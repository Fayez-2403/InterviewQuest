The  th century was the era of the great mechanical systems accompanying the Industrial Revolution
The  th century was the age of the steam engine
During the  th century  the key technology was information gathering  processing  and distribution
Among other developments  we saw the installation of worldwide telephone networks  the invention of radio and television  the birth and unprecedented growth of the computer industry  the launching of communication satellites  and  of course  the Internet
As a result of rapid technological progress  these areas are rapidly converging in the  st century and the differences between collecting  transporting  storing  and processing information are quickly disappearing
Organizations with hundreds of offices spread over a wide geographical area routinely expect to be able to examine the current status of even their most remote outpost at the push of a button
As our ability to gather  process  and distribute information grows  the demand for ever more sophisticated information processing grows even faster
Although the computer industry is still young compared to other industries (
automobiles and air transportation)  computers have made spectacular progress in a short time
During the first two decades of their existence  computer systems were highly centralized  usually within a single large room
Not infrequently  this room had glass walls  through which visitors could gawk at the great electronic wonder inside
A medium-sized company or university might have had INTRODUCTION
one or two computers  while very large institutions had at most a few dozen
The idea that within forty years vastly more powerful computers smaller than postage stamps would be mass produced by the billions was pure science fiction
The merging of computers and communications has had a profound influence on the way computer systems are organized
The once-dominant concept of the ââcomputer centerââ as a room with a large computer to which users bring their work for processing is now totally obsolete (although data centers holding thousands of Internet servers are becoming common)
The old model of a single computer serving all of the organizationâs computational needs has been replaced by one in which a large number of separate but interconnected computers do the job
These systems are called computer networks
The design and organization of these networks are the subjects of this book
Throughout the book we will use the term ââcomputer networkââ to mean a collection of autonomous computers interconnected by a single technology
Two computers are said to be interconnected if they are able to exchange information
The connection need not be via a copper wire; fiber optics  microwaves  infrared  and communication satellites can also be used
Networks come in many sizes  shapes and forms  as we will see later
They are usually connected together to make larger networks  with the Internet being the most well-known example of a network of networks
There is considerable confusion in the literature between a computer network and a distributed system
The key distinction is that in a distributed system  a collection of independent computers appears to its users as a single coherent system
Usually  it has a single model or paradigm that it presents to the users
Often a layer of software on top of the operating system  called middleware  is responsible for implementing this model
A well-known example of a distributed system is the World Wide Web
It runs on top of the Internet and presents a model in which everything looks like a document (Web page)
In a computer network  this coherence  model  and software are absent
Users are exposed to the actual machines  without any attempt by the system to make the machines look and act in a coherent way
If the machines have different hardware and different operating systems  that is fully visible to the users
If a user wants to run a program on a remote machine  heâ  has to log onto that machine and run it there
In effect  a distributed system is a software system built on top of a network
The software gives it a high degree of cohesiveness and transparency
Thus  the distinction between a network and a distributed system lies with the software (especially the operating system)  rather than with the hardware
Nevertheless  there is considerable overlap between the two subjects
For example  both distributed systems and computer networks need to move files around
The difference lies in who invokes the movement  the system or the user
â  ââHeââ should be read as ââhe or sheââ throughout this book
USES OF COMPUTER NETWORKS  Although this book primarily focuses on networks  many of the topics are also important in distributed systems
For more information about distributed systems  see Tanenbaum and Van Steen (   )  USES OF COMPUTER NETWORKS Before we start to examine the technical issues in detail  it is worth devoting some time to pointing out why people are interested in computer networks and what they can be used for
After all  if nobody were interested in computer networks  few of them would be built
We will start with traditional uses at companies  then move on to home networking and recent developments regarding mobile users  and finish with social issues
Business Applications Most companies have a substantial number of computers
For example  a company may have a computer for each worker and use them to design products  write brochures  and do the payroll
Initially  some of these computers may have worked in isolation from the others  but at some point  management may have decided to connect them to be able to distribute information throughout the company
Put in slightly more general form  the issue here is resource sharing
The goal is to make all programs  equipment  and especially data available to anyone on the network without regard to the physical location of the resource or the user
An obvious and widespread example is having a group of office workers share a common printer
None of the individuals really needs a private printer  and a high-volume networked printer is often cheaper  faster  and easier to maintain than a large collection of individual printers
However  probably even more important than sharing physical resources such as printers  and tape backup systems  is sharing information
Companies small and large are vitally dependent on computerized information
Most companies have customer records  product information  inventories  financial statements  tax information  and much more online
If all of its computers suddenly went down  a bank could not last more than five minutes
A modern manufacturing plant  with a computer-controlled assembly line  would not last even  onds
Even a small travel agency or three-person law firm is now highly dependent on computer networks for allowing employees to access relevant information and documents instantly
For smaller companies  all the computers are likely to be in a single office or perhaps a single building  but for larger ones  the computers and employees may be scattered over dozens of offices and plants in many countries
Nevertheless  a sales person in New York might sometimes need access to a product inventory  INTRODUCTION
database in Singapore
Networks called VPNs (Virtual Private Networks) may be used to join the individual networks at different sites into one extended network
In other words  the mere fact that a user happens to be   km away from his data should not prevent him from using the data as though they were local
This goal may be summarized by saying that it is an attempt to end the ââtyranny of geography
ââ In the simplest of terms  one can imagine a companyâs information system as consisting of one or more databases with company information and some number of employees who need to access them remotely
In this model  the data are stored on powerful computers called servers
Often these are centrally housed and maintained by a system administrator
In contrast  the employees have simpler machines  called clients  on their desks  with which they access remote data  for example  to include in spreadsheets they are constructing
(Sometimes we will refer to the human user of the client machine as the ââclient ââ but it should be clear from the context whether we mean the computer or its user
) The client and server machines are connected by a network  as illustrated in Fig
Note that we have shown the network as a simple oval  without any detail
We will use this form when we mean a network in the most abstract sense
When more detail is required  it will be provided
Client Server Network Figure  -
A network with two clients and one server
This whole arrangement is called the client-server model
It is widely used and forms the basis of much network usage
The most popular realization is that of a Web application  in which the server generates Web pages based on its database in response to client requests that may update the database
The client-server model is applicable when the client and server are both in the same building (and belong to the same company)  but also when they are far apart
For example  when a person at home accesses a page on the World Wide Web  the same model is employed  with the remote Web server being the server and the userâs personal   USES OF COMPUTER NETWORKS  computer being the client
Under most conditions  one server can handle a large number (hundreds or thousands) of clients simultaneously
If we look at the client-server model in detail  we see that two processes (
running programs) are involved  one on the client machine and one on the server machine
Communication takes the form of the client process sending a message over the network to the server process
The client process then waits for a reply message
When the server process gets the request  it performs the requested work or looks up the requested data and sends back a reply
These messages are shown in Fig
Client process Server process Client machine Network Reply Request Server machine Figure  -
The client-server model involves requests and replies
A ond goal of setting up a computer network has to do with people rather than information or even computers
A computer network can provide a powerful communication medium among employees
Virtually every company that has two or more computers now has email (electronic mail)  which employees generally use for a great deal of daily communication
In fact  a common gripe around the water cooler is how much email everyone has to deal with  much of it quite meaningless because bosses have discovered that they can send the same (often content-free) message to all their subordinates at the push of a button
Telephone calls between employees may be carried by the computer network instead of by the phone company
This technology is called IP telephony or Voice over IP (VoIP) when Internet technology is used
The microphone and speaker at each end may belong to a VoIP-enabled phone or the employeeâs computer
Companies find this a wonderful way to save on their telephone bills
Other  richer forms of communication are made possible by computer networks
Video can be added to audio so that employees at distant locations can see and hear each other as they hold a meeting
This technique is a powerful tool for eliminating the cost and time previously devoted to travel
Desktop sharing lets remote workers see and interact with a graphical computer screen
This makes it easy for two or more people who work far apart to read and write a shared blackboard or write a report together
When one worker makes a change to an online document  the others can see the change immediately  instead of waiting several days for a letter
Such a speedup makes cooperation among far-flung groups of people easy where it previously had been impossible
More ambitious forms of remote coordination such as telemedicine are only now starting to be used (
INTRODUCTION
remote patient monitoring) but may become much more important
It is sometimes said that communication and transportation are having a race  and whichever wins will make the other obsolete
A third goal for many companies is doing business electronically  especially with customers and suppliers
This new model is called e-commerce (electronic commerce) and it has grown rapidly in recent years
Airlines  bookstores  and other retailers have discovered that many customers like the convenience of shopping from home
Consequently  many companies provide catalogs of their goods and services online and take orders online
Manufacturers of automobiles  aircraft  and computers  among others  buy subsystems from a variety of suppliers and then assemble the parts
Using computer networks  manufacturers can place orders electronically as needed
This reduces the need for large inventories and enhances efficiency
Home Applications In  Ken Olsen was president of the Digital Equipment Corporation  then the number two computer vendor in the world (after IBM)
When asked why Digital was not going after the personal computer market in a big way  he said: ââThere is no reason for any individual to have a computer in his home
ââ History showed otherwise and Digital no longer exists
People initially bought computers for word processing and games
Recently  the biggest reason to buy a home computer was probably for Internet access
Now  many consumer electronic devices  such as set-top boxes  game consoles  and clock radios  come with embedded computers and computer networks  especially wireless networks  and home networks are broadly used for entertainment  including listening to  looking at  and creating music  photos  and videos
Internet access provides home users with connectivity to remote computers
As with companies  home users can access information  communicate with other people  and buy products and services with e-commerce
The main benefit now comes from connecting outside of the home
Bob Metcalfe  the inventor of Ethernet  hypothesized that the value of a network is proportional to the square of the number of users because this is roughly the number of different connections that may be made (Gilder  )
This hypothesis is known as ââMetcalfeâs law
ââ It helps to explain how the tremendous popularity of the Internet comes from its size
Access to remote information comes in many forms
It can be surfing the World Wide Web for information or just for fun
Information available includes the arts  business  cooking  government  health  history  hobbies  recreation  science  sports  travel  and many others
Fun comes in too many ways to mention  plus some ways that are better left unmentioned
Many newspapers have gone online and can be personalized
For example  it is sometimes possible to tell a newspaper that you want everything about corrupt   USES OF COMPUTER NETWORKS  politicians  big fires  scandals involving celebrities  and epidemics  but no football  thank you
Sometimes it is possible to have the selected articles downloaded to your computer while you sleep
As this trend continues  it will cause massive unemployment among  -year-old paperboys  but newspapers like it because distribution has always been the weakest link in the whole production chain
Of course  to make this model work  they will first have to figure out how to make money in this new world  something not entirely obvious since Internet users expect everything to be free
The next step beyond newspapers (plus magazines and scientific journals) is the online digital library
Many professional organizations  such as the ACM ( ) and the IEEE Computer Society ( )  already have all their journals and conference proceedings online
Electronic book readers and online libraries may make printed books obsolete
Skeptics should take note of the effect the printing press had on the medieval illuminated manuscript
Much of this information is accessed using the client-server model  but there is different  popular model for accessing information that goes by the name of peer-to-peer communication (Parameswaran et al
In this form  individuals who form a loose group can communicate with others in the group  as shown in Fig
Every person can  in principle  communicate with one or more other people; there is no fixed division into clients and servers
In a peer-to-peer system there are no fixed clients and servers
Many peer-to-peer systems  such BitTorrent (Cohen  )  do not have any central database of content
Instead  each user maintains his own database locally and provides a list of other nearby people who are members of the system
A new user can then go to any existing member to see what he has and get the names of other members to inspect for more content and more names
This lookup process can be repeated indefinitely to build up a large local database of what is out there
It is an activity that would get tedious for people but computers excel at it
INTRODUCTION
Peer-to-peer communication is often used to share music and videos
It really hit the big time around  with a music sharing service called Napster that was shut down after what was probably the biggest copyright infringement case in all of recorded history (Lam and Tan  ; and Macedonia  )
Legal applications for peer-to-peer communication also exist
These include fans sharing public domain music  families sharing photos and movies  and users downloading public software packages
In fact  one of the most popular Internet applications of all  email  is inherently peer-to-peer
This form of communication is likely to grow considerably in the future
All of the above applications involve interactions between a person and a remote database full of information
The ond broad category of network use is person-to-person communication  basically the  st centuryâs answer to the  th centuryâs telephone
E-mail is already used on a daily basis by millions of people all over the world and its use is growing rapidly
It already routinely contains audio and video as well as text and pictures
Smell may take a while
Any teenager worth his or her salt is addicted to instant messaging
This facility  derived from the UNIX talk program in use since around  allows two people to type messages at each other in real time
There are multi-person messaging services too  such as the Twitter service that lets people send short text messages called ââtweetsââ to their circle of friends or other willing audiences
The Internet can be used by applications to carry audio (
Internet radio stations) and video (
Besides being a cheap way to call to distant friends  these applications can provide rich experiences such as telelearning  meaning attending
classes without the inconvenience of having to get out of bed first
In the long run  the use of networks to enhance human-to-human communication may prove more important than any of the others
It may become hugely important to people who are geographically challenged  giving them the same access to services as people living in the middle of a big city
Between person-to-person communications and accessing information are social network applications
Here  the flow of information is driven by the relationships that people declare between each other
One of the most popular social networking sites is Facebook
It lets people update their personal profiles and shares the updates with other people who they have declared to be their friends
Other social networking applications can make introductions via friends of friends  send news messages to friends such as Twitter above  and much more
Even more loosely  groups of people can work together to create content
A wiki  for example  is a collaborative Web site that the members of a community edit
The most famous wiki is the Wikipedia  an encyclopedia anyone can edit  but there are thousands of other wikis
Our third category is electronic commerce in the broadest sense of the term
Home shopping is already popular and enables users to inspect the online catalogs of thousands of companies
Some of these catalogs are interactive  showing products from different viewpoints and in configurations that can be personalized
USES OF COMPUTER NETWORKS  After the customer buys a product electronically but cannot figure out how to use it  online technical support may be consulted
Another area in which e-commerce is widely used is access to financial institutions
Many people already pay their bills  manage their bank accounts  and handle their investments electronically
This trend will surely continue as networks become more ure
One area that virtually nobody foresaw is electronic flea markets (e-flea?)
Online auctions of ond-hand goods have become a massive industry
Unlike traditional e-commerce  which follows the client-server model  online auctions are peer-to-peer in the sense that consumers can act as both buyers and sellers
Some of these forms of e-commerce have acquired cute little tags based on the fact that ââtoââ and ââ ââ are pronounced the same
The most popular ones are listed in Fig
Tag Full name Example B C Business-to-consumer Ordering books online B B Business-to-business Car manufacturer ordering tires from supplier G C Government-to-consumer Government distributing tax forms electronically C C Consumer-to-consumer Auctioning ond-hand products online P P Peer-to-peer Music sharing Figure  -
Some forms of e-commerce
Our fourth category is entertainment
This has made huge strides in the home in recent years  with the distribution of music  radio and television programs  and movies over the Internet beginning to rival that of traditional mechanisms
Users can find  buy  and download MP  songs and DVD-quality movies and add them to their personal collection
TV shows now reach many homes via IPTV (IP TeleVision) systems that are based on IP technology instead of cable TV or radio transmissions
Media streaming applications let users tune into Internet radio stations or watch recent episodes of their favorite TV shows
Naturally  all of this content can be moved around your house between different devices  displays and speakers  usually with a wireless network
Soon  it may be possible to search for any movie or television program ever made  in any country  and have it displayed on your screen instantly
New films may become interactive  where the user is occasionally prompted for the story direction (should Macbeth murder Duncan or just bide his time?) with alternative scenarios provided for all cases
Live television may also become interactive  with the audience participating in quiz shows  choosing among contestants  and so on
Another form of entertainment is game playing
Already we have multiperson real-time simulation games  like hide-and-seek in a virtual dungeon  and flight   INTRODUCTION
simulators with the players on one team trying to shoot down the players on the opposing team
Virtual worlds provide a persistent setting in which thousands of users can experience a shared reality with three-dimensional graphics
Our last category is ubiquitous computing  in which computing is embedded into everyday life  as in the vision of Mark Weiser (   )
Many homes are already wired with urity systems that include door and window sensors  and there are many more sensors that can be folded in to a smart home monitor  such as energy consumption
Your electricity  gas and water meters could also report usage over the network
This would save money as there would be no need to send out meter readers
And your smoke detectors could call the fire department instead of making a big noise (which has little value if no one is home)
As the cost of sensing and communication drops  more and more measurement and reporting will be done with networks
Increasingly  consumer electronic devices are networked
For example  some high-end cameras already have a wireless network capability and use it to send photos to a nearby display for viewing
Professional sports photographers can also send their photos to their editors in real-time  first wirelessly to an access point then over the Internet
Devices such as televisions that plug into the wall can use power-line networks to send information throughout the house over the wires that carry electricity
It may not be very surprising to have these objects on the network  but objects that we do not think of as computers may sense and communicate information too
For example  your shower may record water usage  give you visual feedback while you lather up  and report to a home environmental monitoring application when you are done to help save on your water bill
A technology called RFID (Radio Frequency IDentification) will push this idea even further in the future
RFID tags are passive (
have no battery) chips the size of stamps and they can already be affixed to books  passports  pets  credit cards  and other items in the home and out
This lets RFID readers locate and communicate with the items over a distance of up to several meters  depending on the kind of RFID
Originally  RFID was commercialized to replace barcodes
It has not succeeded yet because barcodes are free and RFID tags cost a few cents
Of course  RFID tags offer much more and their price is rapidly declining
They may turn the real world into the Internet of things (ITU  )
Mobile Users Mobile computers  such as laptop and handheld computers  are one of the fastest-growing segments of the computer industry
Their sales have already overtaken those of desktop computers
Why would anyone want one? People on the go often want to use their mobile devices to read and send email  tweet  watch movies  download music  play games  or simply to surf the Web for information
They want to do all of the things they do at home and in the office
Naturally  they want to do them from anywhere on land  sea or in the air
USES OF COMPUTER NETWORKS   Connectivity to the Internet enables many of these mobile uses
Since having a wired connection is impossible in cars  boats  and airplanes  there is a lot of interest in wireless networks
Cellular networks operated by the telephone companies are one familiar kind of wireless network that blankets us with coverage for mobile phones
Wireless hotspots based on the
standard are another kind of wireless network for mobile computers
They have sprung up everywhere that people go  resulting in a patchwork of coverage at cafes  hotels  airports  schools  trains and planes
Anyone with a laptop computer and a wireless modem can just turn on their computer on and be connected to the Internet through the hotspot  as though the computer were plugged into a wired network
Wireless networks are of great value to fleets of trucks  taxis  delivery vehicles  and repairpersons for keeping in contact with their home base
For example  in many cities  taxi drivers are independent businessmen  rather than being employees of a taxi company
In some of these cities  the taxis have a display the driver can see
When a customer calls up  a central dispatcher types in the pickup and destination points
This information is displayed on the driversâ displays and a beep sounds
The first driver to hit a button on the display gets the call
Wireless networks are also important to the military
If you have to be able to fight a war anywhere on Earth at short notice  counting on using the local networking infrastructure is probably not a good idea
It is better to bring your own
Although wireless networking and mobile computing are often related  they are not identical  as Fig
Here we see a distinction between fixed wireless and mobile wireless networks
Even notebook computers are sometimes wired
For example  if a traveler plugs a notebook computer into the wired network jack in a hotel room  he has mobility without a wireless network
Wireless Mobile Typical applications No No Desktop computers in offices No Yes A notebook computer used in a hotel room Yes No Networks in unwired buildings Yes Yes Store inventory with a handheld computer Figure  -
Combinations of wireless networks and mobile computing
Conversely  some wireless computers are not mobile
In the home  and in offices or hotels that lack suitable cabling  it can be more convenient to connect desktop computers or media players wirelessly than to install wires
Installing a wireless network may require little more than buying a small box with some electronics in it  unpacking it  and plugging it in
This solution may be far cheaper than having workmen put in cable ducts to wire the building
Finally  there are also true mobile  wireless applications  such as people walking around stores with a handheld computers recording inventory
At many busy   INTRODUCTION
airports  car rental return clerks work in the parking lot with wireless mobile computers
They scan the barcodes or RFID chips of returning cars  and their mobile device  which has a built-in printer  calls the main computer  gets the rental information  and prints out the bill on the spot
Perhaps the key driver of mobile  wireless applications is the mobile phone
Text messaging or texting is tremendously popular
It lets a mobile phone user type a short message that is then delivered by the cellular network to another mobile subscriber
Few people would have predicted ten years ago that having teenagers tediously typing short text messages on mobile phones would be an immense money maker for telephone companies
But texting (or Short Message Service as it is known outside the
) is very profitable since it costs the carrier but a tiny fraction of one cent to relay a text message  a service for which they charge far more
The long-awaited convergence of telephones and the Internet has finally arrived  and it will accelerate the growth of mobile applications
Smart phones  such as the popular iPhone  combine aspects of mobile phones and mobile computers
The ( G and  G) cellular networks to which they connect can provide fast data services for using the Internet as well as handling phone calls
Many advanced phones connect to wireless hotspots too  and automatically switch between networks to choose the best option for the user
Other consumer electronics devices can also use cellular and hotspot networks to stay connected to remote computers
Electronic book readers can download a newly purchased book or the next edition of a magazine or todayâs newspaper wherever they roam
Electronic picture frames can update their displays on cue with fresh images
Since mobile phones know their locations  often because they are equipped with GPS (Global Positioning System) receivers  some services are intentionally location dependent
Mobile maps and directions are an obvious candidate as your GPS-enabled phone and car probably have a better idea of where you are than you do
So  too  are searches for a nearby bookstore or Chinese restaurant  or a local weather forecast
Other services may record location  such as annotating photos and videos with the place at which they were made
This annotation is known as ââgeo-tagging
ââ An area in which mobile phones are now starting to be used is m-commerce (mobile-commerce) (Senn  )
Short text messages from the mobile are used to authorize payments for food in vending machines  movie tickets  and other small items instead of cash and credit cards
The charge then appears on the mobile phone bill
When equipped with NFC (Near Field Communication) technology the mobile can act as an RFID smartcard and interact with a nearby reader for payment
The driving forces behind this phenomenon are the mobile device makers and network operators  who are trying hard to figure out how to get a piece of the e-commerce pie
From the storeâs point of view  this scheme may save them most of the credit card companyâs fee  which can be several percent
USES OF COMPUTER NETWORKS   Of course  this plan may backfire  since customers in a store might use the RFID or barcode readers on their mobile devices to check out competitorsâ prices before buying and use them to get a detailed report on where else an item can be purchased nearby and at what price
One huge thing that m-commerce has going for it is that mobile phone users are accustomed to paying for everything (in contrast to Internet users  who expect everything to be free)
If an Internet Web site charged a fee to allow its customers to pay by credit card  there would be an immense howling noise from the users
If  however  a mobile phone operator its customers to pay for items in a store by waving the phone at the cash register and then tacked on a fee for this convenience  it would probably be accepted as normal
Time will tell
No doubt the uses of mobile and wireless computers will grow rapidly in the future as the size of computers shrinks  probably in ways no one can now foresee
Let us take a quick look at some possibilities
Sensor networks are made up of nodes that gather and wirelessly relay information they sense about the state of the physical world
The nodes may be part of familiar items such as cars or phones  or they may be small separate devices
For example  your car might gather data on its location  speed  vibration  and fuel efficiency from its on-board diagnostic system and upload this information to a database (Hull et al
Those data can help find potholes  plan trips around congested roads  and tell you if you are a ââgas guzzlerââ compared to other drivers on the same stretch of road
Sensor networks are revolutionizing science by providing a wealth of data on behavior that could not previously be observed
One example is tracking the migration of individual zebras by placing a small sensor on each animal (Juang et al
Researchers have packed a wireless computer into a cube  mm on edge (Warneke et al
With mobile computers this small  even small birds  rodents  and ints can be tracked
Even mundane uses  such as in parking meters  can be significant because they make use of data that were not previously available
Wireless parking meters can accept credit or debit card payments with instant verification over the wireless link
They can also report when they are in use over the wireless network
This would let drivers download a recent parking map to their car so they can find an available spot more easily
Of course  when a meter expires  it might also check for the presence of a car (by bouncing a signal off it) and report the expiration to parking enforcement
It has been estimated that city governments in the
alone could collect an additional $  billion this way (Harte et al
Wearable computers are another promising application
Smart watches with radios have been part of our mental space since their appearance in the Dick Tracy comic strip in ; now you can buy them
Other such devices may be implanted  such as pacemakers and insulin pumps
Some of these can be controlled over a wireless network
This lets doctors test and reconfigure them more easily
It could also lead to some nasty problems if the devices are as inure as the average PC and can be hacked easily (Halperin et al
INTRODUCTION
Social Issues Computer networks  like the printing press years ago  allow ordinary citizens to distribute and view content in ways that were not previously possible
But along with the good comes the bad  as this new-found freedom brings with it many unsolved social  political  and ethical issues
Let us just briefly mention a few of them; a thorough study would require a full book  at least
Social networks  message boards  content sharing sites  and a host of other applications allow people to share their views with like-minded individuals
As long as the subjects are restricted to technical topics or hobbies like gardening  not too many problems will arise
The trouble comes with topics that people actually care about  like politics  religion  or sex
Views that are publicly posted may be deeply offensive to some people
Worse yet  they may not be politically correct
Furthermore  opinions need not be limited to text; high-resolution color photographs and video clips are easily shared over computer networks
Some people take a live-and-let-live view  but others feel that posting certain material (
verbal attacks on particular countries or religions  pornography  etc
) is simply unacceptable and that such content must be censored
Different countries have different and conflicting laws in this area
Thus  the debate rages
In the past  people have sued network operators  claiming that they are responsible for the contents of what they carry  just as newspapers and magazines are
The inevitable response is that a network is like a telephone company or the post office and cannot be expected to police what its users say
It should now come only as a slight surprise to learn that some network operators block content for their own reasons
Some users of peer-to-peer applications had their network service cut off because the network operators did not find it profitable to carry the large amounts of traffic sent by those applications
Those same operators would probably like to treat different companies differently
If you are a big company and pay well then you get good service  but if you are a small-time player  you get poor service
Opponents of this practice argue that peer-to-peer and other content should be treated in the same way because they are all just bits to the network
This argument for communications that are not differentiated by their content or source or who is providing the content is known as network neutrality (Wu  )
It is probably safe to say that this debate will go on for a while
Many other parties are involved in the tussle over content
For instance  pirated music and movies fueled the massive growth of peer-to-peer networks  which did not please the copyright holders  who have threatened (and sometimes taken) legal action
There are now automated systems that search peer-to-peer networks and fire off warnings to network operators and users who are suspected of infringing copyright
In the United States  these warnings are known as DMCA takedown notices after the Digital Millennium Copyright Act
This   USES OF COMPUTER NETWORKS   search is an armsâ race because it is hard to reliably catch copyright infringement
Even your printer might be mistaken for a culprit (Piatek et al
Computer networks make it very easy to communicate
They also make it easy for the people who run the network to snoop on the traffic
This sets up conflicts over issues such as employee rights versus employer rights
Many people read and write email at work
Many employers have claimed the right to read and possibly censor employee messages  including messages sent from a home computer outside working hours
Not all employees agree with this  especially the latter part
Another conflict is centered around government versus citizenâs rights
The FBI has installed systems at many Internet service providers to snoop on all incoming and outgoing email for nuggets of interest
One early system was originally called Carnivore  but bad publicity caused it to be renamed to the more innocent-sounding DCS (Blaze and Bellovin  ; Sobel  ; and Zacks  )
The goal of such systems is to spy on millions of people in the hope of perhaps finding information about illegal activities
Unfortunately for the spies  the Fourth Amendment to the
Constitution prohibits government searches without a search warrant  but the government often ignores it
Of course  the government does not have a monopoly on threatening peopleâs privacy
The private tor does its bit too by profiling users
For example  small files called cookies that Web browsers store on usersâ computers allow companies to track usersâ activities in cyberspace and may also allow credit card numbers  social urity numbers  and other confidential information to leak all over the Internet (Berghel  )
Companies that provide Web-based services may maintain large amounts of personal information about their users that allows them to study user activities directly
For example  Google can read your email and show you advertisements based on your interests if you use its email service  Gmail
A new twist with mobile devices is location privacy (Beresford and Stajano  )
As part of the process of providing service to your mobile device the network operators learn where you are at different times of day
This allows them to track your movements
They may know which nightclub you frequent and which medical center you visit
Computer networks also offer the potential to increase privacy by sending anonymous messages
In some situations  this capability may be desirable
Beyond preventing companies from learning your habits  it provides  for example  a way for students  soldiers  employees  and citizens to blow the whistle on illegal behavior on the part of professors  officers  superiors  and politicians without fear of reprisals
On the other hand  in the United States and most other democracies  the law specifically permits an accused person the right to confront and challenge his accuser in court so anonymous accusations cannot be used as evidence
The Internet makes it possible to find information quickly  but a great deal of it is ill considered  misleading  or downright wrong
That medical advice you   INTRODUCTION
plucked from the Internet about the pain in your chest may have come from a Nobel Prize winner or from a high-school dropout
Other information is frequently unwanted
Electronic junk mail (spam) has become a part of life because spammers have collected millions of email addresses and would-be marketers can cheaply send computer-generated messages to them
The resulting flood of spam rivals the flow messages from real people
Fortunately  filtering software is able to read and discard the spam generated by other computers  with lesser or greater degrees of success
Still other content is intended for criminal behavior
Web pages and email messages containing active content (basically  programs or macros that execute on the receiverâs machine) can contain viruses that take over your computer
They might be used to steal your bank account passwords  or to have your computer send spam as part of a botnet or pool of compromised machines
Phishing messages masquerade as originating from a trustworthy party  for example  your bank  to try to trick you into revealing sensitive information  for example  credit card numbers
Identity theft is becoming a serious problem as thieves collect enough information about a victim to obtain credit cards and other documents in the victimâs name
It can be difficult to prevent computers from impersonating people on the Internet
This problem has led to the development of CAPTCHAs  in which a computer asks a person to solve a short recognition task  for example  typing in the letters shown in a distorted image  to show that they are human (von Ahn  )
This process is a variation on the famous Turing test in which a person asks questions over a network to judge whether the entity responding is human
A lot of these problems could be solved if the computer industry took computer urity seriously
If all messages were encrypted and authenticated  it would be harder to commit mischief
Such technology is well established and we will study it in detail in   The problem is that hardware and software vendors know that putting in urity features costs money and their customers are not demanding such features
In addition  a substantial number of the problems are caused by buggy software  which occurs because vendors keep adding more and more features to their programs  which inevitably means more code and thus more bugs
A tax on new features might help  but that might be a tough sell in some quarters
A refund for defective software might be nice  except it would bankrupt the entire software industry in the first year
Computer networks raise new legal problems when they interact with old laws
Electronic gambling provides an example
Computers have been simulating things for decades  so why not simulate slot machines  roulette wheels  blackjack dealers  and more gambling equipment? Well  because it is illegal in a lot of places
The trouble is  gambling is legal in a lot of other places (England  for example) and casino owners there have grasped the potential for Internet gambling
What happens if the gambler  the casino  and the server are all in different countries  with conflicting laws? Good question
NETWORK HARDWARE
NETWORK HARDWARE It is now time to turn our attention from the applications and social aspects of networking (the dessert) to the technical issues involved in network design (the spinach)
There is no generally accepted taxonomy into which all computer networks fit  but two dimensions stand out as important: transmission technology and scale
We will now examine each of these in turn
Broadly speaking  there are two types of transmission technology that are in widespread use: broadcast links and point-to-point links
Point-to-point links connect individual pairs of machines
To go from the source to the destination on a network made up of point-to-point links  short messages  called packets in certain contexts  may have to first visit one or more intermediate machines
Often multiple routes  of different lengths  are possible  so finding good ones is important in point-to-point networks
Point-to-point transmission with exactly one sender and exactly one receiver is sometimes called unicasting
In contrast  on a broadcast network  the communication channel is shared by all the machines on the network; packets sent by any machine are received by all the others
An address field within each packet specifies the intended recipient
Upon receiving a packet  a machine checks the address field
If the packet is intended for the receiving machine  that machine processes the packet; if the packet is intended for some other machine  it is just ignored
A wireless network is a common example of a broadcast link  with communication shared over a coverage region that depends on the wireless channel and the transmitting machine
As an analogy  consider someone standing in a meeting room and shouting ââWatson  come here
I want you
ââ Although the packet may actually be received (heard) by many people  only Watson will respond; the others just ignore it
Broadcast systems usually also allow the possibility of addressing a packet to all destinations by using a special code in the address field
When a packet with this code is transmitted  it is received and processed by every machine on the network
This mode of operation is called broadcasting
Some broadcast systems also support transmission to a subset of the machines  which known as multicasting
An alternative criterion for classifying networks is by scale
Distance is important as a classification metric because different technologies are used at different scales
At the top are the personal area networks  networks that are meant for one person
Beyond these come longer-range networks
These can be divided into local  metropolitan  and wide area networks  each with increasing scale
Finally  the connection of two or more networks is called an internetwork
The worldwide Internet is certainly the best-known (but not the only) example of an internetwork
INTRODUCTION
Soon we will have even larger internetworks with the Interplanetary Internet that connects networks across space (Burleigh et al
m Square meter   m Room m Building  km Campus   km City Interprocessor distance Processors located in same Example km Country  km Continent Planet Personal area network The Internet Local area network Metropolitan area network Wide area network   km Figure  -
Classification of interconnected processors by scale
In this book we will be concerned with networks at all these scales
In the following tions  we give a brief introduction to network hardware by scale
Personal Area Networks PANs (Personal Area Networks) let devices communicate over the range of a person
A common example is a wireless network that connects a computer with its peripherals
Almost every computer has an attached monitor  keyboard  mouse  and printer
Without using wireless  this connection must be done with cables
So many new users have a hard time finding the right cables and plugging them into the right little holes (even though they are usually color coded) that most computer vendors offer the option of sending a technician to the userâs home to do it
To help these users  some companies got together to design a short-range wireless network called Bluetooth to connect these components without wires
The idea is that if your devices have Bluetooth  then you need no cables
You just put them down  turn them on  and they work together
For many people  this ease of operation is a big plus
In the simplest form  Bluetooth networks use the master-slave paradigm of Fig
The system unit (the PC) is normally the master  talking to the mouse  keyboard  etc
The master tells the slaves what addresses to use  when they can broadcast  how long they can transmit  what frequencies they can use  and so on
Bluetooth can be used in other settings  too
It is often used to connect a headset to a mobile phone without cords and it can allow your digital music player   NETWORK HARDWARE   Figure  -
Bluetooth PAN configuration
to connect to your car merely being brought within range
A completely different kind of PAN is formed when an embedded medical device such as a pacemaker  insulin pump  or hearing aid talks to a user-operated remote control
We will discuss Bluetooth in more detail in   PANs can also be built with other technologies that communicate over short ranges  such as RFID on smartcards and library books
We will study RFID in      Local Area Networks The next step up is the LAN (Local Area Network)
A LAN is a privately owned network that operates within and nearby a single building like a home  office or factory
LANs are widely used to connect personal computers and consumer electronics to let them share resources (
printers) and exchange information
When LANs are used by companies  they are called enterprise networks
Wireless LANs are very popular these days  especially in homes  older office buildings  cafeterias  and other places where it is too much trouble to install cables
In these systems  every computer has a radio modem and an antenna that it uses to communicate with other computers
In most cases  each computer talks to a device in the ceiling as shown in Fig
This device  called an AP (Access Point)  wireless router  or base station  relays packets between the wireless computers and also between them and the Internet
Being the AP is like being the popular kid as school because everyone wants to talk to you
However  if other computers are close enough  they can communicate directly with one another in a peer-to-peer configuration
There is a standard for wireless LANs called IEEE
popularly known as WiFi  which has become very widespread
It runs at speeds anywhere from  INTRODUCTION
Ethernet Ports switch To rest of network Access To wired network point Figure  -
Wireless and wired LANs
(a)    (b) Switched Ethernet
to hundreds of Mbps
(In this book we will adhere to tradition and measure line speeds in megabits/  where  Mbps is  bits/  and gigabits/  where  Gbps is  bits/
) We will discuss
in   Wired LANs use a range of different transmission technologies
Most of them use copper wires  but some use optical fiber
LANs are restricted in size  which means that the worst-case transmission time is bounded and known in advance
Knowing these bounds helps with the task of designing network protocols
Typically  wired LANs run at speeds of Mbps to  Gbps  have low delay (microonds or nanoonds)  and make very few errors
Newer LANs can operate at up to   Gbps
Compared to wireless networks  wired LANs exceed them in all dimensions of performance
It is just easier to send signals over a wire or through a fiber than through the air
The topology of many wired LANs is built from point-to-point links
popularly called Ethernet  is  by far  the most common type of wired LAN
Each computer speaks the Ethernet protocol and connects to a box called a switch with a point-to-point link
Hence the name
A switch has multiple ports  each of which can connect to one computer
The job of the switch is to relay packets between computers that are attached to it  using the address in each packet to determine which computer to send it to
To build larger LANs  switches can be plugged into each other using their ports
What happens if you plug them together in a loop? Will the network still work? Luckily  the designers thought of this case
It is the job of the protocol to sort out what paths packets should travel to safely reach the intended computer
We will see how this works in   It is also possible to divide one large physical LAN into two smaller logical LANs
You might wonder why this would be useful
Sometimes  the layout of the network equipment does not match the organizationâs structure
For example  the   NETWORK HARDWARE   engineering and finance departments of a company might have computers on the same physical LAN because they are in the same wing of the building but it might be easier to manage the system if engineering and finance logically each had its own network Virtual LAN or VLAN
In this design each port is tagged with a ââcolor ââ say green for engineering and red for finance
The switch then forwards packets so that computers attached to the green ports are separated from the computers attached to the red ports
Broadcast packets sent on a red port  for example  will not be received on a green port  just as though there were two different LANs
We will cover VLANs at the end of   There are other wired LAN topologies too
In fact  switched Ethernet is a modern version of the original Ethernet design that broadcast all the packets over a single linear cable
At most one machine could successfully transmit at a time  and a distributed arbitration mechanism was used to resolve conflicts
It used a simple algorithm: computers could transmit whenever the cable was idle
If two or more packets collided  each computer just waited a random time and tried later
We will call that version classic Ethernet for clarity  and as you suspected  you will learn about it in   Both wireless and wired broadcast networks can be divided into static and dynamic designs  depending on how the channel is allocated
A typical static allocation would be to divide time into discrete intervals and use a round-robin algorithm  allowing each machine to broadcast only when its time slot comes up
Static allocation wastes channel capacity when a machine has nothing to say during its allocated slot  so most systems attempt to allocate the channel dynamically (
on demand)
Dynamic allocation methods for a common channel are either centralized or decentralized
In the centralized channel allocation method  there is a single entity  for example  the base station in cellular networks  which determines who goes next
It might do this by accepting multiple packets and prioritizing them according to some internal algorithm
In the decentralized channel allocation method  there is no central entity; each machine must decide for itself whether to transmit
You might think that this approach would lead to chaos  but it does not
Later we will study many algorithms designed to bring order out of the potential chaos
It is worth spending a little more time discussing LANs in the home
In the future  it is likely that every appliance in the home will be capable of communicating with every other appliance  and all of them will be accessible over the Internet
This development is likely to be one of those visionary concepts that nobody asked for (like TV remote controls or mobile phones)  but once they arrived nobody can imagine how they lived without them
Many devices are already capable of being networked
These include computers  entertainment devices such as TVs and DVDs  phones and other consumer electronics such as cameras  appliances like clock radios  and infrastructure like utility meters and thermostats
This trend will only continue
For instance  the average home probably has a dozen clocks (
in appliances)  all of which could   INTRODUCTION
adjust to daylight savings time automatically if the clocks were on the Internet
Remote monitoring of the home is a likely winner  as many grown children would be willing to spend some money to help their aging parents live safely in their own homes
While we could think of the home network as just another LAN  it is more likely to have different properties than other networks
First  the networked devices have to be very easy to install
Wireless routers are the most returned consumer electronic item
People buy one because they want a wireless network at home  find that it does not work ââout of the box ââ and then return it rather than listen to elevator music while on hold on the technical helpline
ond  the network and devices have to be foolproof in operation
Air conditioners used to have one knob with four settings: OFF  LOW  MEDIUM  and HIGH
Now they have  -page manuals
Once they are networked  expect the  ter on urity alone to be   pages
This is a problem because only computer users are accustomed to putting up with products that do not work; the car-  television-  and refrigerator-buying public is far less tolerant
They expect products to work   % without the need to hire a geek
Third  low price is essential for success
People will not pay a $  premium for an Internet thermostat because few people regard monitoring their home temperature from work that important
For $  extra  though  it might sell
Fourth  it must be possible to start out with one or two devices and expand the reach of the network gradually
This means no format wars
Telling consumers to buy peripherals with IEEE  (FireWire) interfaces and a few years later retracting that and saying USB
is the interface-of-the-month and then switching that to
gâoops  no  make that
(different wireless networks)âis going to make consumers very skittish
The network interface will have to remain stable for decades  like the television broadcasting standards
Fifth  urity and reliability will be very important
Losing a few files to an email virus is one thing; having a burglar disarm your urity system from his mobile computer and then plunder your house is something quite different
An interesting question is whether home networks will be wired or wireless
Convenience and cost favors wireless networking because there are no wires to fit  or worse  retrofit
urity favors wired networking because the radio waves that wireless networks use are quite good at going through walls
Not everyone is overjoyed at the thought of having the neighbors piggybacking on their Internet connection and reading their email
we will study how encryption can be used to provide urity  but it is easier said than done with inexperienced users
A third option that may be appealing is to reuse the networks that are already in the home
The obvious candidate is the electric wires that are installed throughout the house
Power-line networks let devices that plug into outlets broadcast information throughout the house
You have to plug in the TV anyway  and this way it can get Internet connectivity at the same time
The difficulty is   NETWORK HARDWARE   how to carry both power and data signals at the same time
Part of the answer is that they use different frequency bands
In short  home LANs offer many opportunities and challenges
Most of the latter relate to the need for the networks to be easy to manage  dependable  and ure  especially in the hands of nontechnical users  as well as low cost
Metropolitan Area Networks A MAN (Metropolitan Area Network) covers a city
The best-known examples of MANs are the cable television networks available in many cities
These systems grew from earlier community antenna systems used in areas with poor over-the-air television reception
In those early systems  a large antenna was placed on top of a nearby hill and a signal was then piped to the subscribersâ houses
At first  these were locally designed  ad hoc systems
Then companies began jumping into the business  getting contracts from local governments to wire up entire cities
The next step was television programming and even entire channels designed for cable only
Often these channels were highly specialized  such as all news  all sports  all cooking  all gardening  and so on
But from their inception until the late s  they were intended for television reception only
When the Internet began attracting a mass audience  the cable TV network operators began to realize that with some changes to the system  they could provide two-way Internet service in unused parts of the spectrum
At that point  the cable TV system began to morph from simply a way to distribute television to a metropolitan area network
To a first approximation  a MAN might look something like the system shown in Fig
In this figure we see both television signals and Internet being fed into the centralized cable headend for subsequent distribution to peopleâs homes
We will come back to this subject in detail in   Cable television is not the only MAN  though
Recent developments in highspeed wireless Internet access have resulted in another MAN  which has been standardized as IEEE
and is popularly known as WiMAX
We will look at it in      Wide Area Networks A WAN (Wide Area Network) spans a large geographical area  often a country or continent
We will begin our discussion with wired WANs  using the example of a company with branch offices in different cities
The WAN in Fig
Each of these offices contains computers intended for running user (
application) programs
We will follow traditional usage and call these machines hosts
The rest of the network that connects these hosts is then called the   INTRODUCTION
Internet Antenna Junction box Head end Figure  -
A metropolitan area network based on cable TV
communication subnet  or just subnet for short
The job of the subnet is to carry messages from host to host  just as the telephone system carries words (really just sounds) from speaker to listener
In most WANs  the subnet consists of two distinct components: transmission lines and switching elements
Transmission lines move bits between machines
They can be made of copper wire  optical fiber  or even radio links
Most companies do not have transmission lines lying about  so instead they lease the lines from a telecommunications company
Switching elements  or just switches  are specialized computers that connect two or more transmission lines
When data arrive on an incoming line  the switching element must choose an outgoing line on which to forward them
These switching computers have been called by various names in the past; the name router is now most commonly used
Unfortunately  some people pronounce it âârooterââ while others have it rhyme with ââdoubter
ââ Determining the correct pronunciation will be left as an exercise for the reader
(Note: the perceived correct answer may depend on where you live
) A short comment about the term ââsubnetââ is in order here
Originally  its only meaning was the collection of routers and communication lines that moved packets from the source host to the destination host
Readers should be aware that it has acquired a ond  more recent meaning in conjunction with network addressing
We will discuss that meaning in
and stick with the original meaning (a collection of lines and routers) until then
The WAN as we have described it looks similar to a large wired LAN  but there are some important differences that go beyond long wires
Usually in a WAN  the hosts and subnet are owned and operated by different people
In our   NETWORK HARDWARE   Subnet Router Perth Brisbane Melbourne Transmission line Figure  -
WAN that connects three branch offices in Australia
example  the employees might be responsible for their own computers  while the companyâs IT department is in charge of the rest of the network
We will see clearer boundaries in the coming examples  in which the network provider or telephone company operates the subnet
Separation of the pure communication aspects of the network (the subnet) from the application aspects (the hosts) greatly simplifies the overall network design
A ond difference is that the routers will usually connect different kinds of networking technology
The networks inside the offices may be switched Ethernet  for example  while the long-distance transmission lines may be SONET links (which we will cover in
Some device needs to join them
The astute reader will notice that this goes beyond our definition of a network
This means that many WANs will in fact be internetworks  or composite networks that are made up of more than one network
We will have more to say about internetworks in the next tion
A final difference is in what is connected to the subnet
This could be individual computers  as was the case for connecting to LANs  or it could be entire LANs
This is how larger networks are built from smaller ones
As far as the subnet is concerned  it does the same job
We are now in a position to look at two other varieties of WANs
First  rather than lease dedicated transmission lines  a company might connect its offices to the Internet This allows connections to be made between the offices as virtual links   INTRODUCTION
that use the underlying capacity of the Internet
This arrangement  shown in Fig
Compared to the dedicated arrangement  a VPN has the usual advantage of virtualization  which is that it provides flexible reuse of a resource (Internet connectivity)
Consider how easy it is to add a fourth office to see this
A VPN also has the usual disadvantage of virtualization  which is a lack of control over the underlying resources
With a dedicated line  the capacity is clear
With a VPN your mileage may vary with your Internet service
Internet Perth Brisbane Melbourne Link via the internet Figure  -
WAN using a virtual private network
The ond variation is that the subnet may be run by a different company
The subnet operator is known as a network service provider and the offices are its customers
This structure is shown in Fig
The subnet operator will connect to other customers too  as long as they can pay and it can provide service
Since it would be a disappointing network service if the customers could only send packets to each other  the subnet operator will also connect to other networks that are part of the Internet
Such a subnet operator is called an ISP (Internet Service Provider) and the subnet is an ISP network
Its customers who connect to the ISP receive Internet service
We can use the ISP network to preview some key issues that we will study in later  ters
In most WANs  the network contains many transmission lines  each connecting a pair of routers
If two routers that do not share a transmission line wish to communicate  they must do this indirectly  via other routers
There   NETWORK HARDWARE   ISP network Perth Brisbane Melbourne Transmission line Customer network Figure  -
WAN using an ISP network
may be many paths in the network that connect these two routers
How the network makes the decision as to which path to use is called the routing algorithm
Many such algorithms exist
How each router makes the decision as to where to send a packet next is called the forwarding algorithm
Many of them exist too
We will study some of both types in detail in   Other kinds of WANs make heavy use of wireless technologies
In satellite systems  each computer on the ground has an antenna through which it can send data to and receive data from to a satellite in orbit
All computers can hear the output from the satellite  and in some cases they can also hear the upward transmissions of their fellow computers to the satellite as well
Satellite networks are inherently broadcast and are most useful when the broadcast property is important
The cellular telephone network is another example of a WAN that uses wireless technology
This system has already gone through three generations and a fourth one is on the horizon
The first generation was analog and for voice only
The ond generation was digital and for voice only
The third generation is digital and is for both voice and data
Each cellular base station covers a distance much larger than a wireless LAN  with a range measured in kilometers rather than tens of meters
The base stations are connected to each other by a backbone network that is usually wired
The data rates of cellular networks are often on the order of  Mbps  much smaller than a wireless LAN that can range up to on the order of Mbps
We will have a lot to say about these networks in     INTRODUCTION
Internetworks Many networks exist in the world  often with different hardware and software
People connected to one network often want to communicate with people attached to a different one
The fulfillment of this desire requires that different  and frequently incompatible  networks be connected
A collection of interconnected networks is called an internetwork or internet
These terms will be used in a generic sense  in contrast to the worldwide Internet (which is one specific internet)  which we will always capitalize
The Internet uses ISP networks to connect enterprise networks  home networks  and many other networks
We will look at the Internet in great detail later in this book
Subnets  networks  and internetworks are often confused
The term ââsubnetââ makes the most sense in the context of a wide area network  where it refers to the collection of routers and communication lines owned by the network operator
As an analogy  the telephone system consists of telephone switching offices connected to one another by high-speed lines  and to houses and businesses by low-speed lines
These lines and equipment  owned and managed by the telephone company  form the subnet of the telephone system
The telephones themselves (the hosts in this analogy) are not part of the subnet
A network is formed by the combination of a subnet and its hosts
However  the word âânetworkââ is often used in a loose sense as well
A subnet might be described as a network  as in the case of the ââISP networkââ of Fig
An internetwork might also be described as a network  as in the case of the WAN in Fig
We will follow similar practice  and if we are distinguishing a network from other arrangements  we will stick with our original definition of a collection of computers interconnected by a single technology
Let us say more about what constitutes an internetwork
We know that an internet is formed when distinct networks are interconnected
In our view  connecting a LAN and a WAN or connecting two LANs is the usual way to form an internetwork  but there is little agreement in the industry over terminology in this area
There are two rules of thumb that are useful
First  if different organizations have paid to construct different parts of the network and each maintains its part  we have an internetwork rather than a single network
ond  if the underlying technology is different in different parts (
broadcast versus point-to-point and wired versus wireless)  we probably have an internetwork
To go deeper  we need to talk about how two different networks can be connected
The general name for a machine that makes a connection between two or more networks and provides the necessary translation  both in terms of hardware and software  is a gateway
Gateways are distinguished by the layer at which they operate in the protocol hierarchy
We will have much more to say about layers and protocol hierarchies starting in the next tion  but for now imagine that higher layers are more tied to applications  such as the Web  and lower layers are more tied to transmission links  such as Ethernet
NETWORK HARDWARE   Since the benefit of forming an internet is to connect computers across networks  we do not want to use too low-level a gateway or we will be unable to make connections between different kinds of networks
We do not want to use too high-level a gateway either  or the connection will only work for particular applications
The level in the middle that is ââjust rightââ is often called the network layer  and a router is a gateway that switches packets at the network layer
We can now spot an internet by finding a network that has routers  NETWORK SOFTWARE The first computer networks were designed with the hardware as the main concern and the software as an afterthought
This strategy no longer works
Network software is now highly structured
In the following tions we examine the software structuring technique in some detail
The approach described here forms the keystone of the entire book and will occur repeatedly later on
Protocol Hierarchies To reduce their design complexity  most networks are organized as a stack of layers or levels  each one built upon the one below it
The number of layers  the name of each layer  the contents of each layer  and the function of each layer differ from network to network
The purpose of each layer is to offer certain services to the higher layers while shielding those layers from the details of how the offered services are actually implemented
In a sense  each layer is a kind of virtual machine  offering certain services to the layer above it
This concept is actually a familiar one and is used throughout computer science  where it is variously known as information hiding  abstract data types  data encapsulation  and object-oriented programming
The fundamental idea is that a particular piece of software (or hardware) provides a service to its users but keeps the details of its internal state and algorithms hidden from them
When layer n on one machine carries on a conversation with layer n on another machine  the rules and conventions used in this conversation are collectively known as the layer n protocol
Basically  a protocol is an agreement between the communicating parties on how communication is to proceed
As an analogy  when a woman is introduced to a man  she may choose to stick out her hand
He  in turn  may decide to either shake it or kiss it  depending  for example  on whether she is an American lawyer at a business meeting or a European princess at a formal ball
Violating the protocol will make communication more difficult  if not completely impossible
A five-layer network is illustrated in Fig
The entities comprising the corresponding layers on different machines are called peers
The peers may be   INTRODUCTION
software processes  hardware devices  or even human beings
In other words  it is the peers that communicate by using the protocol to talk to each other
Layer  Layer  Layer  Layer  Layer  Host  Layer  /  interface Layer  /  interface Layer  /  interface Layer  /  interface Layer  protocol Layer  Layer  Layer  Layer  Layer  Host  Layer  protocol Layer  protocol Layer  protocol Layer  protocol Physical medium Figure  -
Layers  protocols  and interfaces
In reality  no data are directly transferred from layer n on one machine to layer n on another machine
Instead  each layer passes data and control information to the layer immediately below it  until the lowest layer is reached
Below layer  is the physical medium through which actual communication occurs
Between each pair of adjacent layers is an interface
The interface defines which primitive operations and services the lower layer makes available to the upper one
When network designers decide how many layers to include in a network and what each one should do  one of the most important considerations is defining clean interfaces between the layers
Doing so  in turn  requires that each layer perform a specific collection of well-understood functions
In addition to minimizing the amount of information that must be passed between layers  clearcut interfaces also make it simpler to replace one layer with a completely different protocol or implementation (
replacing all the telephone lines by satellite channels) because all that is required of the new protocol or implementation is that it offer exactly the same set of services to its upstairs neighbor as the old one did
It is common that different hosts use different implementations of the same protocol (often written by different companies)
In fact  the protocol itself can change in some layer without the layers above and below it even noticing
NETWORK SOFTWARE   A set of layers and protocols is called a network architecture
The specification of an architecture must contain enough information to allow an implementer to write the program or build the hardware for each layer so that it will correctly obey the appropriate protocol
Neither the details of the implementation nor the specification of the interfaces is part of the architecture because these are hidden away inside the machines and not visible from the outside
It is not even necessary that the interfaces on all machines in a network be the same  provided that each machine can correctly use all the protocols
A list of the protocols used by a certain system  one protocol per layer  is called a protocol stack
Network architectures  protocol stacks  and the protocols themselves are the principal subjects of this book
An analogy may help explain the idea of multilayer communication
Imagine two philosophers (peer processes in layer  )  one of whom speaks Urdu and English and one of whom speaks Chinese and French
Since they have no common language  they each engage a translator (peer processes at layer  )  each of whom in turn contacts a retary (peer processes in layer  )
Philosopher  wishes to convey his affection for oryctolagus cuniculus to his peer
To do so  he passes a message (in English) across the  /  interface to his translator  saying ââI like rabbits ââ as illustrated in Fig
The translators have agreed on a neutral language known to both of them  Dutch  so the message is converted to ââIk vind konijnen leuk
ââ The choice of the language is the layer  protocol and is up to the layer  peer processes
The translator then gives the message to a retary for transmission  for example  by email (the layer  protocol)
When the message arrives at the other retary  it is passed to the local translator  who translates it into French and passes it across the  /  interface to the ond philosopher
Note that each protocol is completely independent of the other ones as long as the interfaces are not changed
The translators can switch from Dutch to  say  Finnish  at will  provided that they both agree and neither changes his interface with either layer  or layer
Similarly  the retaries can switch from email to telephone without disturbing (or even informing) the other layers
Each process may add some information intended only for its peer
This information is not passed up to the layer above
Now consider a more technical example: how to provide communication to the top layer of the five-layer network in Fig
A message  M  is produced by an application process running in layer  and given to layer  for transmission
Layer  puts a header in front of the message to identify the message and passes the result to layer
The header includes control information  such as addresses  to allow layer  on the destination machine to deliver the message
Other examples of control information used in some layers are sequence numbers (in case the lower layer does not preserve message order)  sizes  and times
In many networks  no limit is placed on the size of messages transmitted in the layer  protocol but there is nearly always a limit imposed by the layer  protocol
Consequently  layer  must break up the incoming messages into smaller   INTRODUCTION
I like rabbits Location A    Location B Message Philosopher Translator retary Information for the remote translator Information for the remote retary L: Dutch Ik vind konijnen leuk Fax #--- L: Dutch Ik vind konijnen leuk J'aime bien les lapins L: Dutch Ik vind konijnen leuk Fax #--- L: Dutch Ik vind konijnen leuk Figure  -
The philosopher-translator-retary architecture
units  packets  prepending a layer  header to each packet
In this example  M is split into two parts  M  and M  that will be transmitted separately
Layer  decides which of the outgoing lines to use and passes the packets to layer
Layer  adds to each piece not only a header but also a trailer  and gives the resulting unit to layer  for physical transmission
At the receiving machine the message moves upward  from layer to layer  with headers being stripped off as it progresses
None of the headers for layers below n are passed up to layer n
The important thing to understand about Fig
The peer processes in layer   for example  conceptually think of their communication as being ââhorizontal ââ using the layer  protocol
Each one is likely to have procedures called something like SendToOtherSide and GetFrom- OtherSide  even though these procedures actually communicate with lower layers across the  /  interface  and not with the other side
NETWORK SOFTWARE   H  H  H  M  T  H  H  M  T  H  H  H  M  T  H  H  M  T  H  H  M  H  M  H  H  M  H  M  H  M H  M M M Layer  protocol  Layer  protocol Layer  protocol Layer  protocol    Layer Source machine Destination machine Figure  -
Example information flow supporting virtual communication in layer
The peer process abstraction is crucial to all network design
Using it  the unmanageable task of designing the complete network can be broken into several smaller  manageable design problems  namely  the design of the individual layers
Although   is called ââNetwork Software ââ it is worth pointing out that the lower layers of a protocol hierarchy are frequently implemented in hardware or firmware
Nevertheless  complex protocol algorithms are involved  even if they are embedded (in whole or in part) in hardware
Design Issues for the Layers Some of the key design issues that occur in computer networks will come up in layer after layer
Below  we will briefly mention the more important ones
Reliability is the design issue of making a network that operates correctly even though it is made up of a collection of components that are themselves unreliable
Think about the bits of a packet traveling through the network
There is a chance that some of these bits will be received damaged (inverted) due to fluke electrical noise  random wireless signals  hardware flaws  software bugs and so on
How is it possible that we find and fix these errors? One mechanism for finding errors in received information uses codes for error detection
Information that is incorrectly received can then be retransmitted   INTRODUCTION
until it is received correctly
More powerful codes allow for error correction  where the correct message is recovered from the possibly incorrect bits that were originally received
Both of these mechanisms work by adding redundant information
They are used at low layers  to protect packets sent over individual links  and high layers  to check that the right contents were received
Another reliability issue is finding a working path through a network
Often there are multiple paths between a source and destination  and in a large network  there may be some links or routers that are broken
Suppose that the network is down in Germany
Packets sent from London to Rome via Germany will not get through  but we could instead send packets from London to Rome via Paris
The network should automatically make this decision
This topic is called routing
A ond design issue concerns the evolution of the network
Over time  networks grow larger and new designs emerge that need to be connected to the existing network
We have recently seen the key structuring mechanism used to support change by dividing the overall problem and hiding implementation details: protocol layering
There are many other strategies as well
Since there are many computers on the network  every layer needs a mechanism for identifying the senders and receivers that are involved in a particular message
This mechanism is called addressing or naming  in the low and high layers  respectively
An aspect of growth is that different network technologies often have different limitations
For example  not all communication channels preserve the order of messages sent on them  leading to solutions that number messages
Another example is differences in the maximum size of a message that the networks can transmit
This leads to mechanisms for disassembling  transmitting  and then reassembling messages
This overall topic is called internetworking
When networks get large  new problems arise
Cities can have traffic jams  a shortage of telephone numbers  and it is easy to get lost
Not many people have these problems in their own neighborhood  but citywide they may be a big issue
Designs that continue to work well when the network gets large are said to be scalable
A third design issue is resource allocation
Networks provide a service to hosts from their underlying resources  such as the capacity of transmission lines
To do this well  they need mechanisms that divide their resources so that one host does not interfere with another too much
Many designs share network bandwidth dynamically  according to the shortterm needs of hosts  rather than by giving each host a fixed fraction of the bandwidth that it may or may not use
This design is called statistical multiplexing  meaning sharing based on the statistics of demand
It can be applied at low layers for a single link  or at high layers for a network or even applications that use the network
An allocation problem that occurs at every level is how to keep a fast sender from swamping a slow receiver with data
Feedback from the receiver to the   NETWORK SOFTWARE   sender is often used
This subject is called flow control
Sometimes the problem is that the network is oversubscribed because too many computers want to send too much traffic  and the network cannot deliver it all
This overloading of the network is called congestion
One strategy is for each computer to reduce its demand when it experiences congestion
It  too  can be used in all layers
It is interesting to observe that the network has more resources to offer than simply bandwidth
For uses such as carrying live video  the timeliness of delivery matters a great deal
Most networks must provide service to applications that want this real-time delivery at the same time that they provide service to applications that want high throughput
Quality of service is the name given to mechanisms that reconcile these competing demands
The last major design issue is to ure the network by defending it against different kinds of threats
One of the threats we have mentioned previously is that of eavesdropping on communications
Mechanisms that provide confidentiality defend against this threat  and they are used in multiple layers
Mechanisms for authentication prevent someone from impersonating someone else
They might be used to tell fake banking Web sites from the real one  or to let the cellular network check that a call is really coming from your phone so that you will pay the bill
Other mechanisms for integrity prevent surreptitious changes to messages  such as altering ââdebit my account $  ââ to ââdebit my account $
ââ All of these designs are based on cryptography  which we shall study in      Connection-Oriented Versus Connectionless Service Layers can offer two different types of service to the layers above them: connection- oriented and connectionless
In this tion we will look at these two types and examine the differences between them
Connection-oriented service is modeled after the telephone system
To talk to someone  you pick up the phone  dial the number  talk  and then hang up
Similarly  to use a connection-oriented network service  the service user first establishes a connection  uses the connection  and then releases the connection
The essential aspect of a connection is that it acts like a tube: the sender pushes objects (bits) in at one end  and the receiver takes them out at the other end
In most cases the order is preserved so that the bits arrive in the order they were sent
In some cases when a connection is established  the sender  receiver  and subnet conduct a negotiation about the parameters to be used  such as maximum message size  quality of service required  and other issues
Typically  one side makes a proposal and the other side can accept it  reject it  or make a counterproposal
A circuit is another name for a connection with associated resources  such as a fixed bandwidth
This dates from the telephone network in which a circuit was a path over copper wire that carried a phone conversation
In contrast to connection-oriented service  connectionless service is modeled after the postal system
Each message (letter) carries the full destination address INTRODUCTION
and each one is routed through the intermediate nodes inside the system independent of all the subsequent messages
There are different names for messages in different contexts; a packet is a message at the network layer
When the intermediate nodes receive a message in full before sending it on to the next node  this is called store-and-forward switching
The alternative  in which the onward transmission of a message at a node starts before it is completely received by the node  is called cut-through switching
Normally  when two messages are sent to the same destination  the first one sent will be the first one to arrive
However  it is possible that the first one sent can be delayed so that the ond one arrives first
Each kind of service can further be characterized by its reliability
Some services are reliable in the sense that they never lose data
Usually  a reliable service is implemented by having the receiver acknowledge the receipt of each message so the sender is sure that it arrived
The acknowledgement process introduces overhead and delays  which are often worth it but are sometimes undesirable
A typical situation in which a reliable connection-oriented service is appropriate is file transfer
The owner of the file wants to be sure that all the bits arrive correctly and in the same order they were sent
Very few file transfer customers would prefer a service that occasionally scrambles or loses a few bits  even if it is much faster
Reliable connection-oriented service has two minor variations: message sequences and byte streams
In the former variant  the message boundaries are preserved
When two -byte messages are sent  they arrive as two distinct - byte messages  never as one -byte message
In the latter  the connection is simply a stream of bytes  with no message boundaries
When  bytes arrive at the receiver  there is no way to tell if they were sent as one -byte message  two -byte messages  or  -byte messages
If the pages of a book are sent over a network to a phototypesetter as separate messages  it might be important to preserve the message boundaries
On the other hand  to download a DVD movie  a byte stream from the server to the userâs computer is all that is needed
Message boundaries within the movie are not relevant
For some applications  the transit delays introduced by acknowledgements are unacceptable
One such application is digitized voice traffic for voice over IP
It is less disruptive for telephone users to hear a bit of noise on the line from time to time than to experience a delay waiting for acknowledgements
Similarly  when transmitting a video conference  having a few pixels wrong is no problem  but having the image jerk along as the flow stops and starts to correct errors is irritating
Not all applications require connections
For example  spammers send electronic junk-mail to many recipients
The spammer probably does not want to go to the trouble of setting up and later tearing down a connection to a recipient just to send them one item
Nor is percent reliable delivery essential  especially if it costs more
All that is needed is a way to send a single message that has a high   NETWORK SOFTWARE   probability of arrival  but no guarantee
Unreliable (meaning not acknowledged) connectionless service is often called datagram service  in analogy with telegram service  which also does not return an acknowledgement to the sender
Despite it being unreliable  it is the dominant form in most networks for reasons that will become clear later In other situations  the convenience of not having to establish a connection to send one message is desired  but reliability is essential
The acknowledged datagram service can be provided for these applications
It is like sending a registered letter and requesting a return receipt
When the receipt comes back  the sender is absolutely sure that the letter was delivered to the intended party and not lost along the way
Text messaging on mobile phones is an example
Still another service is the request-reply service
In this service the sender transmits a single datagram containing a request; the reply contains the answer
Request-reply is commonly used to implement communication in the client-server model: the client issues a request and the server responds to it
For example  a mobile phone client might send a query to a map server to retrieve the map data for the current location
Figure  -  summarizes the types of services discussed above
Reliable message stream Reliable byte stream Unreliable connection Unreliable datagram Acknowledged datagram Request-reply Service Connectionoriented Connectionless Sequence of pages Movie download Voice over IP Electronic junk mail Text messaging Database query Example Figure  -
Six different types of service
The concept of using unreliable communication may be confusing at first
After all  why would anyone actually prefer unreliable communication to reliable communication? First of all  reliable communication (in our sense  that is  acknowledged) may not be available in a given layer
For example  Ethernet does not provide reliable communication
Packets can occasionally be damaged in transit
It is up to higher protocol levels to recover from this problem
In particular  many reliable services are built on top of an unreliable datagram service
ond  the delays inherent in providing a reliable service may be unacceptable  especially in real-time applications such as multimedia
For these reasons  both reliable and unreliable communication coexist
INTRODUCTION
Service Primitives A service is formally specified by a set of primitives (operations) available to user processes to access the service
These primitives tell the service to perform some action or report on an action taken by a peer entity
If the protocol stack is located in the operating system  as it often is  the primitives are normally system calls
These calls cause a trap to kernel mode  which then turns control of the machine over to the operating system to send the necessary packets
The set of primitives available depends on the nature of the service being provided
The primitives for connection-oriented service are different from those of connectionless service
As a minimal example of the service primitives that might provide a reliable byte stream  consider the primitives listed in Fig
They will be familiar to fans of the Berkeley socket interface  as the primitives are a simplified version of that interface
Primitive Meaning LISTEN Block waiting for an incoming connection CONNECT Establish a connection with a waiting peer ACCEPT Accept an incoming connection from a peer RECEIVE Block waiting for an incoming message SEND Send a message to the peer DISCONNECT Terminate a connection Figure  -
Six service primitives that provide a simple connection-oriented service
These primitives might be used for a request-reply interaction in a client-server environment
To illustrate how  We sketch a simple protocol that implements the service using acknowledged datagrams
First  the server executes LISTEN to indicate that it is prepared to accept incoming connections
A common way to implement LISTEN is to make it a blocking system call
After executing the primitive  the server process is blocked until a request for connection appears
Next  the client process executes CONNECT to establish a connection with the server
The CONNECT call needs to specify who to connect to  so it might have a parameter giving the serverâs address
The operating system then typically sends a packet to the peer asking it to connect  as shown by ( ) in Fig
The client process is suspended until there is a response
When the packet arrives at the server  the operating system sees that the packet is requesting a connection
It checks to see if there is a listener  and if so it unblocks the listener
The server process can then establish the connection with the ACCEPT call
This sends a response ( ) back to the client process to accept the   NETWORK SOFTWARE   Client machine ( ) Connect request ( ) Accept response System calls Operating Kernel system Client process Protocol Drivers stack Server machine System process Kernel Protocol Drivers stack ( ) Request for data ( ) Reply ( ) Disconnect ( ) Disconnect Figure  -
A simple client-server interaction using acknowledged datagrams
connection
The arrival of this response then releases the client
At this point the client and server are both running and they have a connection established
The obvious analogy between this protocol and real life is a customer (client) calling a companyâs customer service manager
At the start of the day  the service manager sits next to his telephone in case it rings
Later  a client places a call
When the manager picks up the phone  the connection is established
The next step is for the server to execute RECEIVE to prepare to accept the first request
Normally  the server does this immediately upon being released from the LISTEN  before the acknowledgement can get back to the client
The RECEIVE call blocks the server
Then the client executes SEND to transmit its request ( ) followed by the execution of RECEIVE to get the reply
The arrival of the request packet at the server machine unblocks the server so it can handle the request
After it has done the work  the server uses SEND to return the answer to the client ( )
The arrival of this packet unblocks the client  which can now inspect the answer
If the client has additional requests  it can make them now
When the client is done  it executes DISCONNECT to terminate the connection ( )
Usually  an initial DISCONNECT is a blocking call  suspending the client and sending a packet to the server saying that the connection is no longer needed
When the server gets the packet  it also issues a DISCONNECT of its own  acknowledging the client and releasing the connection ( )
When the serverâs packet gets back to the client machine  the client process is released and the connection is broken
In a nutshell  this is how connection-oriented communication works
Of course  life is not so simple
Many things can go wrong here
The timing can be wrong (
the CONNECT is done before the LISTEN)  packets can get lost  and much more
We will look at these issues in great detail later  but for the moment  Fig
Given that six packets are required to complete this protocol  one might wonder why a connectionless protocol is not used instead
The answer is that in a perfect world it could be  in which case only two packets would be needed: one   INTRODUCTION
for the request and one for the reply
However  in the face of large messages in either direction (
a megabyte file)  transmission errors  and lost packets  the situation changes
If the reply consisted of hundreds of packets  some of which could be lost during transmission  how would the client know if some pieces were missing? How would the client know whether the last packet actually received was really the last packet sent? Suppose the client wanted a ond file
How could it tell packet  from the ond file from a lost packet  from the first file that suddenly found its way to the client? In short  in the real world  a simple request- reply protocol over an unreliable network is often inadequate
we will study a variety of protocols in detail that overcome these and other problems
For the moment  suffice it to say that having a reliable  ordered byte stream between processes is sometimes very convenient
The Relationship of Services to Protocols Services and protocols are distinct concepts
This distinction is so important that we emphasize it again here
A service is a set of primitives (operations) that a layer provides to the layer above it
The service defines what operations the layer is prepared to perform on behalf of its users  but it says nothing at all about how these operations are implemented
A service relates to an interface between two layers  with the lower layer being the service provider and the upper layer being the service user
A protocol  in contrast  is a set of rules governing the format and meaning of the packets  or messages that are exchanged by the peer entities within a layer
Entities use protocols to implement their service definitions
They are free to change their protocols at will  provided they do not change the service visible to their users
In this way  the service and the protocol are completely decoupled
This is a key concept that any network designer should understand well
To repeat this crucial point  services relate to the interfaces between layers  as illustrated in Fig
In contrast  protocols relate to the packets sent between peer entities on different machines
It is very important not to confuse the two concepts
An analogy with programming languages is worth making
A service is like an abstract data type or an object in an object-oriented language
It defines operations that can be performed on an object but does not specify how these operations are implemented
In contrast  a protocol relates to the implementation of the service and as such is not visible to the user of the service
Many older protocols did not distinguish the service from the protocol
In effect  a typical layer might have had a service primitive SEND PACKET with the user providing a pointer to a fully assembled packet
This arrangement meant that all changes to the protocol were immediately visible to the users
Most network designers now regard such a design as a serious blunder
REFERENCE MODELS   Layer k Layer k +  Layer k -  Protocol Service provided by layer k Layer k Layer k +  Layer k -  Figure  -
The relationship between a service and a protocol  REFERENCE MODELS Now that we have discussed layered networks in the abstract  it is time to look at some examples
We will discuss two important network architectures: the OSI reference model and the TCP/IP reference model
Although the protocols associated with the OSI model are not used any more  the model itself is actually quite general and still valid  and the features discussed at each layer are still very important
The TCP/IP model has the opposite properties: the model itself is not of much use but the protocols are widely used
For this reason we will look at both of them in detail
Also  sometimes you can learn more from failures than from successes
The OSI Reference Model The OSI model (minus the physical medium) is shown in Fig
This model is based on a proposal developed by the International Standards Organization (ISO) as a first step toward international standardization of the protocols used in the various layers (Day and Zimmermann  )
It was revised in  (Day  )
The model is called the ISO OSI (Open Systems Interconnection) Reference Model because it deals with connecting open systemsâthat is  systems that are open for communication with other systems
We will just call it the OSI model for short
The OSI model has seven layers
The principles that were applied to arrive at the seven layers can be briefly summarized as follows:
A layer should be created where a different abstraction is needed Each layer should perform a well-defined function The function of each layer should be chosen with an eye toward defining internationally standardized protocols
INTRODUCTION
Layer Presentation Application Session Transport Network Data link Physical  Interface Host A Name of unit exchanged APDU PPDU SPDU TPDU Packet Frame Bit Presentation Application Session Transport Network Data link Physical Host B Network Network Data link Data link Physical Physical Router Router Internal subnet protocol Application protocol Presentation protocol Transport protocol Session protocol Communication subnet boundary Network layer host-router protocol Data link layer host-router protocol Physical layer host-router protocol Figure  -
The OSI reference model The layer boundaries should be chosen to minimize the information flow across the interfaces The number of layers should be large enough that distinct functions need not be thrown together in the same layer out of necessity and small enough that the architecture does not become unwieldy
Below we will discuss each layer of the model in turn  starting at the bottom layer
Note that the OSI model itself is not a network architecture because it does not specify the exact services and protocols to be used in each layer
It just tells what each layer should do
However  ISO has also produced standards for all the layers  although these are not part of the reference model itself
Each one has been published as a separate international standard
The model (in part) is widely used although the associated protocols have been long forgotten
REFERENCE MODELS   The Physical Layer The physical layer is concerned with transmitting raw bits over a communication channel
The design issues have to do with making sure that when one side sends a  bit it is received by the other side as a  bit  not as a  bit
Typical questions here are what electrical signals should be used to represent a  and a   how many nanoonds a bit lasts  whether transmission may proceed simultaneously in both directions  how the initial connection is established  how it is torn down when both sides are finished  how many pins the network connector has  and what each pin is used for
These design issues largely deal with mechanical  electrical  and timing interfaces  as well as the physical transmission medium  which lies below the physical layer
The Data Link Layer The main task of the data link layer is to transform a raw transmission facility into a line that appears free of undetected transmission errors
It does so by masking the real errors so the network layer does not see them
It accomplishes this task by having the sender break up the input data into data frames (typically a few hundred or a few thousand bytes) and transmit the frames sequentially
If the service is reliable  the receiver confirms correct receipt of each frame by sending back an acknowledgement frame
Another issue that arises in the data link layer (and most of the higher layers as well) is how to keep a fast transmitter from drowning a slow receiver in data
Some traffic regulation mechanism may be needed to let the transmitter know when the receiver can accept more data
Broadcast networks have an additional issue in the data link layer: how to control access to the shared channel
A special sublayer of the data link layer  the medium access control sublayer  deals with this problem
The Network Layer The network layer controls the operation of the subnet
A key design issue is determining how packets are routed from source to destination
Routes can be based on static tables that are ââwired intoââ the network and rarely changed  or more often they can be updated automatically to avoid failed components
They can also be determined at the start of each conversation  for example  a terminal session  such as a login to a remote machine
Finally  they can be highly dynamic  being determined anew for each packet to reflect the current network load
If too many packets are present in the subnet at the same time  they will get in one anotherâs way  forming bottlenecks
Handling congestion is also a responsibility of the network layer  in conjunction with higher layers that adapt the load   INTRODUCTION
they place on the network
More generally  the quality of service provided (delay  transit time  jitter  etc
) is also a network layer issue
When a packet has to travel from one network to another to get to its destination  many problems can arise
The addressing used by the ond network may be different from that used by the first one
The ond one may not accept the packet at all because it is too large
The protocols may differ  and so on
It is up to the network layer to overcome all these problems to allow heterogeneous networks to be interconnected
In broadcast networks  the routing problem is simple  so the network layer is often thin or even nonexistent
The Transport Layer The basic function of the transport layer is to accept data from above it  split it up into smaller units if need be  pass these to the network layer  and ensure that the pieces all arrive correctly at the other end
Furthermore  all this must be done efficiently and in a way that isolates the upper layers from the inevitable changes in the hardware technology over the course of time
The transport layer also determines what type of service to provide to the session layer  and  ultimately  to the users of the network
The most popular type of transport connection is an error-free point-to-point channel that delivers messages or bytes in the order in which they were sent
However  other possible kinds of transport service exist  such as the transporting of isolated messages with no guarantee about the order of delivery  and the broadcasting of messages to multiple destinations
The type of service is determined when the connection is established
(As an aside  an error-free channel is completely impossible to achieve; what people really mean by this term is that the error rate is low enough to ignore in practice
) The transport layer is a true end-to-end layer; it carries data all the way from the source to the destination
In other words  a program on the source machine carries on a conversation with a similar program on the destination machine  using the message headers and control messages
In the lower layers  each protocols is between a machine and its immediate neighbors  and not between the ultimate source and destination machines  which may be separated by many routers
The difference between layers  through   which are chained  and layers  through   which are end-to-end  is illustrated in Fig
The Session Layer The session layer allows users on different machines to establish sessions between them
Sessions offer various services  including dialog control (keeping track of whose turn it is to transmit)  token management (preventing two parties from attempting the same critical operation simultaneously)  and synchronization   REFERENCE MODELS   (checkpointing long transmissions to allow them to pick up from where they left off in the event of a crash and subsequent recovery)
The Presentation Layer Unlike the lower layers  which are mostly concerned with moving bits around  the presentation layer is concerned with the syntax and semantics of the information transmitted
In order to make it possible for computers with different internal data representations to communicate  the data structures to be exchanged can be defined in an abstract way  along with a standard encoding to be used ââon the wire
ââ The presentation layer manages these abstract data structures and allows higher-level data structures (
banking records) to be defined and exchanged
The Application Layer The application layer contains a variety of protocols that are commonly needed by users
One widely used application protocol is HTTP (HyperText Transfer Protocol)  which is the basis for the World Wide Web
When a browser wants a Web page  it sends the name of the page it wants to the server hosting the page using HTTP
The server then sends the page back
Other application protocols are used for file transfer  electronic mail  and network news
The TCP/IP Reference Model Let us now turn from the OSI reference model to the reference model used in the grandparent of all wide area computer networks  the ARPANET  and its successor  the worldwide Internet
Although we will give a brief history of the ARPANET later  it is useful to mention a few key aspects of it now
The ARPANET was a research network sponsored by the DoD (
Department of Defense)
It eventually connected hundreds of universities and government installations  using leased telephone lines
When satellite and radio networks were added later  the existing protocols had trouble interworking with them  so a new reference architecture was needed
Thus  from nearly the beginning  the ability to connect multiple networks in a seamless way was one of the major design goals
This architecture later became known as the TCP/IP Reference Model  after its two primary protocols
It was first described by Cerf and Kahn (   )  and later refined and defined as a standard in the Internet community (Braden  )
The design philosophy behind the model is discussed by Clark (   )
Given the DoDâs worry that some of its precious hosts  routers  and internetwork gateways might get blown to pieces at a momentâs notice by an attack from the Soviet Union  another major goal was that the network be able to survive loss of subnet hardware  without existing conversations being broken off
In other   INTRODUCTION
words  the DoD wanted connections to remain intact as long as the source and destination machines were functioning  even if some of the machines or transmission lines in between were suddenly put out of operation
Furthermore  since applications with divergent requirements were envisioned  ranging from transferring files to real-time speech transmission  a flexible architecture was needed
The Link Layer All these requirements led to the choice of a packet-switching network based on a connectionless layer that runs across different networks
The lowest layer in the model  the link layer describes what links such as serial lines and classic Ethernet must do to meet the needs of this connectionless internet layer
It is not really a layer at all  in the normal sense of the term  but rather an interface between hosts and transmission links
Early material on the TCP/IP model has little to say about it
The Internet Layer The internet layer is the linchpin that holds the whole architecture together
It is shown in Fig
Its job is to permit hosts to inject packets into any network and have them travel independently to the destination (potentially on a different network)
They may even arrive in a completely different order than they were sent  in which case it is the job of higher layers to rearrange them  if in-order delivery is desired
Note that ââinternetââ is used here in a generic sense  even though this layer is present in the Internet
OSI TCP/IP Application Presentation Session Transport Network Data link Physical  Application Transport Internet Link Not present in the model Figure  -
The TCP/IP reference model
The analogy here is with the (snail) mail system
A person can drop a sequence of international letters into a mailbox in one country  and with a little luck    REFERENCE MODELS   most of them will be delivered to the correct address in the destination country
The letters will probably travel through one or more international mail gateways along the way  but this is transparent to the users
Furthermore  that each country (
each network) has its own stamps  preferred envelope sizes  and delivery rules is hidden from the users
The internet layer defines an official packet format and protocol called IP (Internet Protocol)  plus a companion protocol called ICMP (Internet Control Message Protocol) that helps it function
The job of the internet layer is to deliver IP packets where they are supposed to go
Packet routing is clearly a major issue here  as is congestion (though IP has not proven effective at avoiding congestion)
The Transport Layer The layer above the internet layer in the TCP/IP model is now usually called the transport layer
It is designed to allow peer entities on the source and destination hosts to carry on a conversation  just as in the OSI transport layer
Two end-to-end transport protocols have been defined here
The first one  TCP (Transmission Control Protocol)  is a reliable connection-oriented protocol that allows a byte stream originating on one machine to be delivered without error on any other machine in the internet
It segments the incoming byte stream into discrete messages and passes each one on to the internet layer
At the destination  the receiving TCP process reassembles the received messages into the output stream
TCP also handles flow control to make sure a fast sender cannot swamp a slow receiver with more messages than it can handle
The ond protocol in this layer  UDP (User Datagram Protocol)  is an unreliable  connectionless protocol for applications that do not want TCPâs sequencing or flow control and wish to provide their own
It is also widely used for one-shot  client-server-type request-reply queries and applications in which prompt delivery is more important than accurate delivery  such as transmitting speech or video
The relation of IP  TCP  and UDP is shown in Fig
Since the model was developed  IP has been implemented on many other networks
The Application Layer The TCP/IP model does not have session or presentation layers
No need for them was perceived
Instead  applications simply include any session and presentation functions that they require
Experience with the OSI model has proven this view correct: these layers are of little use to most applications
On top of the transport layer is the application layer
It contains all the higher- level protocols
The early ones included virtual terminal (TELNET)  file transfer (FTP)  and electronic mail (SMTP)
Many other protocols have been added to these over the years
Some important ones that we will study  shown in Fig
Link DSL SONET
Ethernet IP ICMP HTTP SMTP RTP DNS TCP UDP Internet Transport Layers Protocols Application Figure  -
The TCP/IP model with some protocols we will study
include the Domain Name System (DNS)  for mapping host names onto their network addresses  HTTP  the protocol for fetching pages on the World Wide Web  and RTP  the protocol for delivering real-time media such as voice or movies
The Model Used in This Book As mentioned earlier  the strength of the OSI reference model is the model itself (minus the presentation and session layers)  which has proven to be exceptionally useful for discussing computer networks
In contrast  the strength of the TCP/IP reference model is the protocols  which have been widely used for many years
Since computer scientists like to have their cake and eat it  too  we will use the hybrid model of Fig
Application  Transport  Network  Link  Physical Figure  -
The reference model used in this book
This model has five layers  running from the physical layer up through the link  network and transport layers to the application layer
The physical layer specifies how to transmit bits across different kinds of media as electrical (or other analog) signals
The link layer is concerned with how to send finite-length messages between directly connected computers with specified levels of reliability
Ethernet and
are examples of link layer protocols
REFERENCE MODELS   The network layer deals with how to combine multiple links into networks  and networks of networks  into internetworks so that we can send packets between distant computers
This includes the task of finding the path along which to send the packets
IP is the main example protocol we will study for this layer
The transport layer strengthens the delivery guarantees of the Network layer  usually with increased reliability  and provide delivery abstractions  such as a reliable byte stream  that match the needs of different applications
TCP is an important example of a transport layer protocol
Finally  the application layer contains programs that make use of the network
Many  but not all  networked applications have user interfaces  such as a Web browser
Our concern  however  is with the portion of the program that uses the network
This is the HTTP protocol in the case of the Web browser
There are also important support programs in the application layer  such as the DNS  that are used by many applications
Our  ter sequence is based on this model
In this way  we retain the value of the OSI model for understanding network architectures  but concentrate primarily on protocols that are important in practice  from TCP/IP and related protocols to newer ones such as
SONET  and Bluetooth
A Comparison of the OSI and TCP/IP Reference Models The OSI and TCP/IP reference models have much in common
Both are based on the concept of a stack of independent protocols
Also  the functionality of the layers is roughly similar
For example  in both models the layers up through and including the transport layer are there to provide an end-to-end  network- independent transport service to processes wishing to communicate
These layers form the transport provider
Again in both models  the layers above transport are application-oriented users of the transport service
Despite these fundamental similarities  the two models also have many differences
In this tion we will focus on the key differences between the two reference models
It is important to note that we are comparing the reference models here  not the corresponding protocol stacks
The protocols themselves will be discussed later
For an entire book comparing and contrasting TCP/IP and OSI  see Piscitello and  in (   )
Three concepts are central to the OSI model:
Services Interfaces Protocols
Probably the biggest contribution of the OSI model is that it makes the distinction between these three concepts explicit
Each layer performs some services for the   INTRODUCTION
layer above it
The service definition tells what the layer does  not how entities above it access it or how the layer works
It defines the layerâs semantics
A layerâs interface tells the processes above it how to access it
It specifies what the parameters are and what results to expect
It  too  says nothing about how the layer works inside
Finally  the peer protocols used in a layer are the layerâs own business
It can use any protocols it wants to  as long as it gets the job done (
provides the offered services)
It can also change them at will without affecting software in higher layers
These ideas fit very nicely with modern ideas about object-oriented programming
An object  like a layer  has a set of methods (operations) that processes outside the object can invoke
The semantics of these methods define the set of services that the object offers
The methodsâ parameters and results form the objectâs interface
The code internal to the object is its protocol and is not visible or of any concern outside the object
The TCP/IP model did not originally clearly distinguish between services  interfaces  and protocols  although people have tried to retrofit it after the fact to make it more OSI-like
For example  the only real services offered by the internet layer are SEND IP PACKET and RECEIVE IP PACKET
As a consequence  the protocols in the OSI model are better hidden than in the TCP/IP model and can be replaced relatively easily as the technology changes
Being able to make such changes transparently is one of the main purposes of having layered protocols in the first place
The OSI reference model was devised before the corresponding protocols were invented
This ordering meant that the model was not biased toward one particular set of protocols  a fact that made it quite general
The downside of this ordering was that the designers did not have much experience with the subject and did not have a good idea of which functionality to put in which layer
For example  the data link layer originally dealt only with point-to-point networks
When broadcast networks came around  a new sublayer had to be hacked into the model
Furthermore  when people started to build real networks using the OSI model and existing protocols  it was discovered that these networks did not match the required service specifications (wonder of wonders)  so convergence sublayers had to be grafted onto the model to provide a place for papering over the differences
Finally  the committee originally expected that each country would have one network  run by the government and using the OSI protocols  so no thought was given to internetworking
To make a long story short  things did not turn out that way
With TCP/IP the reverse was true: the protocols came first  and the model was really just a description of the existing protocols
There was no problem with the protocols fitting the model
They fit perfectly
The only trouble was that the model did not fit any other protocol stacks
Consequently  it was not especially useful for describing other  non-TCP/IP networks
REFERENCE MODELS   Turning from philosophical matters to more specific ones  an obvious difference between the two models is the number of layers: the OSI model has seven layers and the TCP/IP model has four
Both have (inter)network  transport  and application layers  but the other layers are different
Another difference is in the area of connectionless versus connection-oriented communication
The OSI model supports both connectionless and connectionoriented communication in the network layer  but only connection-oriented communication in the transport layer  where it counts (because the transport service is visible to the users)
The TCP/IP model supports only one mode in the network layer (connectionless) but both in the transport layer  giving the users a choice
This choice is especially important for simple request-response protocols
A Critique of the OSI Model and Protocols Neither the OSI model and its protocols nor the TCP/IP model and its protocols are perfect
Quite a bit of criticism can be  and has been  directed at both of them
In this tion and the next one  we will look at some of these criticisms
We will begin with OSI and examine TCP/IP afterward
At the time the ond edition of this book was published (   )  it appeared to many experts in the field that the OSI model and its protocols were going to take over the world and push everything else out of their way
This did not happen
Why? A look back at some of the reasons may be useful
They can be summarized as:
Bad timing Bad technology Bad implementations Bad politics
Bad Timing First let us look at reason one: bad timing
The time at which a standard is established is absolutely critical to its success
David Clark of
has a theory of standards that he calls the apocalypse of the two elephants  which is illustrated in Fig
This figure shows the amount of activity surrounding a new subject
When the subject is first discovered  there is a burst of research activity in the form of discussions  papers  and meetings
After a while this activity subsides  corporations discover the subject  and the billion-dollar wave of investment hits
It is essential that the standards be written in the trough in between the two ââelephants
ââ If they are written too early (before the research results are well   INTRODUCTION
Time Activity Research Standards Billion dollar investment Figure  -
The apocalypse of the two elephants
established)  the subject may still be poorly understood; the result is a bad standard
If they are written too late  so many companies may have already made major investments in different ways of doing things that the standards are effectively ignored
If the interval between the two elephants is very short (because everyone is in a hurry to get started)  the people developing the standards may get crushed
It now appears that the standard OSI protocols got crushed
The competing TCP/IP protocols were already in widespread use by research universities by the time the OSI protocols appeared
While the billion-dollar wave of investment had not yet hit  the academic market was large enough that many vendors had begun cautiously offering TCP/IP products
When OSI came around  they did not want to support a ond protocol stack until they were forced to  so there were no initial offerings
With every company waiting for every other company to go first  no company went first and OSI never happened
Bad Technology The ond reason that OSI never caught on is that both the model and the protocols are flawed
The choice of seven layers was more political than technical  and two of the layers (session and presentation) are nearly empty  whereas two other ones (data link and network) are overfull
The OSI model  along with its associated service definitions and protocols  is extraordinarily complex
When piled up  the printed standards occupy a significant fraction of a meter of paper
They are also difficult to implement and inefficient in operation
In this context  a riddle posed by Paul Mockapetris and cited by Rose (   ) comes to mind: Q: What do you get when you cross a mobster with an international standard? A: Someone who makes you an offer you canât understand
REFERENCE MODELS   In addition to being incomprehensible  another problem with OSI is that some functions  such as addressing  flow control  and error control  reappear again and again in each layer
Saltzer et al
(   )  for example  have pointed out that to be effective  error control must be done in the highest layer  so that repeating it over and over in each of the lower layers is often unnecessary and inefficient
Bad Implementations Given the enormous complexity of the model and the protocols  it will come as no surprise that the initial implementations were huge  unwieldy  and slow
Everyone who tried them got burned
It did not take long for people to associate ââOSIââ with ââpoor quality
ââ Although the products improved in the course of time  the image stuck
In contrast  one of the first implementations of TCP/IP was part of Berkeley UNIX and was quite good (not to mention  free)
People began using it quickly  which led to a large user community  which led to improvements  which led to an even larger community
Here the spiral was upward instead of downward
Bad Politics On account of the initial implementation  many people  especially in academia  thought of TCP/IP as part of UNIX  and UNIX in the s in academia was not unlike parenthood (then incorrectly called motherhood) and apple pie
OSI  on the other hand  was widely thought to be the creature of the European telecommunication ministries  the European Community  and later the
Government
This belief was only partly true  but the very idea of a bunch of government bureaucrats trying to shove a technically inferior standard down the throats of the poor researchers and programmers down in the trenches actually developing computer networks did not aid OSIâs cause
Some people viewed this development in the same light as IBM announcing in the s that PL/I was the language of the future  or the DoD correcting this later by announcing that it was actually Ada
A Critique of the TCP/IP Reference Model The TCP/IP model and protocols have their problems too
First  the model does not clearly distinguish the concepts of services  interfaces  and protocols
Good software engineering practice requires differentiating between the specification and the implementation  something that OSI does very carefully  but TCP/IP does not
Consequently  the TCP/IP model is not much of a guide for designing new networks using new technologies
ond  the TCP/IP model is not at all general and is poorly suited to describing any protocol stack other than TCP/IP
Trying to use the TCP/IP model to describe Bluetooth  for example  is completely impossible
INTRODUCTION
Third  the link layer is not really a layer at all in the normal sense of the term as used in the context of layered protocols
It is an interface (between the network and data link layers)
The distinction between an interface and a layer is crucial  and one should not be sloppy about it
Fourth  the TCP/IP model does not distinguish between the physical and data link layers
These are completely different
The physical layer has to do with the transmission characteristics of copper wire  fiber optics  and wireless communication
The data link layerâs job is to delimit the start and end of frames and get them from one side to the other with the desired degree of reliability
A proper model should include both as separate layers
The TCP/IP model does not do this
Finally  although the IP and TCP protocols were carefully thought out and well implemented  many of the other protocols were ad hoc  generally produced by a couple of graduate students hacking away until they got tired
The protocol implementations were then distributed free  which resulted in their becoming widely used  deeply entrenched  and thus hard to replace
Some of them are a bit of an embarrassment now
The virtual terminal protocol  TELNET  for example  was designed for a ten-character-per-ond mechanical Teletype terminal
It knows nothing of graphical user interfaces and mice
Nevertheless  it is still in use some   years later  EXAMPLE NETWORKS The subject of computer networking covers many different kinds of networks  large and small  well known and less well known
They have different goals  scales  and technologies
In the following tions  we will look at some examples  to get an idea of the variety one finds in the area of computer networking
We will start with the Internet  probably the best known network  and look at its history  evolution  and technology
Then we will consider the mobile phone network
Technically  it is quite different from the Internet  contrasting nicely with it
Next we will introduce IEEE
the dominant standard for wireless LANs
Finally  we will look at RFID and sensor networks  technologies that extend the reach of the network to include the physical world and everyday objects
The Internet The Internet is not really a network at all  but a vast collection of different networks that use certain common protocols and provide certain common services
It is an unusual system in that it was not planned by anyone and is not controlled by anyone
To better understand it  let us start from the beginning and see how it has developed and why
For a wonderful history of the Internet  John Naughtonâs (   ) book is highly recommended
It is one of those rare books that is not only fun to read  but also has   pages of ibid
âs for the serious historian
Some of the material in this tion is based on this book
EXAMPLE NETWORKS   Of course  countless technical books have been written about the Internet and its protocols as well
For more information  see  for example  Maufer (   )
The ARPANET The story begins in the late s
At the height of the Cold War  the
DoD wanted a command-and-control network that could survive a nuclear war
At that time  all military communications used the public telephone network  which was considered vulnerable
The reason for this belief can be gleaned from Fig
Here the black dots represent telephone switching offices  each of which was connected to thousands of telephones
These switching offices were  in turn  connected to higher-level switching offices (toll offices)  to form a national hierarchy with only a small amount of redundancy
The vulnerability of the system was that the destruction of a few key toll offices could fragment it into many isolated islands
(a) Toll office Switching office (b) Figure  -
(a) Structure of the telephone system
(b) Baranâs proposed distributed switching system
Around  the DoD awarded a contract to the RAND Corporation to find a solution
One of its employees  Paul Baran  came up with the highly distributed and fault-tolerant design of Fig
Since the paths between any two switching offices were now much longer than analog signals could travel without distortion  Baran proposed using digital packet-switching technology
Baran wrote several reports for the DoD describing his ideas in detail (Baran  )
Officials at the Pentagon liked the concept and asked AT&T  then the
â national telephone monopoly  to build a prototype
AT&T dismissed Baranâs ideas out of hand
The biggest and richest corporation in the world was not about to allow   INTRODUCTION
some young whippersnapper tell it how to build a telephone system
They said Baranâs network could not be built and the idea was killed
Several years went by and still the DoD did not have a better command-andcontrol system
To understand what happened next  we have to go back all the way to October  when the Soviet Union beat the
into space with the launch of the first artificial satellite  Sputnik
When President Eisenhower tried to find out who was asleep at the switch  he was appalled to find the Army  Navy  and Air Force squabbling over the Pentagonâs research budget
His immediate response was to create a single defense research organization  ARPA  the Advanced Research Projects Agency
ARPA had no scientists or laboratories; in fact  it had nothing more than an office and a small (by Pentagon standards) budget
It did its work by issuing grants and contracts to universities and companies whose ideas looked promising to it
For the first few years  ARPA tried to figure out what its mission should be
In  the attention of Larry Roberts  a program manager at ARPA who was trying to figure out how to provide remote access to computers  turned to networking
He contacted various experts to decide what to do
One of them  Wesley Clark  suggested building a packet-switched subnet  connecting each host to its own router
After some initial skepticism  Roberts bought the idea and presented a somewhat vague paper about it at the ACM SIGOPS Symposium on Operating System Principles held in Gatlinburg  Tennessee in late  (Roberts  )
Much to Robertsâ surprise  another paper at the conference described a similar system that had not only been designed but actually fully implemented under the direction of Donald Davies at the National Physical Laboratory in England
The NPL system was not a national system (it just connected several computers on the NPL campus)  but it demonstrated that packet switching could be made to work
Furthermore  it cited Baranâs now discarded earlier work
Roberts came away from Gatlinburg determined to build what later became known as the ARPANET
The subnet would consist of minicomputers called IMPs (Interface Message Processors) connected by  -kbps transmission lines
For high reliability  each IMP would be connected to at least two other IMPs
The subnet was to be a datagram subnet  so if some lines and IMPs were destroyed  messages could be automatically rerouted along alternative paths
Each node of the network was to consist of an IMP and a host  in the same room  connected by a short wire
A host could send messages of up to  bits to its IMP  which would then break these up into packets of at most  bits and forward them independently toward the destination
Each packet was received in its entirety before being forwarded  so the subnet was the first electronic storeand- forward packet-switching network
ARPA then put out a tender for building the subnet
Twelve companies bid for it
After evaluating all the proposals  ARPA selected BBN  a consulting firm based in Cambridge  Massachusetts  and in December  awarded it a contract   EXAMPLE NETWORKS   to build the subnet and write the subnet software
BBN chose to use specially modified Honeywell DDP-   minicomputers with  K  -bit words of core memory as the IMPs
The IMPs did not have disks  since moving parts were considered unreliable
The IMPs were interconnected by  -kbps lines leased from telephone companies
Although   kbps is now the choice of teenagers who cannot afford DSL or cable  it was then the best money could buy
The software was split into two parts: subnet and host
The subnet software consisted of the IMP end of the host-IMP connection  the IMP-IMP protocol  and a source IMP to destination IMP protocol designed to improve reliability
The original ARPANET design is shown in Fig
Host-IMP protocol Host-host protocol Source IMP to destination IMP protocol IMP-IMP protocol IMP-IMP protocol Host IMP Subnet Figure  -
The original ARPANET design
Outside the subnet  software was also needed  namely  the host end of the host-IMP connection  the host-host protocol  and the application software
It soon became clear that BBN was of the opinion that when it had accepted a message on a host-IMP wire and placed it on the host-IMP wire at the destination  its job was done
Roberts had a problem  though: the hosts needed software too
To deal with it  he convened a meeting of network researchers  mostly graduate students  at Snowbird  Utah  in the summer of
The graduate students expected some network expert to explain the grand design of the network and its software to them and then assign each of them the job of writing part of it
They were astounded when there was no network expert and no grand design
They had to figure out what to do on their own
Nevertheless  somehow an experimental network went online in December  with four nodes: at UCLA  UCSB  SRI  and the University of Utah
These four were chosen because all had a large number of ARPA contracts  and all had different and completely incompatible host computers (just to make it more fun)
The first host-to-host message had been sent two months earlier from the UCLA   INTRODUCTION
node by a team led by Len Kleinrock (a pioneer of the theory of packet switching) to the SRI node
The network grew quickly as more IMPs were delivered and installed; it soon spanned the United States
Figure  -  shows how rapidly the ARPANET grew in the first  years
MIT UCLA UCLA RAND BBN SRI UTAH ILLINOIS MIT LINCOLN CASE CARN RAND BBN HARVARD BURROUGHS SDC STAN UCLA SRI UTAH UCSB SDC UCSB SRI UTAH UCSB NCAR GWC LINCOLN CASE MITRE ETAC RAND TINKER BBN HARVARD NBS SDC AMES USC STAN UCLA CARN SRI UTAH MCCLELLAN UCSB ILLINOIS LINC RADC MIT ILLINOIS MIT LINC RADC UTAH TINKER RAND SRI LBL MCCLELLAN AMES TIP AMES IMP X-PARC FNWC UCSB UCSD STANFORD CCA BBN HARVARD ABERDEEN NBS ETAC ARPA MITRE SAAC BELVOIR CMU UCLA SDC USC NOAA GWC CASE (a) (d) (b) (c) (e) Figure  -
Growth of the ARPANET
(a) December
(e) September
In addition to helping the fledgling ARPANET grow  ARPA also funded research on the use of satellite networks and mobile packet radio networks
In one now famous demonstration  a truck driving around in California used the packet radio network to send messages to SRI  which were then forwarded over the ARPANET to the East Coast  where they were shipped to University College in London over the satellite network
This allowed a researcher in the truck to use a computer in London while driving around in California
This experiment also demonstrated that the existing ARPANET protocols were not suitable for running over different networks
This observation led to more research on protocols  culminating with the invention of the TCP/IP model and protocols (Cerf and Kahn  )
TCP/IP was specifically designed to handle communication over internetworks  something becoming increasingly important as more and more networks were hooked up to the ARPANET
EXAMPLE NETWORKS   To encourage adoption of these new protocols  ARPA awarded several contracts to implement TCP/IP on different computer platforms  including IBM  DEC  and HP systems  as well as for Berkeley UNIX
Researchers at the University of California at Berkeley rewrote TCP/IP with a new programming interface called sockets for the upcoming
BSD release of Berkeley UNIX
They also wrote many application  utility  and management programs to show how convenient it was to use the network with sockets
The timing was perfect
Many universities had just acquired a ond or third VAX computer and a LAN to connect them  but they had no networking software
BSD came along  with TCP/IP  sockets  and many network utilities  the complete package was adopted immediately
Furthermore  with TCP/IP  it was easy for the LANs to connect to the ARPANET  and many did
During the s  additional networks  especially LANs  were connected to the ARPANET
As the scale increased  finding hosts became increasingly expensive  so DNS (Domain Name System) was created to organize machines into domains and map host names onto IP addresses
Since then  DNS has become a generalized  distributed database system for storing a variety of information related to naming
We will study it in detail in   NSFNET By the late s  NSF (the
National Science Foundation) saw the enormous impact the ARPANET was having on university research  allowing scientists across the country to share data and collaborate on research projects
However  to get on the ARPANET a university had to have a research contract with the DoD
Many did not have a contract
NSFâs initial response was to fund the Computer Science Network (CSNET) in
It connected computer science departments and industrial research labs to the ARPANET via dial-up and leased lines
In the late s  the NSF went further and decided to design a successor to the ARPANET that would be open to all university research groups
To have something concrete to start with  NSF decided to build a backbone network to connect its six supercomputer centers  in San Diego  Boulder  Champaign  Pittsburgh  Ithaca  and Princeton
Each supercomputer was given a little brother  consisting of an LSI-  microcomputer called a fuzzball
The fuzzballs were connected with  -kbps leased lines and formed the subnet  the same hardware technology the ARPANET used
The software technology was different however: the fuzzballs spoke TCP/IP right from the start  making it the first TCP/IP WAN
NSF also funded some (eventually about  ) regional networks that connected to the backbone to allow users at thousands of universities  research labs  libraries  and museums to access any of the supercomputers and to communicate with one another
The complete network  including backbone and the regional networks  was called NSFNET
It connected to the ARPANET through a link between an   INTRODUCTION
IMP and a fuzzball in the Carnegie-Mellon machine room
The first NSFNET backbone is illustrated in Fig
NSF Supercomputer center NSF Midlevel network Both Figure  -
The NSFNET backbone in
NSFNET was an instantaneous success and was overloaded from the word go
NSF immediately began planning its successor and awarded a contract to the Michigan-based MERIT consortium to run it
Fiber optic channels at kbps were leased from MCI (since merged with WorldCom) to provide the version  backbone
IBM PC-RTs were used as routers
This  too  was soon overwhelmed  and by  the ond backbone was upgraded to
As growth continued  NSF realized that the government could not continue financing networking forever
Furthermore  commercial organizations wanted to join but were forbidden by NSFâs charter from using networks NSF paid for
Consequently  NSF encouraged MERIT  MCI  and IBM to form a nonprofit corporation  ANS (Advanced Networks and Services)  as the first step along the road to commercialization
In  ANS took over NSFNET and upgraded the
This network operated for  years and was then sold to America Online
But by then  various companies were offering commercial IP service and it was clear the government should now get out of the networking business
To ease the transition and make sure every regional network could communicate with every other regional network  NSF awarded contracts to four different network operators to establish a NAP (Network Access Point)
These operators were PacBell (San Francisco)  Ameritech (Chicago)  MFS (Washington
)  and Sprint (New York City  where for NAP purposes  Pennsauken  New Jersey counts as New York City)
Every network operator that wanted to provide backbone service to the NSF regional networks had to connect to all the NAPs
EXAMPLE NETWORKS   This arrangement meant that a packet originating on any regional network had a choice of backbone carriers to get from its NAP to the destinationâs NAP
Consequently  the backbone carriers were forced to compete for the regional networksâ business on the basis of service and price  which was the idea  of course
As a result  the concept of a single default backbone was replaced by a commercially driven competitive infrastructure
Many people like to criticize the Federal Government for not being innovative  but in the area of networking  it was DoD and NSF that created the infrastructure that formed the basis for the Internet and then handed it over to industry to operate
During the s  many other countries and regions also built national research networks  often patterned on the ARPANET and NSFNET
These included EuropaNET and EBONE in Europe  which started out with  -Mbps lines and then upgraded to  -Mbps lines
Eventually  the network infrastructure in Europe was handed over to industry as well
The Internet has changed a great deal since those early days
It exploded in size with the emergence of the World Wide Web (WWW) in the early s
Recent data from the Internet Systems Consortium puts the number of visible Internet hosts at over million
This guess is only a low-ball estimate  but it far exceeds the few million hosts that were around when the first conference on the WWW was held at CERN in
The way we use the Internet has also changed radically
Initially  applications such as email-for-academics  newsgroups  remote login  and file transfer dominated
Later it switched to email-for-everyman  then the Web and peer-to-peer content distribution  such as the now-shuttered Napster
Now real-time media distribution  social networks (
Facebook)  and microblogging (
Twitter) are taking off
These switches brought richer kinds of media to the Internet and hence much more traffic
In fact  the dominant traffic on the Internet seems to change with some regularity as  for example  new and better ways to work with music or movies can become very popular very quickly
Architecture of the Internet The architecture of the Internet has also changed a great deal as it has grown explosively
In this tion  we will attempt to give a brief overview of what it looks like today
The picture is complicated by continuous upheavals in the businesses of telephone companies (telcos)  cable companies and ISPs that often make it hard to tell who is doing what
One driver of these upheavals is telecommunications convergence  in which one network is used for previously different uses
For example  in a ââtriple playââ one company sells you telephony  TV  and Internet service over the same network connection on the assumption that this will save you money
Consequently  the description given here will be of necessity somewhat simpler than reality
And what is true today may not be true tomorrow
INTRODUCTION
The big picture is shown in Fig
Let us examine this figure piece by piece  starting with a computer at home (at the edges of the figure)
To join the Internet  the computer is connected to an Internet Service Provider  or simply ISP  from who the user purchases Internet access or connectivity
This lets the computer exchange packets with all of the other accessible hosts on the Internet
The user might send packets to surf the Web or for any of a thousand other uses  it does not matter
There are many kinds of Internet access  and they are usually distinguished by how much bandwidth they provide and how much they cost  but the most important attribute is connectivity
Data center Fiber (FTTH) DSL Dialup Cable  G mobile phone Tier  ISP Other ISPs Peering at IXP POP Data path Router Cable modem CMTS Backbone DSLAM DSL modem Figure  -
Overview of the Internet architecture
A common way to connect to an ISP is to use the phone line to your house  in which case your phone company is your ISP
DSL  short for Digital Subscriber Line  reuses the telephone line that connects to your house for digital data transmission
The computer is connected to a device called a DSL modem that converts between digital packets and analog signals that can pass unhindered over the telephone line
At the other end  a device called a DSLAM (Digital Subscriber Line Access Multiplexer) converts between signals and packets
Several other popular ways to connect to an ISP are shown in Fig
DSL is a higher-bandwidth way to use the local telephone line than to send bits over a traditional telephone call instead of a voice conversation
That is called dial-up and done with a different kind of modem at both ends
The word modem is short for ââmodulator demodulatorââ and refers to any device that converts between digital bits and analog signals
Another method is to send signals over the cable TV system
Like DSL  this is a way to reuse existing infrastructure  in this case otherwise unused cable TV   EXAMPLE NETWORKS   channels
The device at the home end is called a cable modem and the device at the cable headend is called the CMTS (Cable Modem Termination System)
DSL and cable provide Internet access at rates from a small fraction of a megabit/ to multiple megabit/  depending on the system
These rates are much greater than dial-up rates  which are limited to   kbps because of the narrow bandwidth used for voice calls
Internet access at much greater than dial-up speeds is called broadband
The name refers to the broader bandwidth that is used for faster networks  rather than any particular speed
The access methods mentioned so far are limited by the bandwidth of the ââlast mileââ or last leg of transmission
By running optical fiber to residences  faster Internet access can be provided at rates on the order of   to Mbps
This design is called FTTH (Fiber to the Home)
For businesses in commercial areas  it may make sense to lease a high-speed transmission line from the offices to the nearest ISP
For example  in North America  a T  line runs at roughly   Mbps
Wireless is used for Internet access too
An example we will explore shortly is that of  G mobile phone networks
They can provide data delivery at rates of  Mbps or higher to mobile phones and fixed subscribers in the coverage area
We can now move packets between the home and the ISP
We call the location at which customer packets enter the ISP network for service the ISPâs POP (Point of Presence)
We will next explain how packets are moved between the POPs of different ISPs
From this point on  the system is fully digital and packet switched
ISP networks may be regional  national  or international in scope
We have already seen that their architecture is made up of long-distance transmission lines that interconnect routers at POPs in the different cities that the ISPs serve
This equipment is called the backbone of the ISP
If a packet is destined for a host served directly by the ISP  that packet is routed over the backbone and delivered to the host
Otherwise  it must be handed over to another ISP
ISPs connect their networks to exchange traffic at IXPs (Internet eXchange Points)
The connected ISPs are said to peer with each other
There are many IXPs in cities around the world
They are drawn vertically in Fig
Basically  an IXP is a room full of routers  at least one per ISP
A LAN in the room connects all the routers  so packets can be forwarded from any ISP backbone to any other ISP backbone
IXPs can be large and independently owned facilities
One of the largest is the Amsterdam Internet Exchange  to which hundreds of ISPs connect and through which they exchange hundreds of gigabits/ of traffic
The peering that happens at IXPs depends on the business relationships between ISPs
There are many possible relationships
For example  a small ISP might pay a larger ISP for Internet connectivity to reach distant hosts  much as a customer purchases service from an Internet provider
In this case  the small ISP is said to pay for transit
Alternatively  two large ISPs might decide to exchange   INTRODUCTION
traffic so that each ISP can deliver some traffic to the other ISP without having to pay for transit
One of the many paradoxes of the Internet is that ISPs who publicly compete with one another for customers often privately cooperate to do peering (Metz  )
The path a packet takes through the Internet depends on the peering choices of the ISPs
If the ISP delivering a packet peers with the destination ISP  it might deliver the packet directly to its peer
Otherwise  it might route the packet to the nearest place at which it connects to a paid transit provider so that provider can deliver the packet
Two example paths across ISPs are drawn in Fig
Often  the path a packet takes will not be the shortest path through the Internet
At the top of the food chain are a small handful of companies  like AT&T and Sprint  that operate large international backbone networks with thousands of routers connected by high-bandwidth fiber optic links
These ISPs do not pay for transit
They are usually called tier  ISPs and are said to form the backbone of the Internet  since everyone else must connect to them to be able to reach the entire Internet
Companies that provide lots of content  such as Google and Yahoo!  locate their computers in data centers that are well connected to the rest of the Internet
These data centers are designed for computers  not humans  and may be filled with rack upon rack of machines called a server farm
Colocation or hosting data centers let customers put equipment such as servers at ISP POPs so that short  fast connections can be made between the servers and the ISP backbones
The Internet hosting industry has become increasingly virtualized so that it is now common to rent a virtual machine that is run on a server farm instead of installing a physical computer
These data centers are so large (tens or hundreds of thousands of machines) that electricity is a major cost  so data centers are sometimes built in areas where electricity is cheap
This ends our quick tour of the Internet
We will have a great deal to say about the individual components and their design  algorithms  and protocols in subsequent  ters
One further point worth mentioning here is that what it means to be on the Internet is changing
It used to be that a machine was on the Internet if it: ( ) ran the TCP/IP protocol stack; ( ) had an IP address; and ( ) could send IP packets to all the other machines on the Internet
However  ISPs often reuse IP addresses depending on which computers are in use at the moment  and home networks often share one IP address between multiple computers
This practice undermines the ond condition
urity measures such as firewalls can also partly block computers from receiving packets  undermining the third condition
Despite these difficulties  it makes sense to regard such machines as being on the Internet while they are connected to their ISPs
Also worth mentioning in passing is that some companies have interconnected all their existing internal networks  often using the same technology as the Internet
These intranets are typically accessible only on company premises or from company notebooks but otherwise work the same way as the Internet
EXAMPLE NETWORKS   Third-Generation Mobile Phone Networks People love to talk on the phone even more than they like to surf the Internet  and this has made the mobile phone network the most successful network in the world
It has more than four billion subscribers worldwide
To put this number in perspective  it is roughly  % of the worldâs population and more than the number of Internet hosts and fixed telephone lines combined (ITU  )
The architecture of the mobile phone network has changed greatly over the past   years along with its tremendous growth
First-generation mobile phone systems transmitted voice calls as continuously varying (analog) signals rather than sequences of (digital) bits
AMPS (Advanced Mobile Phone System)  which was deployed in the United States in  was a widely used firstgeneration system
ond-generation mobile phone systems switched to transmitting voice calls in digital form to increase capacity  improve urity  and offer text messaging
GSM (Global System for Mobile communications)  which was deployed starting in  and has become the most widely used mobile phone system in the world  is a  G system
The third generation  or  G  systems were initially deployed in  and offer both digital voice and broadband digital data services
They also come with a lot of jargon and many different standards to choose from
G is loosely defined by the ITU (an international standards body we will discuss in the next tion) as providing rates of at least  Mbps for stationary or walking users and kbps in a moving vehicle
UMTS (Universal Mobile Telecommunications System)  also called WCDMA (Wideband Code Division Multiple Access)  is the main  G system that is being rapidly deployed worldwide
It can provide up to   Mbps on the downlink and almost  Mbps on the uplink
Future releases will use multiple antennas and radios to provide even greater speeds for users
The scarce resource in  G systems  as in  G and  G systems before them  is radio spectrum
Governments license the right to use parts of the spectrum to the mobile phone network operators  often using a spectrum auction in which network operators submit bids
Having a piece of licensed spectrum makes it easier to design and operate systems  since no one else is allowed transmit on that spectrum  but it often costs a serious amount of money
In the UK in  for example  five  G licenses were auctioned for a total of about $  billion
It is the scarcity of spectrum that led to the cellular network design shown in Fig
To manage the radio interference between users  the coverage area is divided into cells
Within a cell  users are assigned channels that do not interfere with each other and do not cause too much interference for adjacent cells
This allows for good reuse of the spectrum  or frequency reuse  in the neighboring cells  which increases the capacity of the network
In  G systems  which carried each voice call on a specific frequency band  the frequencies were carefully chosen so that they did not conflict with neighboring cells
In this way  a given frequency might only be reused once   INTRODUCTION
in several cells
Modern  G systems allow each cell to use all frequencies  but in a way that results in a tolerable level of interference to the neighboring cells
There are variations on the cellular design  including the use of directional or tored antennas on cell towers to further reduce interference  but the basic idea is the same
Cells Base station Figure  -
Cellular design of mobile phone networks
The architecture of the mobile phone network is very different than that of the Internet
It has several parts  as shown in the simplified version of the UMTS architecture in Fig
First  there is the air interface
This term is a fancy name for the radio communication protocol that is used over the air between the mobile device (
the cell phone) and the cellular base station
Advances in the air interface over the past decades have greatly increased wireless data rates
The UMTS air interface is based on Code Division Multiple Access (CDMA)  a technique that we will study in   The cellular base station together with its controller forms the radio access network
This part is the wireless side of the mobile phone network
The controller node or RNC (Radio Network Controller) controls how the spectrum is used
The base station implements the air interface
It is called Node B  a temporary label that stuck
The rest of the mobile phone network carries the traffic for the radio access network
It is called the core network
The UMTS core network evolved from the core network used for the  G GSM system that came before it
However  something surprising is happening in the UMTS core network
Since the beginning of networking  a war has been going on between the people who support packet networks (
connectionless subnets) and the people who support circuit networks (
connection-oriented subnets)
The main proponents of packets come from the Internet community
In a connectionless design  every packet is routed independently of every other packet
As a consequence  if some routers go down during a session  no harm will be done as long as the system can   EXAMPLE NETWORKS   RNC RNC MSC / MGW GMSC / MGW SGSN GGSN Radio access network Core network Air interface (âUuâ) Node B PSTN Internet Packets Circuits (âIu-CSâ) Access / Core interface (âIuâ) Packets (âIu-PSâ) HSS Figure  -
Architecture of the UMTS  G mobile phone network
dynamically reconfigure itself so that subsequent packets can find some route to the destination  even if it is different from that which previous packets used
The circuit camp comes from the world of telephone companies
In the telephone system  a caller must dial the called partyâs number and wait for a connection before talking or sending data
This connection setup establishes a route through the telephone system that is maintained until the call is terminated
All words or packets follow the same route
If a line or switch on the path goes down  the call is aborted  making it less fault tolerant than a connectionless design
The advantage of circuits is that they can support quality of service more easily
By setting up a connection in advance  the subnet can reserve resources such as link bandwidth  switch buffer space  and CPU
If an attempt is made to set up a call and insufficient resources are available  the call is rejected and the caller gets a kind of busy signal
In this way  once a connection has been set up  the connection will get good service
With a connectionless network  if too many packets arrive at the same router at the same moment  the router will choke and probably lose packets
The sender will eventually notice this and resend them  but the quality of service will be jerky and unsuitable for audio or video unless the network is lightly loaded
Needless to say  providing adequate audio quality is something telephone companies care about very much  hence their preference for connections
The surprise in Fig
This shows the mobile phone network in transition  with mobile phone companies able to implement one or sometimes both of   INTRODUCTION
the alternatives
Older mobile phone networks used a circuit-switched core in the style of the traditional phone network to carry voice calls
This legacy is seen in the UMTS network with the MSC (Mobile Switching Center)  GMSC (Gateway Mobile Switching Center)  and MGW (Media Gateway) elements that set up connections over a circuit-switched core network such as the PSTN (Public Switched Telephone Network)
Data services have become a much more important part of the mobile phone network than they used to be  starting with text messaging and early packet data services such as GPRS (General Packet Radio Service) in the GSM system
These older data services ran at tens of kbps  but users wanted more
Newer mobile phone networks carry packet data at rates of multiple Mbps
For comparison  a voice call is carried at a rate of   kbps  typically  â x less with compression
To carry all this data  the UMTS core network nodes connect directly to a packet-switched network
The SGSN (Serving GPRS Support Node) and the GGSN (Gateway GPRS Support Node) deliver data packets to and from mobiles and interface to external packet networks such as the Internet
This transition is set to continue in the mobile phone networks that are now being planned and deployed
Internet protocols are even used on mobiles to set up connections for voice calls over a packet data network  in the manner of voiceover- IP
IP and packets are used all the way from the radio access through to the core network
Of course  the way that IP networks are designed is also changing to support better quality of service
If it did not  then problems with chopped-up audio and jerky video would not impress paying customers
We will return to this subject in   Another difference between mobile phone networks and the traditional Internet is mobility
When a user moves out of the range of one cellular base station and into the range of another one  the flow of data must be re-routed from the old to the new cell base station
This technique is known as handover or handoff  and it is illustrated in Fig
(a) (b) Figure  -
Mobile phone handover (a) before  (b) after
Either the mobile device or the base station may request a handover when the quality of the signal drops
In some cell networks  usually those based on CDMA   EXAMPLE NETWORKS   technology  it is possible to connect to the new base station before disconnecting from the old base station
This improves the connection quality for the mobile because there is no break in service; the mobile is actually connected to two base stations for a short while
This way of doing a handover is called a soft handover to distinguish it from a hard handover  in which the mobile disconnects from the old base station before connecting to the new one
A related issue is how to find a mobile in the first place when there is an incoming call
Each mobile phone network has a HSS (Home Subscriber Server) in the core network that knows the location of each subscriber  as well as other profile information that is used for authentication and authorization
In this way  each mobile can be found by contacting the HSS
A final area to discuss is urity
Historically  phone companies have taken urity much more seriously than Internet companies for a long time because of the need to bill for service and avoid (payment) fraud
Unfortunately that is not saying much
Nevertheless  in the evolution from  G through  G technologies  mobile phone companies have been able to roll out some basic urity mechanisms for mobiles
Starting with the  G GSM system  the mobile phone was divided into a handset and a removable chip containing the subscriberâs identity and account information
The chip is informally called a SIM card  short for Subscriber Identity Module
SIM cards can be switched to different handsets to activate them  and they provide a basis for urity
When GSM customers travel to other countries on vacation or business  they often bring their handsets but buy a new SIM card for few dollars upon arrival in order to make local calls with no roaming charges
To reduce fraud  information on SIM cards is also used by the mobile phone network to authenticate subscribers and check that they are allowed to use the network
With UMTS  the mobile also uses the information on the SIM card to check that it is talking to a legitimate network
Another aspect of urity is privacy
Wireless signals are broadcast to all nearby receivers  so to make it difficult to eavesdrop on conversations  cryptographic keys on the SIM card are used to encrypt transmissions
This approach provides much better privacy than in  G systems  which were easily tapped  but is not a panacea due to weaknesses in the encryption schemes
Mobile phone networks are destined to play a central role in future networks
They are now more about mobile broadband applications than voice calls  and this has major implications for the air interfaces  core network architecture  and urity of future networks
G technologies that are faster and better are on the drawing board under the name of LTE (Long Term Evolution)  even as  G design and deployment continues
Other wireless technologies also offer broadband Internet access to fixed and mobile clients  notably
networks under the common name of WiMAX
It is entirely possible that LTE and WiMAX are on a collision course with each other and it is hard to predict what will happen to them
INTRODUCTION
Wireless LANs:
Almost as soon as laptop computers appeared  many people had a dream of walking into an office and magically having their laptop computer be connected to the Internet
Consequently  various groups began working on ways to accomplish this goal
The most practical approach is to equip both the office and the laptop computers with short-range radio transmitters and receivers to allow them to talk
Work in this field rapidly led to wireless LANs being marketed by a variety of companies
The trouble was that no two of them were compatible
The proliferation of standards meant that a computer equipped with a brand X radio would not work in a room equipped with a brand Y base station
In the mid s  the industry decided that a wireless LAN standard might be a good idea  so the IEEE committee that had standardized wired LANs was given the task of drawing up a wireless LAN standard
The first decision was the easiest: what to call it
All the other LAN standards had numbers like     and
so the wireless LAN standard was dubbed    A common slang name for it is WiFi but it is an important standard and deserves respect  so we will call it by its proper name  The rest was harder
The first problem was to find a suitable frequency band that was available  preferably worldwide
The approach taken was the opposite of that used in mobile phone networks
Instead of expensive  licensed spectrum
systems operate in unlicensed bands such as the ISM (Industrial  Scientific  and Medical) bands defined by ITU-R (
All devices are allowed to use this spectrum provided that they limit their transmit power to let different devices coexist
Of course  this means that
radios may find themselves competing with cordless phones  garage door openers  and microwave ovens
networks are made up of clients  such as laptops and mobile phones  and infrastructure called APs (access points) that is installed in buildings
Access points are sometimes called base stations
The access points connect to the wired network  and all communication between clients goes through an access point
It is also possible for clients that are in radio range to talk directly  such as two computers in an office without an access point
This arrangement is called an ad hoc network
It is used much less often than the access point mode
Both modes are shown in Fig
transmission is complicated by wireless conditions that vary with even small changes in the environment
At the frequencies used for
radio signals can be reflected off solid objects so that multiple echoes of a transmission may reach a receiver along different paths
The echoes can cancel or reinforce each other  causing the received signal to fluctuate greatly
This phenomenon is called multipath fading  and it is shown in Fig
The key idea for overcoming variable wireless conditions is path diversity  or the sending of information along multiple  independent paths
In this way  the   EXAMPLE NETWORKS   (a) (b) Access To wired network point Figure  -
(a) Wireless network with an access point
(b) Ad hoc network
information is likely to be received even if one of the paths happens to be poor due to a fade
These independent paths are typically built into the digital modulation scheme at the physical layer
Options include using different frequencies across the allowed band  following different spatial paths between different pairs of antennas  or repeating bits over different periods of time
Reflector Faded signal Wireless transmitter Non-faded signal Multiple paths Wireless receiver Figure  -
Multipath fading
Different versions of
have used all of these techniques
The initial (   ) standard defined a wireless LAN that ran at either  Mbps or  Mbps by hopping between frequencies or spreading the signal across the allowed spectrum
Almost immediately  people complained that it was too slow  so work began on faster standards
The spread spectrum design was extended and became the (   )
b standard running at rates up to   Mbps
a (   ) and
g (   ) standards switched to a different modulation scheme called OFDM (Orthogonal Frequency Division Multiplexing)
It divides a wide band of spectrum into many narrow slices over which different bits are sent in parallel
This improved scheme  which we will study in
boosted the
a/g bit   INTRODUCTION
rates up to   Mbps
That is a significant increase  but people still wanted more throughput to support more demanding uses
The latest version is
It uses wider frequency bands and up to four antennas per computer to achieve rates up to Mbps
Since wireless is inherently a broadcast medium
radios also have to deal with the problem that multiple transmissions that are sent at the same time will collide  which may interfere with reception
To handle this problem
uses a CSMA (Carrier Sense Multiple Access) scheme that draws on ideas from classic wired Ethernet  which  ironically  drew from an early wireless network developed in Hawaii and called ALOHA
Computers wait for a short random interval before transmitting  and defer their transmissions if they hear that someone else is already transmitting
This scheme makes it less likely that two computers will send at the same time
It does not work as well as in the case of wired networks  though
To see why  examine Fig
Suppose that computer A is transmitting to computer B  but the radio range of Aâs transmitter is too short to reach computer C
If C wants to transmit to B it can listen before starting  but the fact that it does not hear anything does not mean that its transmission will succeed
The inability of C to hear A before starting causes some collisions to occur
After any collision  the sender then waits another  longer  random delay and retransmits the packet
Despite this and some other issues  the scheme works well enough in practice
A B C Range of A's radio Range of C's radio Figure  -
The range of a single radio may not cover the entire system
Another problem is that of mobility
If a mobile client is moved away from the access point it is using and into the range of a different access point  some way of handing it off is needed
The solution is that an
network can consist of multiple cells  each with its own access point  and a distribution system that connects the cells
The distribution system is often switched Ethernet  but it can use any technology
As the clients move  they may find another access point with a better signal than the one they are currently using and change their association
From the outside  the entire system looks like a single wired LAN
EXAMPLE NETWORKS   That said  mobility in
has been of limited value so far compared to mobility in the mobile phone network
is used by nomadic clients that go from one fixed location to another  rather than being used on-the-go
Mobility is not really needed for nomadic usage
mobility is used  it extends over a single
network  which might cover at most a large building
Future schemes will need to provide mobility across different networks and across different technologies (   )
Finally  there is the problem of urity
Since wireless transmissions are broadcast  it is easy for nearby computers to receive packets of information that were not intended for them
To prevent this  the
standard included an encryption scheme known as WEP (Wired Equivalent Privacy)
The idea was to make wireless urity like that of wired urity
It is a good idea  but unfortunately the scheme was flawed and soon broken (Borisov et al
It has since been replaced with newer schemes that have different cryptographic details in the
i standard  also called WiFi Protected Access  initially called WPA but now replaced by WPA
has caused a revolution in wireless networking that is set to continue
Beyond buildings  it is starting to be installed in trains  planes  boats  and automobiles so that people can surf the Internet wherever they go
Mobile phones and all manner of consumer electronics  from game consoles to digital cameras  can communicate with it
We will come back to it in detail in      RFID and Sensor Networks The networks we have studied so far are made up of computing devices that are easy to recognize  from computers to mobile phones
With Radio Frequency IDentification (RFID)  everyday objects can also be part of a computer network
An RFID tag looks like a postage stamp-sized sticker that can be affixed to (or embedded in) an object so that it can be tracked
The object might be a cow  a passport  a book or a shipping pallet
The tag consists of a small microchip with a unique identifier and an antenna that receives radio transmissions
RFID readers installed at tracking points find tags when they come into range and interrogate them for their information as shown in Fig
Applications include checking identities  managing the supply chain  timing races  and replacing barcodes
There are many kinds of RFID  each with different properties  but perhaps the most fascinating aspect of RFID technology is that most RFID tags have neither an electric plug nor a battery
Instead  all of the energy needed to operate them is supplied in the form of radio waves by RFID readers
This technology is called passive RFID to distinguish it from the (less common) active RFID in which there is a power source on the tag
One common form of RFID is UHF RFID (Ultra-High Frequency RFID)
It is used on shipping pallets and some drivers licenses
Readers send signals in   INTRODUCTION
RFID reader RFID tag Figure  -
RFID used to network everyday objects
the   -   MHz band in the United States
Tags communicate at distances of several meters by changing the way they reflect the reader signals; the reader is able to pick up these reflections
This way of operating is called backscatter
Another popular kind of RFID is HF RFID (High Frequency RFID)
It operates at
MHz and is likely to be in your passport  credit cards  books  and noncontact payment systems
HF RFID has a short range  typically a meter or less  because the physical mechanism is based on induction rather than backscatter
There are also other forms of RFID using other frequencies  such as LF RFID (Low Frequency RFID)  which was developed before HF RFID and used for animal tracking
It is the kind of RFID likely to be in your cat
RFID readers must somehow solve the problem of dealing with multiple tags within reading range
This means that a tag cannot simply respond when it hears a reader  or the signals from multiple tags may collide
The solution is similar to the approach taken in
: tags wait for a short random interval before responding with their identification  which allows the reader to narrow down individual tags and interrogate them further
urity is another problem
The ability of RFID readers to easily track an object  and hence the person who uses it  can be an invasion of privacy
Unfortunately  it is difficult to ure RFID tags because they lack the computation and communication power to run strong cryptographic algorithms
Instead  weak measures like passwords (which can easily be cracked) are used
If an identity card can be remotely read by an official at a border  what is to stop the same card from being tracked by other people without your knowledge? Not much
RFID tags started as identification chips  but are rapidly turning into fullfledged computers
For example  many tags have memory that can be updated and later queried  so that information about what has happened to the tagged object can be stored with it
Rieback et al
(   ) demonstrated that this means that all of the usual problems of computer malware apply  only now your cat or your passport might be used to spread an RFID virus
A step up in capability from RFID is the sensor network
Sensor networks are deployed to monitor aspects of the physical world
So far  they have mostly been used for scientific experimentation  such as monitoring bird habitats  volcanic activity  and zebra migration  but business applications including healthcare    EXAMPLE NETWORKS   monitoring equipment for vibration  and tracking of frozen  refrigerated  or otherwise perishable goods cannot be too far behind
Sensor nodes are small computers  often the size of a key fob  that have temperature  vibration  and other sensors
Many nodes are placed in the environment that is to be monitored
Typically  they have batteries  though they may scavenge energy from vibrations or the sun
As with RFID  having enough energy is a key challenge  and the nodes must communicate carefully to be able to deliver their sensor information to an external collection point
A common strategy is for the nodes to self-organize to relay messages for each other  as shown in Fig
This design is called a multihop network
Data collection point Sensor node Wireless hop Figure  -
Multihop topology of a sensor network
RFID and sensor networks are likely to become much more capable and pervasive in the future
Researchers have already combined the best of both technologies by prototyping programmable RFID tags with light  movement  and other sensors (Sample et al
)  NETWORK STANDARDIZATION Many network vendors and suppliers exist  each with its own ideas of how things should be done
Without coordination  there would be complete chaos  and users would get nothing done
The only way out is to agree on some network standards
Not only do good standards allow different computers to communicate  but they also increase the market for products adhering to the standards
A larger market leads to mass production  economies of scale in manufacturing  better implementations  and other benefits that decrease price and further increase acceptance
In this tion we will take a quick look at the important but little-known  world of international standardization
But let us first discuss what belongs in a   INTRODUCTION
A reasonable person might assume that a standard tells you how a protocol should work so that you can do a good job of implementing it
That person would be wrong
Standards define what is needed for interoperability: no more  no less
That lets the larger market emerge and also lets companies compete on the basis of how good their products are
For example  the
standard defines many transmission rates but does not say when a sender should use which rate  which is a key factor in good performance
That is up to whoever makes the product
Often getting to interoperability this way is difficult  since there are many implementation choices and standards usually define many options
there were so many problems that  in a strategy that has become common practice  a trade group called the WiFi Alliance was started to work on interoperability within the
Similarly  a protocol standard defines the protocol over the wire but not the service interface inside the box  except to help explain the protocol
Real service interfaces are often proprietary
For example  the way TCP interfaces to IP within a computer does not matter for talking to a remote host
It only matters that the remote host speaks TCP/IP
In fact  TCP and IP are commonly implemented together without any distinct interface
That said  good service interfaces  like good APIs  are valuable for getting protocols used  and the best ones (such as Berkeley sockets) can become very popular
Standards fall into two categories: de facto and de jure
De facto (Latin for ââfrom the factââ) standards are those that have just happened  without any formal plan
HTTP  the protocol on which the Web runs  started life as a de facto standard
It was part of early WWW browsers developed by Tim Berners-Lee at CERN  and its use took off with the growth of the Web
Bluetooth is another example
It was originally developed by Ericsson but now everyone is using it
De jure (Latin for ââby lawââ) standards  in contrast  are adopted through the rules of some formal standardization body
International standardization authorities are generally divided into two classes: those established by treaty among national governments  and those comprising voluntary  nontreaty organizations
In the area of computer network standards  there are several organizations of each type  notably ITU  ISO  IETF and IEEE  all of which we will discuss below
In practice  the relationships between standards  companies  and standardization bodies are complicated
De facto standards often evolve into de jure standards  especially if they are successful
This happened in the case of HTTP  which was quickly picked up by IETF
Standards bodies often ratify each othersâ standards  in what looks like patting one another on the back  to increase the market for a technology
These days  many ad hoc business alliances that are formed around particular technologies also play a significant role in developing and refining network standards
For example  GPP (Third Generation Partnership Project) is a collaboration between telecommunications associations that drives the UMTS  G mobile phone standards
NETWORK STANDARDIZATION   Whoâs Who in the Telecommunications World The legal status of the worldâs telephone companies varies considerably from country to country
At one extreme is the United States  which has over  separate  (mostly very small) privately owned telephone companies
A few more were added with the breakup of AT&T in  (which was then the worldâs largest corporation  providing telephone service to about   percent of Americaâs telephones)  and the Telecommunications Act of  that overhauled regulation to foster competition
At the other extreme are countries in which the national government has a complete monopoly on all communication  including the mail  telegraph  telephone  and often radio and television
Much of the world falls into this category
In some cases the telecommunication authority is a nationalized company  and in others it is simply a branch of the government  usually known as the PTT (Post  Telegraph & Telephone administration)
Worldwide  the trend is toward liberalization and competition and away from government monopoly
Most European countries have now (partially) privatized their PTTs  but elsewhere the process is still only slowly gaining steam
With all these different suppliers of services  there is clearly a need to provide compatibility on a worldwide scale to ensure that people (and computers) in one country can call their counterparts in another one
Actually  this need has existed for a long time
In  representatives from many European governments met to form the predecessor to todayâs ITU (International Telecommunication Union)
Its job was to standardize international telecommunications  which in those days meant telegraphy
Even then it was clear that if half the countries used Morse code and the other half used some other code  there was going to be a problem
When the telephone was put into international service  ITU took over the job of standardizing telephony (pronounced te-LEF-ony) as well
In  ITU became an agency of the United Nations
ITU has about governmental members  including almost every member of the United Nations
Since the United States does not have a PTT  somebody else had to represent it in ITU
This task fell to the State Department  probably on the grounds that ITU had to do with foreign countries  the State Departmentâs specialty
ITU also has more than tor and associate members
They include telephone companies (
AT&T  Vodafone  Sprint)  telecom equipment manufacturers (
Cisco  Nokia  Nortel)  computer vendors (
Microsoft  Agilent  Toshiba)  chip manufacturers (
Intel  Motorola  TI)  and other interested companies (
Boeing  CBS  VeriSign)
ITU has three main tors
We will focus primarily on ITU-T  the Telecommunications Standardization tor  which is concerned with telephone and data communication systems
Before  this tor was called CCITT  which is an acronym for its French name  ComiteÂ´ Consultatif International TeÂ´leÂ´graphique et TeÂ´leÂ´phonique
ITU-R  the Radiocommunications tor  is concerned with   INTRODUCTION
coordinating the use by competing interest groups of radio frequencies worldwide
The other tor is ITU-D  the Development tor
It promotes the development of information and communication technologies to narrow the ââdigital divideââ between countries with effective access to the information technologies and countries with limited access
ITU-Tâs task is to make technical recommendations about telephone  telegraph  and data communication interfaces
These often become internationally recognized standards  though technically the recommendations are only suggestions that governments can adopt or ignore  as they wish (because governments are like  -year-old boysâthey do not take kindly to being given orders)
In practice  a country that wishes to adopt a telephone standard different from that used by the rest of the world is free to do so  but at the price of cutting itself off from everyone else
This might work for North Korea  but elsewhere it would be a real problem
The real work of ITU-T is done in its Study Groups
There are currently   Study Groups  often as large as people  that cover topics ranging from telephone billing to multimedia services to urity
SG for example  standardizes the DSL technologies popularly used to connect to the Internet
In order to make it possible to get anything at all done  the Study Groups are divided into Working Parties  which are in turn divided into Expert Teams  which are in turn divided into ad hoc groups
Once a bureaucracy  always a bureaucracy
Despite all this  ITU-T actually does get things done
Since its inception  it has produced more than  recommendations  many of which are widely used in practice
For example  Recommendation H
(also an ISO standard known as MPEG-  AVC) is widely used for video compression  and X
public key certificates are used for ure Web browsing and digitally signed email
As the field of telecommunications completes the transition started in the s from being entirely national to being entirely global  standards will become increasingly important  and more and more organizations will want to become involved in setting them
For more information about ITU  see Irmer (   )
Whoâs Who in the International Standards World International standards are produced and published by ISO (International Standards Organizationâ )  a voluntary nontreaty organization founded in
Its members are the national standards organizations of the member countries
These members include ANSI (
)  BSI (Great Britain)  AFNOR (France)  DIN (Germany)  and others
ISO issues standards on a truly vast number of subjects  ranging from nuts and bolts (literally) to telephone pole coatings [not to mention cocoa beans (ISO )  fishing nets (ISO )  womenâs underwear (ISO ) and quite a few â  For the purist  ISOâs true name is the International Organization for Standardization
NETWORK STANDARDIZATION   other subjects one might not think were subject to standardization]
On issues of telecommunication standards  ISO and ITU-T often cooperate (ISO is a member of ITU-T) to avoid the irony of two official and mutually incompatible international standards
Over   standards have been issued  including the OSI standards
ISO has over Technical Committees (TCs)  numbered in the order of their creation  each dealing with a specific subject
TC  deals with the nuts and bolts (standardizing screw thread pitches)
JTC  deals with information technology  including networks  computers  and software
It is the first (and so far only) Joint Technical Committee  created in  by merging TC  with activities in IEC  yet another standardization body
Each TC has subcommittees (SCs) divided into working groups (WGs)
The real work is done largely in the WGs by over    volunteers worldwide
Many of these ââvolunteersââ are assigned to work on ISO matters by their employers  whose products are being standardized
Others are government officials keen on having their countryâs way of doing things become the international standard
Academic experts also are active in many of the WGs
The procedure used by ISO for adopting standards has been designed to achieve as broad a consensus as possible
The process begins when one of the national standards organizations feels the need for an international standard in some area
A working group is then formed to come up with a CD (Committee Draft)
The CD is then circulated to all the member bodies  which get  months to criticize it
If a substantial majority approves  a revised document  called a DIS (Draft International Standard) is produced and circulated for comments and voting
Based on the results of this round  the final text of the IS (International Standard) is prepared  approved  and published
In areas of great controversy  a CD or DIS may have to go through several versions before acquiring enough votes  and the whole process can take years
NIST (National Institute of Standards and Technology) is part of the
Department of Commerce
It used to be called the National Bureau of Standards
It issues standards that are mandatory for purchases made by the
Government  except for those of the Department of Defense  which defines its own standards
Another major player in the standards world is IEEE (Institute of Electrical and Electronics Engineers)  the largest professional organization in the world
In addition to publishing scores of journals and running hundreds of conferences each year  IEEE has a standardization group that develops standards in the area of electrical engineering and computing
IEEEâs committee has standardized many kinds of LANs
We will study some of its output later in this book
The actual work is done by a collection of working groups  which are listed in Fig
The success rate of the various working groups has been low; having an    number is no guarantee of success
Still  the impact of the success stories (especially
) on the industry and the world has been enormous
INTRODUCTION
Number Topic
Overview and architecture of LANs
â Logical link control
* Ethernet
â Token bus (was briefly used in manufacturing plants)
Token ring (IBMâs entry into the LAN world)
â Dual queue dual bus (early metropolitan area network)
â Technical advisory group on broadband technologies
â  Technical advisory group on fiber optic technologies
â Isochronous LANs (for real-time applications)
â Virtual LANs and urity
* Wireless LANs (WiFi)
â Demand priority (Hewlett-Packardâs AnyLAN)
Unlucky number; nobody wanted it
â Cable modems (defunct: an industry consortium got there first)
* Personal area networks (Bluetooth  Zigbee)
* Broadband wireless (WiMAX)
Resilient packet ring
Technical advisory group on radio regulatory issues
Technical advisory group on coexistence of all these standards
Mobile broadband wireless (similar to
Media independent handoff (for roaming over technologies)
Wireless regional area network Figure  -
The working groups
The important ones are marked with *
The ones marked with â are hibernating
The one marked with â  gave up and disbanded itself
Whoâs Who in the Internet Standards World The worldwide Internet has its own standardization mechanisms  very different from those of ITU-T and ISO
The difference can be crudely summed up by saying that the people who come to ITU or ISO standardization meetings wear suits  while the people who come to Internet standardization meetings wear jeans (except when they meet in San Diego  when they wear shorts and T-shirts)
ITU-T and ISO meetings are populated by corporate officials and government civil servants for whom standardization is their job
They regard standardization as a Good Thing and devote their lives to it
Internet people  on the other hand  prefer anarchy as a matter of principle
However  with hundreds of millions of   NETWORK STANDARDIZATION   people all doing their own thing  little communication can occur
Thus  standards  however regrettable  are sometimes needed
In this context  David Clark of
once made a now-famous remark about Internet standardization consisting of âârough consensus and running code
ââ When the ARPANET was set up  DoD created an informal committee to oversee it
In  the committee was renamed the IAB (Internet Activities Board) and was given a slighter broader mission  namely  to keep the researchers involved with the ARPANET and the Internet pointed more or less in the same direction  an activity not unlike herding cats
The meaning of the acronym ââIABââ was later changed to Internet Architecture Board
Each of the approximately ten members of the IAB headed a task force on some issue of importance
The IAB met several times a year to discuss results and to give feedback to the DoD and NSF  which were providing most of the funding at this time
When a standard was needed (
a new routing algorithm)  the IAB members would thrash it out and then announce the change so the graduate students who were the heart of the software effort could implement it
Communication was done by a series of technical reports called RFCs (Request For Comments)
RFCs are stored online and can be fetched by anyone interested in them from  /rfc
They are numbered in chronological order of creation
Over  now exist
We will refer to many RFCs in this book
By  the Internet had grown so large that this highly informal style no longer worked
Many vendors by then offered TCP/IP products and did not want to change them just because ten researchers had thought of a better idea
In the summer of  the IAB was reorganized again
The researchers were moved to the IRTF (Internet Research Task Force)  which was made subsidiary to IAB  along with the IETF (Internet Engineering Task Force)
The IAB was repopulated with people representing a broader range of organizations than just the research community
It was initially a self-perpetuating group  with members serving for a  -year term and new members being appointed by the old ones
Later  the Internet Society was created  populated by people interested in the Internet
The Internet Society is thus in a sense comparable to ACM or IEEE
It is governed by elected trustees who appoint the IABâs members
The idea of this split was to have the IRTF concentrate on long-term research while the IETF dealt with short-term engineering issues
The IETF was divided up into working groups  each with a specific problem to solve
The chairmen of these working groups initially met as a steering committee to direct the engineering effort
The working group topics include new applications  user information  OSI integration  routing and addressing  urity  network management  and standards
Eventually  so many working groups were formed (more than  ) that they were grouped into areas and the area chairmen met as the steering committee
In addition  a more formal standardization process was adopted  patterned after ISOs
To become a Proposed Standard  the basic idea must be explained in an RFC and have sufficient interest in the community to warrant consideration
INTRODUCTION
To advance to the Draft Standard stage  a working implementation must have been rigorously tested by at least two independent sites for at least  months
If the IAB is convinced that the idea is sound and the software works  it can declare the RFC to be an Internet Standard
Some Internet Standards have become DoD standards (MIL-STD)  making them mandatory for DoD suppliers
For Web standards  the World Wide Web Consortium (W C) develops protocols and guidelines to facilitate the long-term growth of the Web
It is an industry consortium led by Tim Berners-Lee and set up in  as the Web really begun to take off
W C now has more than members from around the world and has produced more than W C Recommendations  as its standards are called  covering topics such as HTML and Web privacy  METRIC UNITS To avoid any confusion  it is worth stating explicitly that in this book  as in computer science in general  metric units are used instead of traditional English units (the furlong-stone-fortnight system)
The principal metric prefixes are listed in Fig
The prefixes are typically abbreviated by their first letters  with the units greater than  capitalized (KB  MB  etc
One exception (for historical reasons) is kbps for kilobits/
Thus  a  -Mbps communication line transmits bits/ and a   -p (or   -ps) clock ticks every  â  onds
Since milli and micro both begin with the letter ââm ââ a choice had to be made
Normally  ââmââ is used for milli and ââÎ¼ââ (the Greek letter mu) is used for micro
Explicit Prefix Exp
Explicit Prefix  â
milli  Kilo  â
micro  Mega  â
nano     Giga  â
pico      Tera  â
femto      Peta  â
atto      Exa  â
zepto      Zetta  â
yocto      Yotta Figure  -
The principal metric prefixes
It is also worth pointing out that for measuring memory  disk  file  and database sizes  in common industry practice  the units have slightly different meanings
There  kilo means (   ) rather than (   ) because memories are always a power of two
Thus  a  -KB memory contains  bytes  not  bytes
Note also the capital ââBââ in that usage to mean ââbytesââ (units of eight   METRIC UNITS   bits)  instead of a lowercase ââbââ that means ââbits
ââ Similarly  a  -MB memory contains (    ) bytes  a  -GB memory contains (    ) bytes  and a  -TB database contains (    ) bytes
However  a  -kbps communication line transmits  bits per ond and a  -Mbps LAN runs at   bits/ because these speeds are not powers of two
Unfortunately  many people tend to mix up these two systems  especially for disk sizes
To avoid ambiguity  in this book  we will use the symbols KB  MB  GB  and TB for   and bytes  respectively  and the symbols kbps  Mbps  Gbps  and Tbps for   and  bits/  respectively  OUTLINE OF THE REST OF THE BOOK This book discusses both the principles and practice of computer networking
Most  ters start with a discussion of the relevant principles  followed by a number of examples that illustrate these principles
These examples are usually taken from the Internet and wireless networks such as the mobile phone network since these are both important and very different
Other examples will be given where relevant
The book is structured according to the hybrid model of Fig
Starting with
we begin working our way up the protocol hierarchy beginning at the bottom
We provide some background in the field of data communication that covers both wired and wireless transmission systems
This material is concerned with how to deliver information over physical channels  although we cover only the architectural rather than the hardware aspects
Several examples of the physical layer  such as the public switched telephone network  the mobile telephone network  and the cable television network are also discussed
ters  and  discuss the data link layer in two parts  looks at the problem of how to send packets across a link  including error detection and correction
We look at DSL (used for broadband Internet access over phone lines) as a real-world example of a data link protocol
we examine the medium access sublayer
This is the part of the data link layer that deals with how to share a channel between multiple computers
The examples we look at include wireless  such as
and RFID  and wired LANs such as classic Ethernet
Link layer switches that connect LANs  such as switched Ethernet  are also discussed here
ter  deals with the network layer  especially routing
Many routing algorithms  both static and dynamic  are covered
Even with good routing algorithms  though  if more traffic is offered than the network can handle  some packets will be delayed or discarded
We discuss this issue from how to prevent congestion to how to guarantee a certain quality of service
Connecting heterogeneous networks to form internetworks also leads to numerous problems that are discussed here
The network layer in the Internet is given extensive coverage
INTRODUCTION
ter  deals with the transport layer
Much of the emphasis is on connection- oriented protocols and reliability  since many applications need these
Both Internet transport protocols  UDP and TCP  are covered in detail  as are their performance issues
ter  deals with the application layer  its protocols  and its applications
The first topic is DNS  which is the Internetâs telephone book
Next comes email  including a discussion of its protocols
Then we move on to the Web  with detailed discussions of static and dynamic content  and what happens on the client and server sides
We follow this with a look at networked multimedia  including streaming audio and video
Finally  we discuss content-delivery networks  including peer-to-peer technology
ter  is about network urity
This topic has aspects that relate to all layers  so it is easiest to treat it after all the layers have been thoroughly explained
The  ter starts with an introduction to cryptography
Later  it shows how cryptography can be used to ure communication  email  and the Web
The  ter ends with a discussion of some areas in which urity collides with privacy  freedom of speech  censorship  and other social issues
ter  contains an annotated list of suggested readings arranged by  ter
It is intended to help those readers who would like to pursue their study of networking further
The  ter also has an alphabetical bibliography of all the references cited in this book
The authorsâ Web site at Pearson: http:// /tanenbaum has a page with links to many tutorials  FAQs  companies  industry consortia  professional organizations  standards organizations  technologies  papers  and more  SUMMARY Computer networks have many uses  both for companies and for individuals  in the home and while on the move
Companies use networks of computers to share corporate information  typically using the client-server model with employee desktops acting as clients accessing powerful servers in the machine room
For individuals  networks offer access to a variety of information and entertainment resources  as well as a way to buy and sell products and services
Individuals often access the Internet via their phone or cable providers at home  though increasingly wireless access is used for laptops and phones
Technology advances are enabling new kinds of mobile applications and networks with computers embedded in appliances and other consumer devices
The same advances raise social issues such as privacy concerns
Roughly speaking  networks can be divided into LANs  MANs  WANs  and internetworks
LANs typical cover a building and operate at high speeds
MANs   SUMMARY   usually cover a city
An example is the cable television system  which is now used by many people to access the Internet
WANs may cover a country or a continent
Some of the technologies used to build these networks are point-to-point (
a cable) while others are broadcast (
Networks can be interconnected with routers to form internetworks  of which the Internet is the largest and best known example
Wireless networks  for example
LANs and  G mobile telephony  are also becoming extremely popular
Network software is built around protocols  which are rules by which processes communicate
Most networks support protocol hierarchies  with each layer providing services to the layer above it and insulating them from the details of the protocols used in the lower layers
Protocol stacks are typically based either on the OSI model or on the TCP/IP model
Both have link  network  transport  and application layers  but they differ on the other layers
Design issues include reliability  resource allocation  growth  urity  and more
Much of this book deals with protocols and their design
Networks provide various services to their users
These services can range from connectionless best-efforts packet delivery to connection-oriented guaranteed delivery
In some networks  connectionless service is provided in one layer and connection-oriented service is provided in the layer above it
Well-known networks include the Internet  the  G mobile telephone network  and
The Internet evolved from the ARPANET  to which other networks were added to form an internetwork
The present-day Internet is actually a collection of many thousands of networks that use the TCP/IP protocol stack
The  G mobile telephone network provides wireless and mobile access to the Internet at speeds of multiple Mbps  and  of course  carries voice calls as well
Wireless LANs based on the IEEE
standard are deployed in many homes and cafes and can provide connectivity at rates in excess of Mbps
New kinds of networks are emerging too  such as embedded sensor networks and networks based on RFID technology
Enabling multiple computers to talk to each other requires a large amount of standardization  both in the hardware and software
Organizations such as ITU-T  ISO  IEEE  and IAB manage different parts of the standardization process
Imagine that you have trained your St
Bernard  Bernie  to carry a box of three  -mm tapes instead of a flask of brandy
(When your disk fills up  you consider that an emergency
) These tapes each contain  gigabytes
The dog can travel to your side  wherever you may be  at   km/hour
For what range of distances does Bernie have a higher data rate than a transmission line whose data rate (excluding overhead) is Mbps? How does your answer change if (i) Bernieâs speed is doubled; (ii) each tape capacity is doubled; (iii) the data rate of the transmission line is doubled
INTRODUCTION
An alternative to a LAN is simply a big timesharing system with terminals for all users
Give two advantages of a client-server system using a LAN The performance of a client-server system is strongly influenced by two major network characteristics: the bandwidth of the network (that is  how many bits/ it can transport) and the latency (that is  how many onds it takes for the first bit to get from the client to the server)
Give an example of a network that exhibits high bandwidth but also high latency
Then give an example of one that has both low bandwidth and low latency Besides bandwidth and latency  what other parameter is needed to give a good characterization of the quality of service offered by a network used for (i) digitized voice traffic? (ii) video traffic? (iii) financial transaction traffic?
A factor in the delay of a store-and-forward packet-switching system is how long it takes to store and forward a packet through a switch
If switching time is   Î¼  is this likely to be a major factor in the response of a client-server system where the client is in New York and the server is in California? Assume the propagation speed in copper and fiber to be  /  the speed of light in vacuum A client-server system uses a satellite network  with the satellite at a height of   km
What is the best-case delay in response to a request?
In the future  when everyone has a home terminal connected to a computer network  instant public referendums on important pending legislation will become possible
Ultimately  existing legislatures could be eliminated  to let the will of the people be expressed directly
The positive aspects of such a direct democracy are fairly obvious; discuss some of the negative aspects Five routers are to be connected in a point-to-point subnet
Between each pair of routers  the designers may put a high-speed line  a medium-speed line  a low-speed line  or no line
If it takes ms of computer time to generate and inspect each topology  how long will it take to inspect all of them?
A disadvantage of a broadcast subnet is the capacity wasted when multiple hosts attempt to access the channel at the same time
As a simplistic example  suppose that time is divided into discrete slots  with each of the n hosts attempting to use the channel with probability p during each slot
What fraction of the slots will be wasted due to collisions?
What are two reasons for using layered protocols? What is one possible disadvantage of using layered protocols?
The president of the Specialty Paint Corp
gets the idea to work with a local beer brewer to produce an invisible beer can (as an anti-litter measure)
The president tells her legal department to look into it  and they in turn ask engineering for help
As a result  the chief engineer calls his counterpart at the brewery to discuss the technical aspects of the project
The engineers then report back to their respective legal departments  which then confer by telephone to arrange the legal aspects
Finally  the two corporate presidents discuss the financial side of the deal
What principle of a multilayer protocol in the sense of the OSI model does this communication mechanism violate?
Two networks each provide reliable connection-oriented service
One of them offers a reliable byte stream and the other offers a reliable message stream
Are these identical? If so  why is the distinction made? If not  give an example of how they differ What does âânegotiationââ mean when discussing network protocols? Give an example In Fig
Are any other services implicit in this figure? If so  where? If not  why not?
In some networks  the data link layer handles transmission errors by requesting that damaged frames be retransmitted
If the probability of a frameâs being damaged is p  what is the mean number of transmissions required to send a frame? Assume that acknowledgements are never lost A system has an n-layer protocol hierarchy
Applications generate messages of length M bytes
At each of the layers  an h-byte header is added
What fraction of the network bandwidth is filled with headers?
What is the main difference between TCP and UDP?
The subnet of Fig
How many bombs would it take to partition the nodes into two disconnected sets? Assume that any bomb wipes out a node and all of the links connected to it The Internet is roughly doubling in size every   months
Although no one really knows for sure  one estimate put the number of hosts on it at million in
Use these data to compute the expected number of Internet hosts in the year
Do you believe this? Explain why or why not When a file is transferred between two computers  two acknowledgement strategies are possible
In the first one  the file is chopped up into packets  which are individually acknowledged by the receiver  but the file transfer as a whole is not acknowledged
In the ond one  the packets are not acknowledged individually  but the entire file is acknowledged when it arrives
Discuss these two approaches Mobile phone network operators need to know where their subscribersâ mobile phones (hence their users) are located
Explain why this is bad for users
Now give reasons why this is good for users How long was a bit in the original
standard in meters? Use a transmission speed of   Mbps and assume the propagation speed in coax is  /  the speed of light in vacuum An image is  Ã  pixels with  bytes/pixel
Assume the image is uncompressed
How long does it take to transmit it over a  -kbps modem channel? Over a  -Mbps cable modem? Over a  -Mbps Ethernet? Over   -Mbps Ethernet? Over gigabit Ethernet?
Ethernet and wireless networks have some similarities and some differences
One property of Ethernet is that only one frame at a time can be transmitted on an Ethernet
share this property with Ethernet? Discuss your answer List two advantages and two disadvantages of having international standards for network protocols
INTRODUCTION   When a system has a permanent part and a removable part (such as a CD-ROM drive and the CD-ROM)  it is important that the system be standardized  so that different companies can make both the permanent and removable parts and everything still works together
Give three examples outside the computer industry where such international standards exist
Now give three areas outside the computer industry where they do not exist Suppose the algorithms used to implement the operations at layer k is changed
How does this impact operations at layers k â  and k +  ?
Suppose there is a change in the service (set of operations) provided by layer k
How does this impact services at layers k-  and k+ ?
Provide a list of reasons for why the response time of a client may be larger than the best-case delay What are the disadvantages of using small  fixed-length cells in ATM?
Make a list of activities that you do every day in which computer networks are used
How would your life be altered if these networks were suddenly switched off?
Find out what networks are used at your school or place of work
Describe the network types  topologies  and switching methods used there The ping program allows you to send a test packet to a given location and see how long it takes to get there and back
Try using ping to see how long it takes to get from your location to several known locations
From these data  plot the one-way transit time over the Internet as a function of distance
It is best to use universities since the location of their servers is known very accurately
For example    is in Berkeley  California;   is in Cambridge  Massachusetts;   is in Amsterdam; The Netherlands;
is in Sydney  Australia; and
is in Cape Town  South Africa Go to IETFâs Web site     to see what they are doing
Pick a project you like and write a half-page report on the problem and the proposed solution The Internet is made up of a large number of networks
Their arrangement determines the topology of the Internet
A considerable amount of information about the Internet topology is available on line
Use a search engine to find out more about the Internet topology and write a short report summarizing your findings Search the Internet to find out some of the important peering points used for routing packets in the Internet at present Write a program that implements message flow from the top layer to the bottom layer of the  -layer protocol model
Your program should include a separate protocol function for each layer
Protocol headers are sequence up to   characters
Each protocol function has two parameters: a message passed from the higher layer protocol (a char buffer) and the size of the message
This function attaches its header in front of the message  prints the new message on the standard output  and then invokes the protocol function of the lower-layer protocol
Program input is an application message (a sequence of   characters or less)
THE PHYSICAL LAYER In this  ter we will look at the lowest layer in our protocol model  the physical layer
It defines the electrical  timing and other interfaces by which bits are sent as signals over channels
The physical layer is the foundation on which the network is built
The properties of different kinds of physical channels determine the performance (
throughput  latency  and error rate) so it is a good place to start our journey into networkland
We will begin with a theoretical analysis of data transmission  only to discover that Mother (Parent?) Nature puts some limits on what can be sent over a channel
Then we will cover three kinds of transmission media: guided (copper wire and fiber optics)  wireless (terrestrial radio)  and satellite
Each of these technologies has different properties that affect the design and performance of the networks that use them
This material will provide background information on the key transmission technologies used in modern networks
Next comes digital modulation  which is all about how analog signals are converted into digital bits and back again
After that we will look at multiplexing schemes  exploring how multiple conversations can be put on the same transmission medium at the same time without interfering with one another
Finally  we will look at three examples of communication systems used in practice for wide area computer networks: the (fixed) telephone system  the mobile phone system  and the cable television system
Each of these is important in practice  so we will devote a fair amount of space to each one
THE PHYSICAL LAYER
THE THEORETICAL BASIS FOR DATA COMMUNICATION Information can be transmitted on wires by varying some physical property such as voltage or current
By representing the value of this voltage or current as a single-valued function of time  f(t)  we can model the behavior of the signal and analyze it mathematically
This analysis is the subject of the following tions
Fourier Analysis In the early  th century  the French mathematician Jean-Baptiste Fourier proved that any reasonably behaved periodic function  g(t) with period T  can be constructed as the sum of a (possibly infinite) number of sines and cosines: g(t) = c + n =  Î£ â an sin( Ïnft ) + n =  Î£ â bn cos( Ïnft) ( - ) where f =  /T is the fundamental frequency  an and bn are the sine and cosine amplitudes of the nth harmonics (terms)  and c is a constant
Such a decomposition is called a Fourier series
From the Fourier series  the function can be reconstructed
That is  if the period  T  is known and the amplitudes are given  the original function of time can be found by performing the sums of Eq
A data signal that has a finite duration  which all of them do  can be handled by just imagining that it repeats the entire pattern over and over forever (
the interval from T to  T is the same as from  to T  etc
The an amplitudes can be computed for any given g(t) by multiplying both sides of Eq
( - ) by sin( Ïkft ) and then integrating from  to T
Since  â« T sin( Ïkft) sin( Ïnft ) dt = â§â¨â© T /  for k = n  for k â  n only one term of the summation survives: an
The bn summation vanishes completely
Similarly  by multiplying Eq
( - ) by cos( Ïkft ) and integrating between  and T  we can derive bn
By just integrating both sides of the equation as it stands  we can find c
The results of performing these operations are as follows: an = T â« T g(t) sin( Ïnft ) dt bn = T â« T g(t) cos( Ïnft ) dt c = T â« T g(t) dt    Bandwidth-Limited Signals The relevance of all of this to data communication is that real channels affect different frequency signals differently
Let us consider a specific example: the transmission of the ASCII character ââbââ encoded in an  -bit byte
The bit pattern that is to be transmitted is
The left-hand part of Fig
The Fourier analysis of this signal yields the coefficients: an = Ïn  [cos(Ïn / ) â cos( Ïn / ) + cos( Ïn / ) â cos( Ïn / )] bn = Ïn  [sin( Ïn / ) â sin(Ïn / ) + sin( Ïn / ) â sin( Ïn / )] c =  /  The root-mean-square amplitudes  âan  + bn for the first few terms are shown on the right-hand side of Fig
These values are of interest because their squares are proportional to the energy transmitted at the corresponding frequency
No transmission facility can transmit signals without losing some power in the process
If all the Fourier components were equally diminished  the resulting signal would be reduced in amplitude but not distorted [
it would have the same nice squared-off shape as Fig
Unfortunately  all transmission facilities diminish different Fourier components by different amounts  thus introducing distortion
Usually  for a wire  the amplitudes are transmitted mostly undiminished from  up to some frequency fc [measured in cycles/ or Hertz (Hz)]  with all frequencies above this cutoff frequency attenuated
The width of the frequency range transmitted without being strongly attenuated is called the bandwidth
In practice  the cutoff is not really sharp  so often the quoted bandwidth is from  to the frequency at which the received power has fallen by half
The bandwidth is a physical property of the transmission medium that depends on  for example  the construction  thickness  and length of a wire or fiber
Filters are often used to further limit the bandwidth of a signal
wireless channels are allowed to use up to roughly   MHz  for example  so
radios filter the signal bandwidth to this size
As another example  traditional (analog) television channels occupy  MHz each  on a wire or over the air
This filtering lets more signals share a given region of spectrum  which improves the overall efficiency of the system
It means that the frequency range for some signals will not start at zero  but this does not matter
The bandwidth is still the width of the band of frequencies that are passed  and the information that can be carried depends only on this width and not on the starting and ending frequencies
Signals that run from  up to a maximum frequency are called baseband signals
Signals that are shifted to occupy a higher range of frequencies  as is the case for all wireless transmissions  are called passband signals
Now let us consider how the signal of Fig
- (a) would look if the bandwidth were so low that only the lowest frequencies were transmitted [
if the function were being approximated by the first few terms of Eq
Figure  - (b) shows the signal that results from a channel that allows only the first harmonic   THE PHYSICAL LAYER
Time T    Time rms amplitude
Harmonic number  harmonic  harmonics  harmonics  harmonics     Harmonic number (a) (b) (c) (d) (e) Figure  -
(a) A binary signal and its root-mean-square Fourier amplitudes
(b)â(e) Successive approximations to the original signal
THE THEORETICAL BASIS FOR DATA COMMUNICATION   (the fundamental  f) to pass through
Similarly  Fig
- (c)â(e) show the spectra and reconstructed functions for higher-bandwidth channels
For digital transmission  the goal is to receive a signal with just enough fidelity to reconstruct the sequence of bits that was sent
We can already do this easily in Fig
Given a bit rate of b bits/  the time required to send the  bits in our example  bit at a time is  /b   so the frequency of the first harmonic of this signal is b /  Hz
An ordinary telephone line  often called a voice-grade line  has an artificially introduced cutoff frequency just above  Hz
The presence of this restriction means that the number of the highest harmonic passed through is roughly /(b/ )  or  /b (the cutoff is not sharp)
For some data rates  the numbers work out as shown in Fig
From these numbers  it is clear that trying to send at  bps over a voice-grade telephone line will transform Fig
- (c)  making accurate reception of the original binary bit stream tricky
It should be obvious that at data rates much higher than
kbps  there is no hope at all for binary signals  even if the transmission facility is completely noiseless
In other words  limiting the bandwidth limits the data rate  even for perfect channels
However  coding schemes that make use of several voltage levels do exist and can achieve higher data rates
We will discuss these later in this  ter
Bps T (m) First harmonic (Hz) # Harmonics sent
Relation between data rate and harmonics for our example
There is much confusion about bandwidth because it means different things to electrical engineers and to computer scientists
To electrical engineers  (analog) bandwidth is (as we have described above) a quantity measured in Hz
To computer scientists  (digital) bandwidth is the maximum data rate of a channel  a quantity measured in bits/
That data rate is the end result of using the analog bandwidth of a physical channel for digital transmission  and the two are related  as we discuss next
In this book  it will be clear from the context whether we mean analog bandwidth (Hz) or digital bandwidth (bits/)
THE PHYSICAL LAYER
The Maximum Data Rate of a Channel As early as  an AT&T engineer  Henry Nyquist  realized that even a perfect channel has a finite transmission capacity
He derived an equation expressing the maximum data rate for a finite-bandwidth noiseless channel
In  Claude Shannon carried Nyquistâs work further and extended it to the case of a channel subject to random (that is  thermodynamic) noise (Shannon  )
This paper is the most important paper in all of information theory
We will just briefly summarize their now classical results here
Nyquist proved that if an arbitrary signal has been run through a low-pass filter of bandwidth B  the filtered signal can be completely reconstructed by making only  B (exact) samples per ond
Sampling the line faster than  B times per ond is pointless because the higher-frequency components that such sampling could recover have already been filtered out
If the signal consists of V discrete levels  Nyquistâs theorem states: maximum data rate =  B log  V bits / ( - ) For example  a noiseless  -kHz channel cannot transmit binary (
two-level) signals at a rate exceeding  bps
So far we have considered only noiseless channels
If random noise is present  the situation deteriorates rapidly
And there is always random (thermal) noise present due to the motion of the molecules in the system
The amount of thermal noise present is measured by the ratio of the signal power to the noise power  called the SNR (Signal-to-Noise Ratio)
If we denote the signal power by S and the noise power by N  the signal-to-noise ratio is S/N
Usually  the ratio is expressed on a log scale as the quantity   log  S /N because it can vary over a tremendous range
The units of this log scale are called decibels (dB)  with ââdeciââ meaning   and ââbelââ chosen to honor Alexander Graham Bell  who invented the telephone
An S /N ratio of   is   dB  a ratio of is   dB  a ratio of  is   dB  and so on
The manufacturers of stereo amplifiers often characterize the bandwidth (frequency range) over which their products are linear by giving the  - dB frequency on each end
These are the points at which the amplification factor has been approximately halved (because   log
Shannonâs major result is that the maximum data rate or capacity of a noisy channel whose bandwidth is B Hz and whose signal-to-noise ratio is S/N  is given by: maximum number of bits/ = B log  (  + S/N) ( - ) This tells us the best capacities that real channels can have
For example  ADSL (Asymmetric Digital Subscriber Line)  which provides Internet access over normal telephone lines  uses a bandwidth of around  MHz
The SNR depends strongly on the distance of the home from the telephone exchange  and an SNR of around   dB for short lines of  to  km is very good
With these characteristics    THE THEORETICAL BASIS FOR DATA COMMUNICATION   the channel can never transmit much more than   Mbps  no matter how many or how few signal levels are used and no matter how often or how infrequently samples are taken
In practice  ADSL is specified up to   Mbps  though users often see lower rates
This data rate is actually very good  with over   years of communications techniques having greatly reduced the gap between the Shannon capacity and the capacity of real systems
Shannonâs result was derived from information-theory arguments and applies to any channel subject to thermal noise
Counterexamples should be treated in the same category as perpetual motion machines
For ADSL to exceed   Mbps  it must either improve the SNR (for example by inserting digital repeaters in the lines closer to the customers) or use more bandwidth  as is done with the evolution to ASDL +  GUIDED TRANSMISSION MEDIA The purpose of the physical layer is to transport bits from one machine to another
Various physical media can be used for the actual transmission
Each one has its own niche in terms of bandwidth  delay  cost  and ease of installation and maintenance
Media are roughly grouped into guided media  such as copper wire and fiber optics  and unguided media  such as terrestrial wireless  satellite  and lasers through the air
We will look at guided media in this tion  and unguided media in the next tions
Magnetic Media One of the most common ways to transport data from one computer to another is to write them onto magnetic tape or removable media (
recordable DVDs)  physically transport the tape or disks to the destination machine  and read them back in again
Although this method is not as sophisticated as using a geosynchronous communication satellite  it is often more cost effective  especially for applications in which high bandwidth or cost per bit transported is the key factor
A simple calculation will make this point clear
An industry-standard Ultrium tape can hold gigabytes
A box   Ã   Ã   cm can hold about  of these tapes  for a total capacity of terabytes  or  terabits (
A box of tapes can be delivered anywhere in the United States in   hours by Federal Express and other companies
The effective bandwidth of this transmission is  terabits/    or a bit over   Gbps
If the destination is only an hour away by road  the bandwidth is increased to over  Gbps
No computer network can even approach this
Of course  networks are getting faster  but tape densities are increasing  too
If we now look at cost  we get a similar picture
The cost of an Ultrium tape is around $  when bought in bulk
A tape can be reused at least   times  so the   THE PHYSICAL LAYER
tape cost is maybe $ per box per usage
Add to this another $ for shipping (probably much less)  and we have a cost of roughly $ to ship TB
This amounts to shipping a gigabyte for a little over half a cent
No network can beat that
The moral of the story is: Never underestimate the bandwidth of a station wagon full of tapes hurtling down the highway
Twisted Pairs Although the bandwidth characteristics of magnetic tape are excellent  the delay characteristics are poor
Transmission time is measured in minutes or hours  not millionds
For many applications an online connection is needed
One of the oldest and still most common transmission media is twisted pair
A twisted pair consists of two insulated copper wires  typically about  mm thick
The wires are twisted together in a helical form  just like a DNA molecule
Twisting is done because two parallel wires constitute a fine antenna
When the wires are twisted  the waves from different twists cancel out  so the wire radiates less effectively
A signal is usually carried as the difference in voltage between the two wires in the pair
This provides better immunity to external noise because the noise tends to affect both wires the same  leaving the differential unchanged
The most common application of the twisted pair is the telephone system
Nearly all telephones are connected to the telephone company (telco) office by a twisted pair
Both telephone calls and ADSL Internet access run over these lines
Twisted pairs can run several kilometers without amplification  but for longer distances the signal becomes too attenuated and repeaters are needed
When many twisted pairs run in parallel for a substantial distance  such as all the wires coming from an apartment building to the telephone company office  they are bundled together and encased in a protective sheath
The pairs in these bundles would interfere with one another if it were not for the twisting
In parts of the world where telephone lines run on poles above ground  it is common to see bundles several centimeters in diameter
Twisted pairs can be used for transmitting either analog or digital information
The bandwidth depends on the thickness of the wire and the distance traveled  but several megabits/ can be achieved for a few kilometers in many cases
Due to their adequate performance and low cost  twisted pairs are widely used and are likely to remain so for years to come
Twisted-pair cabling comes in several varieties
The garden variety deployed in many office buildings is called Category  cabling  or ââCat
ââ A category  twisted pair consists of two insulated wires gently twisted together
Four such pairs are typically grouped in a plastic sheath to protect the wires and keep them together
This arrangement is shown in Fig
Different LAN standards may use the twisted pairs differently
For example -Mbps Ethernet uses two (out of the four) pairs  one pair for each direction
GUIDED TRANSMISSION MEDIA   Twisted pair Figure  -
Category  UTP cable with four twisted pairs
To reach higher speeds  -Gbps Ethernet uses all four pairs in both directions simultaneously; this requires the receiver to factor out the signal that is transmitted locally
Some general terminology is now in order
Links that can be used in both directions at the same time  like a two-lane road  are called full-duplex links
In contrast  links that can be used in either direction  but only one way at a time  like a single-track railroad line
are called half-duplex links
A third category consists of links that allow traffic in only one direction  like a one-way street
They are called simplex links
Returning to twisted pair  Cat  replaced earlier Category  cables with a similar cable that uses the same connector  but has more twists per meter
More twists result in less crosstalk and a better-quality signal over longer distances  making the cables more suitable for high-speed computer communication  especially   -Mbps and  -Gbps Ethernet LANs
New wiring is more likely to be Category  or even Category
These categories has more stringent specifications to handle signals with greater bandwidths
Some cables in Category  and above are rated for signals of MHz and can support the  -Gbps links that will soon be deployed
Through Category   these wiring types are referred to as UTP (Unshielded Twisted Pair) as they consist simply of wires and insulators
In contrast to these  Category  cables have shielding on the individual twisted pairs  as well as around the entire cable (but inside the plastic protective sheath)
Shielding reduces the susceptibility to external interference and crosstalk with other nearby cables to meet demanding performance specifications
The cables are reminiscent of the high-quality  but bulky and expensive shielded twisted pair cables that IBM introduced in the early s  but which did not prove popular outside of IBM installations
Evidently  it is time to try again
Coaxial Cable Another common transmission medium is the coaxial cable (known to its many friends as just ââcoaxââ and pronounced ââco-axââ)
It has better shielding and greater bandwidth than unshielded twisted pairs  so it can span longer distances at   THE PHYSICAL LAYER
higher speeds
Two kinds of coaxial cable are widely used
One kind   -ohm cable  is commonly used when it is intended for digital transmission from the start
The other kind   -ohm cable  is commonly used for analog transmission and cable television
This distinction is based on historical  rather than technical  factors (
early dipole antennas had an impedance of ohms  and it was easy to use existing  :  impedance-matching transformers)
Starting in the mid- s  cable TV operators began to provide Internet access over cable  which has made  -ohm cable more important for data communication
A coaxial cable consists of a stiff copper wire as the core  surrounded by an insulating material
The insulator is encased by a cylindrical conductor  often as a closely woven braided mesh
The outer conductor is covered in a protective plastic sheath
A cutaway view of a coaxial cable is shown in Fig
Copper core Insulating material Braided outer conductor Protective plastic covering Figure  -
A coaxial cable
The construction and shielding of the coaxial cable give it a good combination of high bandwidth and excellent noise immunity
The bandwidth possible depends on the cable quality and length
Modern cables have a bandwidth of up to a few GHz
Coaxial cables used to be widely used within the telephone system for long-distance lines but have now largely been replaced by fiber optics on longhaul routes
Coax is still widely used for cable television and metropolitan area networks  however
Power Lines The telephone and cable television networks are not the only sources of wiring that can be reused for data communication
There is a yet more common kind of wiring: electrical power lines
Power lines deliver electrical power to houses  and electrical wiring within houses distributes the power to electrical outlets
The use of power lines for data communication is an old idea
Power lines have been used by electricity companies for low-rate communication such as remote metering for many years  as well in the home to control devices (
the X  standard)
In recent years there has been renewed interest in high-rate communication over these lines  both inside the home as a LAN and outside the home   GUIDED TRANSMISSION MEDIA   for broadband Internet access
We will concentrate on the most common scenario: using electrical wires inside the home
The convenience of using power lines for networking should be clear
Simply plug a TV and a receiver into the wall  which you must do anyway because they need power  and they can send and receive movies over the electrical wiring
This configuration is shown in Fig
There is no other plug or radio
The data signal is superimposed on the low-frequency power signal (on the active or ââhotââ wire) as both signals use the wiring at the same time
Power signal Electric cable Data signal Figure  -
A network that uses household electrical wiring
The difficulty with using household electrical wiring for a network is that it was designed to distribute power signals
This task is quite different than distributing data signals  at which household wiring does a horrible job
Electrical signals are sent at  â  Hz and the wiring attenuates the much higher frequency (MHz) signals needed for high-rate data communication
The electrical properties of the wiring vary from one house to the next and change as appliances are turned on and off  which causes data signals to bounce around the wiring
Transient currents when appliances switch on and off create electrical noise over a wide range of frequencies
And without the careful twisting of twisted pairs  electrical wiring acts as a fine antenna  picking up external signals and radiating signals of its own
This behavior means that to meet regulatory requirements  the data signal must exclude licensed frequencies such as the amateur radio bands
Despite these difficulties  it is practical to send at least Mbps over typical household electrical wiring by using communication schemes that resist impaired frequencies and bursts of errors
Many products use various proprietary standards for power-line networking  so international standards are actively under development
Fiber Optics Many people in the computer industry take enormous pride in how fast computer technology is improving as it follows Mooreâs law  which predicts a doubling of the number of transistors per chip roughly every two years (Schaller  THE PHYSICAL LAYER
The original (   ) IBM PC ran at a clock speed of
Twentyeight years later  PCs could run a four-core CPU at  GHz
This increase is a gain of a factor of around  or   per decade
Impressive
In the same period  wide area communication links went from   Mbps (a T  line in the telephone system) to Gbps (a modern long distance line)
This gain is similarly impressive  more than a factor of  and close to   per decade  while at the same time the error rate went from  â  per bit to almost zero
Furthermore  single CPUs are beginning to approach physical limits  which is why it is now the number of CPUs that is being increased per chip
In contrast  the achievable bandwidth with fiber technology is in excess of   Gbps (  Tbps) and we are nowhere near reaching these limits
The current practical limit of around Gbps is due to our inability to convert between electrical and optical signals any faster
To build higher-capacity links  many channels are simply carried in parallel over a single fiber
In this tion we will study fiber optics to learn how that transmission technology works
In the ongoing race between computing and communication  communication may yet win because of fiber optic networks
The implication of this would be essentially infinite bandwidth and a new conventional wisdom that computers are hopelessly slow so that networks should try to avoid computation at all costs  no matter how much bandwidth that wastes
This change will take a while to sink in to a generation of computer scientists and engineers taught to think in terms of the low Shannon limits imposed by copper
Of course  this scenario does not tell the whole story because it does not include cost
The cost to install fiber over the last mile to reach consumers and bypass the low bandwidth of wires and limited availability of spectrum is tremendous
It also costs more energy to move bits than to compute
We may always have islands of inequities where either computation or communication is essentially free
For example  at the edge of the Internet we throw computation and storage at the problem of compressing and caching content  all to make better use of Internet access links
Within the Internet  we may do the reverse  with companies such as Google moving huge amounts of data across the network to where it is cheaper to store or compute on it
Fiber optics are used for long-haul transmission in network backbones  highspeed LANs (although so far  copper has always managed catch up eventually)  and high-speed Internet access such as FttH (Fiber to the Home)
An optical transmission system has three key components: the light source  the transmission medium  and the detector
Conventionally  a pulse of light indicates a  bit and the absence of light indicates a  bit
The transmission medium is an ultra-thin fiber of glass
The detector generates an electrical pulse when light falls on it
By attaching a light source to one end of an optical fiber and a detector to the other  we have a unidirectional data transmission system that accepts an electrical signal  converts and transmits it by light pulses  and then reconverts the output to an electrical signal at the receiving end
GUIDED TRANSMISSION MEDIA This transmission system would leak light and be useless in practice were it not for an interesting principle of physics
When a light ray passes from one medium to anotherâfor example  from fused silica to airâthe ray is refracted (bent) at the silica/air boundary  as shown in Fig
Here we see a light ray incident on the boundary at an angle Î±  emerging at an angle Î²
The amount of refraction depends on the properties of the two media (in particular  their indices of refraction)
For angles of incidence above a certain critical value  the light is refracted back into the silica; none of it escapes into the air
Thus  a light ray incident at or above the critical angle is trapped inside the fiber  as shown in Fig
Total internal reflection
Air/silica boundary Silica Light source Air (a) (b) Î²  Î²  Î²  Î±  Î±  Î±  Figure  -
(a) Three examples of a light ray from inside a silica fiber impinging on the air/silica boundary at different angles
(b) Light trapped by total internal reflection
The sketch of Fig
Each ray is said to have a different mode  so a fiber having this property is called a multimode fiber
However  if the fiberâs diameter is reduced to a few wavelengths of light the fiber acts like a wave guide and the light can propagate only in a straight line  without bouncing  yielding a single-mode fiber
Single-mode fibers are more expensive but are widely used for longer distances
Currently available single-mode fibers can transmit data at Gbps for km without amplification
Even higher data rates have been achieved in the laboratory for shorter distances
Transmission of Light Through Fiber Optical fibers are made of glass  which  in turn  is made from sand  an inexpensive raw material available in unlimited amounts
Glassmaking was known to the ancient Egyptians  but their glass had to be no more than  mm thick or the THE PHYSICAL LAYER
light could not shine through
Glass transparent enough to be useful for windows was developed during the Renaissance
The glass used for modern optical fibers is so transparent that if the oceans were full of it instead of water  the seabed would be as visible from the surface as the ground is from an airplane on a clear day
The attenuation of light through glass depends on the wavelength of the light (as well as on some physical properties of the glass)
It is defined as the ratio of input to output signal power
For the kind of glass used in fibers  the attenuation is shown in Fig
For example  a factor of two loss of signal power gives an attenuation of   log =  dB
The figure shows the near-infrared part of the spectrum  which is what is used in practice
Visible light has slightly shorter wavelengths  from
(  micron is  â  meters
) The true metric purist would refer to these wavelengths as nm to nm  but we will stick with traditional usage
Wavelength (microns)
Î¼ Band Attenuation (dB/km)
Attenuation of light through fiber in the infrared region
Three wavelength bands are most commonly used at present for optical communication
They are centered at     and
microns  respectively
All three bands are   to   GHz wide
It has higher attenuation and so is used for shorter distances  but at that wavelength the lasers and electronics could be made from the same material (gallium arsenide)
The last two bands have good attenuation properties (less than  % loss per kilometer)
GUIDED TRANSMISSION MEDIA Light pulses sent down a fiber spread out in length as they propagate
This spreading is called chromatic dispersion
The amount of it is wavelength dependent
One way to keep these spread-out pulses from overlapping is to increase the distance between them  but this can be done only by reducing the signaling rate
Fortunately  it has been discovered that making the pulses in a special shape related to the reciprocal of the hyperbolic cosine causes nearly all the dispersion effects cancel out  so it is possible to send pulses for thousands of kilometers without appreciable shape distortion
These pulses are called solitons
A considerable amount of research is going on to take solitons out of the lab and into the field
Fiber Cables Fiber optic cables are similar to coax  except without the braid
Figure  - (a) shows a single fiber viewed from the side
At the center is the glass core through which the light propagates
In multimode fibers  the core is typically   microns in diameter  about the thickness of a human hair
In single-mode fibers  the core is  to   microns
Jacket (plastic) Core Cladding Sheath Jacket Cladding (glass) Core (glass) (a) (b) Figure  -
(a) Side view of a single fiber
(b) End view of a sheath with three fibers
The core is surrounded by a glass cladding with a lower index of refraction than the core  to keep all the light in the core
Next comes a thin plastic jacket to protect the cladding
Fibers are typically grouped in bundles  protected by an outer sheath
Figure  - (b) shows a sheath with three fibers
Terrestrial fiber sheaths are normally laid in the ground within a meter of the surface  where they are occasionally subject to attacks by backhoes or gophers
Near the shore  transoceanic fiber sheaths are buried in trenches by a kind of seaplow
In deep water  they just lie on the bottom  where they can be snagged by fishing trawlers or attacked by giant squid
Fibers can be connected in three different ways
First  they can terminate in connectors and be plugged into fiber sockets
Connectors lose about   to  % of the light  but they make it easy to reconfigure systems
ond  they can be spliced mechanically
Mechanical splices just lay the two carefully cut ends next to each other in a special sleeve and clamp them in THE PHYSICAL LAYER
Alignment can be improved by passing light through the junction and then making small adjustments to maximize the signal
Mechanical splices take trained personnel about  minutes and result in a  % light loss
Third  two pieces of fiber can be fused (melted) to form a solid connection
A fusion splice is almost as good as a single drawn fiber  but even here  a small amount of attenuation occurs
For all three kinds of splices  reflections can occur at the point of the splice  and the reflected energy can interfere with the signal
Two kinds of light sources are typically used to do the signaling
These are LEDs (Light Emitting Diodes) and semiconductor lasers
They have different properties  as shown in Fig
They can be tuned in wavelength by inserting Fabry-Perot or Mach-Zehnder interferometers between the source and the fiber
Fabry-Perot interferometers are simple resonant cavities consisting of two parallel mirrors
The light is incident perpendicular to the mirrors
The length of the cavity selects out those wavelengths that fit inside an integral number of times
Mach-Zehnder interferometers separate the light into two beams
The two beams travel slightly different distances
They are recombined at the end and are in phase for only certain wavelengths
Item LED Semiconductor laser Data rate Low High Fiber type Multi-mode Multi-mode or single-mode Distance Short Long Lifetime Long life Short life Temperature sensitivity Minor Substantial Cost Low cost Expensive Figure  -
A comparison of semiconductor diodes and LEDs as light sources
The receiving end of an optical fiber consists of a photodiode  which gives off an electrical pulse when struck by light
The response time of photodiodes  which convert the signal from the optical to the electrical domain  limits data rates to about Gbps
Thermal noise is also an issue  so a pulse of light must carry enough energy to be detected
By making the pulses powerful enough  the error rate can be made arbitrarily small
Comparison of Fiber Optics and Copper Wire It is instructive to compare fiber to copper
Fiber has many advantages
To start with  it can handle much higher bandwidths than copper
This alone would require its use in high-end networks
Due to the low attenuation  repeaters are needed only about every   km on long lines  versus about every  km for copper    GUIDED TRANSMISSION MEDIA resulting in a big cost saving
Fiber also has the advantage of not being affected by power surges  electromagnetic interference  or power failures
Nor is it affected by corrosive chemicals in the air  important for harsh factory environments
Oddly enough  telephone companies like fiber for a different reason: it is thin and lightweight
Many existing cable ducts are completely full  so there is no room to add new capacity
Removing all the copper and replacing it with fiber empties the ducts  and the copper has excellent resale value to copper refiners who see it as very high-grade ore
Also  fiber is much lighter than copper
One thousand twisted pairs  km long weigh  kg
Two fibers have more capacity and weigh only kg  which reduces the need for expensive mechanical support systems that must be maintained
For new routes  fiber wins hands down due to its much lower installation cost
Finally  fibers do not leak light and are difficult to tap
These properties give fiber good urity against potential wiretappers
On the downside  fiber is a less familiar technology requiring skills not all engineers have  and fibers can be damaged easily by being bent too much
Since optical transmission is inherently unidirectional  two-way communication requires either two fibers or two frequency bands on one fiber
Finally  fiber interfaces cost more than electrical interfaces
Nevertheless  the future of all fixed data communication over more than short distances is clearly with fiber
For a discussion of all aspects of fiber optics and their networks  see Hecht (   )  WIRELESS TRANSMISSION Our age has given rise to information junkies: people who need to be online all the time
For these mobile users  twisted pair  coax  and fiber optics are of no use
They need to get their ââhitsââ of data for their laptop  notebook  shirt pocket  palmtop  or wristwatch computers without being tethered to the terrestrial communication infrastructure
For these users  wireless communication is the answer
In the following tions  we will look at wireless communication in general
It has many other important applications besides providing connectivity to users who want to surf the Web from the beach
Wireless has advantages for even fixed devices in some circumstances
For example  if running a fiber to a building is difficult due to the terrain (mountains  jungles  swamps  etc
)  wireless may be better
It is noteworthy that modern wireless digital communication began in the Hawaiian Islands  where large chunks of Pacific Ocean separated the users from their computer center and the telephone system was inadequate
The Electromagnetic Spectrum When electrons move  they create electromagnetic waves that can propagate through space (even in a vacuum)
These waves were predicted by the British physicist James Clerk Maxwell in  and first observed by the German THE PHYSICAL LAYER
physicist Heinrich Hertz in
The number of oscillations per ond of a wave is called its frequency  f  and is measured in Hz (in honor of Heinrich Hertz)
The distance between two conutive maxima (or minima) is called the wavelength  which is universally designated by the Greek letter Î» (lambda)
When an antenna of the appropriate size is attached to an electrical circuit  the electromagnetic waves can be broadcast efficiently and received by a receiver some distance away
All wireless communication is based on this principle
In a vacuum  all electromagnetic waves travel at the same speed  no matter what their frequency
This speed  usually called the speed of light  c  is approximately  Ã m/  or about  foot (  cm) per nanoond
(A case could be made for redefining the foot as the distance light travels in a vacuum in  n rather than basing it on the shoe size of some long-dead king
) In copper or fiber the speed slows to about  /  of this value and becomes slightly frequency dependent
The speed of light is the ultimate speed limit
No object or signal can ever move faster than it
The fundamental relation between f  Î»  and c (in a vacuum) is Î»f = c ( - ) Since c is a constant  if we know f  we can find Î»  and vice versa
As a rule of thumb  when Î» is in meters and f is in MHz  Î»f â¼â¼
For example -MHz waves are about  meters long  -MHz waves are
meters long  and
The electromagnetic spectrum is shown in Fig
The radio  microwave  infrared  and visible light portions of the spectrum can all be used for transmitting information by modulating the amplitude  frequency  or phase of the waves
Ultraviolet light  X-rays  and gamma rays would be even better  due to their higher frequencies  but they are hard to produce and modulate  do not propagate well through buildings  and are dangerous to living things
The bands listed at the bottom of Fig
The terms LF  MF  and HF refer to Low  Medium  and High Frequency  respectively
Clearly  when the names were assigned nobody expected to go above   MHz  so the higher bands were later named the Very  Ultra  Super  Extremely  and Tremendously High Frequency bands
Beyond that there are no names  but Incredibly  Astonishingly  and Prodigiously High Frequency (IHF  AHF  and PHF) would sound nice
We know from Shannon [Eq
( - )] that the amount of information that a signal such as an electromagnetic wave can carry depends on the received power and is proportional to its bandwidth
Many GHz of bandwidth are available to tap for data transmission in the microwave band  and even more in fiber because it is further to the right in our logarithmic scale
As an example  consider the
If we use   WIRELESS TRANSMISSION         Radio Microwave Infrared UV X-ray Gamma ray f (Hz) Visible light        f (Hz) Twisted pair Coax Satellite TV Terrestrial microwave Fiber optics Maritime AM radio FM radio Band LF MF HF VHF UHF SHF EHF THF Figure  -
The electromagnetic spectrum and its uses for communication
( - ) to find the start and end frequencies from the start and end wavelengths  we find the frequency range to be about   GHz
With a reasonable signalto- noise ratio of   dB  this is Tbps
Most transmissions use a relatively narrow frequency band (
Îf / f <<  )
They concentrate their signals in this narrow band to use the spectrum efficiently and obtain reasonable data rates by transmitting with enough power
However  in some cases  a wider band is used  with three variations
In frequency hopping spread spectrum  the transmitter hops from frequency to frequency hundreds of times per ond
It is popular for military communication because it makes transmissions hard to detect and next to impossible to jam
It also offers good resistance to multipath fading and narrowband interference because the receiver will not be stuck on an impaired frequency for long enough to shut down communication
This robustness makes it useful for crowded parts of the spectrum  such as the ISM bands we will describe shortly
This technique is used commercially  for example  in Bluetooth and older versions of    As a curious footnote  the technique was coinvented by the Austrian-born sex goddess Hedy Lamarr  the first woman to appear nude in a motion picture (the  Czech film Extase)
Her first husband was an armaments manufacturer who told her how easy it was to block the radio signals then used to control torpedoes
When she discovered that he was selling weapons to Hitler  she was horrified  disguised herself as a maid to escape him  and fled to Hollywood to continue her career as a movie actress
In her spare time  she invented frequency hopping to help the Allied war effort
Her scheme used   frequencies  the number of keys THE PHYSICAL LAYER
(and frequencies) on the piano
For their invention  she and her friend  the musical composer George Antheil  received
However  they were unable to convince the
Navy that their invention had any practical use and never received any royalties
Only years after the patent expired did it become popular
A ond form of spread spectrum  direct sequence spread spectrum  uses a code sequence to spread the data signal over a wider frequency band
It is widely used commercially as a spectrally efficient way to let multiple signals share the same frequency band
These signals can be given different codes  a method called CDMA (Code Division Multiple Access) that we will return to later in this  ter
This method is shown in contrast with frequency hopping in Fig
It forms the basis of  G mobile phone networks and is also used in GPS (Global Positioning System)
Even without different codes  direct sequence spread spectrum  like frequency hopping spread spectrum  can tolerate narrowband interference and multipath fading because only a fraction of the desired signal is lost
It is used in this role in older
b wireless LANs
For a fascinating and detailed history of spread spectrum communication  see Scholtz (   )
Ultrawideband underlay (CDMA user with different code) Direct sequence spread spectrum Frequency hopping spread spectrum Frequency (CDMA user with different code) Figure  -
Spread spectrum and ultra-wideband (UWB) communication
A third method of communication with a wider band is UWB (Ultra- WideBand) communication
UWB sends a series of rapid pulses  varying their positions to communicate information
The rapid transitions lead to a signal that is spread thinly over a very wide frequency band
UWB is defined as signals that have a bandwidth of at least MHz or at least  % of the center frequency of their frequency band
UWB is also shown in Fig
With this much bandwidth  UWB has the potential to communicate at high rates
Because it is spread across a wide band of frequencies  it can tolerate a substantial amount of relatively strong interference from other narrowband signals
Just as importantly  since UWB has very little energy at any given frequency when used for short-range transmission  it does not cause harmful interference to those other narrowband radio signals
It is said to underlay the other signals
This peaceful coexistence has led to its application in wireless PANs that run at up to  Gbps  although commercial success has been mixed
It can also be used for imaging through solid objects (ground  walls  and bodies) or as part of precise location systems
WIRELESS TRANSMISSION We will now discuss how the various parts of the electromagnetic spectrum of Fig
We will assume that all transmissions use a narrow frequency band unless otherwise stated
Radio Transmission Radio frequency (RF) waves are easy to generate  can travel long distances  and can penetrate buildings easily  so they are widely used for communication  both indoors and outdoors
Radio waves also are omnidirectional  meaning that they travel in all directions from the source  so the transmitter and receiver do not have to be carefully aligned physically
Sometimes omnidirectional radio is good  but sometimes it is bad
In the s  General Motors decided to equip all its new Cadillacs with computer-controlled antilock brakes
When the driver stepped on the brake pedal  the computer pulsed the brakes on and off instead of locking them on hard
One fine day an Ohio Highway Patrolman began using his new mobile radio to call headquarters  and suddenly the Cadillac next to him began behaving like a bucking bronco
When the officer pulled the car over  the driver claimed that he had done nothing and that the car had gone crazy
Eventually  a pattern began to emerge: Cadillacs would sometimes go berserk  but only on major highways in Ohio and then only when the Highway Patrol was watching
For a long  long time General Motors could not understand why Cadillacs worked fine in all the other states and also on minor roads in Ohio
Only after much searching did they discover that the Cadillacâs wiring made a fine antenna for the frequency used by the Ohio Highway Patrolâs new radio system
The properties of radio waves are frequency dependent
At low frequencies  radio waves pass through obstacles well  but the power falls off sharply with distance from the sourceâat least as fast as  /r  in airâas the signal energy is spread more thinly over a larger surface
This attenuation is called path loss
At high frequencies  radio waves tend to travel in straight lines and bounce off obstacles
Path loss still reduces power  though the received signal can depend strongly on reflections as well
High-frequency radio waves are also absorbed by rain and other obstacles to a larger extent than are low-frequency ones
At all frequencies  radio waves are subject to interference from motors and other electrical equipment
It is interesting to compare the attenuation of radio waves to that of signals in guided media
With fiber  coax and twisted pair  the signal drops by the same fraction per unit distance  for example   dB per   m for twisted pair
With radio  the signal drops by the same fraction as the distance doubles  for example  dB per doubling in free space
This behavior means that radio waves can travel long distances  and interference between users is a problem
For this reason  all governments tightly regulate the use of radio transmitters  with few notable exceptions  which are discussed later in this  ter
THE PHYSICAL LAYER
In the VLF  LF  and MF bands  radio waves follow the ground  as illustrated in Fig
These waves can be detected for perhaps  km at the lower frequencies  less at the higher ones
AM radio broadcasting uses the MF band  which is why the ground waves from Boston AM radio stations cannot be heard easily in New York
Radio waves in these bands pass through buildings easily  which is why portable radios work indoors
The main problem with using these bands for data communication is their low bandwidth [see Eq
e r e h p s o n o I Earth's surface Earth's surface (a) (b) Ground wave Figure  -
(a) In the VLF  LF  and MF bands  radio waves follow the curvature of the earth
(b) In the HF band  they bounce off the ionosphere
In the HF and VHF bands  the ground waves tend to be absorbed by the earth
However  the waves that reach the ionosphere  a layer of charged particles circling the earth at a height of to km  are refracted by it and sent back to earth  as shown in Fig
Under certain atmospheric conditions  the signals can bounce several times
Amateur radio operators (hams) use these bands to talk long distance
The military also communicate in the HF and VHF bands
Microwave Transmission Above MHz  the waves travel in nearly straight lines and can therefore be narrowly focused
Concentrating all the energy into a small beam by means of a parabolic antenna (like the familiar satellite TV dish) gives a much higher signalto- noise ratio  but the transmitting and receiving antennas must be accurately aligned with each other
In addition  this directionality allows multiple transmitters lined up in a row to communicate with multiple receivers in a row without interference  provided some minimum spacing rules are observed
Before fiber optics  for decades these microwaves formed the heart of the long-distance telephone transmission system
In fact  MCI  one of AT&Tâs first competitors after it was deregulated  built its entire system with microwave communications passing between towers tens of kilometers apart
Even the companyâs name reflected this (MCI stood for Microwave Communications  Inc
MCI has since gone over to fiber and through a long series of corporate mergers and bankruptcies in the telecommunications shuffle has become part of Verizon
WIRELESS TRANSMISSION Microwaves travel in a straight line  so if the towers are too far apart  the earth will get in the way (think about a Seattle-to-Amsterdam link)
Thus  repeaters are needed periodically
The higher the towers are  the farther apart they can be
The distance between repeaters goes up very roughly with the square root of the tower height
For   -meter-high towers  repeaters can be   km apart
Unlike radio waves at lower frequencies  microwaves do not pass through buildings well
In addition  even though the beam may be well focused at the transmitter  there is still some divergence in space
Some waves may be refracted off low-lying atmospheric layers and may take slightly longer to arrive than the direct waves
The delayed waves may arrive out of phase with the direct wave and thus cancel the signal
This effect is called multipath fading and is often a serious problem
It is weather and frequency dependent
Some operators keep  % of their channels idle as spares to switch on when multipath fading temporarily wipes out some frequency band
The demand for more and more spectrum drives operators to yet higher frequencies
Bands up to   GHz are now in routine use  but at about  GHz a new problem sets in: absorption by water
These waves are only a few centimeters long and are absorbed by rain
This effect would be fine if one were planning to build a huge outdoor microwave oven for roasting passing birds  but for communication it is a severe problem
As with multipath fading  the only solution is to shut off links that are being rained on and route around them
In summary  microwave communication is so widely used for long-distance telephone communication  mobile phones  television distribution  and other purposes that a severe shortage of spectrum has developed
It has several key advantages over fiber
The main one is that no right of way is needed to lay down cables
By buying a small plot of ground every   km and putting a microwave tower on it  one can bypass the telephone system entirely
This is how MCI managed to get started as a new long-distance telephone company so quickly
(Sprint  another early competitor to the deregulated AT&T  went a completely different route: it was formed by the Southern Pacific Railroad  which already owned a large amount of right of way and just buried fiber next to the tracks
) Microwave is also relatively inexpensive
Putting up two simple towers (which can be just big poles with four guy wires) and putting antennas on each one may be cheaper than burying   km of fiber through a congested urban area or up over a mountain  and it may also be cheaper than leasing the telephone companyâs fiber  especially if the telephone company has not yet even fully paid for the copper it ripped out when it put in the fiber
The Politics of the Electromagnetic Spectrum To prevent total chaos  there are national and international agreements about who gets to use which frequencies
Since everyone wants a higher data rate  everyone wants more spectrum
National governments allocate spectrum for AM THE PHYSICAL LAYER
and FM radio  television  and mobile phones  as well as for telephone companies  police  maritime  navigation  military  government  and many other competing users
Worldwide  an agency of ITU-R (WRC) tries to coordinate this allocation so devices that work in multiple countries can be manufactured
However  countries are not bound by ITU-Râs recommendations  and the FCC (Federal Communication Commission)  which does the allocation for the United States  has occasionally rejected ITU-Râs recommendations (usually because they required some politically powerful group to give up some piece of the spectrum)
Even when a piece of spectrum has been allocated to some use  such as mobile phones  there is the additional issue of which carrier is allowed to use which frequencies
Three algorithms were widely used in the past
The oldest algorithm  often called the beauty contest  requires each carrier to explain why its proposal serves the public interest best
Government officials then decide which of the nice stories they enjoy most
Having some government official award property worth billions of dollars to his favorite company often leads to bribery  corruption  nepotism  and worse
Furthermore  even a scrupulously honest government official who thought that a foreign company could do a better job than any of the national companies would have a lot of explaining to do
This observation led to algorithm   holding a lottery among the interested companies
The problem with that idea is that companies with no interest in using the spectrum can enter the lottery
If  say  a fast food restaurant or shoe store chain wins  it can resell the spectrum to a carrier at a huge profit and with no risk
Bestowing huge windfalls on alert but otherwise random companies has been severely criticized by many  which led to algorithm  : auction off the bandwidth to the highest bidder
When the British government auctioned off the frequencies needed for third-generation mobile systems in  it expected to get about $  billion
It actually received about $  billion because the carriers got into a feeding frenzy  scared to death of missing the mobile boat
This event switched on nearby governmentsâ greedy bits and inspired them to hold their own auctions
It worked  but it also left some of the carriers with so much debt that they are close to bankruptcy
Even in the best cases  it will take many years to recoup the licensing fee
A completely different approach to allocating frequencies is to not allocate them at all
Instead  let everyone transmit at will  but regulate the power used so that stations have such a short range that they do not interfere with each other
Accordingly  most governments have set aside some frequency bands  called the ISM (Industrial  Scientific  Medical) bands for unlicensed usage
Garage door openers  cordless phones  radio-controlled toys  wireless mice  and numerous other wireless household devices use the ISM bands
To minimize interference between these uncoordinated devices  the FCC mandates that all devices in the ISM bands limit their transmit power (
to  watt) and use other techniques to spread their signals over a range of frequencies
Devices may also need to take care to avoid interference with radar installations
WIRELESS TRANSMISSION The location of these bands varies somewhat from country to country
In the United States  for example  the bands that networking devices use in practice without requiring a FCC license are shown in Fig
The   -MHz band was used for early versions of
but it is crowded
b/g and Bluetooth  though it is subject to interference from microwave ovens and radar installations
The  -GHz part of the spectrum includes U-NII (Unlicensed National Information Infrastructure) bands
The  -GHz bands are relatively undeveloped but  since they have the most bandwidth and are used by
a  they are quickly gaining in popularity
MHz MHz MHz
GHz U-NII bands
GHz ISM band
MHz MHz MHz ISM band MHz ISM band Figure  -
ISM and U-NII bands used in the United States by wireless devices
The unlicensed bands have been a roaring success over the past decade
The ability to use the spectrum freely has unleashed a huge amount of innovation in wireless LANs and PANs  evidenced by the widespread deployment of technologies such as
and Bluetooth
To continue this innovation  more spectrum is needed
One exciting development in the
is the FCC decision in  to allow unlicensed use of white spaces around MHz
White spaces are frequency bands that have been allocated but are not being used locally
The transition from analog to all-digital television broadcasts in the
in  freed up white spaces around MHz
The only difficulty is that  to use the white spaces  unlicensed devices must be able to detect any nearby licensed transmitters  including wireless microphones  that have first rights to use the frequency band
Another flurry of activity is happening around the  -GHz band
The FCC opened   GHz to   GHz for unlicensed operation in
This range is an enormous portion of spectrum  more than all the other ISM bands combined  so it can support the kind of high-speed networks that would be needed to stream high-definition TV through the air across your living room
At   GHz  radio THE PHYSICAL LAYER
waves are absorbed by oxygen
This means that signals do not propagate far  making them well suited to short-range networks
The high frequencies (  GHz is in the Extremely High Frequency or ââmillimeterââ band  just below infrared radiation) posed an initial challenge for equipment makers  but products are now on the market
Infrared Transmission Unguided infrared waves are widely used for short-range communication
The remote controls used for televisions  VCRs  and stereos all use infrared communication
They are relatively directional  cheap  and easy to build but have a major drawback: they do not pass through solid objects
(Try standing between your remote control and your television and see if it still works
) In general  as we go from long-wave radio toward visible light  the waves behave more and more like light and less and less like radio
On the other hand  the fact that infrared waves do not pass through solid walls well is also a plus
It means that an infrared system in one room of a building will not interfere with a similar system in adjacent rooms or buildings: you cannot control your neighborâs television with your remote control
Furthermore  urity of infrared systems against eavesdropping is better than that of radio systems precisely for this reason
Therefore  no government license is needed to operate an infrared system  in contrast to radio systems  which must be licensed outside the ISM bands
Infrared communication has a limited use on the desktop  for example  to connect notebook computers and printers with the IrDA (Infrared Data Association) standard  but it is not a major player in the communication game
Light Transmission Unguided optical signaling or free-space optics has been in use for centuries
Paul Revere used binary optical signaling from the Old North Church just prior to his famous ride
A more modern application is to connect the LANs in two buildings via lasers mounted on their rooftops
Optical signaling using lasers is inherently unidirectional  so each end needs its own laser and its own photodetector
This scheme offers very high bandwidth at very low cost and is relatively ure because it is difficult to tap a narrow laser beam
It is also relatively easy to install and  unlike microwave transmission  does not require an FCC license
The laserâs strength  a very narrow beam  is also its weakness here
Aiming a laser beam  mm wide at a target the size of a pin head meters away requires the marksmanship of a latter-day Annie Oakley
Usually  lenses are put into the system to defocus the beam slightly
To add to the difficulty  wind and temperature changes can distort the beam and laser beams also cannot penetrate rain or thick fog  although they normally work well on sunny days
However  many of these factors are not an issue when the use is to connect two spacecraft
WIRELESS TRANSMISSION One of the authors (AST) once attended a conference at a modern hotel in Europe at which the conference organizers thoughtfully provided a room full of terminals to allow the attendees to read their email during boring presentations
Since the local PTT was unwilling to install a large number of telephone lines for just  days  the organizers put a laser on the roof and aimed it at their universityâs computer science building a few kilometers away
They tested it the night before the conference and it worked perfectly
on a bright  sunny day  the link failed completely and stayed down all day
The pattern repeated itself the next two days
It was not until after the conference that the organizers discovered the problem: heat from the sun during the daytime caused convection currents to rise up from the roof of the building  as shown in Fig
This turbulent air diverted the beam and made it dance around the detector  much like a shimmering road on a hot day
The lesson here is that to work well in difficult conditions as well as good conditions  unguided optical links need to be engineered with a sufficient margin of error
Laser beam misses the detector Laser Photodetector Region of turbulent seeing Heat rising off the building Figure  -
Convection currents can interfere with laser communication systems
A bidirectional system with two lasers is pictured here
Unguided optical communication may seem like an exotic networking technology today  but it might soon become much more prevalent
We are surrounded THE PHYSICAL LAYER
by cameras (that sense light) and displays (that emit light using LEDs and other technology)
Data communication can be layered on top of these displays by encoding information in the pattern at which LEDs turn on and off that is below the threshold of human perception
Communicating with visible light in this way is inherently safe and creates a low-speed network in the immediate vicinity of the display
This could enable all sorts of fanciful ubiquitous computing scenarios
The flashing lights on emergency vehicles might alert nearby traffic lights and vehicles to help clear a path
Informational signs might broadcast maps
Even festive lights might broadcast songs that are synchronized with their display  COMMUNICATION SATELLITES In the s and early s  people tried to set up communication systems by bouncing signals off metallized weather balloons
Unfortunately  the received signals were too weak to be of any practical use
Navy noticed a kind of permanent weather balloon in the skyâthe moonâand built an operational system for ship-to-shore communication by bouncing signals off it
Further progress in the celestial communication field had to wait until the first communication satellite was launched
The key difference between an artificial satellite and a real one is that the artificial one can amplify the signals before sending them back  turning a strange curiosity into a powerful communication system
Communication satellites have some interesting properties that make them attractive for many applications
In its simplest form  a communication satellite can be thought of as a big microwave repeater in the sky
It contains several transponders  each of which listens to some portion of the spectrum  amplifies the incoming signal  and then rebroadcasts it at another frequency to avoid interference with the incoming signal
This mode of operation is known as a bent pipe
Digital processing can be added to separately manipulate or redirect data streams in the overall band  or digital information can even be received by the satellite and rebroadcast
Regenerating signals in this way improves performance compared to a bent pipe because the satellite does not amplify noise in the upward signal
The downward beams can be broad  covering a substantial fraction of the earthâs surface  or narrow  covering an area only hundreds of kilometers in diameter
According to Keplerâs law  the orbital period of a satellite varies as the radius of the orbit to the  /  power
The higher the satellite  the longer the period
Near the surface of the earth  the period is about   minutes
Consequently  low-orbit satellites pass out of view fairly quickly  so many of them are needed to provide continuous coverage and ground antennas must track them
At an altitude of about   km  the period is   hours
At an altitude of    km  the period is about one month  as anyone who has observed the moon regularly can testify
COMMUNICATION SATELLITES A satelliteâs period is important  but it is not the only issue in determining where to place it
Another issue is the presence of the Van Allen belts  layers of highly charged particles trapped by the earthâs magnetic field
Any satellite flying within them would be destroyed fairly quickly by the particles
These factors lead to three regions in which satellites can be placed safely
These regions and some of their properties are illustrated in Fig
Below we will briefly describe the satellites that inhabit each of these regions
Altitude (km) Type        GEO MEO Upper Van Allen belt Lower Van Allen belt LEO Latency (ms)  â   â  Sats needed    Figure  -
Communication satellites and some of their properties  including altitude above the earth  round-trip delay time  and number of satellites needed for global coverage
Geostationary Satellites In  the science fiction writer Arthur C
Clarke calculated that a satellite at an altitude of   km in a circular equatorial orbit would appear to remain motionless in the sky  so it would not need to be tracked (Clarke  )
He went on to describe a complete communication system that used these (manned) geostationary satellites  including the orbits  solar panels  radio frequencies  and launch procedures
Unfortunately  he concluded that satellites were impractical due to the impossibility of putting power-hungry  fragile vacuum tube amplifiers into orbit  so he never pursued this idea further  although he wrote some science fiction stories about it
The invention of the transistor changed all that  and the first artificial communication satellite  Telstar  was launched in July
Since then  communication satellites have become a multibillion dollar business and the only aspect of outer space that has become highly profitable
These high-flying satellites are often called GEO (Geostationary Earth Orbit) satellites
THE PHYSICAL LAYER
With current technology  it is unwise to have geostationary satellites spaced much closer than  degrees in the   -degree equatorial plane  to avoid interference
With a spacing of  degrees  there can only be   /  = of these satellites in the sky at once
However  each transponder can use multiple frequencies and polarizations to increase the available bandwidth
To prevent total chaos in the sky  orbit slot allocation is done by ITU
This process is highly political  with countries barely out of the stone age demanding ââtheirââ orbit slots (for the purpose of leasing them to the highest bidder)
Other countries  however  maintain that national property rights do not extend up to the moon and that no country has a legal right to the orbit slots above its territory
To add to the fight  commercial telecommunication is not the only application
Television broadcasters  governments  and the military also want a piece of the orbiting pie
Modern satellites can be quite large  weighing over  kg and consuming several kilowatts of electric power produced by the solar panels
The effects of solar  lunar  and planetary gravity tend to move them away from their assigned orbit slots and orientations  an effect countered by on-board rocket motors
This fine-tuning activity is called station keeping
However  when the fuel for the motors has been exhausted (typically after about   years) the satellite drifts and tumbles helplessly  so it has to be turned off
Eventually  the orbit decays and the satellite reenters the atmosphere and burns up (or very rarely crashes to earth)
Orbit slots are not the only bone of contention
Frequencies are an issue  too  because the downlink transmissions interfere with existing microwave users
Consequently  ITU has allocated certain frequency bands to satellite users
The main ones are listed in Fig
The C band was the first to be designated for commercial satellite traffic
Two frequency ranges are assigned in it  the lower one for downlink traffic (from the satellite) and the upper one for uplink traffic (to the satellite)
To allow traffic to go both ways at the same time  two channels are required
These channels are already overcrowded because they are also used by the common carriers for terrestrial microwave links
The L and S bands were added by international agreement in
However  they are narrow and also crowded
Band Downlink Uplink Bandwidth Problems L
GHz   MHz Low bandwidth; crowded S
GHz   MHz Low bandwidth; crowded C
GHz MHz Terrestrial interference Ku   GHz   GHz MHz Rain Ka   GHz   GHz  MHz Rain  equipment cost Figure  -
The principal satellite bands
COMMUNICATION SATELLITES The next-highest band available to commercial telecommunication carriers is the Ku (K under) band
This band is not (yet) congested  and at its higher frequencies  satellites can be spaced as close as  degree
However  another problem exists: rain
Water absorbs these short microwaves well
Fortunately  heavy storms are usually localized  so using several widely separated ground stations instead of just one circumvents the problem  but at the price of extra antennas  extra cables  and extra electronics to enable rapid switching between stations
Bandwidth has also been allocated in the Ka (K above) band for commercial satellite traffic  but the equipment needed to use it is expensive
In addition to these commercial bands  many government and military bands also exist
A modern satellite has around   transponders  most often with a  -MHz bandwidth
Usually  each transponder operates as a bent pipe  but recent satellites have some on-board processing capacity  allowing more sophisticated operation
In the earliest satellites  the division of the transponders into channels was static: the bandwidth was simply split up into fixed frequency bands
Nowadays  each transponder beam is divided into time slots  with various users taking turns
We will study these two techniques (frequency division multiplexing and time division multiplexing) in detail later in this  ter
The first geostationary satellites had a single spatial beam that illuminated about  /  of the earthâs surface  called its footprint
With the enormous decline in the price  size  and power requirements of microelectronics  a much more sophisticated broadcasting strategy has become possible
Each satellite is equipped with multiple antennas and multiple transponders
Each downward beam can be focused on a small geographical area  so multiple upward and downward transmissions can take place simultaneously
Typically  these so-called spot beams are elliptically shaped  and can be as small as a few hundred km in diameter
A communication satellite for the United States typically has one wide beam for the contiguous   states  plus spot beams for Alaska and Hawaii
A recent development in the communication satellite world is the development of low-cost microstations  sometimes called VSATs (Very Small Aperture Terminals) (Abramson  )
These tiny terminals have  -meter or smaller antennas (versus   m for a standard GEO antenna) and can put out about  watt of power
The uplink is generally good for up to  Mbps  but the downlink is often up to several megabits/
Direct broadcast satellite television uses this technology for one-way transmission
In many VSAT systems  the microstations do not have enough power to communicate directly with one another (via the satellite  of course)
Instead  a special ground station  the hub  with a large  high-gain antenna is needed to relay traffic between VSATs  as shown in Fig
In this mode of operation  either the sender or the receiver has a large antenna and a powerful amplifier
The trade-off is a longer delay in return for having cheaper end-user stations
VSATs have great potential in rural areas
It is not widely appreciated  but over half the worldâs population lives more than hourâs walk from the nearest THE PHYSICAL LAYER
Communication satellite    Hub VSAT Figure  -
VSATs using a hub
Stringing telephone wires to thousands of small villages is far beyond the budgets of most Third World governments  but installing  -meter VSAT dishes powered by solar cells is often feasible
VSATs provide the technology that will wire the world
Communication satellites have several properties that are radically different from terrestrial point-to-point links
To begin with  even though signals to and from a satellite travel at the speed of light (nearly    km/)  the long round-trip distance introduces a substantial delay for GEO satellites
Depending on the distance between the user and the ground station and the elevation of the satellite above the horizon  the end-to-end transit time is between and m
A typical value is m (   m for a VSAT system with a hub)
For comparison purposes  terrestrial microwave links have a propagation delay of roughly  Î¼ /km  and coaxial cable or fiber optic links have a delay of approximately  Î¼/km
The latter are slower than the former because electromagnetic signals travel faster in air than in solid materials
Another important property of satellites is that they are inherently broadcast media
It does not cost more to send a message to thousands of stations within a transponderâs footprint than it does to send to one
For some applications  this property is very useful
For example  one could imagine a satellite broadcasting popular Web pages to the caches of a large number of computers spread over a wide area
Even when broadcasting can be simulated with point-to-point lines    COMMUNICATION SATELLITES satellite broadcasting may be much cheaper
On the other hand  from a privacy point of view  satellites are a complete disaster: everybody can hear everything
Encryption is essential when urity is required
Satellites also have the property that the cost of transmitting a message is independent of the distance traversed
A call across the ocean costs no more to service than a call across the street
Satellites also have excellent error rates and can be deployed almost instantly  a major consideration for disaster response and military communication
Medium-Earth Orbit Satellites At much lower altitudes  between the two Van Allen belts  we find the MEO (Medium-Earth Orbit) satellites
As viewed from the earth  these drift slowly in longitude  taking something like  hours to circle the earth
Accordingly  they must be tracked as they move through the sky
Because they are lower than the GEOs  they have a smaller footprint on the ground and require less powerful transmitters to reach them
Currently they are used for navigation systems rather than telecommunications  so we will not examine them further here
The constellation of roughly   GPS (Global Positioning System) satellites orbiting at about   km are examples of MEO satellites
Low-Earth Orbit Satellites Moving down in altitude  we come to the LEO (Low-Earth Orbit) satellites
Due to their rapid motion  large numbers of them are needed for a complete system
On the other hand  because the satellites are so close to the earth  the ground stations do not need much power  and the round-trip delay is only a few millionds
The launch cost is substantially cheaper too
In this tion we will examine two examples of satellite constellations for voice service  Iridium and Globalstar
For the first   years of the satellite era  low-orbit satellites were rarely used because they zip into and out of view so quickly
In  Motorola broke new ground by filing an application with the FCC asking for permission to launch   low-orbit satellites for the Iridium project (element   is iridium)
The plan was later revised to use only   satellites  so the project should have been renamed Dysprosium (element  )  but that probably sounded too much like a disease
The idea was that as soon as one satellite went out of view  another would replace it
This proposal set off a feeding frenzy among other communication companies
All of a sudden  everyone wanted to launch a chain of low-orbit satellites
After seven years of cobbling together partners and financing  communication service began in November
Unfortunately  the commercial demand for large  heavy satellite telephones was negligible because the mobile phone network had grown in a spectacular way since
As a consequence  Iridium was not THE PHYSICAL LAYER
profitable and was forced into bankruptcy in August  in one of the most spectacular corporate fiascos in history
The satellites and other assets (worth $  billion) were later purchased by an investor for $  million at a kind of extraterrestrial garage sale
Other satellite business ventures promptly followed suit
The Iridium service restarted in March  and has been growing ever since
It provides voice  data  paging  fax  and navigation service everywhere on land  air  and sea  via hand-held devices that communicate directly with the Iridium satellites
Customers include the maritime  aviation  and oil exploration industries  as well as people traveling in parts of the world lacking a telecom infrastructure (
deserts  mountains  the South Pole  and some Third World countries)
The Iridium satellites are positioned at an altitude of km  in circular polar orbits
They are arranged in north-south necklaces  with one satellite every   degrees of latitude  as shown in Fig
Each satellite has a maximum of   cells (spot beams) and a capacity of  channels  some of which are used for paging and navigation  while others are used for data and voice
Each satellite has four neighbors Figure  -
The Iridium satellites form six necklaces around the earth
With six satellite necklaces the entire earth is covered  as suggested by Fig
An interesting property of Iridium is that communication between distant customers takes place in space  as shown in Fig
Here we see a caller at the North Pole contacting a satellite directly overhead
Each satellite has four neighbors with which it can communicate  two in the same necklace (shown) and two in adjacent necklaces (not shown)
The satellites relay the call across this grid until it is finally sent down to the callee at the South Pole
An alternative design to Iridium is Globalstar
It is based on   LEO satellites but uses a different switching scheme than that of Iridium
Whereas Iridium relays calls from satellite to satellite  which requires sophisticated switching equipment in the satellites  Globalstar uses a traditional bent-pipe design
The call originating at the North Pole in Fig
-  (b) is sent back to earth and picked   COMMUNICATION SATELLITES Bent-pipe satellite Satellite switches in space Switching on the ground (a) (b) Figure  -
(a) Relaying in space
(b) Relaying on the ground
up by the large ground station at Santaâs Workshop
The call is then routed via a terrestrial network to the ground station nearest the callee and delivered by a bent-pipe connection as shown
The advantage of this scheme is that it puts much of the complexity on the ground  where it is easier to manage
Also  the use of large ground station antennas that can put out a powerful signal and receive a weak one means that lower-powered telephones can be used
After all  the telephone puts out only a few milliwatts of power  so the signal that gets back to the ground station is fairly weak  even after having been amplified by the satellite
Satellites continue to be launched at a rate of around   per year  including ever-larger satellites that now weigh over  kilograms
But there are also very small satellites for the more budget-conscious organization
To make space research more accessible  academics from Cal Poly and Stanford got together in  to define a standard for miniature satellites and an associated launcher that would greatly lower launch costs (Nugent et al
CubeSats are satellites in units of   cm Ã   cm Ã   cm cubes  each weighing no more than  kilogram  that can be launched for as little as $  each
The launcher flies as a ondary payload on commercial space missions
It is basically a tube that takes up to three units of cubesats and uses springs to release them into orbit
Roughly   cubesats have launched so far  with many more in the works
Most of them communicate with ground stations on the UHF and VHF bands
Satellites Versus Fiber A comparison between satellite communication and terrestrial communication is instructive
As recently as   years ago  a case could be made that the future of communication lay with communication satellites
After all  the telephone system THE PHYSICAL LAYER
had changed little in the previous years and showed no signs of changing in the next years
This glacial movement was caused in no small part by the regulatory environment in which the telephone companies were expected to provide good voice service at reasonable prices (which they did)  and in return got a guaranteed profit on their investment
For people with data to transmit  -bps modems were available
That was pretty much all there was
The introduction of competition in  in the United States and somewhat later in Europe changed all that radically
Telephone companies began replacing their long-haul networks with fiber and introduced high-bandwidth services like ADSL (Asymmetric Digital Subscriber Line)
They also stopped their long-time practice of charging artificially high prices to long-distance users to subsidize local service
All of a sudden  terrestrial fiber connections looked like the winner
Nevertheless  communication satellites have some major niche markets that fiber does not (and  sometimes  cannot) address
First  when rapid deployment is critical  satellites win easily
A quick response is useful for military communication systems in times of war and disaster response in times of peace
Following the massive December  Sumatra earthquake and subsequent tsunami  for example  communications satellites were able to restore communications to first responders within   hours
This rapid response was possible because there is a developed satellite service provider market in which large players  such as Intelsat with over   satellites  can rent out capacity pretty much anywhere it is needed
For customers served by existing satellite networks  a VSAT can be set up easily and quickly to provide a megabit/ link to elsewhere in the world
A ond niche is for communication in places where the terrestrial infrastructure is poorly developed
Many people nowadays want to communicate everywhere they go
Mobile phone networks cover those locations with good population density  but do not do an adequate job in other places (
at sea or in the desert)
Conversely  Iridium provides voice service everywhere on Earth  even at the South Pole
Terrestrial infrastructure can also be expensive to install  depending on the terrain and necessary rights of way
Indonesia  for example  has its own satellite for domestic telephone traffic
Launching one satellite was cheaper than stringing thousands of undersea cables among the   islands in the archipelago
A third niche is when broadcasting is essential
A message sent by satellite can be received by thousands of ground stations at once
Satellites are used to distribute much network TV programming to local stations for this reason
There is now a large market for satellite broadcasts of digital TV and radio directly to end users with satellite receivers in their homes and cars
All sorts of other content can be broadcast too
For example  an organization transmitting a stream of stock  bond  or commodity prices to thousands of dealers might find a satellite system to be much cheaper than simulating broadcasting on the ground
In short  it looks like the mainstream communication of the future will be terrestrial fiber optics combined with cellular radio  but for some specialized uses    COMMUNICATION SATELLITES satellites are better
However  there is one caveat that applies to all of this: economics
Although fiber offers more bandwidth  it is conceivable that terrestrial and satellite communication could compete aggressively on price
If advances in technology radically cut the cost of deploying a satellite (
if some future space vehicle can toss out dozens of satellites on one launch) or low-orbit satellites catch on in a big way  it is not certain that fiber will win all markets  DIGITAL MODULATION AND MULTIPLEXING Now that we have studied the properties of wired and wireless channels  we turn our attention to the problem of sending digital information
Wires and wireless channels carry analog signals such as continuously varying voltage  light intensity  or sound intensity
To send digital information  we must devise analog signals to represent bits
The process of converting between bits and signals that represent them is called digital modulation
We will start with schemes that directly convert bits into a signal
These schemes result in baseband transmission  in which the signal occupies frequencies from zero up to a maximum that depends on the signaling rate
It is common for wires
Then we will consider schemes that regulate the amplitude  phase  or frequency of a carrier signal to convey bits
These schemes result in passband transmission  in which the signal occupies a band of frequencies around the frequency of the carrier signal
It is common for wireless and optical channels for which the signals must reside in a given frequency band
Channels are often shared by multiple signals
After all  it is much more convenient to use a single wire to carry several signals than to install a wire for every signal
This kind of sharing is called multiplexing
It can be accomplished in several different ways
We will present methods for time  frequency  and code division multiplexing
The modulation and multiplexing techniques we describe in this tion are all widely used for wires  fiber  terrestrial wireless  and satellite channels
In the following tions  we will look at examples of networks to see them in action
Baseband Transmission The most straightforward form of digital modulation is to use a positive voltage to represent a  and a negative voltage to represent a
For an optical fiber  the presence of light might represent a  and the absence of light might represent a
This scheme is called NRZ (Non-Return-to-Zero)
The odd name is for historical reasons  and simply means that the signal follows the data
An example is shown in Fig
Once sent  the NRZ signal propagates down the wire
At the other end  the receiver converts it into bits by sampling the signal at regular intervals of time
THE PHYSICAL LAYER
(Clock that is XORed with bits) (a) Bit stream (b) Non-Return to Zero (NRZ) (c) NRZ Invert (NRZI) (d) Manchester (e) Bipolar encoding (also Alternate Mark Inversion  AMI)     Figure  -
Line codes: (a) Bits  (b) NRZ  (c) NRZI  (d) Manchester  (e) Bipolar or AMI
This signal will not look exactly like the signal that was sent
It will be attenuated and distorted by the channel and noise at the receiver
To decode the bits  the receiver maps the signal samples to the closest symbols
For NRZ  a positive voltage will be taken to indicate that a  was sent and a negative voltage will be taken to indicate that a  was sent
NRZ is a good starting point for our studies because it is simple  but it is seldom used by itself in practice
More complex schemes can convert bits to signals that better meet engineering considerations
These schemes are called line codes
Below  we describe line codes that help with bandwidth efficiency  clock recovery  and DC balance
Bandwidth Efficiency With NRZ  the signal may cycle between the positive and negative levels up to every  bits (in the case of alternating  s and  s)
This means that we need a bandwidth of at least B/  Hz when the bit rate is B bits/
This relation comes from the Nyquist rate [Eq
It is a fundamental limit  so we cannot run NRZ faster without using more bandwidth
Bandwidth is often a limited resource  even for wired channels  Higher-frequency signals are increasingly attenuated  making them less useful  and higher-frequency signals also require faster electronics
One strategy for using limited bandwidth more efficiently is to use more than two signaling levels
By using four voltages  for instance  we can send  bits at once as a single symbol
This design will work as long as the signal at the receiver is sufficiently strong to distinguish the four levels
The rate at which the signal changes is then half the bit rate  so the needed bandwidth has been reduced
DIGITAL MODULATION AND MULTIPLEXING We call the rate at which the signal changes the symbol rate to distinguish it from the bit rate
The bit rate is the symbol rate multiplied by the number of bits per symbol
An older name for the symbol rate  particularly in the context of devices called telephone modems that convey digital data over telephone lines  is the baud rate
In the literature  the terms ââbit rateââ and ââbaud rateââ are often used incorrectly
Note that the number of signal levels does not need to be a power of two
Often it is not  with some of the levels used for protecting against errors and simplifying the design of the receiver
Clock Recovery For all schemes that encode bits into symbols  the receiver must know when one symbol ends and the next symbol begins to correctly decode the bits
With NRZ  in which the symbols are simply voltage levels  a long run of  s or  s leaves the signal unchanged
After a while it is hard to tell the bits apart  as   zeros look much like   zeros unless you have a very accurate clock
Accurate clocks would help with this problem  but they are an expensive solution for commodity equipment
Remember  we are timing bits on links that run at many megabits/  so the clock would have to drift less than a fraction of a microond over the longest permitted run
This might be reasonable for slow links or short messages  but it is not a general solution
One strategy is to send a separate clock signal to the receiver
Another clock line is no big deal for computer buses or short cables in which there are many lines in parallel  but it is wasteful for most network links since if we had another line to send a signal we could use it to send data
A clever trick here is to mix the clock signal with the data signal by XORing them together so that no extra line is needed
The results are shown in Fig
The clock makes a clock transition in every bit time  so it runs at twice the bit rate
When it is XORed with the  level it makes a low-to-high transition that is simply the clock
This transition is a logical
When it is XORed with the  level it is inverted and makes a high-tolow transition
This transition is a logical
This scheme is called Manchester encoding and was used for classic Ethernet
The downside of Manchester encoding is that it requires twice as much bandwidth as NRZ because of the clock  and we have learned that bandwidth often matters
A different strategy is based on the idea that we should code the data to ensure that there are enough transitions in the signal
Consider that NRZ will have clock recovery problems only for long runs of  s and  s
If there are frequent transitions  it will be easy for the receiver to stay synchronized with the incoming stream of symbols
As a step in the right direction  we can simplify the situation by coding a  as a transition and a  as no transition  or vice versa
This coding is called NRZI (Non-Return-to-Zero Inverted)  a twist on NRZ
An example is shown in THE PHYSICAL LAYER
The popular USB (Universal Serial Bus) standard for connecting computer peripherals uses NRZI
With it  long runs of  s do not cause a problem
Of course  long runs of  s still cause a problem that we must fix
If we were the telephone company  we might simply require that the sender not transmit too many  s
Older digital telephone lines in the
called T  lines  did in fact require that no more than   conutive  s be sent for them to work correctly
To really fix the problem we can break up runs of  s by mapping small groups of bits to be transmitted so that groups with successive  s are mapped to slightly longer patterns that do not have too many conutive  s
A well-known code to do this is called  B/ B
Every  bits is mapped into a -bit pattern with a fixed translation table
The five bit patterns are chosen so that there will never be a run of more than three conutive  s
The mapping is shown in Fig
This scheme adds  % overhead  which is better than the   % overhead of Manchester encoding
Since there are   input combinations and   output combinations  some of the output combinations are not used
Putting aside the combinations with too many successive  s  there are still some codes left
As a bonus  we can use these nondata codes to represent physical layer control signals
For example  in some uses ââ ââ represents an idle line and ââ ââ represents the start of a frame
Data ( B) Codeword ( B) Data ( B) Codeword ( B)                         Figure  -
B/ B mapping
An alternative approach is to make the data look random  known as scrambling
In this case it is very likely that there will be frequent transitions
A scrambler works by XORing the data with a pseudorandom sequence before it is transmitted
This mixing will make the data as random as the pseudorandom sequence (assuming it is independent of the pseudorandom sequence)
The receiver then XORs the incoming bits with the same pseudorandom sequence to recover the real data
For this to be practical  the pseudorandom sequence must be easy to create
It is commonly given as the seed to a simple random number generator
Scrambling is attractive because it adds no bandwidth or time overhead
In fact  it often helps to condition the signal so that it does not have its energy in   DIGITAL MODULATION AND MULTIPLEXING dominant frequency components (caused by repetitive data patterns) that might radiate electromagnetic interference
Scrambling helps because random signals tend to be ââwhite ââ or have energy spread across the frequency components
However  scrambling does not guarantee that there will be no long runs
It is possible to get unlucky occasionally
If the data are the same as the pseudorandom sequence  they will XOR to all  s
This outcome does not generally occur with a long pseudorandom sequence that is difficult to predict
However  with a short or predictable sequence  it might be possible for malicious users to send bit patterns that cause long runs of  s after scrambling and cause links to fail
Early versions of the standards for sending IP packets over SONET links in the telephone system had this defect (Malis and Simpson  )
It was possible for users to send certain ââkiller packetsââ that were guaranteed to cause problems
Balanced Signals Signals that have as much positive voltage as negative voltage even over short periods of time are called balanced signals
They average to zero  which means that they have no DC electrical component
The lack of a DC component is an advantage because some channels  such as coaxial cable or lines with transformers  strongly attenuate a DC component due to their physical properties
Also  one method of connecting the receiver to the channel called capacitive coupling passes only the AC portion of a signal
In either case  if we send a signal whose average is not zero  we waste energy as the DC component will be filtered out
Balancing helps to provide transitions for clock recovery since there is a mix of positive and negative voltages
It also provides a simple way to calibrate receivers because the average of the signal can be measured and used as a decision threshold to decode symbols
With unbalanced signals  the average may be drift away from the true decision level due to a density of  s  for example  which would cause more symbols to be decoded with errors
A straightforward way to construct a balanced code is to use two voltage levels to represent a logical   (say +  V or â  V) with  V representing a logical zero
To send a   the transmitter alternates between the +  V and â  V levels so that they always average out
This scheme is called bipolar encoding
In telephone networks it is called AMI (Alternate Mark Inversion)  building on old terminology in which a  is called a ââmarkââ and a  is called a ââspace
ââ An example is given in Fig
Bipolar encoding adds a voltage level to achieve balance
Alternatively we can use a mapping like  B/ B to achieve balance (as well as transitions for clock recovery)
An example of this kind of balanced code is the  B/  B line code
It maps  bits of input to   bits of output  so it is  % efficient  just like the  B/ B line code
The  bits are split into a group of  bits  which is mapped to  bits  and a group of  bits  which is mapped to  bits
The  -bit and  -bit symbols are THE PHYSICAL LAYER
then concatenated
In each group  some input patterns can be mapped to balanced output patterns that have the same number of  s and  s
For example  ââ  ââ is mapped to ââ ââ which is balanced
But there are not enough combinations for all output patterns to be balanced
For these cases  each input pattern is mapped to two output patterns
One will have an extra  and the alternate will have an extra
For example  ââ  ââ is mapped to both ââ   ââ and its complement ââ
ââ As input bits are mapped to output bits  the encoder remembers the disparity from the previous symbol
The disparity is the total number of  s or  s by which the signal is out of balance
The encoder then selects either an output pattern or its alternate to reduce the disparity
With  B/  B  the disparity will be at most  bits
Thus  the signal will never be far from balanced
There will also never be more than five conutive  s or  s  to help with clock recovery
Passband Transmission Often  we want to use a range of frequencies that does not start at zero to send information across a channel
For wireless channels  it is not practical to send very low frequency signals because the size of the antenna needs to be a fraction of the signal wavelength  which becomes large
In any case  regulatory constraints and the need to avoid interference usually dictate the choice of frequencies
Even for wires  placing a signal in a given frequency band is useful to let different kinds of signals coexist on the channel
This kind of transmission is called passband transmission because an arbitrary band of frequencies is used to pass the signal
Fortunately  our fundamental results from earlier in the  ter are all in terms of bandwidth  or the width of the frequency band
The absolute frequency values do not matter for capacity
This means that we can take a baseband signal that occupies  to B Hz and shift it up to occupy a passband of S to S +B Hz without changing the amount of information that it can carry  even though the signal will look different
To process a signal at the receiver  we can shift it back down to baseband  where it is more convenient to detect symbols
Digital modulation is accomplished with passband transmission by regulating or modulating a carrier signal that sits in the passband
We can modulate the amplitude  frequency  or phase of the carrier signal
Each of these methods has a corresponding name
In ASK (Amplitude Shift Keying)  two different amplitudes are used to represent  and
An example with a nonzero and a zero level is shown in Fig
More than two levels can be used to represent more symbols
Similarly  with FSK (Frequency Shift Keying)  two or more different tones are used
The example in Fig
-  (c) uses just two frequencies
In the simplest form of PSK (Phase Shift Keying)  the carrier wave is systematically shifted  or degrees at each symbol period
Because there are two phases  it is called BPSK (Binary Phase Shift Keying)
ââBinaryââ here refers to the two symbols  not that the symbols represent  bits
An example is shown in Fig
A   DIGITAL MODULATION AND MULTIPLEXING better scheme that uses the channel bandwidth more efficiently is to use four shifts
or degrees  to transmit  bits of information per symbol
This version is called QPSK (Quadrature Phase Shift Keying)
Phase changes  (a) (b) (c) (d)    Figure  -
(a) A binary signal
(b) Amplitude shift keying
(c) Frequency shift keying
(d) Phase shift keying
We can combine these schemes and use more levels to transmit more bits per symbol
Only one of frequency and phase can be modulated at a time because they are related  with frequency being the rate of change of phase over time
Usually  amplitude and phase are modulated in combination
Three examples are shown in Fig
In each example  the points give the legal amplitude and phase combinations of each symbol
The phase of a dot is indicated by the angle a line from it to the origin makes with the positive x-axis
The amplitude of a dot is the distance from the origin
This figure is a representation of QPSK
This kind of diagram is called a constellation diagram
Sixteen combinations of amplitudes and phase are used  so the modulation scheme can be used to transmit THE PHYSICAL LAYER
(a)    (b)  (c)  Figure  -
bits per symbol
It is called QAM-   where QAM stands for Quadrature Amplitude Modulation
Figure  -  (c) is a still denser modulation scheme with   different combinations  so  bits can be transmitted per symbol
It is called QAM-
Even higher-order QAMs are used too
As you might suspect from these constellations  it is easier to build electronics to produce symbols as a combination of values on each axis than as a combination of amplitude and phase values
That is why the patterns look like squares rather than concentric circles
The constellations we have seen so far do not show how bits are assigned to symbols
When making the assignment  an important consideration is that a small burst of noise at the receiver not lead to many bit errors
This might happen if we assigned conutive bit values to adjacent symbols
With QAM-   for example  if one symbol stood for  and the neighboring symbol stood for  if the receiver mistakenly picks the adjacent symbol it will cause all of the bits to be wrong
A better solution is to map bits to symbols so that adjacent symbols differ in only  bit position
This mapping is called a Gray code
Now if the receiver decodes the symbol in error  it will make only a single bit error in the expected case that the decoded symbol is close to the transmitted symbol
Frequency Division Multiplexing The modulation schemes we have seen let us send one signal to convey bits along a wired or wireless link
However  economies of scale play an important role in how we use networks
It costs essentially the same amount of money to install and maintain a high-bandwidth transmission line as a low-bandwidth line between two different offices (
the costs come from having to dig the trench and not from what kind of cable or fiber goes into it)
Consequently  multiplexing schemes have been developed to share lines among many signals
DIGITAL MODULATION AND MULTIPLEXING A B C D E When  is sent: Point Decodes as Bit errors A   B   C   D   E              Q I Figure  -
Gray-coded QAM-
FDM (Frequency Division Multiplexing) takes advantage of passband transmission to share a channel
It divides the spectrum into frequency bands  with each user having exclusive possession of some band in which to send their signal
AM radio broadcasting illustrates FDM
The allocated spectrum is about  MHz  roughly to  kHz
Different frequencies are allocated to different logical channels (stations)  each operating in a portion of the spectrum  with the interchannel separation great enough to prevent interference
For a more detailed example  in Fig
Filters limit the usable bandwidth to about  Hz per voice-grade channel
When many channels are multiplexed together  Hz is allocated per channel
The excess is called a guard band
It keeps the channels well separated
First the voice channels are raised in frequency  each by a different amount
Then they can be combined because no two channels now occupy the same portion of the spectrum
Notice that even though there are gaps between the channels thanks to the guard bands  there is some overlap between adjacent channels
The overlap is there because real filters do not have ideal sharp edges
This means that a strong spike at the edge of one channel will be felt in the adjacent one as nonthermal noise
This scheme has been used to multiplex calls in the telephone system for many years  but multiplexing in time is now preferred instead
However  FDM continues to be used in telephone networks  as well as cellular  terrestrial wireless  and satellite networks at a higher level of granularity
When sending digital data  it is possible to divide the spectrum efficiently without using guard bands
In OFDM (Orthogonal Frequency Division Multiplexing)  the channel bandwidth is divided into many subcarriers that independently send data (
The subcarriers are packed tightly together in the frequency domain
Thus  signals from each subcarrier extend into adjacent ones
However  as seen in Fig
Channel  Channel  Channel    Attenuation factor   Frequency (kHz) (c) Channel  Channel  Channel  Frequency (kHz) (b) Frequency (Hz) (a)  Figure  -
Frequency division multiplexing
(a) The original bandwidths
(b) The bandwidths raised in frequency
(c) The multiplexed channel
designed so that it is zero at the center of the adjacent subcarriers
The subcarriers can therefore be sampled at their center frequencies without interference from their neighbors
To make this work  a guard time is needed to repeat a portion of the symbol signals in time so that they have the desired frequency response
However  this overhead is much less than is needed for many guard bands
Frequency Power f  f  f  f  f  Separation f One OFDM subcarrier(shaded) Figure  -
Orthogonal frequency division multiplexing (OFDM)
The idea of OFDM has been around for a long time  but it is only in the last decade that it has been widely adopted  following the realization that it is possible   DIGITAL MODULATION AND MULTIPLEXING to implement OFDM efficiently in terms of a Fourier transform of digital data over all subcarriers (instead of separately modulating each subcarrier)
OFDM is used in
cable networks and power line networking  and is planned for fourth-generation cellular systems
Usually  one high-rate stream of digital information is split into many low-rate streams that are transmitted on the subcarriers in parallel
This division is valuable because degradations of the channel are easier to cope with at the subcarrier level; some subcarriers may be very degraded and excluded in favor of subcarriers that are received well
Time Division Multiplexing An alternative to FDM is TDM (Time Division Multiplexing)
Here  the users take turns (in a round-robin fashion)  each one periodically getting the entire bandwidth for a little burst of time
An example of three streams being multiplexed with TDM is shown in Fig
Bits from each input stream are taken in a fixed time slot and output to the aggregate stream
This stream runs at the sum rate of the individual streams
For this to work  the streams must be synchronized in time
Small intervals of guard time analogous to a frequency guard band may be added to accommodate small timing variations
Round-robin TDM multiplexer    Guard time  Figure  -
Time Division Multiplexing (TDM)
TDM is used widely as part of the telephone and cellular networks
To avoid one point of confusion  let us be clear that it is quite different from the alternative STDM (Statistical Time Division Multiplexing)
The prefix ââstatisticalââ is added to indicate that the individual streams contribute to the multiplexed stream not on a fixed schedule  but according to the statistics of their demand
STDM is packet switching by another name
Code Division Multiplexing There is a third kind of multiplexing that works in a completely different way than FDM and TDM
CDM (Code Division Multiplexing) is a form of spread spectrum communication in which a narrowband signal is spread out over a wider frequency band
This can make it more tolerant of interference  as well as allowing multiple signals from different users to share the same frequency band
Because code division multiplexing is mostly used for the latter purpose it is commonly called CDMA (Code Division Multiple Access)
THE PHYSICAL LAYER
CDMA allows each station to transmit over the entire frequency spectrum all the time
Multiple simultaneous transmissions are separated using coding theory
Before getting into the algorithm  let us consider an analogy: an airport lounge with many pairs of people conversing
TDM is comparable to pairs of people in the room taking turns speaking
FDM is comparable to the pairs of people speaking at different pitches  some high-pitched and some low-pitched such that each pair can hold its own conversation at the same time as but independently of the others
CDMA is comparable to each pair of people talking at once  but in a different language
The French-speaking couple just hones in on the French  rejecting everything that is not French as noise
Thus  the key to CDMA is to be able to extract the desired signal while rejecting everything else as random noise
A somewhat simplified description of CDMA follows
In CDMA  each bit time is subdivided into m short intervals called chips
Typically  there are   or chips per bit  but in the example given here we will use  chips/bit for simplicity
Each station is assigned a unique m-bit code called a chip sequence
For pedagogical purposes  it is convenient to use a bipolar notation to write these codes as sequences of â  and +
We will show chip sequences in parentheses
To transmit a  bit  a station sends its chip sequence
To transmit a  bit  it sends the negation of its chip sequence
No other patterns are permitted
Thus  for m =   if station A is assigned the chip sequence (â  â  â  +  +  â  +  + )  it can send a  bit by transmiting the chip sequence and a  by transmitting (+  +  +  â  â  +  â  â )
It is really signals with these voltage levels that are sent  but it is sufficient for us to think in terms of the sequences
Increasing the amount of information to be sent from b bits/ to mb chips/ for each station means that the bandwidth needed for CDMA is greater by a factor of m than the bandwidth needed for a station not using CDMA (assuming no changes in the modulation or encoding techniques)
If we have a  -MHz band available for stations  with FDM each one would have   kHz and could send at   kbps (assuming  bit per Hz)
With CDMA  each station uses the full  MHz  so the chip rate is chips per bit to spread the stationâs bit rate of   kbps across the channel
Each station has its own unique chip sequence
Let us use the symbol S to indicate the m-chip vector for station S  and S for its negation
All chip sequences are pairwise orthogonal  by which we mean that the normalized inner product of any two distinct chip sequences  S and T (written as S T)  is
It is known how to generate such orthogonal chip sequences using a method known as Walsh codes
In mathematical terms  orthogonality of the chip sequences can be expressed as follows: S T â¡ m  i =  Î£m SiTi =  ( - )   DIGITAL MODULATION AND MULTIPLEXING In plain English  as many pairs are the same as are different
This orthogonality property will prove crucial later
Note that if S T =   then S T is also
The normalized inner product of any chip sequence with itself is  : S S = m  i =  Î£m SiSi = m  i =  Î£m Si  = m  i =  Î£m (Â± )  =  This follows because each of the m terms in the inner product is   so the sum is m
Also note that S S = â
(b) A = (â  â  â  +  +  â  +  + ) B = (â  â  +  â  +  +  +  â ) C = (â  +  â  +  +  +  â  â ) D = (â  +  â  â  â  â  +  â ) (a) (c) (d) S  = C = (â  +  â  +  +  +  â  â ) S =B+C =(â  + +  â ) S  = A+B = ( â  +   â   + ) S  = A+B+C = (â  +  â  +  +  â  â  + ) S  = A+B+C+D = (â   â   +   +  â ) S  = A+B+C+D = (â  â   â   â  +  ) S  C = [ + â + + + â â ]/  =  S  C = [ + + + + + + + ]/  =  S  C = [ + + + + â + â ]/  =  S  C = [ + + + + â + â ]/  =  S  C = [ + + + + + â + ]/  =  S  C = [ â + â + â â + ]/  = â  Figure  -
(a) Chip sequences for four stations
(b) Signals the sequences represent (c) Six examples of transmissions
(d) Recovery of station Câs signal
During each bit time  a station can transmit a  (by sending its chip sequence)  it can transmit a  (by sending the negative of its chip sequence)  or it can be silent and transmit nothing
We assume for now that all stations are synchronized in time  so all chip sequences begin at the same instant
When two or more stations transmit simultaneously  their bipolar sequences add linearly
For example  if in one chip period three stations output +  and one station outputs â  +  will be received
One can think of this as signals that add as voltages superimposed on the channel: three stations output +  V and one station outputs â  V  so that  V is received
For instance  in Fig
-  (c) we see six examples of one or more stations transmitting  bit at the same time
In the first example  C transmits a  bit  so we just get Câs chip sequence
In the ond example  both B and C transmit  bits  so we get the sum of their bipolar chip sequences  namely: (â  â  +  â  +  +  +  â ) + (â  +  â  +  +  +  â  â ) = (â  +  +  â ) To recover the bit stream of an individual station  the receiver must know that stationâs chip sequence in advance
It does the recovery by computing the normalized inner product of the received chip sequence and the chip sequence of the station whose bit stream it is trying to recover
If the received chip sequence is S and the receiver is trying to listen to a station whose chip sequence is C  it just computes the normalized inner product  S C
THE PHYSICAL LAYER
To see why this works  just imagine that two stations  A and C  both transmit a  bit at the same time that B transmits a  bit  as is the case in the third example
The receiver sees the sum  S = A + B + C  and computes S C = (A + B + C) C = A C + B C + C C =  +  +  =  The first two terms vanish because all pairs of chip sequences have been carefully chosen to be orthogonal  as shown in Eq
Now it should be clear why this property must be imposed on the chip sequences
To make the decoding process more concrete  we show six examples in Fig
Suppose that the receiver is interested in extracting the bit sent by station C from each of the six signals S  through S
It calculates the bit by summing the pairwise products of the received S and the C vector of Fig
-  (a) and then taking  /  of the result (since m =  here)
The examples include cases where C is silent  sends a  bit  and sends a  bit  individually and in combination with other transmissions
As shown  the correct bit is decoded each time
It is just like speaking French
In principle  given enough computing capacity  the receiver can listen to all the senders at once by running the decoding algorithm for each of them in parallel
In real life  suffice it to say that this is easier said than done  and it is useful to know which senders might be transmitting
In the ideal  noiseless CDMA system we have studied here  the number of stations that send concurrently can be made arbitrarily large by using longer chip sequences
For  n stations  Walsh codes can provide  n orthogonal chip sequences of length  n
However  one significant limitation is that we have assumed that all the chips are synchronized in time at the receiver
This synchronization is not even approximately true in some applications  such as cellular networks (in which CDMA has been widely deployed starting in the s)
It leads to different designs
We will return to this topic later in the  ter and describe how asynchronous CDMA differs from synchronous CDMA
As well as cellular networks  CDMA is used by satellites and cable networks
We have glossed over many complicating factors in this brief introduction
Engineers who want to gain a deep understanding of CDMA should read Viterbi (   ) and Lee and Miller (   )
These references require quite a bit of background in communication engineering  however  THE PUBLIC SWITCHED TELEPHONE NETWORK When two computers owned by the same company or organization and located close to each other need to communicate  it is often easiest just to run a cable between them
LANs work this way
However  when the distances are large or there are many computers or the cables have to pass through a public road or other public right of way  the costs of running private cables are usually prohibitive
THE PUBLIC SWITCHED TELEPHONE NETWORK Furthermore  in just about every country in the world  stringing private transmission lines across (or underneath) public property is also illegal
Consequently  the network designers must rely on the existing telecommunication facilities
These facilities  especially the PSTN (Public Switched Telephone Network)  were usually designed many years ago  with a completely different goal in mind: transmitting the human voice in a more-or-less recognizable form
Their suitability for use in computer-computer communication is often marginal at best
To see the size of the problem  consider that a cheap commodity cable running between two computers can transfer data at  Gbps or more
In contrast  typical ADSL  the blazingly fast alternative to a telephone modem  runs at around  Mbps
The difference between the two is the difference between cruising in an airplane and taking a leisurely stroll
Nonetheless  the telephone system is tightly intertwined with (wide area) computer networks  so it is worth devoting some time to study it in detail
The limiting factor for networking purposes turns out to be the ââlast mileââ over which customers connect  not the trunks and switches inside the telephone network
This situation is changing with the gradual rollout of fiber and digital technology at the edge of the network  but it will take time and money
During the long wait  computer systems designers used to working with systems that give at least three orders of magnitude better performance have devoted much time and effort to figure out how to use the telephone network efficiently
In the following tions we will describe the telephone system and show how it works
For additional information about the innards of the telephone system see Bellamy (   )
Structure of the Telephone System Soon after Alexander Graham Bell patented the telephone in  (just a few hours ahead of his rival  Elisha Gray)  there was an enormous demand for his new invention
The initial market was for the sale of telephones  which came in pairs
It was up to the customer to string a single wire between them
If a telephone owner wanted to talk to n other telephone owners  separate wires had to be strung to all n houses
Within a year  the cities were covered with wires passing over houses and trees in a wild jumble
It became immediately obvious that the model of connecting every telephone to every other telephone  as shown in Fig
To his credit  Bell saw this problem early on and formed the Bell Telephone Company  which opened its first switching office (in New Haven  Connecticut) in
The company ran a wire to each customerâs house or office
To make a call  the customer would crank the phone to make a ringing sound in the telephone company office to attract the attention of an operator  who would then manually connect the caller to the callee by using a short jumper cable to connect the caller to the callee
The model of a single switching office is illustrated in Fig
THE PHYSICAL LAYER
(a) (b) (c) Figure  -
(a) Fully interconnected network
(b) Centralized switch
(c) Two-level hierarchy
Pretty soon  Bell System switching offices were springing up everywhere and people wanted to make long-distance calls between cities  so the Bell System began to connect the switching offices
The original problem soon returned: to connect every switching office to every other switching office by means of a wire between them quickly became unmanageable  so ond-level switching offices were invented
After a while  multiple ond-level offices were needed  as illustrated in Fig
Eventually  the hierarchy grew to five levels
By  the three major parts of the telephone system were in place: the switching offices  the wires between the customers and the switching offices (by now balanced  insulated  twisted pairs instead of open wires with an earth return)  and the long-distance connections between the switching offices
For a short technical history of the telephone system  see Hawley (   )
While there have been improvements in all three areas since then  the basic Bell System model has remained essentially intact for over years
The following description is highly simplified but gives the essential flavor nevertheless
Each telephone has two copper wires coming out of it that go directly to the telephone companyâs nearest end office (also called a local central office)
The distance is typically  to   km  being shorter in cities than in rural areas
In the United States alone there are about   end offices
The two-wire connections between each subscriberâs telephone and the end office are known in the trade as the local loop
If the worldâs local loops were stretched out end to end  they would extend to the moon and back  times
At one time   % of AT&Tâs capital value was the copper in the local loops
AT&T was then  in effect  the worldâs largest copper mine
Fortunately  this fact was not well known in the investment community
Had it been known  some corporate raider might have bought AT&T  ended all telephone service in the United States  ripped out all the wire  and sold it to a copper refiner for a quick payback
THE PUBLIC SWITCHED TELEPHONE NETWORK If a subscriber attached to a given end office calls another subscriber attached to the same end office  the switching mechanism within the office sets up a direct electrical connection between the two local loops
This connection remains intact for the duration of the call
If the called telephone is attached to another end office  a different procedure has to be used
Each end office has a number of outgoing lines to one or more nearby switching centers  called toll offices (or  if they are within the same local area  tandem offices)
These lines are called toll connecting trunks
The number of different kinds of switching centers and their topology varies from country to country depending on the countryâs telephone density
If both the callerâs and calleeâs end offices happen to have a toll connecting trunk to the same toll office (a likely occurrence if they are relatively close by)  the connection may be established within the toll office
A telephone network consisting only of telephones (the small dots)  end offices (the large dots)  and toll offices (the squares) is shown in Fig
If the caller and callee do not have a toll office in common  a path will have to be established between two toll offices
The toll offices communicate with each other via high-bandwidth intertoll trunks (also called interoffice trunks)
Prior to the  breakup of AT&T  the
telephone system used hierarchical routing to find a path  going to higher levels of the hierarchy until there was a switching office in common
This was then replaced with more flexible  nonhierarchical routing
Figure  -  shows how a long-distance connection might be routed
Telephone End office Toll office Intermediate switching office(s) End Telephone office Toll office Local loop Toll connecting trunk Very high bandwidth intertoll trunks Toll connecting trunk Local loop Figure  -
A typical circuit route for a long-distance call
A variety of transmission media are used for telecommunication
Unlike modern office buildings  where the wiring is commonly Category   local loops to homes mostly consist of Category  twisted pairs  with fiber just starting to appear
Between switching offices  coaxial cables  microwaves  and especially fiber optics are widely used
In the past  transmission throughout the telephone system was analog  with the actual voice signal being transmitted as an electrical voltage from source to destination
With the advent of fiber optics  digital electronics  and computers  all the trunks and switches are now digital  leaving the local loop as the last piece of THE PHYSICAL LAYER
analog technology in the system
Digital transmission is preferred because it is not necessary to accurately reproduce an analog waveform after it has passed through many amplifiers on a long call
Being able to correctly distinguish a  from a  is enough
This property makes digital transmission more reliable than analog
It is also cheaper and easier to maintain
In summary  the telephone system consists of three major components:
Local loops (analog twisted pairs going to houses and businesses) Trunks (digital fiber optic links connecting the switching offices) Switching offices (where calls are moved from one trunk to another)
After a short digression on the politics of telephones  we will come back to each of these three components in some detail
The local loops provide everyone access to the whole system  so they are critical
Unfortunately  they are also the weakest link in the system
For the long-haul trunks  the main issue is how to collect multiple calls together and send them out over the same fiber
This calls for multiplexing  and we apply FDM and TDM to do it
Finally  there are two fundamentally different ways of doing switching; we will look at both
The Politics of Telephones For decades prior to  the Bell System provided both local and long-distance service throughout most of the United States
In the s  the
Federal Government came to believe that this was an illegal monopoly and sued to break it up
The government won  and on January  AT&T was broken up into AT&T Long Lines BOCs (Bell Operating Companies)  and a few other pieces
The   BOCs were grouped into seven regional BOCs (RBOCs) to make them economically viable
The entire nature of telecommunication in the United States was changed overnight by court order (not by an act of Congress)
The exact specifications of the divestiture were described in the so-called MFJ (Modified Final Judgment)  an oxymoron if ever there was oneâif the judgment could be modified  it clearly was not final
This event led to increased competition  better service  and lower long-distance rates for consumers and businesses
However  prices for local service rose as the cross subsidies from longdistance calling were eliminated and local service had to become self supporting
Many other countries have now introduced competition along similar lines
Of direct relevance to our studies is that the new competitive framework caused a key technical feature to be added to the architecture of the telephone network
To make it clear who could do what  the United States was divided up into LATAs (Local Access and Transport Areas)
Very roughly  a LATA is about as big as the area covered by one area code
Within each LATA  there was one LEC (Local Exchange Carrier) with a monopoly on traditional telephone   THE PUBLIC SWITCHED TELEPHONE NETWORK service within its area
The most important LECs were the BOCs  although some LATAs contained one or more of the  independent telephone companies operating as LECs
The new feature was that all inter-LATA traffic was handled by a different kind of company  an IXC (IntereXchange Carrier)
Originally  AT&T Long Lines was the only serious IXC  but now there are well-established competitors such as Verizon and Sprint in the IXC business
One of the concerns at the breakup was to ensure that all the IXCs would be treated equally in terms of line quality  tariffs  and the number of digits their customers would have to dial to use them
The way this is handled is illustrated in Fig
Here we see three example LATAs  each with several end offices
LATAs  and  also have a small hierarchy with tandem offices (intra-LATA toll offices)
To local loops IXC # âs toll office IXC # âs toll office IXC POP Tandem office End office LATA  LATA  LATA  Figure  -
The relationship of LATAs  LECs  and IXCs
All the circles are LEC switching offices
Each hexagon belongs to the IXC whose number is in it
Any IXC that wishes to handle calls originating in a LATA can build a switching office called a POP (Point of Presence) there
The LEC is required to connect each IXC to every end office  either directly  as in LATAs  and   or indirectly  as in LATA
Furthermore  the terms of the connection  both technical and financial  must be identical for all IXCs
This requirement enables  a subscriber in  say  LATA   to choose which IXC to use for calling subscribers in LATA
As part of the MFJ  the IXCs were forbidden to offer local telephone service and the LECs were forbidden to offer inter-LATA telephone service  although THE PHYSICAL LAYER
both were free to enter any other business  such as operating fried chicken restaurants
In  that was a fairly unambiguous statement
Unfortunately  technology has a funny way of making the law obsolete
Neither cable television nor mobile phones were covered by the agreement
As cable television went from one way to two way and mobile phones exploded in popularity  both LECs and IXCs began buying up or merging with cable and mobile operators
By  Congress saw that trying to maintain a distinction between the various kinds of companies was no longer tenable and drafted a bill to preserve accessibility for competition but allow cable TV companies  local telephone companies  long-distance carriers  and mobile operators to enter one anotherâs businesses
The idea was that any company could then offer its customers a single integrated package containing cable TV  telephone  and information services and that different companies would compete on service and price
The bill was enacted into law in February  as a major overhaul of telecommunications regulation
As a result  some BOCs became IXCs and some other companies  such as cable television operators  began offering local telephone service in competition with the LECs
One interesting property of the  law is the requirement that LECs implement local number portability
This means that a customer can change local telephone companies without having to get a new telephone number
Portability for mobile phone numbers (and between fixed and mobile lines) followed suit in
These provisions removed a huge hurdle for many people  making them much more inclined to switch LECs
As a result  the
telecommunications landscape became much more competitive  and other countries have followed suit
Often other countries wait to see how this kind of experiment works out in the
If it works well  they do the same thing; if it works badly  they try something else
The Local Loop: Modems  ADSL  and Fiber It is now time to start our detailed study of how the telephone system works
Let us begin with the part that most people are familiar with: the two-wire local loop coming from a telephone company end office into houses
The local loop is also frequently referred to as the ââlast mile ââ although the length can be up to several miles
It has carried analog information for over years and is likely to continue doing so for some years to come  due to the high cost of converting to digital
Much effort has been devoted to squeezing data networking out of the copper local loops that are already deployed
Telephone modems send digital data between computers over the narrow channel the telephone network provides for a voice call
They were once widely used  but have been largely displaced by broadband technologies such as ADSL that
reuse the local loop to send digital data from a customer to the end office  where they are siphoned off to the Internet
THE PUBLIC SWITCHED TELEPHONE NETWORK Both modems and ADSL must deal with the limitations of old local loops: relatively narrow bandwidth  attenuation and distortion of signals  and susceptibility to electrical noise such as crosstalk
In some places  the local loop has been modernized by installing optical fiber to (or very close to) the home
Fiber is the way of the future
These installations support computer networks from the ground up  with the local loop having ample bandwidth for data services
The limiting factor is what people will pay  not the physics of the local loop
In this tion we will study the local loop  both old and new
We will cover telephone modems  ADSL  and fiber to the home
Telephone Modems To send bits over the local loop  or any other physical channel for that matter  they must be converted to analog signals that can be transmitted over the channel
This conversion is accomplished using the methods for digital modulation that we studied in the previous tion
At the other end of the channel  the analog signal is converted back to bits
A device that converts between a stream of digital bits and an analog signal that represents the bits is called a modem  which is short for ââmodulator demodulator
ââ Modems come in many varieties: telephone modems  DSL modems  cable modems  wireless modems  etc
The modem may be built into the computer (which is now common for telephone modems) or be a separate box (which is common for DSL and cable modems)
Logically  the modem is inserted between the (digital) computer and the (analog) telephone system  as seen in Fig
End office Modem Codec Computer Local loop (analog) Trunk (digital  fiber) Digital line Analog line Codec Modem ISP  ISP  Figure  -
The use of both analog and digital transmission for a computerto- computer call
Conversion is done by the modems and codecs
Telephone modems are used to send bits between two computers over a voice-grade telephone line  in place of the conversation that usually fills the line
The main difficulty in doing so is that a voice-grade telephone line is limited to  Hz  about what is sufficient to carry a conversation
This bandwidth is more than four orders of magnitude less than the bandwidth that is used for Ethernet or THE PHYSICAL LAYER    (WiFi)
Unsurprisingly  the data rates of telephone modems are also four orders of magnitude less than that of Ethernet and    Let us run the numbers to see why this is the case
The Nyquist theorem tells us that even with a perfect -Hz line (which a telephone line is decidedly not)  there is no point in sending symbols at a rate faster than  baud
In practice  most modems send at a rate of  symbols/  or  baud  and focus on getting multiple bits per symbol while allowing traffic in both directions at the same time (by using different frequencies for different directions)
The humble -bps modem uses  volts for a logical  and  volt for a logical   with  bit per symbol
One step up  it can use four different symbols  as in the four phases of QPSK  so with  bits/symbol it can get a data rate of  bps
A long progression of higher rates has been achieved as technology has improved
Higher rates require a larger set of symbols or constellation
With many symbols  even a small amount of noise in the detected amplitude or phase can result in an error
To reduce the chance of errors  standards for the higher-speed modems use some of the symbols for error correction
The schemes are known as TCM (Trellis Coded Modulation) (Ungerboeck  )
modem standard uses   constellation points to transmit  data bits and  check bit per symbol at  baud to achieve  bps with error correction
The next step above  bps is   bps
It is called V
bis and transmits  data bits and  check bit per symbol at  baud
Then comes V
which achieves   bps by transmitting   data bits/symbol at  baud
The constellation now has thousands of points
The final modem in this series is V
bis which uses   data bits/symbol at  baud to achieve   bps
Why stop here? The reason that standard modems stop at   is that the Shannon limit for the telephone system is about   kbps based on the average length of local loops and the quality of these lines
Going faster than this would violate the laws of physics (department of thermodynamics)
However  there is one way we can change the situation
At the telephone company end office  the data are converted to digital form for transmission within the telephone network (the core of the telephone network converted from analog to digital long ago)
The  -kbps limit is for the situation in which there are two local loops  one at each end
Each of these adds noise to the signal
If we could get rid of one of these local loops  we would increase the SNR and the maximum rate would be doubled
This approach is how  -kbps modems are made to work
One end  typically an ISP  gets a high-quality digital feed from the nearest end office
Thus  when one end of the connection is a high-quality signal  as it is with most ISPs now  the maximum data rate can be as high as   kbps
Between two home users with modems and analog lines  the maximum is still
The reason that  -kbps modems (rather than  -kbps modems) are in use has to do with the Nyquist theorem
A telephone channel is carried inside the telephone system as digital samples
Each telephone channel is  Hz wide when   THE PUBLIC SWITCHED TELEPHONE NETWORK the guard bands are included
The number of samples per ond needed to reconstruct it is thus
The number of bits per sample in the
is   one of which may be used for control purposes  allowing   bits/ of user data
In Europe  all  bits are available to users  so  -bit/ modems could have been used  but to get international agreement on a standard    was chosen
The end result is the V
modem standards
They provide for a  -kbps downstream channel (ISP to user) and a
The asymmetry is because there is usually more data transported from the ISP to the user than the other way
It also means that more of the limited bandwidth can be allocated to the downstream channel to increase the chances of it actually working at   kbps
Digital Subscriber Lines When the telephone industry finally got to   kbps  it patted itself on the back for a job well done
Meanwhile  the cable TV industry was offering speeds up to   Mbps on shared cables
As Internet access became an increasingly important part of their business  the telephone companies (LECs) began to realize they needed a more competitive product
Their answer was to offer new digital services over the local loop
Initially  there were many overlapping high-speed offerings  all under the general name of xDSL (Digital Subscriber Line)  for various x
Services with more bandwidth than standard telephone service are sometimes called broadband  although the term really is more of a marketing concept than a specific technical concept
Later  we will discuss what has become the most popular of these services  ADSL (Asymmetric DSL)
We will also use the term DSL or xDSL as shorthand for all flavors
The reason that modems are so slow is that telephones were invented for carrying the human voice and the entire system has been carefully optimized for this purpose
Data have always been stepchildren
At the point where each local loop terminates in the end office  the wire runs through a filter that attenuates all frequencies below Hz and above  Hz
The cutoff is not sharpâ   Hz and  Hz are the  -dB pointsâso the bandwidth is usually quoted as  Hz even though the distance between the  dB points is  Hz
Data on the wire are thus also restricted to this narrow band
The trick that makes xDSL work is that when a customer subscribes to it  the incoming line is connected to a different kind of switch  one that does not have this filter  thus making the entire capacity of the local loop available
The limiting factor then becomes the physics of the local loop  which supports roughly  MHz  not the artificial  Hz bandwidth created by the filter
Unfortunately  the capacity of the local loop falls rather quickly with distance from the end office as the signal is increasingly degraded along the wire
It also depends on the thickness and general quality of the twisted pair
A plot of the THE PHYSICAL LAYER
potential bandwidth as a function of distance is given in Fig
This figure assumes that all the other factors are optimal (new wires  modest bundles  etc
Meters  Mbps Figure  -
Bandwidth versus distance over Category  UTP for DSL
The implication of this figure creates a problem for the telephone company
When it picks a speed to offer  it is simultaneously picking a radius from its end offices beyond which the service cannot be offered
This means that when distant customers try to sign up for the service  they may be told ââThanks a lot for your interest  but you live meters too far from the nearest end office to get this service
Could you please move?ââ The lower the chosen speed is  the larger the radius and the more customers are covered
But the lower the speed  the less attractive the service is and the fewer the people who will be willing to pay for it
This is where business meets technology
The xDSL services have all been designed with certain goals in mind
First  the services must work over the existing Category  twisted pair local loops
ond  they must not affect customersâ existing telephones and fax machines
Third  they must be much faster than   kbps
Fourth  they should be always on  with just a monthly charge and no per-minute charge
To meet the technical goals  the available
MHz spectrum on the local loop is divided into independent channels of
This arrangement is shown in Fig
The OFDM scheme  which we saw in the previous tion  is used to send data over these channels  though it is often called DMT (Discrete MultiTone) in the context of ADSL
Channel  is used for POTS (Plain Old Telephone Service)
Channels  â  are not used  to keep the voice and data signals from interfering with each other
Of the remaining channels  one is used for upstream control and one is used for downstream control
The rest are available for user data
In principle  each of the remaining channels can be used for a full-duplex data stream  but harmonics  crosstalk  and other effects keep practical systems well   THE PUBLIC SWITCHED TELEPHONE NETWORK Power Voice Upstream Downstream  -kHz Channels  kHz Figure  -
Operation of ADSL using discrete multitone modulation
below the theoretical limit
It is up to the provider to determine how many channels are used for upstream and how many for downstream
A  /  mix of upstream and downstream is technically possible  but most providers allocate something like  â  % of the bandwidth to the downstream channel since most users download more data than they upload
This choice gives rise to the ââAââ in ADSL
A common split is   channels for upstream and the rest downstream
It is also possible to have a few of the highest upstream channels be bidirectional for increased bandwidth  although making this optimization requires adding a special circuit to cancel echoes
The international ADSL standard  known as    was approved in
It allows speeds of as much as  Mbps downstream and  Mbps upstream
It was superseded by a ond generation in  called ADSL  with various improvements to allow speeds of as much as   Mbps downstream and  Mbps upstream
Now we have ADSL +  which doubles the downstream speed to   Mbps by doubling the bandwidth to use
MHz over the twisted pair
However  the numbers quoted here are best-case speeds for good lines close (within  to  km) to the exchange
Few lines support these rates  and few providers offer these speeds
Typically  providers offer something like  Mbps downstream and kbps upstream (standard service)   Mbps downstream and  Mbps upstream (improved service)  and  Mbps downstream and  Mbps upstream (premium service)
Within each channel  QAM modulation is used at a rate of roughly  symbols/
The line quality in each channel is constantly monitored and the data rate is adjusted by using a larger or smaller constellation  like those in Fig
Different channels may have different data rates  with up to   bits per symbol sent on a channel with a high SNR  and down to  or no bits per symbol sent on a channel with a low SNR depending on the standard
A typical ADSL arrangement is shown in Fig
In this scheme  a telephone company technician must install a NID (Network Interface Device) on the customerâs premises
This small plastic box marks the end of the telephone companyâs property and the start of the customerâs property
Close to the NID (or sometimes combined with it) is a splitter  an analog filter that separates the THE PHYSICAL LAYER
â   -Hz band used by POTS from the data
The POTS signal is routed to the existing telephone or fax machine
The data signal is routed to an ADSL modem  which uses digital signal processing to implement OFDM
Since most ADSL modems are external  the computer must be connected to them at high speed
Usually  this is done using Ethernet  a USB cable  or    DSLAM Splitter Codec Splitter Telephone To ISP ADSL modem Ethernet Computer Telephone line Telephone company end office Customer premises Voice switch NID Figure  -
A typical ADSL equipment configuration
At the other end of the wire  on the end office side  a corresponding splitter is installed
Here  the voice portion of the signal is filtered out and sent to the normal voice switch
The signal above   kHz is routed to a new kind of device called a DSLAM (Digital Subscriber Line Access Multiplexer)  which contains the same kind of digital signal processor as the ADSL modem
Once the bits have been recovered from the signal  packets are formed and sent off to the ISP
This complete separation between the voice system and ADSL makes it relatively easy for a telephone company to deploy ADSL
All that is needed is buying a DSLAM and splitter and attaching the ADSL subscribers to the splitter
Other high-bandwidth services (
ISDN) require much greater changes to the existing switching equipment
One disadvantage of the design of Fig
Installing these can only be done by a telephone company technician  necessitating an expensive ââtruck rollââ (
sending a technician to the customerâs premises)
Therefore  an alternative  splitterless design  informally called    has also been standardized
It is the same as Fig
The existing telephone line is used as is
The only difference is that a microfilter has to be inserted into each telephone jack   THE PUBLIC SWITCHED TELEPHONE NETWORK between the telephone or ADSL modem and the wire
The microfilter for the telephone is a low-pass filter eliminating frequencies above  Hz; the microfilter for the ADSL modem is a high-pass filter eliminating frequencies below   kHz
However  this system is not as reliable as having a splitter  so   can be used only up to
Mbps (versus  Mbps for ADSL with a splitter)
For more information about ADSL  see Starr (   )
Fiber To The Home Deployed copper local loops limit the performance of ADSL and telephone modems
To let them provide faster and better network services  telephone companies are upgrading local loops at every opportunity by installing optical fiber all the way to houses and offices
The result is called FttH (Fiber To The Home)
While FttH technology has been available for some time  deployments only began to take off in  with growth in the demand for high-speed Internet from customers used to DSL and cable who wanted to download movies
Around  % of
houses are now connected to FttH with Internet access speeds of up to Mbps
Several variations of the form ââFttXââ (where X stands for the basement  curb  or neighborhood) exist
They are used to note that the fiber deployment may reach close to the house
In this case  copper (twisted pair or coaxial cable) provides fast enough speeds over the last short distance
The choice of how far to lay the fiber is an economic one  balancing cost with expected revenue
In any case  the point is that optical fiber has crossed the traditional barrier of the ââlast mile
ââ We will focus on FttH in our discussion
Like the copper wires before it  the fiber local loop is passive
This means no powered equipment is required to amplify or otherwise process signals
The fiber simply carries signals between the home and the end office
This in turn reduces cost and improves reliability
Usually  the fibers from the houses are joined together so that only a single fiber reaches the end office per group of up to houses
In the downstream direction  optical splitters divide the signal from the end office so that it reaches all the houses
Encryption is needed for urity if only one house should be able to decode the signal
In the upstream direction  optical combiners merge the signals from the houses into a single signal that is received at the end office
This architecture is called a PON (Passive Optical Network)  and it is shown in Fig
It is common to use one wavelength shared between all the houses for downstream transmission  and another wavelength for upstream transmission
Even with the splitting  the tremendous bandwidth and low attenuation of fiber mean that PONs can provide high rates to users over distances of up to   km
The actual data rates and other details depend on the type of PON
Two kinds are common
GPONs (Gigabit-capable PONs) come from the world of telecommunications  so they are defined by an ITU standard
EPONs (Ethernet PONs) THE PHYSICAL LAYER
Fiber Optical End office splitter/combiner Rest of network Figure  -
Passive optical network for Fiber To The Home
are more in tune with the world of networking  so they are defined by an IEEE standard
Both run at around a gigabit and can carry traffic for different services  including Internet  video  and voice
For example  GPONs provide
Gbps downstream and
Gbps upstream
Some protocol is needed to share the capacity of the single fiber at the end office between the different houses
The downstream direction is easy
The end office can send messages to each different house in whatever order it likes
In the upstream direction  however  messages from different houses cannot be sent at the same time  or different signals would collide
The houses also cannot hear each otherâs transmissions so they cannot listen before transmitting
The solution is that equipment at the houses requests and is granted time slots to use by equipment in the end office
For this to work  there is a ranging process to adjust the transmission times from the houses so that all the signals received at the end office are synchronized
The design is similar to cable modems  which we cover later in this  ter
For more information on the future of PONs  see Grobe and Elbers (   )
Trunks and Multiplexing Trunks in the telephone network are not only much faster than the local loops  they are different in two other respects
The core of the telephone network carries digital information  not analog information; that is  bits not voice
This necessitates a conversion at the end office to digital form for transmission over the longhaul trunks
The trunks carry thousands  even millions  of calls simultaneously
This sharing is important for achieving economies of scale  since it costs essentially the same amount of money to install and maintain a high-bandwidth trunk as a low-bandwidth trunk between two switching offices
It is accomplished with versions of TDM and FDM multiplexing
Below we will briefly examine how voice signals are digitized so that they can be transported by the telephone network
After that  we will see how TDM is used to carry bits on trunks  including the TDM system used for fiber optics   THE PUBLIC SWITCHED TELEPHONE NETWORK (SONET)
Then we will turn to FDM as it is applied to fiber optics  which is called wavelength division multiplexing
Digitizing Voice Signals Early in the development of the telephone network  the core handled voice calls as analog information
FDM techniques were used for many years to multiplex -Hz voice channels (comprised of  Hz plus guard bands) into larger and larger units
For example calls in the   kHzâtoâ   kHz band is known as a group and five groups (a total of   calls) are known as a supergroup  and so on
These FDM methods are still used over some copper wires and microwave channels
However  FDM requires analog circuitry and is not amenable to being done by a computer
In contrast  TDM can be handled entirely by digital electronics  so it has become far more widespread in recent years
Since TDM can only be used for digital data and the local loops produce analog signals  a conversion is needed from analog to digital in the end office  where all the individual local loops come together to be combined onto outgoing trunks
The analog signals are digitized in the end office by a device called a codec (short for ââcoder-decoderââ)
The codec makes  samples per ond (   Î¼/sample) because the Nyquist theorem says that this is sufficient to capture all the information from the  -kHz telephone channel bandwidth
At a lower sampling rate  information would be lost; at a higher one  no extra information would be gained
Each sample of the amplitude of the signal is quantized to an  -bit number
This technique is called PCM (Pulse Code Modulation)
It forms the heart of the modern telephone system
As a consequence  virtually all time intervals within the telephone system are multiples of Î¼
The standard uncompressed data rate for a voice-grade telephone call is thus  bits every Î¼  or   kbps
At the other end of the call  an analog signal is recreated from the quantized samples by playing them out (and smoothing them) over time
It will not be exactly the same as the original analog signal  even though we sampled at the Nyquist rate  because the samples were quantized
To reduce the error due to quantization  the quantization levels are unevenly spaced
A logarithmic scale is used that gives relatively more bits to smaller signal amplitudes and relatively fewer bits to large signal amplitudes
In this way the error is proportional to the signal amplitude
Two versions of quantization are widely used: Î¼-law  used in North America and Japan  and A-law  used in Europe and the rest of the world
Both versions are specified in standard ITU G An equivalent way to think about this process is to imagine that the dynamic range of the signal (or the ratio between the largest and smallest possible values) is compressed before it is (evenly) quantized  and then expanded when the analog signal is recreated
For this reason it is called THE PHYSICAL LAYER
companding
It is also possible to compress the samples after they are digitized so that they require much less than   kbps
However  we will leave this topic for when we explore audio applications such as voice over IP
Time Division Multiplexing TDM based on PCM is used to carry multiple voice calls over trunks by sending a sample from each call every Î¼
When digital transmission began emerging as a feasible technology  ITU (then called CCITT) was unable to reach agreement on an international standard for PCM
Consequently  a variety of incompatible schemes are now in use in different countries around the world
The method used in North America and Japan is the T  carrier  depicted in Fig
(Technically speaking  the format is called DS  and the carrier is called T  but following widespread industry tradition  we will not make that subtle distinction here
) The T  carrier consists of   voice channels multiplexed together
Each of the   channels  in turn  gets to insert  bits into the output stream
Channel  Channel  Channel  Channel  Channel  -bit frame (   Î¼)  Data bits per channel per sample Bit  is a framing code Bit  is for signaling Figure  -
The T  carrier (
A frame consists of   Ã  = bits plus one extra bit for control purposes  yielding bits every Î¼
This gives a gross data rate of
Mbps  of which  kbps is for signaling
The   rd bit is used for frame synchronization and signaling
In one variation  the   rd bit is used across a group of   frames called an extended superframe
Six of the bits  in the  th  th   th   th   th  and  th positions  take on the alternating pattern     Normally  the receiver keeps checking for this pattern to make sure that it has not lost synchronization
Six more bits are used to send an error check code to help the receiver confirm that it is synchronized
If it does get out of sync  the receiver can scan for the pattern and validate the error check code to get resynchronized
The remaining     THE PUBLIC SWITCHED TELEPHONE NETWORK bits are used for control information for operating and maintaining the network  such as performance reporting from the remote end
The T  format has several variations
The earlier versions sent signaling information in-band  meaning in the same channel as the data  by using some of the data bits
This design is one form of channel-associated signaling  because each channel has its own private signaling subchannel
In one arrangement  the least significant bit out of an  -bit sample on each channel is used in every sixth frame
It has the colorful name of robbed-bit signaling
The idea is that a few stolen bits will not matter for voice calls
No one will hear the difference
For data  however  it is another story
Delivering the wrong bits is unhelpful  to say the least
If older versions of T  are used to carry data  only  of  bits  or   kbps can be used in each of the   channels
Instead  newer versions of T  provide clear channels in which all of the bits may be used to send data
Clear channels are what businesses who lease a T  line want when they send data across the telephone network in place of voice samples
Signaling for any voice calls is then handled out-of-band  meaning in a separate channel from the data
Often  the signaling is done with common-channel signaling in which there is a shared signaling channel
One of the   channels may be used for this purpose
Outside North America and Japan  the
This carrier has -bit data samples packed into the basic   -Î¼ frame
Thirty of the channels are used for information and up to two are used for signaling
Each group of four frames provides   signaling bits  half of which are used for signaling (whether channel-associated or common-channel) and half of which are used for frame synchronization or are reserved for each country to use as it wishes
Time division multiplexing allows multiple T  carriers to be multiplexed into higher-order carriers
Figure  -  shows how this can be done
At the left we see four T  channels being multiplexed into one T  channel
The multiplexing at T  and above is done bit for bit  rather than byte for byte with the   voice channels that make up a T  frame
Four T  streams at
Mbps should generate
Mbps  but T  is actually
The extra bits are used for framing and recovery in case the carrier slips
T  and T  are widely used by customers  whereas T  and T  are only used within the telephone system itself  so they are not well known
At the next level  seven T  streams are combined bitwise to form a T  stream
Then six T  streams are joined to form a T  stream
At each step a small amount of overhead is added for framing and recovery in case the synchronization between sender and receiver is lost
Just as there is little agreement on the basic carrier between the United States and the rest of the world  there is equally little agreement on how it is to be multiplexed into higher-bandwidth carriers
scheme of stepping up by  and  did not strike everyone else as the way to go  so the ITU standard calls for multiplexing four streams into one stream at each level
Also  the framing and THE PHYSICAL LAYER
:  :  :   T  streams in  T  stream out
Mbps T   T  streams in  T  streams in Figure  -
Multiplexing T  streams into higher carriers
recovery data are different in the
and ITU standards
The ITU hierarchy for    and  channels runs at speeds of
SONET/SDH In the early days of fiber optics  every telephone company had its own proprietary optical TDM system
After AT&T was broken up in  local telephone companies had to connect to multiple long-distance carriers  all with different optical TDM systems  so the need for standardization became obvious
In  Bellcore  the RBOCâs research arm  began working on a standard  called SONET (Synchronous Optical NETwork)
Later  ITU joined the effort  which resulted in a SONET standard and a set of parallel ITU recommendations (G
The ITU recommendations are called SDH (Synchronous Digital Hierarchy) but differ from SONET only in minor ways
Virtually all the long-distance telephone traffic in the United States  and much of it elsewhere  now uses trunks running SONET in the physical layer
For additional information about SONET  see Bellamy (   )  Goralski (   )  and Shepard (   )
The SONET design had four major goals
First and foremost  SONET had to make it possible for different carriers to interwork
Achieving this goal required defining a common signaling standard with respect to wavelength  timing  framing structure  and other issues
ond  some means was needed to unify the
European  and Japanese digital systems  all of which were based on  -kbps PCM channels but combined them in different (and incompatible) ways
Third  SONET had to provide a way to multiplex multiple digital channels
At the time SONET was devised  the highest-speed digital carrier actually used widely in the United States was T  at
T  was defined  but not used   THE PUBLIC SWITCHED TELEPHONE NETWORK much  and nothing was even defined above T  speed
Part of SONETâs mission was to continue the hierarchy to gigabits/ and beyond
A standard way to multiplex slower channels into one SONET channel was also needed
Fourth  SONET had to provide support for operations  administration  and maintenance (OAM)  which are needed to manage the network
Previous systems did not do this very well
An early decision was to make SONET a traditional TDM system  with the entire bandwidth of the fiber devoted to one channel containing time slots for the various subchannels
As such  SONET is a synchronous system
Each sender and receiver is tied to a common clock
The master clock that controls the system has an accuracy of about  part in
Bits on a SONET line are sent out at extremely precise intervals  controlled by the master clock
The basic SONET frame is a block of bytes put out every Î¼
Since SONET is synchronous  frames are emitted whether or not there are any useful data to send
Having  frames/ exactly matches the sampling rate of the PCM channels used in all digital telephony systems
The   -byte SONET frames are best described as a rectangle of bytes columns wide by  rows high
Thus   Ã =  bits are transmitted  times per ond  for a gross data rate of
This layout is the basic SONET channel  called STS-  (Synchronous Transport Signal- )
All SONET trunks are multiples of STS-
The first three columns of each frame are reserved for system management information  as illustrated in Fig
In this block  the first three rows contain the tion overhead; the next six contain the line overhead
The tion overhead is generated and checked at the start and end of each tion  whereas the line overhead is generated and checked at the start and end of each line
A SONET transmitter sends back-to-back   -byte frames  without gaps between them  even when there are no data (in which case it sends dummy data)
From the receiverâs point of view  all it sees is a continuous bit stream  so how does it know where each frame begins? The answer is that the first  bytes of each frame contain a fixed pattern that the receiver searches for
If it finds this pattern in the same place in a large number of conutive frames  it assumes that it is in sync with the sender
In theory  a user could insert this pattern into the payload in a regular way  but in practice it cannot be done due to the multiplexing of multiple users into the same frame and other reasons
The remaining   columns of each frame hold   Ã  Ã  Ã  =
Mbps of user data
This user data could be voice samples  T  and other carriers swallowed whole  or packets
SONET is simply a convenient container for transporting bits
The SPE (Synchronous Payload Envelope)  which carries the user data does not always begin in row   column
The SPE can begin anywhere within the frame
A pointer to the first byte is contained in the first row of the line overhead
The first column of the SPE is the path overhead (
the header for the end-to-end path sublayer protocol)
THE PHYSICAL LAYER
Sonet frame (   Î¼) Sonet frame (   Î¼)  Rows      Columns  Columns for overhead tion SPE overhead Line overhead Path overhead Figure  -
Two back-to-back SONET frames
The ability to allow the SPE to begin anywhere within the SONET frame and even to span two frames  as shown in Fig
For example  if a payload arrives at the source while a dummy SONET frame is being constructed  it can be inserted into the current frame instead of being held until the start of the next one
The SONET/SDH multiplexing hierarchy is shown in Fig
Rates from STS-  to STS-   have been defined  ranging from roughly a T  line to   Gbps
Even higher rates will surely be defined over time  with OC- at Gbps being the next in line if and when it becomes technologically feasible
The optical carrier corresponding to STS-n is called OC-n but is bit for bit the same except for a certain bit reordering needed for synchronization
The SDH names are different  and they start at OC-  because ITU-based systems do not have a rate near
We have shown the common rates  which proceed from OC-  in multiples of four
The gross data rate includes all the overhead
The SPE data rate excludes the line and tion overhead
The user data rate excludes all overhead and counts only the   payload columns
As an aside  when a carrier  such as OC-  is not multiplexed  but carries the data from only a single source  the letter c (for concatenated) is appended to the designation  so OC-  indicates a
-Mbps carrier consisting of three separate OC-  carriers  but OC- c indicates a data stream from a single source at
The three OC-  streams within an OC- c stream are interleaved by columnâfirst column  from stream   then column  from stream   then column  from stream   followed by column  from stream   and so onâleading to a frame columns wide and  rows deep
THE PUBLIC SWITCHED TELEPHONE NETWORK SONET SDH Data rate (Mbps) Electrical Optical Optical Gross SPE User STS-  OC-
STS-  OC-  STM-
STS-  OC-  STM-
STS-  OC-  STM-
STS-   OC-   STM-
STS-   OC-   STM-
SONET and SDH multiplex rates
Wavelength Division Multiplexing A form of frequency division multiplexing is used as well as TDM to harness the tremendous bandwidth of fiber optic channels
It is called WDM (Wavelength Division Multiplexing)
The basic principle of WDM on fibers is depicted in Fig
Here four fibers come together at an optical combiner  each with its energy present at a different wavelength
The four beams are combined onto a single shared fiber for transmission to a distant destination
At the far end  the beam is split up over as many fibers as there were on the input side
Each output fiber contains a short  specially constructed core that filters out all but one wavelength
The resulting signals can be routed to their destination or recombined in different ways for additional multiplexed transport
There is really nothing new here
This way of operating is just frequency division multiplexing at very high frequencies  with the term WDM owing to the description of fiber optic channels by their wavelength or ââcolorââ rather than frequency
As long as each channel has its own frequency (
wavelength) range and all the ranges are disjoint  they can be multiplexed together on the long-haul fiber
The only difference with electrical FDM is that an optical system using a diffraction grating is completely passive and thus highly reliable
The reason WDM is popular is that the energy on a single channel is typically only a few gigahertz wide because that is the current limit of how fast we can convert between electrical and optical signals
By running many channels in parallel on different wavelengths  the aggregate bandwidth is increased linearly with the number of channels
Since the bandwidth of a single fiber band is about   GHz (see Fig
- )  there is theoretically room for   -Gbps channels even at  bit/Hz (and higher rates are also possible)
WDM technology has been progressing at a rate that puts computer technology to shame
WDM was invented around
The first commercial systems had eight channels of
Gbps per channel
By  systems with   channels THE PHYSICAL LAYER
Spectrum on the shared fiber Power Î» Fiber  spectrum Power Î» Fiber  spectrum Power Î» Fiber  spectrum Power Î» Î»  Î» +Î» +Î» +Î»  Fiber  spectrum Power Î» Fiber  Î»  Fiber  Î»  Fiber  Combiner Splitter Î»  Long-haul shared fiber Î»  Î»  Î»  Fiber  Î»  Filter Figure  -
Wavelength division multiplexing
Gbps were on the market
By  there were products with channels of   Gbps and   channels of   Gbps  capable of moving up to
This bandwidth is enough to transmit   full-length DVD movies per ond
The channels are also packed tightly on the fiber  with  or as little as   GHz of separation
Technology demonstrations by companies after bragging rights have shown   times this capacity in the lab  but going from the lab to the field usually takes at least a few years
When the number of channels is very large and the wavelengths are spaced close together  the system is referred to as DWDM (Dense WDM)
One of the drivers of WDM technology is the development of all-optical components
Previously  every km it was necessary to split up all the channels and convert each one to an electrical signal for amplification separately before reconverting them to optical signals and combining them
Nowadays  all-optical amplifiers can regenerate the entire signal once every  km without the need for multiple opto-electrical conversions
In the example of Fig
Bits from input fiber  go to output fiber   bits from input fiber  go to output fiber   etc
However  it is also possible to build WDM systems that are switched in the optical domain
In such a device  the output filters are tunable using Fabry-Perot or Mach-Zehnder interferometers
These devices allow the selected frequencies to be changed dynamically by a control computer
This ability provides a large amount of flexibility to provision many different wavelength paths through the telephone network from a fixed set of fibers
For more information about optical networks and WDM  see Ramaswami et al
THE PUBLIC SWITCHED TELEPHONE NETWORK    Switching From the point of view of the average telephone engineer  the phone system is divided into two principal parts: outside plant (the local loops and trunks  since they are physically outside the switching offices) and inside plant (the switches  which are inside the switching offices)
We have just looked at the outside plant
Now it is time to examine the inside plant
Two different switching techniques are used by the network nowadays: circuit switching and packet switching
The traditional telephone system is based on circuit switching  but packet switching is beginning to make inroads with the rise of voice over IP technology
We will go into circuit switching in some detail and contrast it with packet switching
Both kinds of switching are important enough that we will come back to them when we get to the network layer
Circuit Switching Conceptually  when you or your computer places a telephone call  the switching equipment within the telephone system seeks out a physical path all the way from your telephone to the receiverâs telephone
This technique is called circuit switching
It is shown schematically in Fig
Each of the six rectangles represents a carrier switching office (end office  toll office  etc
In this example  each office has three incoming lines and three outgoing lines
When a call passes through a switching office  a physical connection is (conceptually) established between the line on which the call came in and one of the output lines  as shown by the dotted lines
In the early days of the telephone  the connection was made by the operator plugging a jumper cable into the input and output sockets
In fact  a surprising little story is associated with the invention of automatic circuit switching equipment
It was invented by a  th-century Missouri undertaker named Almon B
Shortly after the telephone was invented  when someone died  one of the survivors would call the town operator and say ââPlease connect me to an undertaker
ââ Unfortunately for Mr
Strowger  there were two undertakers in his town  and the other oneâs wife was the town telephone operator
He quickly saw that either he was going to have to invent automatic telephone switching equipment or he was going to go out of business
He chose the first option
For nearly years  the circuit-switching equipment used worldwide was known as Strowger gear
(History does not record whether the now-unemployed switchboard operator got a job as an information operator  answering questions such as ââWhat is the phone number of an undertaker?ââ) The model shown in Fig
Nevertheless  the basic idea is valid: once a call has been set up  a dedicated path between both ends exists and will continue to exist until the call is finished
THE PHYSICAL LAYER
(a) (b) Switching office Physical (copper) connection set up when call is made Packets queued for subsequent transmission Computer Computer Figure  -
(a) Circuit switching
(b) Packet switching
An important property of circuit switching is the need to set up an end-to-end path before any data can be sent
The elapsed time between the end of dialing and the start of ringing can easily be     more on long-distance or international calls
During this time interval  the telephone system is hunting for a path  as shown in Fig
Note that before data transmission can even begin  the call request signal must propagate all the way to the destination and be acknowledged
For many computer applications (
point-of-sale credit verification)  long setup times are undesirable
As a consequence of the reserved path between the calling parties  once the setup has been completed  the only delay for data is the propagation time for the electromagnetic signal  about  m per  km
Also as a consequence of the established path  there is no danger of congestionâthat is  once the call has been put through  you never get busy signals
Of course  you might get one before the connection has been established due to lack of switching or trunk capacity
Packet Switching The alternative to circuit switching is packet switching  shown in Fig
There is no need to set up a dedicated path in advance  unlike   THE PUBLIC SWITCHED TELEPHONE NETWORK Call request signal Data AB trunk A B C (a) D A B C (b) D BC trunk CD trunk Call accept signal Propagation delay Queuing delay Pkt  Pkt  Pkt  Pkt  Pkt  Pkt  Pkt  Pkt  Pkt  Time spent hunting for an outgoing trunk Time Figure  -
Timing of events in (a) circuit switching  (b) packet switching
with circuit switching
It is up to routers to use store-and-forward transmission to send each packet on its way to the destination on its own
This procedure is unlike circuit switching  in which the result of the connection setup is the reservation of bandwidth all the way from the sender to the receiver
All data on the circuit follows this path
Among other properties  having all the data follow the same path means that it cannot arrive out of order
With packet switching there is no fixed path  so different packets can follow different paths  depending on network conditions at the time they are sent  and they may arrive out of order
Packet-switching networks place a tight upper limit on the size of packets
This ensures that no user can monopolize any transmission line for very long (
many millionds)  so that packet-switched networks can handle interactive traffic
It also reduces delay since the first packet of a long message can be forwarded before the ond one has fully arrived
However  the store-and-forward delay of accumulating a packet in the routerâs memory before it is sent on to the THE PHYSICAL LAYER
next router exceeds that of circuit switching
With circuit switching  the bits just flow through the wire continuously
Packet and circuit switching also differ in other ways
Because no bandwidth is reserved with packet switching  packets may have to wait to be forwarded
This introduces queuing delay and congestion if many packets are sent at the same time
On the other hand  there is no danger of getting a busy signal and being unable to use the network
Thus  congestion occurs at different times with circuit switching (at setup time) and packet switching (when packets are sent)
If a circuit has been reserved for a particular user and there is no traffic  its bandwidth is wasted
It cannot be used for other traffic
Packet switching does not waste bandwidth and thus is more efficient from a system perspective
Understanding this trade-off is crucial for comprehending the difference between circuit switching and packet switching
The trade-off is between guaranteed service and wasting resources versus not guaranteeing service and not wasting resources
Packet switching is more fault tolerant than circuit switching
In fact  that is why it was invented
If a switch goes down  all of the circuits using it are terminated and no more traffic can be sent on any of them
With packet switching  packets can be routed around dead switches
A final difference between circuit and packet switching is the charging algorithm
With circuit switching  charging has historically been based on distance and time
For mobile phones  distance usually does not play a role  except for international calls  and time plays only a coarse role (
a calling plan with  free minutes costs more than one with  free minutes and sometimes nights or weekends are cheap)
With packet switching  connect time is not an issue  but the volume of traffic is
For home users  ISPs usually charge a flat monthly rate because it is less work for them and their customers can understand this model  but backbone carriers charge regional networks based on the volume of their traffic
The differences are summarized in Fig
Traditionally  telephone networks have used circuit switching to provide high-quality telephone calls  and computer networks have used packet switching for simplicity and efficiency
However  there are notable exceptions
Some older computer networks have been circuit switched under the covers (
) and some newer telephone networks use packet switching with voice over IP technology
This looks just like a standard telephone call on the outside to users  but inside the network packets of voice data are switched
This approach has let upstarts market cheap international calls via calling cards  though perhaps with lower call quality than the incumbents  THE MOBILE TELEPHONE SYSTEM The traditional telephone system  even if it someday gets multigigabit end-toend fiber  will still not be able to satisfy a growing group of users: people on the go
People now expect to make phone calls and to use their phones to check   THE MOBILE TELEPHONE SYSTEM Item Circuit switched Packet switched Call setup Required Not needed Dedicated physical path Yes No Each packet follows the same route Yes No Packets arrive in order Yes No Is a switch crash fatal Yes No Bandwidth available Fixed Dynamic Time of possible congestion At setup time On every packet Potentially wasted bandwidth Yes No Store-and-forward transmission No Yes Charging Per minute Per packet Figure  -
A comparison of circuit-switched and packet-switched networks
email and surf the Web from airplanes  cars  swimming pools  and while jogging in the park
Consequently  there is a tremendous amount of interest in wireless telephony
In the following tions we will study this topic in some detail
The mobile phone system is used for wide area voice and data communication
Mobile phones (sometimes called cell phones) have gone through three distinct generations  widely called  G  G  and  G
The generations are:
Analog voice Digital voice Digital voice and data (Internet  email  etc
(Mobile phones should not be confused with cordless phones that consist of a base station and a handset sold as a set for use within the home
These are never used for networking  so we will not examine them further
) Although most of our discussion will be about the technology of these systems  it is interesting to note how political and tiny marketing decisions can have a huge impact
The first mobile system was devised in the
by AT&T and mandated for the whole country by the FCC
As a result  the entire
had a single (analog) system and a mobile phone purchased in California also worked in New York
In contrast  when mobile phones came to Europe  every country devised its own system  which resulted in a fiasco
Europe learned from its mistake and when digital came around  the government- run PTTs got together and standardized on a single system (GSM)  so any European mobile phone will work anywhere in Europe
By then  the
had decided that government should not be in the standardization business  so it left digital to the marketplace
This decision resulted in different equipment manufacturers producing different kinds of mobile phones
As a consequence  in the
THE PHYSICAL LAYER
two majorâand completely incompatibleâdigital mobile phone systems were deployed  as well as other minor systems
Despite an initial lead by the
mobile phone ownership and usage in Europe is now far greater than in the
Having a single system that works anywhere in Europe and with any provider is part of the reason  but there is more
A ond area where the
and Europe differed is in the humble matter of phone numbers
mobile phones are mixed in with regular (fixed) telephones
Thus  there is no way for a caller to see if  say  (  )   - is a fixed telephone (cheap or free call) or a mobile phone (expensive call)
To keep people from getting nervous about placing calls  the telephone companies decided to make the mobile phone owner pay for incoming calls
As a consequence  many people hesitated buying a mobile phone for fear of running up a big bill by just receiving calls
In Europe  mobile phone numbers have a special area code (analogous to and numbers) so they are instantly recognizable
Consequently  the usual rule of ââcaller paysââ also applies to mobile phones in Europe (except for international calls  where costs are split)
A third issue that has had a large impact on adoption is the widespread use of prepaid mobile phones in Europe (up to  % in some areas)
These can be purchased in many stores with no more formality than buying a digital camera
You pay and you go
They are preloaded with a balance of  for example or   euros and can be recharged (using a ret PIN code) when the balance drops to zero
As a consequence  practically every teenager and many small children in Europe have (usually prepaid) mobile phones so their parents can locate them  without the danger of the child running up a huge bill
If the mobile phone is used only occasionally  its use is essentially free since there is no monthly charge or charge for incoming calls
First-Generation ( G) Mobile Phones: Analog Voice Enough about the politics and marketing aspects of mobile phones
Now let us look at the technology  starting with the earliest system
Mobile radiotelephones were used sporadically for maritime and military communication during the early decades of the  th century
In  the first system for car-based telephones was set up in St
This system used a single large transmitter on top of a tall building and had a single channel  used for both sending and receiving
To talk  the user had to push a button that enabled the transmitter and disabled the receiver
Such systems  known as push-to-talk systems  were installed in several cities beginning in the late s
CB radio  taxis  and police cars often use this technology
In the s  IMTS (Improved Mobile Telephone System) was installed
It  too  used a high-powered (  -watt) transmitter on top of a hill but it had two frequencies  one for sending and one for receiving  so the push-to-talk button was   THE MOBILE TELEPHONE SYSTEM no longer needed
Since all communication from the mobile telephones went inbound on a different channel than the outbound signals  the mobile users could not hear each other (unlike the push-to-talk system used in taxis)
IMTS supported   channels spread out from MHz to MHz
Due to the small number of channels  users often had to wait a long time before getting a dial tone
Also  due to the large power of the hilltop transmitters  adjacent systems had to be several hundred kilometers apart to avoid interference
All in all  the limited capacity made the system impractical
Advanced Mobile Phone System All that changed with AMPS (Advanced Mobile Phone System)  invented by Bell Labs and first installed in the United States in
It was also used in England  where it was called TACS  and in Japan  where it was called MCS-L
AMPS was formally retired in  but we will look at it to understand the context for the  G and  G systems that improved on it
In all mobile phone systems  a geographic region is divided up into cells  which is why the devices are sometimes called cell phones
In AMPS  the cells are typically   to   km across; in digital systems  the cells are smaller
Each cell uses some set of frequencies not used by any of its neighbors
The key idea that gives cellular systems far more capacity than previous systems is the use of relatively small cells and the reuse of transmission frequencies in nearby (but not adjacent) cells
Whereas an IMTS system km across can have only one call on each frequency  an AMPS system might have  -km cells in the same area and be able to have   to   calls on each frequency  in widely separated cells
Thus  the cellular design increases the system capacity by at least an order of magnitude  more as the cells get smaller
Furthermore  smaller cells mean that less power is needed  which leads to smaller and cheaper transmitters and handsets
The idea of frequency reuse is illustrated in Fig
The cells are normally roughly circular  but they are easier to model as hexagons
They are grouped in units of seven cells
Each letter indicates a group of frequencies
Notice that for each frequency set  there is a buffer about two cells wide where that frequency is not reused  providing for good separation and low interference
Finding locations high in the air to place base station antennas is a major issue
This problem has led some telecommunication carriers to forge alliances with the Roman Catholic Church  since the latter owns a substantial number of exalted potential antenna sites worldwide  all conveniently under a single management
In an area where the number of users has grown to the point that the system is overloaded  the power can be reduced and the overloaded cells split into smaller THE PHYSICAL LAYER
G F A B C D E G F A B C D E G F A B C D E (a) (b) Figure  -
(a) Frequencies are not reused in adjacent cells
(b) To add more users  smaller cells can be used
microcells to permit more frequency reuse  as shown in Fig
Telephone companies sometimes create temporary microcells  using portable towers with satellite links at sporting events  rock concerts  and other places where large numbers of mobile users congregate for a few hours
At the center of each cell is a base station to which all the telephones in the cell transmit
The base station consists of a computer and transmitter/receiver connected to an antenna
In a small system  all the base stations are connected to a single device called an MSC (Mobile Switching Center) or MTSO (Mobile Telephone Switching Office)
In a larger one  several MSCs may be needed  all of which are connected to a ond-level MSC  and so on
The MSCs are essentially end offices as in the telephone system  and are in fact connected to at least one telephone system end office
The MSCs communicate with the base stations  each other  and the PSTN using a packet-switching network
At any instant  each mobile telephone is logically in one specific cell and under the control of that cellâs base station
When a mobile telephone physically leaves a cell  its base station notices the telephoneâs signal fading away and asks all the surrounding base stations how much power they are getting from it
When the answers come back  the base station then transfers ownership to the cell getting the strongest signal; under most conditions that is the cell where the telephone is now located
The telephone is then informed of its new boss  and if a call is in progress  it is asked to switch to a new channel (because the old one is not reused in any of the adjacent cells)
This process  called handoff  takes about m
Channel assignment is done by the MSC  the nerve center of the system
The base stations are really just dumb radio relays
THE MOBILE TELEPHONE SYSTEM Channels AMPS uses FDM to separate the channels
The system uses full-duplex channels  each consisting of a pair of simplex channels
This arrangement is known as FDD (Frequency Division Duplex)
The simplex channels from to MHz are used for mobile to base station transmission  and simplex channels from to MHz are used for base station to mobile transmission
Each of these simplex channels is   kHz wide
The channels are divided into four categories
Control channels (base to mobile) are used to manage the system
Paging channels (base to mobile) alert mobile users to calls for them
Access channels (bidirectional) are used for call setup and channel assignment
Finally  data channels (bidirectional) carry voice  fax  or data
Since the same frequencies cannot be reused in nearby cells and   channels are reserved in each cell for control  the actual number of voice channels available per cell is much smaller than  typically about
Call Management Each mobile telephone in AMPS has a  -bit serial number and a  -digit telephone number in its programmable read-only memory
The telephone number is represented as a  -digit area code in   bits and a  -digit subscriber number in   bits
When a phone is switched on  it scans a preprogrammed list of   control channels to find the most powerful signal
The phone then broadcasts its  -bit serial number and  -bit telephone number
Like all the control information in AMPS  this packet is sent in digital form  multiple times  and with an error-correcting code  even though the voice channels themselves are analog
When the base station hears the announcement  it tells the MSC  which records the existence of its new customer and also informs the customerâs home MSC of his current location
During normal operation  the mobile telephone reregisters about once every   minutes
To make a call  a mobile user switches on the phone  enters the number to be called on the keypad  and hits the SEND button
The phone then transmits the number to be called and its own identity on the access channel
If a collision occurs there  it tries again later
When the base station gets the request  it informs the MSC
If the caller is a customer of the MSCâs company (or one of its partners)  the MSC looks for an idle channel for the call
If one is found  the channel number is sent back on the control channel
The mobile phone then automatically switches to the selected voice channel and waits until the called party picks up the phone
Incoming calls work differently
To start with  all idle phones continuously listen to the paging channel to detect messages directed at them
When a call is placed to a mobile phone (either from a fixed phone or another mobile phone)  a packet is sent to the calleeâs home MSC to find out where it is
A packet is then THE PHYSICAL LAYER
sent to the base station in its current cell  which sends a broadcast on the paging channel of the form ââUnit are you there?ââ The called phone responds with a ââYesââ on the access channel
The base then says something like: ââUnit call for you on channel
ââ At this point  the called phone switches to channel  and starts making ringing sounds (or playing some melody the owner was given as a birthday present)
ond-Generation ( G) Mobile Phones: Digital Voice The first generation of mobile phones was analog; the ond generation is digital
Switching to digital has several advantages
It provides capacity gains by allowing voice signals to be digitized and compressed
It improves urity by allowing voice and control signals to be encrypted
This in turn deters fraud and eavesdropping  whether from intentional scanning or echoes of other calls due to RF propagation
Finally  it enables new services such as text messaging
Just as there was no worldwide standardization during the first generation  there was also no worldwide standardization during the ond  either
Several different systems were developed  and three have been widely deployed
DAMPS (Digital Advanced Mobile Phone System) is a digital version of AMPS that coexists with AMPS and uses TDM to place multiple calls on the same frequency channel
It is described in International Standard IS-  and its successor IS-
GSM (Global System for Mobile communications) has emerged as the dominant system  and while it was slow to catch on in the
it is now used virtually everywhere in the world
Like D-AMPS  GSM is based on a mix of FDM and TDM
CDMA (Code Division Multiple Access)  described in International Standard IS-   is a completely different kind of system and is based on neither FDM mor TDM
While CDMA has not become the dominant  G system  its technology has become the basis for  G systems
Also  the name PCS (Personal Communications Services) is sometimes used in the marketing literature to indicate a ond-generation (
digital) system
Originally it meant a mobile phone using the  MHz band  but that distinction is rarely made now
We will now describe GSM  since it is the dominant  G system
In the next tion we will have more to say about CDMA when we describe  G systems
GSMâThe Global System for Mobile Communications GSM started life in the s as an effort to produce a single European  G standard
The task was assigned to a telecommunications group called (in French) Groupe SpecialeÂ´ Mobile
The first GSM systems were deployed starting in  and were a quick success
It soon became clear that GSM was going to be more than a European success  with uptake stretching to countries as far away as Australia  so GSM was renamed to have a more worldwide appeal
THE MOBILE TELEPHONE SYSTEM GSM and the other mobile phone systems we will study retain from  G systems a design based on cells  frequency reuse across cells  and mobility with handoffs as subscribers move
It is the details that differ
Here  we will briefly discuss some of the main properties of GSM
However  the printed GSM standard is over  [sic] pages long
A large fraction of this material relates to engineering aspects of the system  especially the design of receivers to handle multipath signal propagation  and synchronizing transmitters and receivers
None of this will be even mentioned here
The mobile itself is now divided into the handset and a removable chip with subscriber and account information called a SIM card  short for Subscriber Identity Module
It is the SIM card that activates the handset and contains rets that let the mobile and the network identify each other and encrypt conversations
A SIM card can be removed and plugged into a different handset to turn that handset into your mobile as far as the network is concerned
VLR MSC Air interface Cell tower and base station SIM PSTN card Handset BSC HLR BSC Figure  -
GSM mobile network architecture
The mobile talks to cell base stations over an air interface that we will describe in a moment
The cell base stations are each connected to a BSC (Base Station Controller) that controls the radio resources of cells and handles handoff
The BSC in turn is connected to an MSC (as in AMPS) that routes calls and connects to the PSTN (Public Switched Telephone Network)
To be able to route calls  the MSC needs to know where mobiles can currently be found
It maintains a database of nearby mobiles that are associated with the cells it manages
This database is called the VLR (Visitor Location Register)
There is also a database in the mobile network that gives the last known location of each mobile
It is called the HLR (Home Location Register)
This database is used to route incoming calls to the right locations
Both databases must be kept up to date as mobiles move from cell to cell
We will now describe the air interface in some detail
GSM runs on a range of frequencies worldwide  including   and  MHz
More spectrum is allocated than for AMPS in order to support a much larger number of users
GSM THE PHYSICAL LAYER
is a frequency division duplex cellular system  like AMPS
That is  each mobile transmits on one frequency and receives on another  higher frequency (  MHz higher for GSM versus   MHz higher for AMPS)
However  unlike with AMPS  with GSM a single frequency pair is split by time-division multiplexing into time slots
In this way it is shared by multiple mobiles
To handle multiple mobiles  GSM channels are much wider than the AMPS channels (  -kHz versus   kHz)
One   -kHz channel is shown in Fig
A GSM system operating in the   -MHz region has pairs of simplex channels
Each simplex channel is kHz wide and supports eight separate connections on it  using time division multiplexing
Each currently active station is assigned one time slot on one channel pair
Theoretically  channels can be supported in each cell  but many of them are not available  to avoid frequency conflicts with neighboring cells
Transmitting and receiving does not happen in the same time slot because the GSM radios cannot transmit and receive at the same time and it takes time to switch from one to the other
If the mobile device assigned to
MHz and time slot  wanted to transmit to the base station  it would use the lower four shaded slots (and the ones following them in time)  putting some data in each slot until all the data had been sent
MHz Frequency Base to mobile Mobile to base    Channel TDM frame Time Figure  -
GSM uses frequency channels  each of which uses an eightslot TDM system
The TDM slots shown in Fig
Each TDM slot has a specific structure  and groups of TDM slots form multiframes  also with a specific structure
A simplified version of this hierarchy is shown in Fig
Here we can see that each TDM slot consists of a   -bit data frame that occupies the channel for Î¼ (including a  -Î¼ guard time   THE MOBILE TELEPHONE SYSTEM after each slot)
Each data frame starts and ends with three  bits  for frame delineation purposes
It also contains two  -bit Information fields  each one having a control bit that indicates whether the following Information field is for voice or data
Between the Information fields is a  -bit Sync (training) field that is used by the receiver to synchronize to the senderâs frame boundaries
CTL            -Bit multiframe sent in m    -Bit TDM frame sent in
âbit (  Î¼) guard time Reserved for future use Information Sync Information   -Bit data frame sent in Î¼ Bits    Voice/data bit Figure  -
A portion of the GSM framing structure
A data frame is transmitted in Î¼  but a transmitter is only allowed to send one data frame every
m  since it is sharing the channel with seven other stations
The gross rate of each channel is    bps  divided among eight users
However  as with AMPS  the overhead eats up a large fraction of the bandwidth  ultimately leaving
kbps worth of payload per user before error correction
After error correction kbps is left for speech
While this is substantially less than   kbps PCM for uncompressed voice signals in the fixed telephone network  compression on the mobile device can reach these levels with little loss of quality
As can be seen from Fig
Of the   TDM frames in a multiframe  slot   is used for control and slot   is reserved for future use  so only   are available for user traffic
However  in addition to the  -slot multiframe shown in Fig
Some of these slots are used to hold several control channels used to manage the system
The broadcast control channel is a continuous stream of output from the base station containing the base stationâs identity and the channel status
All mobile stations monitor their signal strength to see when they have moved into a new cell
THE PHYSICAL LAYER
The dedicated control channel is used for location updating  registration  and call setup
In particular  each BSC maintains a database of mobile stations currently under its jurisdiction  the VLR
Information needed to maintain the VLR is sent on the dedicated control channel
Finally  there is the common control channel  which is split up into three logical subchannels
The first of these subchannels is the paging channel  which the base station uses to announce incoming calls
Each mobile station monitors it continuously to watch for calls it should answer
The ond is the random access channel  which allows users to request a slot on the dedicated control channel
If two requests collide  they are garbled and have to be retried later
Using the dedicated control channel slot  the station can set up a call
The assigned slot is announced on the third subchannel  the access grant channel
Finally  GSM differs from AMPS in how handoff is handled
In AMPS  the MSC manages it completely without help from the mobile devices
With time slots in GSM  the mobile is neither sending nor receiving most of the time
The idle slots are an opportunity for the mobile to measure signal quality to other nearby base stations
It does so and sends this information to the BSC
The BSC can use it to determine when a mobile is leaving one cell and entering another so it can perform the handoff
This design is called MAHO (Mobile Assisted HandOff)
Third-Generation ( G) Mobile Phones: Digital Voice and Data The first generation of mobile phones was analog voice  and the ond generation was digital voice
The third generation of mobile phones  or  G as it is called  is all about digital voice and data
A number of factors are driving the industry
First  data traffic already exceeds voice traffic on the fixed network and is growing exponentially  whereas voice traffic is essentially flat
Many industry experts expect data traffic to dominate voice on mobile devices as well soon
ond  the telephone  entertainment  and computer industries have all gone digital and are rapidly converging
Many people are drooling over lightweight  portable devices that act as a telephone  music and video player  email terminal  Web interface  gaming machine  and more  all with worldwide wireless connectivity to the Internet at high bandwidth
Appleâs iPhone is a good example of this kind of  G device
With it  people get hooked on wireless data services  and AT&T wireless data volumes are rising steeply with the popularity of iPhones
The trouble is  the iPhone uses a
G network (an enhanced  G network  but not a true  G network) and there is not enough data capacity to keep users happy
G mobile telephony is all about providing enough wireless bandwidth to keep these future users happy
ITU tried to get a bit more specific about this vision starting back around
It issued a blueprint for getting there called IMT-  where IMT stood   THE MOBILE TELEPHONE SYSTEM for International Mobile Telecommunications
The basic services that the IMT- network was supposed to provide to its users are:
High-quality voice transmission Messaging (replacing email  fax  SMS  chat  etc
) Multimedia (playing music  viewing videos  films  television  etc
) Internet access (Web surfing  including pages with audio and video)
Additional services might be video conferencing  telepresence  group game playing  and m-commerce (waving your telephone at the cashier to pay in a store)
Furthermore  all these services are supposed to be available worldwide (with automatic connection via a satellite when no terrestrial network can be located)  instantly (always on)  and with quality of service guarantees
ITU envisioned a single worldwide technology for IMT-  so manufacturers could build a single device that could be sold and used anywhere in the world (like CD players and computers and unlike mobile phones and televisions)
Having a single technology would also make life much simpler for network operators and would encourage more people to use the services
Format wars  such as the Betamax versus VHS battle with videorecorders  are not good for business
As it turned out  this was a bit optimistic
The number  stood for three things: ( ) the year it was supposed to go into service  ( ) the frequency it was supposed to operate at (in MHz)  and ( ) the bandwidth the service should have (in kbps)
It did not make it on any of the three counts
Nothing was implemented by
ITU recommended that all governments reserve spectrum at  GHz so devices could roam seamlessly from country to country
China reserved the required bandwidth but nobody else did
Finally  it was recognized that  Mbps is not currently feasible for users who are too mobile (due to the difficulty of performing handoffs quickly enough)
More realistic is  Mbps for stationary indoor users (which will compete head-on with ADSL)  kbps for people walking  and kbps for connections in cars
Despite these initial setbacks  much has been accomplished since then
Several IMT proposals were made and  after some winnowing  it came down to two main ones
The first one  WCDMA (Wideband CDMA)  was proposed by Ericsson and was pushed by the European Union  which called it UMTS (Universal Mobile Telecommunications System)
The other contender was CDMA  proposed by Qualcomm
Both of these systems are more similar than different in that they are based on broadband CDMA; WCDMA uses  -MHz channels and CDMA uses
If the Ericsson and Qualcomm engineers were put in a room and told to come to a common design  they probably could find one fairly quickly
The trouble is that the real problem is not engineering  but politics (as usual)
Europe wanted a system that interworked with GSM  whereas the
wanted a THE PHYSICAL LAYER
system that was compatible with one already widely deployed in the
Each side also supported its local company (Ericsson is based in Sweden; Qualcomm is in California)
Finally  Ericsson and Qualcomm were involved in numerous lawsuits over their respective CDMA patents
Worldwide   â  % of mobile subscribers already use  G technologies
In North America and Europe  around a third of mobile subscribers are  G
Japan was an early adopter and now nearly all mobile phones in Japan are  G
These figures include the deployment of both UMTS and CDMA  and  G continues to be one great cauldron of activity as the market shakes out
To add to the confusion  UMTS became a single  G standard with multiple incompatible options  including CDMA
This change was an effort to unify the various camps  but it just papers over the technical differences and obscures the focus of ongoing efforts
We will use UMTS to mean WCDMA  as distinct from CDMA
We will focus our discussion on the use of CDMA in cellular networks  as it is the distinguishing feature of both systems
CDMA is neither FDM nor TDM but a kind of mix in which each user sends on the same frequency band at the same time
When it was first proposed for cellular systems  the industry gave it approximately the same reaction that Columbus first got from Queen Isabella when he proposed reaching India by sailing in the wrong direction
However  through the persistence of a single company  Qualcomm  CDMA succeeded as a  G system (IS-  ) and matured to the point that it became the technical basis for  G
To make CDMA work in the mobile phone setting requires more than the basic CDMA technique that we described in the previous tion
Specifically  we described synchronous CDMA  in which the chip sequences are exactly orthogonal
This design works when all users are synchronized on the start time of their chip sequences  as in the case of the base station transmitting to mobiles
The base station can transmit the chip sequences starting at the same time so that the signals will be orthogonal and able to be separated
However  it is difficult to synchronize the transmissions of independent mobile phones
Without care  their transmissions would arrive at the base station at different times  with no guarantee of orthogonality
To let mobiles send to the base station without synchronization  we want code sequences that are orthogonal to each other at all possible offsets  not simply when they are aligned at the start
While it is not possible to find sequences that are exactly orthogonal for this general case  long pseudorandom sequences come close enough
They have the property that  with high probability  they have a low cross-correlation with each other at all offsets
This means that when one sequence is multiplied by another sequence and summed up to compute the inner product  the result will be small; it would be zero if they were orthogonal
(Intuitively  random sequences should always look different from each other
Multiplying them together should then produce a random signal  which will sum to a small result
) This lets a receiver filter unwanted transmissions out of the received signal
Also  the auto-correlation of   THE MOBILE TELEPHONE SYSTEM pseudorandom sequences is also small  with high probability  except at a zero offset
This means that when one sequence is multiplied by a delayed copy of itself and summed  the result will be small  except when the delay is zero
(Intuitively  a delayed random sequence looks like a different random sequence  and we are back to the cross-correlation case
) This lets a receiver lock onto the beginning of the wanted transmission in the received signal
The use of pseudorandom sequences lets the base station receive CDMA messages from unsynchronized mobiles
However  an implicit assumption in our discussion of CDMA is that the power levels of all mobiles are the same at the receiver
If they are not  a small cross-correlation with a powerful signal might overwhelm a large auto-correlation with a weak signal
Thus  the transmit power on mobiles must be controlled to minimize interference between competing signals
It is this interference that limits the capacity of CDMA systems
The power levels received at a base station depend on how far away the transmitters are as well as how much power they transmit
There may be many mobile stations at varying distances from the base station
A good heuristic to equalize the received power is for each mobile station to transmit to the base station at the inverse of the power level it receives from the base station
In other words  a mobile station receiving a weak signal from the base station will use more power than one getting a strong signal
For more accuracy  the base station also gives each mobile feedback to increase  decrease  or hold steady its transmit power
The feedback is frequent ( times per ond) because good power control is important to minimize interference
Another improvement over the basic CDMA scheme we described earlier is to allow different users to send data at different rates
This trick is accomplished naturally in CDMA by fixing the rate at which chips are transmitted and assigning users chip sequences of different lengths
For example  in WCDMA  the chip rate is
Mchips/ and the spreading codes vary from  to chips
With a   - chip code  around   kbps is left after error correction  and this capacity is sufficient for a voice call
With a  -chip code  the user data rate is close to  Mbps
Intermediate-length codes give intermediate rates; to get to multiple Mbps  the mobile must use more than one  -MHz channel at once
Now let us describe the advantages of CDMA  given that we have dealt with the problems of getting it to work
It has three main advantages
First  CDMA can improve capacity by taking advantage of small periods when some transmitters are silent
In polite voice calls  one party is silent while the other talks
On average  the line is busy only  % of the time
However  the pauses may be small and are difficult to predict
With TDM or FDM systems  it is not possible to reassign time slots or frequency channels quickly enough to benefit from these small silences
However  in CDMA  by simply not transmitting one user lowers the interference for other users  and it is likely that some fraction of users will not be transmitting in a busy cell at any given time
Thus CDMA takes advantage of expected silences to allow a larger number of simultaneous calls
THE PHYSICAL LAYER
ond  with CDMA each cell uses the same frequencies
Unlike GSM and AMPS  FDM is not needed to separate the transmissions of different users
This eliminates complicated frequency planning tasks and improves capacity
It also makes it easy for a base station to use multiple directional antennas  or tored antennas  instead of an omnidirectional antenna
Directional antennas concentrate a signal in the intended direction and reduce the signal  and hence interference  in other directions
This in turn increases capacity
Three tor designs are common
The base station must track the mobile as it moves from tor to tor
This tracking is easy with CDMA because all frequencies are used in all tors
Third  CDMA facilitates soft handoff  in which the mobile is acquired by the new base station before the previous one signs off
In this way there is no loss of continuity
Soft handoff is shown in Fig
It is easy with CDMA because all frequencies are used in each cell
The alternative is a hard handoff  in which the old base station drops the call before the new one acquires it
If the new one is unable to acquire it (
because there is no available frequency)  the call is disconnected abruptly
Users tend to notice this  but it is inevitable occasionally with the current design
Hard handoff is the norm with FDM designs to avoid the cost of having the mobile transmit or receive on two frequencies simultaneously
(a) (b) (c) Figure  -
Soft handoff (a) before  (b) during  and (c) after
Much has been written about  G  most of it praising it as the greatest thing since sliced bread
Meanwhile  many operators have taken cautious steps in the direction of  G by going to what is sometimes called
G  although
G might be more accurate
One such system is EDGE (Enhanced Data rates for GSM Evolution)  which is just GSM with more bits per symbol
The trouble is  more bits per symbol also means more errors per symbol  so EDGE has nine different schemes for modulation and error correction  differing in terms of how much of the bandwidth is devoted to fixing the errors introduced by the higher speed
EDGE is one step along an evolutionary path that is defined from GSM to WCDMA
Similarly  there is an evolutionary path defined for operators to upgrade from IS-  to CDMA networks
Even though  G networks are not fully deployed yet  some researchers regard  G as a done deal
These people are already working on  G systems under the   THE MOBILE TELEPHONE SYSTEM name of LTE (Long Term Evolution)
Some of the proposed features of  G include: high bandwidth; ubiquity (connectivity everywhere); seamless integration with other wired and wireless IP networks  including
access points; adaptive resource and spectrum management; and high quality of service for multimedia
For more information see Astely et al
(   ) and Larmo et al
Meanwhile  wireless networks with  G levels of performance are already available
The main example is
also known as WiMAX
For an overview of mobile WiMAX see Ahmadi (   )
To say the industry is in a state of flux is a huge understatement
Check back in a few years to see what has happened  CABLE TELEVISION We have now studied both the fixed and wireless telephone systems in a fair amount of detail
Both will clearly play a major role in future networks
But there is another major player that has emerged over the past decade for Internet access: cable television networks
Many people nowadays get their telephone and Internet service over cable
In the following tions we will look at cable television as a network in more detail and contrast it with the telephone systems we have just studied
Some relevant references for more information are Donaldson and Jones (   )  Dutta-Roy (   )  and Fellows and Jones (   )
Community Antenna Television Cable television was conceived in the late s as a way to provide better reception to people living in rural or mountainous areas
The system initially consisted of a big antenna on top of a hill to pluck the television signal out of the air  an amplifier  called the headend  to strengthen it  and a coaxial cable to deliver it to peopleâs houses  as illustrated in Fig
Tap Coaxial cable Drop cable Headend Antenna for picking up distant signals Figure  -
An early cable television system
In the early years  cable television was called Community Antenna Television
It was very much a mom-and-pop operation; anyone handy with electronics THE PHYSICAL LAYER
could set up a service for his town  and the users would chip in to pay the costs
As the number of subscribers grew  additional cables were spliced onto the original cable and amplifiers were added as needed
Transmission was one way  from the headend to the users
By  thousands of independent systems existed
In  Time Inc
started a new channel  Home Box Office  with new content (movies) distributed only on cable
Other cable-only channels followed  focusing on news  sports  cooking  and many other topics
This development gave rise to two changes in the industry
First  large corporations began buying up existing cable systems and laying new cable to acquire new subscribers
ond  there was now a need to connect multiple systems  often in distant cities  in order to distribute the new cable channels
The cable companies began to lay cable between the cities to connect them all into a single system
This pattern was analogous to what happened in the telephone industry   years earlier with the connection of previously isolated end offices to make long-distance calling possible
Internet over Cable Over the course of the years the cable system grew and the cables between the various cities were replaced by high-bandwidth fiber  similar to what happened in the telephone system
A system with fiber for the long-haul runs and coaxial cable to the houses is called an HFC (Hybrid Fiber Coax) system
The electrooptical converters that interface between the optical and electrical parts of the system are called fiber nodes
Because the bandwidth of fiber is so much greater than that of coax  a fiber node can feed multiple coaxial cables
Part of a modern HFC system is shown in Fig
Over the past decade  many cable operators decided to get into the Internet access business  and often the telephony business as well
Technical differences between the cable plant and telephone plant had an effect on what had to be done to achieve these goals
For one thing  all the one-way amplifiers in the system had to be replaced by two-way amplifiers to support upstream as well as downstream transmissions
While this was happening  early Internet over cable systems used the cable television network for downstream transmissions and a dialup connection via the telephone network for upstream transmissions
It was a clever workaround  but not much of a network compared to what it could be
However  there is another difference between the HFC system of Fig
Down in the neighborhoods  a single cable is shared by many houses  whereas in the telephone system  every house has its own private local loop
When used for television broadcasting  this sharing is a natural fit
All the programs are broadcast on the cable and it does not matter whether there are   viewers or   viewers
When the same cable is used for Internet access  however  it matters a lot if there are   users or
If one user decides to download a very large file  that bandwidth is potentially being taken away from other users
The more users there   CABLE TELEVISION Copper twisted pair Switch Toll office Headend High-bandwidth fiber trunk End office Local loop (a) (b) House High-bandwidth fiber trunk Coaxial cable House Tap Fiber node Fiber Fiber Figure  -
(a) Cable television
(b) The fixed telephone system
are  the more competition there is for bandwidth
The telephone system does not have this particular property: downloading a large file over an ADSL line does not reduce your neighborâs bandwidth
On the other hand  the bandwidth of coax is much higher than that of twisted pairs  so you can get lucky if your neighbors do not use the Internet much
The way the cable industry has tackled this problem is to split up long cables and connect each one directly to a fiber node
The bandwidth from the headend to each fiber node is effectively infinite  so as long as there are not too many subscribers on each cable segment  the amount of traffic is manageable
Typical THE PHYSICAL LAYER
cables nowadays have   â houses  but as more and more people subscribe to Internet over cable  the load may become too great  requiring more splitting and more fiber nodes
Spectrum Allocation Throwing off all the TV channels and using the cable infrastructure strictly for Internet access would probably generate a fair number of irate customers  so cable companies are hesitant to do this
Furthermore  most cities heavily regulate what is on the cable  so the cable operators would not be allowed to do this even if they really wanted to
As a consequence  they needed to find a way to have television and Internet peacefully coexist on the same cable
The solution is to build on frequency division multiplexing
Cable television channels in North America occupy the  â   MHz region (except for FM radio  from   to MHz)
These channels are  -MHz wide  including guard bands  and can carry one traditional analog television channel or several digital television channels
In Europe the low end is usually   MHz and the channels are  â  MHz wide for the higher resolution required by PAL and AM  but otherwise the allocation scheme is similar
The low part of the band is not used
Modern cables can also operate well above MHz  often at up to MHz or more
The solution chosen was to introduce upstream channels in the  â  MHz band (slightly higher in Europe) and use the frequencies at the high end for the downstream signals
The cable spectrum is illustrated in Fig
TV TV Downstream data Downstream frequencies Upstream data Upstream frequencies FM    MHz  Figure  -
Frequency allocation in a typical cable TV system used for Internet access
Note that since the television signals are all downstream  it is possible to use upstream amplifiers that work only in the  â  MHz region and downstream amplifiers that work only at   MHz and up  as shown in the figure
Thus  we get an asymmetry in the upstream and downstream bandwidths because more spectrum is available above television than below it
On the other hand  most users want more downstream traffic  so cable operators are not unhappy with this fact   CABLE TELEVISION of life
As we saw earlier  telephone companies usually offer an asymmetric DSL service  even though they have no technical reason for doing so
In addition to upgrading the amplifiers  the operator has to upgrade the headend  too  from a dumb amplifier to an intelligent digital computer system with a high-bandwidth fiber interface to an ISP
Often the name gets upgraded as well  from ââheadendââ to CMTS (Cable Modem Termination System)
In the following text  we will refrain from doing a name upgrade and stick with the traditional ââheadend
ââ    Cable Modems Internet access requires a cable modem  a device that has two interfaces on it: one to the computer and one to the cable network
In the early years of cable Internet  each operator had a proprietary cable modem  which was installed by a cable company technician
However  it soon became apparent that an open standard would create a competitive cable modem market and drive down prices  thus encouraging use of the service
Furthermore  having the customers buy cable modems in stores and install them themselves (as they do with wireless access points) would eliminate the dreaded truck rolls
Consequently  the larger cable operators teamed up with a company called CableLabs to produce a cable modem standard and to test products for compliance
This standard  called DOCSIS (Data Over Cable Service Interface Specification)  has mostly replaced proprietary modems
DOCSIS version
came out in  and was soon followed by DOCSIS
It increased upstream rates to better support symmetric services such as IP telephony
The most recent version of the standard is DOCSIS
which came out in
It uses more bandwidth to increase rates in both directions
The European version of these standards is called EuroDOCSIS
Not all cable operators like the idea of a standard  however  since many of them were making good money leasing their modems to their captive customers
An open standard with dozens of manufacturers selling cable modems in stores ends this lucrative practice
The modem-to-computer interface is straightforward
It is normally Ethernet  or occasionally USB
The other end is more complicated as it uses all of FDM  TDM  and CDMA to share the bandwidth of the cable between subscribers
When a cable modem is plugged in and powered up  it scans the downstream channels looking for a special packet periodically put out by the headend to provide system parameters to modems that have just come online
Upon finding this packet  the new modem announces its presence on one of the upstream channels
The headend responds by assigning the modem to its upstream and downstream channels
These assignments can be changed later if the headend deems it necessary to balance the load
The use of  -MHz or  -MHz channels is the FDM part
Each cable modem sends data on one upstream and one downstream channel  or multiple channels THE PHYSICAL LAYER
under DOCSIS   The usual scheme is to take each  (or  ) MHz downstream channel and modulate it with QAM-  or  if the cable quality is exceptionally good  QAM-
With a  -MHz channel and QAM-   we get about   Mbps
When the overhead is subtracted  the net payload is about   Mbps
With QAM-  the net payload is about   Mbps
The European values are  /  larger
For upstream  there is more RF noise because the system was not originally designed for data  and noise from multiple subscribers is funneled to the headend  so a more conservative scheme is used
This ranges from QPSK to QAM- where some of the symbols are used for error protection with Trellis Coded Modulation
With fewer bits per symbol on the upstream  the asymmetry between upstream and downstream rates is much more than suggested by Fig
TDM is then used to share bandwidth on the upstream across multiple subscribers
Otherwise their transmissions would collide at the headend
Time is divided into minislots and different subscribers send in different minislots
To make this work  the modem determines its distance from the headend by sending it a special packet and seeing how long it takes to get the response
This process is called ranging
It is important for the modem to know its distance to get the timing right
Each upstream packet must fit in one or more conutive minislots at the headend when it is received
The headend announces the start of a new round of minislots periodically  but the starting gun is not heard at all modems simultaneously due to the propagation time down the cable
By knowing how far it is from the headend  each modem can compute how long ago the first minislot really started
Minislot length is network dependent
A typical payload is  bytes
During initialization  the headend assigns each modem to a minislot to use for requesting upstream bandwidth
When a computer wants to send a packet  it transfers the packet to the modem  which then requests the necessary number of minislots for it
If the request is accepted  the headend puts an acknowledgement on the downstream channel telling the modem which minislots have been reserved for its packet
The packet is then sent  starting in the minislot allocated to it
Additional packets can be requested using a field in the header
As a rule  multiple modems will be assigned the same minislot  which leads to contention
Two different possibilities exist for dealing with it
The first is that CDMA is used to share the minislot between subscribers
This solves the contention problem because all subscribers with a CDMA code sequence can send at the same time  albeit at a reduced rate
The ond option is that CDMA is not used  in which case there may be no acknowledgement to the request because of a collision
In this case  the modem just waits a random time and tries again
After each successive failure  the randomization time is doubled
(For readers already somewhat familiar with networking  this algorithm is just slotted ALOHA with binary exponential backoff
Ethernet cannot be used on cable because stations cannot sense the medium
We will come back to these issues in  ) The downstream channels are managed differently from the upstream channels
For starters  there is only one sender (the headend)  so there is no contention   CABLE TELEVISION and no need for minislots  which is actually just statistical time division multiplexing
For another  the amount of traffic downstream is usually much larger than upstream  so a fixed packet size of bytes is used
Part of that is a Reed- Solomon error-correcting code and some other overhead  leaving a user payload of bytes
These numbers were chosen for compatibility with digital television using MPEG-  so the TV and downstream data channels are formatted the same way
Logically  the connections are as depicted in Fig
Typical details of the upstream and downstream channels in North America
ADSL Versus Cable Which is better  ADSL or cable? That is like asking which operating system is better
Or which language is better
Or which religion
Which answer you get depends on whom you ask
Let us compare ADSL and cable on a few points
Both use fiber in the backbone  but they differ on the edge
Cable uses coax; ADSL uses twisted pair
The theoretical carrying capacity of coax is hundreds of times more than twisted pair
However  the full capacity of the cable is not available for data users because much of the cableâs bandwidth is wasted on useless stuff such as television programs
In practice  it is hard to generalize about effective capacity
ADSL providers give specific statements about the bandwidth (
Mbps downstream  kbps upstream) and generally achieve about  % of it consistently
Cable providers may artificially cap the bandwidth to each user to help them make performance predictions  but they cannot really give guarantees because the effective capacity depends on how many people are currently active on the userâs cable segment
Sometimes it may be better than ADSL and sometimes it may be worse
What can be annoying  though  is the unpredictability
Having great service one minute does not guarantee great service the next minute since the biggest bandwidth hog in town may have just turned on his computer
THE PHYSICAL LAYER
As an ADSL system acquires more users  their increasing numbers have little effect on existing users  since each user has a dedicated connection
With cable  as more subscribers sign up for Internet service  performance for existing users will drop
The only cure is for the cable operator to split busy cables and connect each one to a fiber node directly
Doing so costs time and money  so there are business pressures to avoid it
As an aside  we have already studied another system with a shared channel like cable: the mobile telephone system
Here  too  a group of usersâwe could call them cellmatesâshare a fixed amount of bandwidth
For voice traffic  which is fairly smooth  the bandwidth is rigidly divided in fixed chunks among the active users using FDM and TDM
But for data traffic  this rigid division is very inefficient because data users are frequently idle  in which case their reserved bandwidth is wasted
As with cable  a more dynamic means is used to allocate the shared bandwidth
Availability is an issue on which ADSL and cable differ
Everyone has a telephone  but not all users are close enough to their end offices to get ADSL
On the other hand  not everyone has cable  but if you do have cable and the company provides Internet access  you can get it
Distance to the fiber node or headend is not an issue
It is also worth noting that since cable started out as a television distribution medium  few businesses have it
Being a point-to-point medium  ADSL is inherently more ure than cable
Any cable user can easily read all the packets going down the cable
For this reason  any decent cable provider will encrypt all traffic in both directions
Nevertheless  having your neighbor get your encrypted messages is still less ure than having him not get anything at all
The telephone system is generally more reliable than cable
For example  it has backup power and continues to work normally even during a power outage
With cable  if the power to any amplifier along the chain fails  all downstream users are cut off instantly
Finally  most ADSL providers offer a choice of ISPs
Sometimes they are even required to do so by law
Such is not always the case with cable operators
The conclusion is that ADSL and cable are much more alike than they are different
They offer comparable service and  as competition between them heats up  probably comparable prices  SUMMARY The physical layer is the basis of all networks
Nature imposes two fundamental limits on all channels  and these determine their bandwidth
These limits are the Nyquist limit  which deals with noiseless channels  and the Shannon limit  which deals with noisy channels
SUMMARY Transmission media can be guided or unguided
The principal guided media are twisted pair  coaxial cable  and fiber optics
Unguided media include terrestrial radio  microwaves  infrared  lasers through the air  and satellites
Digital modulation methods send bits over guided and unguided media as analog signals
Line codes operate at baseband  and signals can be placed in a passband by modulating the amplitude  frequency  and phase of a carrier
Channels can be shared between users with time  frequency and code division multiplexing
A key element in most wide area networks is the telephone system
Its main components are the local loops  trunks  and switches
ADSL offers speeds up to   Mbps over the local loop by dividing it into many subcarriers that run in parallel
This far exceeds the rates of telephone modems
PONs bring fiber to the home for even greater access rates than ADSL
Trunks carry digital information
They are multiplexed with WDM to provision many high capacity links over individual fibers  as well as with TDM to share each high rate link between users
Both circuit switching and packet switching are important
For mobile applications  the fixed telephone system is not suitable
Mobile phones are currently in widespread use for voice  and increasingly for data
They have gone through three generations
The first generation  G  was analog and dominated by AMPS
G was digital  with GSM presently the most widely deployed mobile phone system in the world
G is digital and based on broadband CDMA  with WCDMA and also CDMA now being deployed
An alternative system for network access is the cable television system
It has gradually evolved from coaxial cable to hybrid fiber coax  and from television to television and Internet
Potentially  it offers very high bandwidth  but the bandwidth in practice depends heavily on the other users because it is shared
Compute the Fourier coefficients for the function f(t) = t (  â¤ t â¤  ) A noiseless  -kHz channel is sampled every  m
What is the maximum data rate? How does the maximum data rate change if the channel is noisy  with a signal-to-noise ratio of   dB?
Television channels are  MHz wide
How many bits/ can be sent if four-level digital signals are used? Assume a noiseless channel If a binary signal is sent over a  -kHz channel whose signal-to-noise ratio is   dB  what is the maximum achievable data rate?
What signal-to-noise ratio is needed to put a T  carrier on a  -kHz line?
What are the advantages of fiber optics over copper as a transmission medium? Is there any downside of using fiber optics over copper? THE PHYSICAL LAYER
How much bandwidth is there in
microns of spectrum at a wavelength of  micron?
It is desired to send a sequence of computer screen images over an optical fiber
The screen is  Ã  pixels  each pixel being   bits
There are   screen images per ond
How much bandwidth is needed  and how many microns of wavelength are needed for this band at
Is the Nyquist theorem true for high-quality single-mode optical fiber or only for copper wire?
Radio antennas often work best when the diameter of the antenna is equal to the wavelength of the radio wave
Reasonable antennas range from  cm to  meters in diameter
What frequency range does this cover?
A laser beam  mm wide is aimed at a detector  mm wide m away on the roof of a building
How much of an angular diversion (in degrees) does the laser have to have before it misses the detector?
The   low-orbit satellites in the Iridium project are divided into six necklaces around the earth
At the altitude they are using  the period is   minutes
What is the average interval for handoffs for a stationary transmitter?
Calculate the end-to-end transit time for a packet for both GEO (altitude:   km)  MEO (altitude:   km) and LEO (altitude: km) satellites What is the latency of a call originating at the North Pole to reach the South Pole if the call is routed via Iridium satellites? Assume that the switching time at the satellites is   microonds and earthâs radius is  km What is the minimum bandwidth needed to achieve a data rate of B bits/ if the signal is transmitted using NRZ  MLT-  and Manchester encoding? Explain your answer Prove that in  B/ B encoding  a signal transition will occur at least every four bit times How many end office codes were there pre-  when each end office was named by its three-digit area code and the first three digits of the local number? Area codes started with a digit in the range  â  had a  or  as the ond digit  and ended with any digit
The first two digits of a local number were always in the range  â
The third digit could be any digit A simple telephone system consists of two end offices and a single toll office to which each end office is connected by a  -MHz full-duplex trunk
The average telephone is used to make four calls per  -hour workday
The mean call duration is  min
Ten percent of the calls are long distance (
pass through the toll office)
What is the maximum number of telephones an end office can support? (Assume  kHz per circuit
) Explain why a telephone company may decide to support a lesser number of telephones than this maximum number at the end office A regional telephone company has   million subscribers
Each of their telephones is connected to a central office by a copper twisted pair
The average length of these twisted pairs is   km
How much is the copper in the local loops worth? Assume
PROBLEMS that the cross tion of each strand is a circle  mm in diameter  the density of copper is
grams/cm  and that copper sells for $  per kilogram Is an oil pipeline a simplex system  a half-duplex system  a full-duplex system  or none of the above? What about a river or a walkie-talkie-style communication?
The cost of a fast microprocessor has dropped to the point where it is now possible to put one in each modem
How does that affect the handling of telephone line errors? Does it negate the need for error checking/correction in layer  ?
A modem constellation diagram similar to Fig
-  has data points at the following coordinates: (   )  (  â )  (â   )  and (â  â )
How many bps can a modem with these parameters achieve at  symbols/ond?
What is the maximum bit rate achievable in a V
standard modem if the baud rate is  and no error correction is used?
How many frequencies does a full-duplex QAM-  modem use?
Ten signals  each requiring  Hz  are multiplexed onto a single channel using FDM
What is the minimum bandwidth required for the multiplexed channel? Assume that the guard bands are Hz wide Why has the PCM sampling time been set at Î¼?
What is the percent overhead on a T  carrier? That is  what percent of the
Mbps are not delivered to the end user? How does it relate to the percent overhead in OC-  or OC-   lines?
Compare the maximum data rate of a noiseless  -kHz channel using (a) Analog encoding (
QPSK) with  bits per sample
(b) The T  PCM system If a T  carrier system slips and loses track of where it is  it tries to resynchronize using the first bit in each frame
How many frames will have to be inspected on average to resynchronize with a probability of
of being wrong?
What is the difference  if any  between the demodulator part of a modem and the coder part of a codec? (After all  both convert analog signals to digital ones
SONET clocks have a drift rate of about  part in
How long does it take for the drift to equal the width of  bit? Do you see any practical implications of this calculation? If so  what?
How long will it take to transmit a  -GB file from one VSAT to another using a hub as shown in Figure  -  ? Assume that the uplink is  Mbps  the downlink is  Mbps  and circuit switching is used with
circuit setup time Calculate the transmit time in the previous problem if packet switching is used instead
Assume that the packet size is   KB  the switching delay in the satellite and hub is   microonds  and the packet header size is   bytes In Fig
Show how this number can be derived from the SONET OC-  parameters
What will be the gross  SPE  and user data rates of an OC- line? THE PHYSICAL LAYER   To accommodate lower data rates than STS-  SONET has a system of virtual tributaries (VTs)
A VT is a partial payload that can be inserted into an STS-  frame and combined with other partial payloads to fill the data frame
uses  columns  VT  uses  columns  VT  uses  columns  and VT  uses   columns of an STS-  frame
Which VT can accommodate (a) A DS-  service (
Mbps)? (b) European CEPT-  service (
Mbps)? (c) A DS-  service (
What is the available user bandwidth in an OC-  c connection?
Three packet-switching networks each contain n nodes
The first network has a star topology with a central switch  the ond is a (bidirectional) ring  and the third is fully interconnected  with a wire from every node to every other node
What are the best-  average-  and worst-case transmission paths in hops?
Compare the delay in sending an x-bit message over a k-hop path in a circuit-switched network and in a (lightly loaded) packet-switched network
The circuit setup time is s   the propagation delay is d  per hop  the packet size is p bits  and the data rate is b bps
Under what conditions does the packet network have a lower delay? Also  explain the conditions under which a packet-switched network is preferable to a circuitswitched network Suppose that x bits of user data are to be transmitted over a k-hop path in a packetswitched network as a series of packets  each containing p data bits and h header bits  with x >> p + h
The bit rate of the lines is b bps and the propagation delay is negligible
What value of p minimizes the total delay?
In a typical mobile phone system with hexagonal cells  it is forbidden to reuse a frequency band in an adjacent cell
If frequencies are available  how many can be used in a given cell?
The actual layout of cells is seldom as regular that as shown in Fig
Even the shapes of individual cells are typically irregular
Give a possible reason why this might be
How do these irregular shapes affect frequency assignment to each cell?
Make a rough estimate of the number of PCS microcells m in diameter it would take to cover San Francisco (   square km) Sometimes when a mobile user crosses the boundary from one cell to another  the current call is abruptly terminated  even though all transmitters and receivers are functioning perfectly
Suppose that A  B  and C are simultaneously transmitting  bits  using a CDMA system with the chip sequences of Fig
What is the resulting chip sequence?
Consider a different way of looking at the orthogonality property of CDMA chip sequences
Each bit in a pair of sequences can match or not match
Express the orthogonality property in terms of matches and mismatches A CDMA receiver gets the following chips: (â  +  â  +  â  â  +  + )
Assuming the chip sequences defined in Fig
In Figure  -   there are four stations that can transmit
Suppose four more stations are added
Provide the chip sequences of these stations At the low end  the telephone system is star shaped  with all the local loops in a neighborhood converging on an end office
In contrast  cable television consists of a single long cable snaking its way past all the houses in the same neighborhood
Suppose that a future TV cable were  -Gbps fiber instead of copper
Could it be used to simulate the telephone model of everybody having their own private line to the end office? If so  how many one-telephone houses could be hooked up to a single fiber?
A cable company decides to provide Internet access over cable in a neighborhood consisting of  houses
The company uses a coaxial cable and spectrum allocation allowing Mbps downstream bandwidth per cable
To attract customers  the company decides to guarantee at least  Mbps downstream bandwidth to each house at any time
Describe what the cable company needs to do to provide this guarantee Using the spectral allocation shown in Fig
How fast can a cable user receive data if the network is otherwise idle? Assume that the user interface is (a)  -Mbps Ethernet (b)   -Mbps Ethernet (c)  -Mbps Wireless Multiplexing STS-  multiple data streams  called tributaries  plays an important role in SONET
A  :  multiplexer multiplexes three input STS-  tributaries onto one output STS-  stream
This multiplexing is done byte for byte
That is  the first three output bytes are the first bytes of tributaries  and   respectively
the next three output bytes are the ond bytes of tributaries  and   respectively  and so on
Write a program that simulates this  :  multiplexer
Your program should consist of five processes
The main process creates four processes  one each for the three STS-  tributaries and one for the multiplexer
Each tributary process reads in an STS-  frame from an input file as a sequence of bytes
They send their frames (byte by byte) to the multiplexer process
The multiplexer process receives these bytes and outputs an STS-  frame (byte by byte) by writing it to standard output
Use pipes for communication among processes Write a program to implement CDMA
Assume that the length of a chip sequence is eight and the number of stations transmitting is four
Your program consists of three sets of processes: four transmitter processes (t  t  t  and t )  one joiner process  and four receiver processes (r  r  r  and r )
The main program  which also acts as the joiner process first reads four chip sequences (bipolar notation) from the standard input and a sequence of  bits (  bit per transmitter process to be transmitted)  and forks off four pairs of transmitter and receiver processes
Each pair of transmitter/receiver processes (t  r ; t  r ; t  r ; t  r ) is assigned one chip sequence and each transmitter process is assigned  bit (first bit to t  ond bit to t  and so on)
Next  each transmitter process computes the signal to be transmitted (a sequence of  bits) and sends it to the joiner process
After receiving signals from all four transmitter processes  the joiner process combines the signals and sends the combined signal to THE PHYSICAL LAYER
the four receiver processes
Each receiver process then computes the bit it has received and prints it to standard output
Use pipes for communication between processes
THE DATA LINK LAYER In this  ter we will study the design principles for the ond layer in our model  the data link layer
This study deals with algorithms for achieving reliable  efficient communication of whole units of information called frames (rather than individual bits  as in the physical layer) between two adjacent machines
By adjacent  we mean that the two machines are connected by a communication channel that acts conceptually like a wire (
a coaxial cable  telephone line  or wireless channel)
The essential property of a channel that makes it ââwire-likeââ is that the bits are delivered in exactly the same order in which they are sent
At first you might think this problem is so trivial that there is nothing to studyâmachine A just puts the bits on the wire  and machine B just takes them off
Unfortunately  communication channels make errors occasionally
Furthermore  they have only a finite data rate  and there is a nonzero propagation delay between the time a bit is sent and the time it is received
These limitations have important implications for the efficiency of the data transfer
The protocols used for communications must take all these factors into consideration
These protocols are the subject of this  ter
After an introduction to the key design issues present in the data link layer  we will start our study of its protocols by looking at the nature of errors and how they can be detected and corrected
Then we will study a series of increasingly complex protocols  each one solving more and more of the problems present in this layer
Finally  we will conclude with some examples of data link protocols
THE DATA LINK LAYER
DATA LINK LAYER DESIGN ISSUES The data link layer uses the services of the physical layer to send and receive bits over communication channels
It has a number of functions  including:
Providing a well-defined service interface to the network layer Dealing with transmission errors Regulating the flow of data so that slow receivers are not swamped by fast senders
To accomplish these goals  the data link layer takes the packets it gets from the network layer and encapsulates them into frames for transmission
Each frame contains a frame header  a payload field for holding the packet  and a frame trailer  as illustrated in Fig
Frame management forms the heart of what the data link layer does
In the following tions we will examine all the abovementioned issues in detail
Header Payload field Trailer Frame Sending machine Packet Packet Receiving machine Header Payload field Trailer Figure  -
Relationship between packets and frames
Although this  ter is explicitly about the data link layer and its protocols  many of the principles we will study here  such as error control and flow control  are found in transport and other protocols as well
That is because reliability is an overall goal  and it is achieved when all the layers work together
In fact  in many networks  these functions are found mostly in the upper layers  with the data link layer doing the minimal job that is ââgood enough
ââ However  no matter where they are found  the principles are pretty much the same
They often show up in their simplest and purest forms in the data link layer  making this a good place to examine them in detail
Services Provided to the Network Layer The function of the data link layer is to provide services to the network layer
The principal service is transferring data from the network layer on the source machine to the network layer on the destination machine
On the source machine is   DATA LINK LAYER DESIGN ISSUES an entity  call it a process  in the network layer that hands some bits to the data link layer for transmission to the destination
The job of the data link layer is to transmit the bits to the destination machine so they can be handed over to the network layer there  as shown in Fig
The actual transmission follows the path of Fig
For this reason  we will implicitly use the model of Fig
Host  Host  Host  Host  Virtual data path Actual data path (a) (b) Figure  -
(a) Virtual communication
(b) Actual communication
The data link layer can be designed to offer various services
The actual services that are offered vary from protocol to protocol
Three reasonable possibilities that we will consider in turn are:
Unacknowledged connectionless service Acknowledged connectionless service Acknowledged connection-oriented service
Unacknowledged connectionless service consists of having the source machine send independent frames to the destination machine without having the destination machine acknowledge them
Ethernet is a good example of a data link layer that provides this class of service
No logical connection is established beforehand or released afterward
If a frame is lost due to noise on the line  no THE DATA LINK LAYER
attempt is made to detect the loss or recover from it in the data link layer
This class of service is appropriate when the error rate is very low  so recovery is left to higher layers
It is also appropriate for real-time traffic  such as voice  in which late data are worse than bad data
The next step up in terms of reliability is acknowledged connectionless service
When this service is offered  there are still no logical connections used  but each frame sent is individually acknowledged
In this way  the sender knows whether a frame has arrived correctly or been lost
If it has not arrived within a specified time interval  it can be sent again
This service is useful over unreliable channels  such as wireless systems
(WiFi) is a good example of this class of service
It is perhaps worth emphasizing that providing acknowledgements in the data link layer is just an optimization  never a requirement
The network layer can always send a packet and wait for it to be acknowledged by its peer on the remote machine
If the acknowledgement is not forthcoming before the timer expires  the sender can just send the entire message again
The trouble with this strategy is that it can be inefficient
Links usually have a strict maximum frame length imposed by the hardware  and known propagation delays
The network layer does not know these parameters
It might send a large packet that is broken up into  say frames  of which  are lost on average
It would then take a very long time for the packet to get through
Instead  if individual frames are acknowledged and retransmitted  then errors can be corrected more directly and more quickly
On reliable channels  such as fiber  the overhead of a heavyweight data link protocol may be unnecessary  but on (inherently unreliable) wireless channels it is well worth the cost
Getting back to our services  the most sophisticated service the data link layer can provide to the network layer is connection-oriented service
With this service  the source and destination machines establish a connection before any data are transferred
Each frame sent over the connection is numbered  and the data link layer guarantees that each frame sent is indeed received
Furthermore  it guarantees that each frame is received exactly once and that all frames are received in the right order
Connection-oriented service thus provides the network layer processes with the equivalent of a reliable bit stream
It is appropriate over long  unreliable links such as a satellite channel or a long-distance telephone circuit
If acknowledged connectionless service were used  it is conceivable that lost acknowledgements could cause a frame to be sent and received several times  wasting bandwidth
When connection-oriented service is used  transfers go through three distinct phases
In the first phase  the connection is established by having both sides initialize variables and counters needed to keep track of which frames have been received and which ones have not
In the ond phase  one or more frames are actually transmitted
In the third and final phase  the connection is released  freeing up the variables  buffers  and other resources used to maintain the connection
DATA LINK LAYER DESIGN ISSUES    Framing To provide service to the network layer  the data link layer must use the service provided to it by the physical layer
What the physical layer does is accept a raw bit stream and attempt to deliver it to the destination
If the channel is noisy  as it is for most wireless and some wired links  the physical layer will add some redundancy to its signals to reduce the bit error rate to a tolerable level
However  the bit stream received by the data link layer is not guaranteed to be error free
Some bits may have different values and the number of bits received may be less than  equal to  or more than the number of bits transmitted
It is up to the data link layer to detect and  if necessary  correct errors
The usual approach is for the data link layer to break up the bit stream into discrete frames  compute a short token called a checksum for each frame  and include the checksum in the frame when it is transmitted
(Checksum algorithms will be discussed later in this  ter
) When a frame arrives at the destination  the checksum is recomputed
If the newly computed checksum is different from the one contained in the frame  the data link layer knows that an error has occurred and takes steps to deal with it (
discarding the bad frame and possibly also sending back an error report)
Breaking up the bit stream into frames is more difficult than it at first appears
A good design must make it easy for a receiver to find the start of new frames while using little of the channel bandwidth
We will look at four methods:
Byte count Flag bytes with byte stuffing Flag bits with bit stuffing Physical layer coding violations
The first framing method uses a field in the header to specify the number of bytes in the frame
When the data link layer at the destination sees the byte count  it knows how many bytes follow and hence where the end of the frame is
This technique is shown in Fig
The trouble with this algorithm is that the count can be garbled by a transmission error
For example  if the byte count of  in the ond frame of Fig
It will then be unable to locate the correct start of the next frame
Even if the checksum is incorrect so the destination knows that the frame is bad  it still has no way of telling where the next frame starts
Sending a frame back to the source asking for a retransmission does not help either  since the destination does not know how many bytes to skip over to get to the start of the retransmission
For this reason  the byte count method is rarely used by itself
THE DATA LINK LAYER
(b) (a)              Byte count One byte Error Frame bytes Frame  Frame bytes Frame  (Wrong) Frame bytes Frame bytes Now a byte count Figure  -
A byte stream
(a) Without errors
(b) With one error
The ond framing method gets around the problem of resynchronization after an error by having each frame start and end with special bytes
Often the same byte  called a flag byte  is used as both the starting and ending delimiter
This byte is shown in Fig
Two conutive flag bytes indicate the end of one frame and the start of the next
Thus  if the receiver ever loses synchronization it can just search for two flag bytes to find the end of the current frame and the start of the next frame
However  there is a still a problem we have to solve
It may happen that the flag byte occurs in the data  especially when binary data such as photographs or songs are being transmitted
This situation would interfere with the framing
One way to solve this problem is to have the senderâs data link layer insert a special escape byte (ESC) just before each ââaccidentalââ flag byte in the data
Thus  a framing flag byte can be distinguished from one in the data by the absence or presence of an escape byte before it
The data link layer on the receiving end removes the escape bytes before giving the data to the network layer
This technique is called byte stuffing
Of course  the next question is: what happens if an escape byte occurs in the middle of the data? The answer is that it  too  is stuffed with an escape byte
At the receiver  the first escape byte is removed  leaving the data byte that follows it (which might be another escape byte or the flag byte)
Some examples are shown in Fig
In all cases  the byte sequence delivered after destuffing is exactly the same as the original byte sequence
We can still search for a frame boundary by looking for two flag bytes in a row  without bothering to undo escapes
The byte-stuffing scheme depicted in Fig
-  is a slight simplification of the one used in PPP (Point-to-Point Protocol)  which is used to carry packets over communications links
We will discuss PPP near the end of this  ter
DATA LINK LAYER DESIGN ISSUES A ESC FLAG B A ESC ESC B A ESC ESC ESC FLAG B A ESC ESC ESC ESC B A FLAG B A ESC B A ESC FLAG B A ESC ESC B FLAG Header Payload field Trailer FLAG Original bytes After stuffing (a) (b) Figure  -
(a) A frame delimited by flag bytes
(b) Four examples of byte sequences before and after byte stuffing
The third method of delimiting the bit stream gets around a disadvantage of byte stuffing  which is that it is tied to the use of  -bit bytes
Framing can be also be done at the bit level  so frames can contain an arbitrary number of bits made up of units of any size
It was developed for the once very popular HDLC (Highlevel Data Link Control) protocol
Each frame begins and ends with a special bit pattern  or  x E in hexadecimal
This pattern is a flag byte
Whenever the senderâs data link layer encounters five conutive  s in the data  it automatically stuffs a  bit into the outgoing bit stream
This bit stuffing is analogous to byte stuffing  in which an escape byte is stuffed into the outgoing character stream before a flag byte in the data
It also ensures a minimum density of transitions that help the physical layer maintain synchronization
USB (Universal Serial Bus) uses bit stuffing for this reason
When the receiver sees five conutive incoming  bits  followed by a  bit  it automatically destuffs (
deletes) the  bit
Just as byte stuffing is completely transparent to the network layer in both computers  so is bit stuffing
If the user data contain the flag pattern   this flag is transmitted as  but stored in the receiverâs memory as
Figure  -  gives an example of bit stuffing
With bit stuffing  the boundary between two frames can be unambiguously recognized by the flag pattern
Thus  if the receiver loses track of where it is  all it has to do is scan the input for flag sequences  since they can only occur at frame boundaries and never within the data
THE DATA LINK LAYER
Stuffed bits (a) (b) (c)         Figure  -
Bit stuffing
(a) The original data
(b) The data as they appear on the line
(c) The data as they are stored in the receiverâs memory after destuffing
With both bit and byte stuffing  a side effect is that the length of a frame now depends on the contents of the data it carries
For instance  if there are no flag bytes in the data  bytes might be carried in a frame of roughly bytes
If  however  the data consists solely of flag bytes  each flag byte will be escaped and the frame will become roughly bytes long
With bit stuffing  the increase would be roughly
% as  bit is added to every byte
The last method of framing is to use a shortcut from the physical layer
that the encoding of bits as signals often includes redundancy to help the receiver
This redundancy means that some signals will not occur in regular data
For example  in the  B/ B line code  data bits are mapped to  signal bits to ensure sufficient bit transitions
This means that   out of the   signal possibilities are not used
We can use some reserved signals to indicate the start and end of frames
In effect  we are using ââcoding violationsââ to delimit frames
The beauty of this scheme is that  because they are reserved signals  it is easy to find the start and end of frames and there is no need to stuff the data
Many data link protocols use a combination of these methods for safety
A common pattern used for Ethernet and
is to have a frame begin with a well-defined pattern called a preamble
This pattern might be quite long (  bits is typical for
) to allow the receiver to prepare for an incoming packet
The preamble is then followed by a length (
count) field in the header that is used to locate the end of the frame
Error Control Having solved the problem of marking the start and end of each frame  we come to the next problem: how to make sure all frames are eventually delivered to the network layer at the destination and in the proper order
Assume for the moment that the receiver can tell whether a frame that it receives contains correct or faulty information (we will look at the codes that are used to detect and correct transmission errors in  )
For unacknowledged connectionless service it might be fine if the sender just kept outputting frames without regard to whether   DATA LINK LAYER DESIGN ISSUES they were arriving properly
But for reliable  connection-oriented service it would not be fine at all
The usual way to ensure reliable delivery is to provide the sender with some feedback about what is happening at the other end of the line
Typically  the protocol calls for the receiver to send back special control frames bearing positive or negative acknowledgements about the incoming frames
If the sender receives a positive acknowledgement about a frame  it knows the frame has arrived safely
On the other hand  a negative acknowledgement means that something has gone wrong and the frame must be transmitted again
An additional complication comes from the possibility that hardware troubles may cause a frame to vanish completely (
in a noise burst)
In this case  the receiver will not react at all  since it has no reason to react
Similarly  if the acknowledgement frame is lost  the sender will not know how to proceed
It should be clear that a protocol in which the sender transmits a frame and then waits for an acknowledgement  positive or negative  will hang forever if a frame is ever lost due to  for example  malfunctioning hardware or a faulty communication channel
This possibility is dealt with by introducing timers into the data link layer
When the sender transmits a frame  it generally also starts a timer
The timer is set to expire after an interval long enough for the frame to reach the destination  be processed there  and have the acknowledgement propagate back to the sender
Normally  the frame will be correctly received and the acknowledgement will get back before the timer runs out  in which case the timer will be canceled
However  if either the frame or the acknowledgement is lost  the timer will go off  alerting the sender to a potential problem
The obvious solution is to just transmit the frame again
However  when frames may be transmitted multiple times there is a danger that the receiver will accept the same frame two or more times and pass it to the network layer more than once
To prevent this from happening  it is generally necessary to assign sequence numbers to outgoing frames  so that the receiver can distinguish retransmissions from originals
The whole issue of managing the timers and sequence numbers so as to ensure that each frame is ultimately passed to the network layer at the destination exactly once  no more and no less  is an important part of the duties of the data link layer (and higher layers)
Later in this  ter  we will look at a series of increasingly sophisticated examples to see how this management is done
Flow Control Another important design issue that occurs in the data link layer (and higher layers as well) is what to do with a sender that systematically wants to transmit frames faster than the receiver can accept them
This situation can occur when the sender is running on a fast  powerful computer and the receiver is running on a slow  low-end machine
A common situation is when a smart phone requests a Web page from a far more powerful server  which then turns on the fire hose and THE DATA LINK LAYER
blasts the data at the poor helpless phone until it is completely swamped
Even if the transmission is error free  the receiver may be unable to handle the frames as fast as they arrive and will lose some
Clearly  something has to be done to prevent this situation
Two approaches are commonly used
In the first one  feedback-based flow control  the receiver sends back information to the sender giving it permission to send more data  or at least telling the sender how the receiver is doing
In the ond one  rate-based flow control  the protocol has a built-in mechanism that limits the rate at which senders may transmit data  without using feedback from the receiver
In this  ter we will study feedback-based flow control schemes  primarily because rate-based schemes are only seen as part of the transport layer (
Feedback-based schemes are seen at both the link layer and higher layers
The latter is more common these days  in which case the link layer hardware is designed to run fast enough that it does not cause loss
For example  hardware implementations of the link layer as NICs (Network Interface Cards) are sometimes said to run at ââwire speed ââ meaning that they can handle frames as fast as they can arrive on the link
Any overruns are then not a link problem  so they are handled by higher layers
Various feedback-based flow control schemes are known  but most of them use the same basic principle
The protocol contains well-defined rules about when a sender may transmit the next frame
These rules often prohibit frames from being sent until the receiver has granted permission  either implicitly or explicitly
For example  when a connection is set up the receiver might say: ââYou may send me n frames now  but after they have been sent  do not send any more until I have told you to continue
ââ We will examine the details shortly  ERROR DETECTION AND CORRECTION We saw in
that communication channels have a range of characteristics
Some channels  like optical fiber in telecommunications networks  have tiny error rates so that transmission errors are a rare occurrence
But other channels  especially wireless links and aging local loops  have error rates that are orders of magnitude larger
For these links  transmission errors are the norm
They cannot be avoided at a reasonable expense or cost in terms of performance
The conclusion is that transmission errors are here to stay
We have to learn how to deal with them
Network designers have developed two basic strategies for dealing with errors
Both add redundant information to the data that is sent
One strategy is to include enough redundant information to enable the receiver to deduce what the transmitted data must have been
The other is to include only enough redundancy to allow the receiver to deduce that an error has occurred (but not which error)   ERROR DETECTION AND CORRECTION and have it request a retransmission
The former strategy uses error-correcting codes and the latter uses error-detecting codes
The use of error-correcting codes is often referred to as FEC (Forward Error Correction)
Each of these techniques occupies a different ecological niche
On channels that are highly reliable  such as fiber  it is cheaper to use an error-detecting code and just retransmit the occasional block found to be faulty
However  on channels such as wireless links that make many errors  it is better to add redundancy to each block so that the receiver is able to figure out what the originally transmitted block was
FEC is used on noisy channels because retransmissions are just as likely to be in error as the first transmission
A key consideration for these codes is the type of errors that are likely to occur
Neither error-correcting codes nor error-detecting codes can handle all possible errors since the redundant bits that offer protection are as likely to be received in error as the data bits (which can compromise their protection)
It would be nice if the channel treated redundant bits differently than data bits  but it does not
They are all just bits to the channel
This means that to avoid undetected errors the code must be strong enough to handle the expected errors
One model is that errors are caused by extreme values of thermal noise that overwhelm the signal briefly and occasionally  giving rise to isolated single-bit errors
Another model is that errors tend to come in bursts rather than singly
This model follows from the physical processes that generate themâsuch as a deep fade on a wireless channel or transient electrical interference on a wired channel/ Both models matter in practice  and they have different trade-offs
Having the errors come in bursts has both advantages and disadvantages over isolated singlebit errors
On the advantage side  computer data are always sent in blocks of bits
Suppose that the block size was  bits and the error rate was
If errors were independent  most blocks would contain an error
If the errors came in bursts of  however  only one block in would be affected  on average
The disadvantage of burst errors is that when they do occur they are much harder to correct than isolated errors
Other types of errors also exist
Sometimes  the location of an error will be known  perhaps because the physical layer received an analog signal that was far from the expected value for a  or  and declared the bit to be lost
This situation is called an erasure channel
It is easier to correct errors in erasure channels than in channels that flip bits because even if the value of the bit has been lost  at least we know which bit is in error
However  we often do not have the benefit of erasures
We will examine both error-correcting codes and error-detecting codes next
Please keep two points in mind  though
First  we cover these codes in the link layer because this is the first place that we have run up against the problem of reliably transmitting groups of bits
However  the codes are widely used because reliability is an overall concern
Error-correcting codes are also seen in the physical layer  particularly for noisy channels  and in higher layers  particularly for THE DATA LINK LAYER
real-time media and content distribution
Error-detecting codes are commonly used in link  network  and transport layers
The ond point to bear in mind is that error codes are applied mathematics
Unless you are particularly adept at Galois fields or the properties of sparse matrices  you should get codes with good properties from a reliable source rather than making up your own
In fact  this is what many protocol standards do  with the same codes coming up again and again
In the material below  we will study a simple code in detail and then briefly describe advanced codes
In this way  we can understand the trade-offs from the simple code and talk about the codes that are used in practice via the advanced codes
Error-Correcting Codes We will examine four different error-correcting codes:
Hamming codes Binary convolutional codes Reed-Solomon codes Low-Density Parity Check codes
All of these codes add redundancy to the information that is sent
A frame consists of m data (
message) bits and r redundant (
check) bits
In a block code  the r check bits are computed solely as a function of the m data bits with which they are associated  as though the m bits were looked up in a large table to find their corresponding r check bits
In a systematic code  the m data bits are sent directly  along with the check bits  rather than being encoded themselves before they are sent
In a linear code  the r check bits are computed as a linear function of the m data bits
Exclusive OR (XOR) or modulo  addition is a popular choice
This means that encoding can be done with operations such as matrix multiplications or simple logic circuits
The codes we will look at in this tion are linear  systematic block codes unless otherwise noted
Let the total length of a block be n (
n = m + r)
We will describe this as an (n m) code
An n-bit unit containing data and check bits is referred to as an nbit codeword
The code rate  or simply rate  is the fraction of the codeword that carries information that is not redundant  or m/n
The rates used in practice vary widely
They might be  /  for a noisy channel  in which case half of the received information is redundant  or close to  for a high-quality channel  with only a small number of check bits added to a large message
To understand how errors can be handled  it is necessary to first look closely at what an error really is
Given any two codewords that may be transmitted or receivedâsay  and    âit is possible to determine how many   ERROR DETECTION AND CORRECTION corresponding bits differ
In this case   bits differ
To determine how many bits differ  just XOR the two codewords and count the number of  bits in the result
For example:      The number of bit positions in which two codewords differ is called the Hamming distance (Hamming  )
Its significance is that if two codewords are a Hamming distance d apart  it will require d single-bit errors to convert one into the other
Given the algorithm for computing the check bits  it is possible to construct a complete list of the legal codewords  and from this list to find the two codewords with the smallest Hamming distance
This distance is the Hamming distance of the complete code
In most data transmission applications  all  m possible data messages are legal  but due to the way the check bits are computed  not all of the  n possible codewords are used
In fact  when there are r check bits  only the small fraction of  m / n or  / r of the possible messages will be legal codewords
It is the sparseness with which the message is embedded in the space of codewords that allows the receiver to detect and correct errors
The error-detecting and error-correcting properties of a block code depend on its Hamming distance
To reliably detect d errors  you need a distance d +  code because with such a code there is no way that d single-bit errors can change a valid codeword into another valid codeword
When the receiver sees an illegal codeword  it can tell that a transmission error has occurred
Similarly  to correct d errors  you need a distance  d +  code because that way the legal codewords are so far apart that even with d changes the original codeword is still closer than any other codeword
This means the original codeword can be uniquely determined based on the assumption that a larger number of errors are less likely
As a simple example of an error-correcting code  consider a code with only four valid codewords:      and   This code has a distance of   which means that it can correct double errors or detect quadruple errors
If the codeword   arrives and we expect only single- or double-bit errors  the receiver will know that the original must have been
If  however  a triple error changes   into    the error will not be corrected properly
Alternatively  if we expect all of these errors  we can detect them
None of the received codewords are legal codewords so an error must have occurred
It should be apparent that in this example we cannot both correct double errors and detect quadruple errors because this would require us to interpret a received codeword in two different ways
THE DATA LINK LAYER
In our example  the task of decoding by finding the legal codeword that is closest to the received codeword can be done by inspection
Unfortunately  in the most general case where all codewords need to be evaluated as candidates  this task can be a time-consuming search
Instead  practical codes are designed so that they admit shortcuts to find what was likely the original codeword
Imagine that we want to design a code with m message bits and r check bits that will allow all single errors to be corrected
Each of the  m legal messages has n illegal codewords at a distance of  from it
These are formed by systematically inverting each of the n bits in the n-bit codeword formed from it
Thus  each of the  m legal messages requires n +  bit patterns dedicated to it
Since the total number of bit patterns is  n  we must have (n +  ) m â¤  n
Using n = m + r  this requirement becomes (m + r +  ) â¤  r ( - ) Given m  this puts a lower limit on the number of check bits needed to correct single errors
This theoretical lower limit can  in fact  be achieved using a method due to Hamming (   )
In Hamming codes the bits of the codeword are numbered conutively  starting with bit  at the left end  bit  to its immediate right  and so on
The bits that are powers of  (   etc
) are check bits
The rest (  etc
) are filled up with the m data bits
This pattern is shown for an (   ) Hamming code with  data bits and  check bits in Fig
Each check bit forces the modulo  sum  or parity  of some collection of bits  including itself  to be even (or odd)
A bit may be included in several check bit computations
To see which check bits the data bit in position k contributes to  rewrite k as a sum of powers of
For example =  +  +  and   =  +  +  +
A bit is checked by just those check bits occurring in its expansion (
bit   is checked by bits  and  )
In the example  the check bits are computed for even parity sums for a message that is the ASCII letter ââA
ââ Sent codeword Received codeword     p  p  m  p  m  m  m  p  m  m  m  Check bits Channel    bit error Syndrome    Check results A    Flip bit  A    Message Message Figure  -
Example of an ( ) Hamming code correcting a single-bit error
This construction gives a code with a Hamming distance of   which means that it can correct single errors (or detect double errors)
The reason for the very careful numbering of message and check bits becomes apparent in the decoding   ERROR DETECTION AND CORRECTION process
When a codeword arrives  the receiver redoes the check bit computations including the values of the received check bits
We call these the check results
If the check bits are correct then  for even parity sums  each check result should be zero
In this case the codeword is accepted as valid
If the check results are not all zero  however  an error has been detected
The set of check results forms the error syndrome that is used to pinpoint and correct the error
This gives a syndrome of  or  +  =
By the design of the scheme  this means that the fifth bit is in error
Flipping the incorrect bit (which might be a check bit or a data bit) and discarding the check bits gives the correct message of an ASCII ââA
ââ Hamming distances are valuable for understanding block codes  and Hamming codes are used in error-correcting memory
However  most networks use stronger codes
The ond code we will look at is a convolutional code
This code is the only one we will cover that is not a block code
In a convolutional code  an encoder processes a sequence of input bits and generates a sequence of output bits
There is no natural message size or encoding boundary as in a block code
The output depends on the current and previous input bits
That is  the encoder has memory
The number of previous bits on which the output depends is called the constraint length of the code
Convolutional codes are specified in terms of their rate and constraint length
Convolutional codes are widely used in deployed networks  for example  as part of the GSM mobile phone system  in satellite communications  and in    As an example  a popular convolutional code is shown in Fig
This code is known as the NASA convolutional code of r =  /  and k =   since it was first used for the Voyager space missions starting in
Since then it has been liberally reused  for example  as part of    Input bit Output bit  S  S  S  S  S  S  Output bit  Figure  -
The NASA binary convolutional code used in    In Fig
Since it deals with bits and performs linear operations  this is a binary  linear convolutional code
Since  input bit produces  output bits  the code rate is  /
It is not systematic since none of the output bits is simply the input bit
THE DATA LINK LAYER
The internal state is kept in six memory registers
Each time another bit is input the values in the registers are shifted to the right
For example  if is input and the initial state is all zeros  the internal state  written left to right  will become    and   after the first  ond  and third bits have been input
The output bits will be followed by and then
It takes seven shifts to flush an input completely so that it does not affect the output
The constraint length of this code is thus k =
A convolutional code is decoded by finding the sequence of input bits that is most likely to have produced the observed sequence of output bits (which includes any errors)
For small values of k  this is done with a widely used algorithm developed by Viterbi (Forney  )
The algorithm walks the observed sequence  keeping for each step and for each possible internal state the input sequence that would have produced the observed sequence with the fewest errors
The input sequence requiring the fewest errors at the end is the most likely message
Convolutional codes have been popular in practice because it is easy to factor the uncertainty of a bit being a  or a  into the decoding
For example  suppose â V is the logical  level and + V is the logical  level  we might receive
V for  bits
Instead of mapping these signals to  and  right away  we would like to treat
V as ââvery likely a  ââ and â
V as ââmaybe a  ââ and correct the sequence as a whole
Extensions of the Viterbi algorithm can work with these uncertainties to provide stronger error correction
This approach of working with the uncertainty of a bit is called soft-decision decoding
Conversely  deciding whether each bit is a  or a  before subsequent error correction is called hard-decision decoding
The third kind of error-correcting code we will describe is the Reed-Solomon code
Like Hamming codes  Reed-Solomon codes are linear block codes  and they are often systematic too
Unlike Hamming codes  which operate on individual bits  Reed-Solomon codes operate on m bit symbols
Naturally  the mathematics are more involved  so we will describe their operation by analogy
Reed-Solomon codes are based on the fact that every n degree polynomial is uniquely determined by n +  points
For example  a line having the form ax + b is determined by two points
Extra points on the same line are redundant  which is helpful for error correction
Imagine that we have two data points that represent a line and we send those two data points plus two check points chosen to lie on the same line
If one of the points is received in error  we can still recover the data points by fitting a line to the received points
Three of the points will lie on the line  and one point  the one in error  will not
By finding the line we have corrected the error
Reed-Solomon codes are actually defined as polynomials that operate over finite fields  but they work in a similar manner
For m bit symbols  the codewords are  mâ  symbols long
A popular choice is to make m =  so that symbols are bytes
A codeword is then bytes long
The (   ) code is widely used; it adds   redundant symbols to data symbols
Decoding with error correction   ERROR DETECTION AND CORRECTION is done with an algorithm developed by Berlekamp and Massey that can efficiently perform the fitting task for moderate-length codes (Massey  )
Reed-Solomon codes are widely used in practice because of their strong error-correction properties  particularly for burst errors
They are used for DSL  data over cable  satellite communications  and perhaps most ubiquitously on CDs  DVDs  and Blu-ray discs
Because they are based on m bit symbols  a single-bit error and an m-bit burst error are both treated simply as one symbol error
When  t redundant symbols are added  a Reed-Solomon code is able to correct up to t errors in any of the transmitted symbols
This means  for example  that the (   ) code  which has   redundant symbols  can correct up to   symbol errors
Since the symbols may be conutive and they are each  bits  an error burst of up to bits can be corrected
The situation is even better if the error model is one of erasures (
a scratch on a CD that obliterates some symbols)
In this case  up to  t errors can be corrected
Reed-Solomon codes are often used in combination with other codes such as a convolutional code
The thinking is as follows
Convolutional codes are effective at handling isolated bit errors  but they will fail  likely with a burst of errors  if there are too many errors in the received bit stream
By adding a Reed-Solomon code within the convolutional code  the Reed-Solomon decoding can mop up the error bursts  a task at which it is very good
The overall code then provides good protection against both single and burst errors
The final error-correcting code we will cover is the LDPC (Low-Density Parity Check) code
LDPC codes are linear block codes that were invented by Robert Gallagher in his doctoral thesis (Gallagher  )
Like most theses  they were promptly forgotten  only to be reinvented in  when advances in computing power had made them practical
In an LDPC code  each output bit is formed from only a fraction of the input bits
This leads to a matrix representation of the code that has a low density of  s  hence the name for the code
The received codewords are decoded with an approximation algorithm that iteratively improves on a best fit of the received data to a legal codeword
This corrects errors
LDPC codes are practical for large block sizes and have excellent error-correction abilities that outperform many other codes (including the ones we have looked at) in practice
For this reason they are rapidly being included in new protocols
They are part of the standard for digital video broadcasting Gbps Ethernet  power-line networks  and the latest version of    Expect to see more of them in future networks
Error-Detecting Codes Error-correcting codes are widely used on wireless links  which are notoriously noisy and error prone when compared to optical fibers
Without error-correcting codes  it would be hard to get anything through
However  over fiber or THE DATA LINK LAYER
high-quality copper  the error rate is much lower  so error detection and retransmission is usually more efficient there for dealing with the occasional error
We will examine three different error-detecting codes
They are all linear  systematic block codes:
Parity Checksums Cyclic Redundancy Checks (CRCs)
To see how they can be more efficient than error-correcting codes  consider the first error-detecting code  in which a single parity bit is appended to the data
The parity bit is chosen so that the number of  bits in the codeword is even (or odd)
Doing this is equivalent to computing the (even) parity bit as the modulo  sum or XOR of the data bits
For example  when    is sent in even parity  a bit is added to the end to make it
With odd parity    becomes
A code with a single parity bit has a distance of   since any single-bit error produces a codeword with the wrong parity
This means that it can detect single-bit errors
Consider a channel on which errors are isolated and the error rate is  â  per bit
This may seem a tiny error rate  but it is at best a fair rate for a long wired cable that is challenging for error detection
Typical LAN links provide bit error rates of  â
Let the block size be  bits
To provide error correction for -bit blocks  we know from Eq
( - ) that   check bits are needed
Thus  a megabit of data would require   check bits
To merely detect a block with a single  -bit error  one parity bit per block will suffice
Once every  blocks  a block will be found to be in error and an extra block ( bits) will have to be transmitted to repair the error
The total overhead for the error detection and retransmission method is only  bits per megabit of data  versus   bits for a Hamming code
One difficulty with this scheme is that a single parity bit can only reliably detect a single-bit error in the block
If the block is badly garbled by a long burst error  the probability that the error will be detected is only
which is hardly acceptable
The odds can be improved considerably if each block to be sent is regarded as a rectangular matrix n bits wide and k bits high
Now  if we compute and send one parity bit for each row  up to k bit errors will be reliably detected as long as there is at most one error per row
However  there is something else we can do that provides better protection against burst errors: we can compute the parity bits over the data in a different order than the order in which the data bits are transmitted
Doing so is called interleaving
In this case  we will compute a parity bit for each of the n columns and send all the data bits as k rows  sending the rows from top to bottom and the bits in each row from left to right in the usual manner
At the last row  we send the n parity bits
This transmission order is shown in Fig
-  for n =  and k =
ERROR DETECTION AND CORRECTION Burst error Channel Transmit order Parity bits    N c l w o r k Parity errors    N e t w o r k                 Figure  -
Interleaving of parity bits to detect a burst error
Interleaving is a general technique to convert a code that detects (or corrects) isolated errors into a code that detects (or corrects) burst errors
(A burst error does not imply that all the bits are wrong; it just implies that at least the first and last are wrong
) At most  bit in each of the n columns will be affected  so the parity bits on those columns will detect the error
This method uses n parity bits on blocks of kn data bits to detect a single burst error of length n or less
A burst of length n +  will pass undetected  however  if the first bit is inverted  the last bit is inverted  and all the other bits are correct
If the block is badly garbled by a long burst or by multiple shorter bursts  the probability that any of the n columns will have the correct parity by accident is
so the probability of a bad block being accepted when it should not be is  ân
The ond kind of error-detecting code  the checksum  is closely related to groups of parity bits
The word ââchecksumââ is often used to mean a group of check bits associated with a message  regardless of how are calculated
A group of parity bits is one example of a checksum
However  there are other  stronger checksums based on a running sum of the data bits of the message
The checksum is usually placed at the end of the message  as the complement of the sum function
This way  errors may be detected by summing the entire received codeword  both data bits and checksum
If the result comes out to be zero  no error has been detected
One example of a checksum is the  -bit Internet checksum used on all Internet packets as part of the IP protocol (Braden et al
This checksum is a sum of the message bits divided into  -bit words
Because this method operates on words rather than on bits  as in parity  errors that leave the parity unchanged can still alter the sum and be detected
For example  if the lowest order bit in two different words is flipped from a  to a   a parity check across these bits would fail to detect an error
However  two  s will be added to the  -bit checksum to produce a different result
The error can then be detected
THE DATA LINK LAYER
The Internet checksum is computed in oneâs complement arithmetic instead of as the modulo sum
In oneâs complement arithmetic  a negative number is the bitwise complement of its positive counterpart
Modern computers run twoâs complement arithmetic  in which a negative number is the oneâs complement plus one
On a twoâs complement computer  the oneâs complement sum is equivalent to taking the sum modulo and adding any overflow of the high order bits back into the low-order bits
This algorithm gives a more uniform coverage of the data by the checksum bits
Otherwise  two high-order bits can be added  overflow  and be lost without changing the sum
There is another benefit  too
Oneâs complement has two representations of zero  all  s and all  s
This allows one value (
all  s) to indicate that there is no checksum  without the need for another field
For decades  it has always been assumed that frames to be checksummed contain random bits
All analyses of checksum algorithms have been made under this assumption
Inspection of real data by Partridge et al
(   ) has shown this assumption to be quite wrong
As a consequence  undetected errors are in some cases much more common than had been previously thought
The Internet checksum in particular is efficient and simple but provides weak protection in some cases precisely because it is a simple sum
It does not detect the deletion or addition of zero data  nor swapping parts of the message  and it provides weak protection against message splices in which parts of two packets are put together
These errors may seem very unlikely to occur by random processes  but they are just the sort of errors that can occur with buggy hardware
A better choice is Fletcherâs checksum (Fletcher  )
It includes a positional component  adding the product of the data and its position to the running sum
This provides stronger detection of changes in the position of data
Although the two preceding schemes may sometimes be adequate at higher layers  in practice  a third and stronger kind of error-detecting code is in widespread use at the link layer: the CRC (Cyclic Redundancy Check)  also known as a polynomial code
Polynomial codes are based upon treating bit strings as representations of polynomials with coefficients of  and  only
A k-bit frame is regarded as the coefficient list for a polynomial with k terms  ranging from x k â  to x
Such a polynomial is said to be of degree k â
The high-order (leftmost) bit is the coefficient of x k â   the next bit is the coefficient of x k â   and so on
For example    has  bits and thus represents a six-term polynomial with coefficients   and  :  x  +  x  +  x  +  x  +  x  +  x
Polynomial arithmetic is done modulo   according to the rules of algebraic field theory
It does not have carries for addition or borrows for subtraction
Both addition and subtraction are identical to exclusive OR
For example:      +  +  â  â       Long division is carried out in exactly the same way as it is in binary except that   ERROR DETECTION AND CORRECTION the subtraction is again done modulo
A divisor is said ââto go intoââ a dividend if the dividend has as many bits as the divisor
When the polynomial code method is employed  the sender and receiver must agree upon a generator polynomial  G(x)  in advance
Both the high- and loworder bits of the generator must be
To compute the CRC for some frame with m bits corresponding to the polynomial M(x)  the frame must be longer than the generator polynomial
The idea is to append a CRC to the end of the frame in such a way that the polynomial represented by the checksummed frame is divisible by G(x)
When the receiver gets the checksummed frame  it tries dividing it by G(x)
If there is a remainder  there has been a transmission error
The algorithm for computing the CRC is as follows:
Let r be the degree of G(x)
Append r zero bits to the low-order end of the frame so it now contains m + r bits and corresponds to the polynomial x rM(x) Divide the bit string corresponding to G(x) into the bit string corresponding to x rM(x)  using modulo  division Subtract the remainder (which is always r or fewer bits) from the bit string corresponding to x rM(x) using modulo  subtraction
The result is the checksummed frame to be transmitted
Call its polynomial T(x)
Figure  -  illustrates the calculation for a frame   using the generator G(x) = x  + x +
It should be clear that T(x) is divisible (modulo  ) by G(x)
In any division problem  if you diminish the dividend by the remainder  what is left over is divisible by the divisor
For example  in base if you divide    by    the remainder is
If you then subtract  from  what is left over (  ) is divisible by
Now let us analyze the power of this method
What kinds of errors will be detected? Imagine that a transmission error occurs  so that instead of the bit string for T(x) arriving  T(x) + E(x) arrives
Each  bit in E(x) corresponds to a bit that has been inverted
If there are k  bits in E(x)  k single-bit errors have occurred
A single burst error is characterized by an initial   a mixture of  s and  s  and a final   with all other bits being
Upon receiving the checksummed frame  the receiver divides it by G(x); that is  it computes [T(x) + E(x)]/G(x)
T(x)/G(x) is   so the result of the computation is simply E(x)/G(x)
Those errors that happen to correspond to polynomials containing G(x) as a factor will slip by; all other errors will be caught
If there has been a single-bit error  E(x) = x i  where i determines which bit is in error
If G(x) contains two or more terms  it will never divide into E(x)  so all single-bit errors will be detected
THE DATA LINK LAYER
Remainder Quotient (thrown away) Frame with four zeros appended      Frame with four zeros appended minus remainder Transmitted frame:  Frame:      Generator: Figure  -
Example calculation of the CRC
If there have been two isolated single-bit errors  E(x) = x i + x j  where i > j
Alternatively  this can be written as E(x) = x j(x i â j +  )
If we assume that G(x) is not divisible by x  a sufficient condition for all double errors to be detected is that G(x) does not divide x k +  for any k up to the maximum value of i â j (
up to the maximum frame length)
Simple  low-degree polynomials that give protection to long frames are known
For example  x   + x   +  will not divide x k +  for any value of k below
If there are an odd number of bits in error  E(X) contains an odd number of terms (
x  + x  +   but not x  +  )
Interestingly  no polynomial with an odd number of terms has x +  as a factor in the modulo  system
By making x +  a factor of G(x)  we can catch all errors with an odd number of inverted bits
Finally  and importantly  a polynomial code with r check bits will detect all burst errors of length â¤ r
A burst error of length k can be represented by x i(x k â  +
+  )  where i determines how far from the right-hand end of the received frame the burst is located
If G(x) contains an x  term  it will not have x i as a factor  so if the degree of the parenthesized expression is less than the degree of G(x)  the remainder can never be zero
ERROR DETECTION AND CORRECTION If the burst length is r +   the remainder of the division by G(x) will be zero if and only if the burst is identical to G(x)
By definition of a burst  the first and last bits must be   so whether it matches depends on the r â  intermediate bits
If all combinations are regarded as equally likely  the probability of such an incorrect frame being accepted as valid is Â½r â
It can also be shown that when an error burst longer than r +  bits occurs or when several shorter bursts occur  the probability of a bad frame getting through unnoticed is Â½r  assuming that all bit patterns are equally likely
Certain polynomials have become international standards
The one used in IEEE followed the example of Ethernet and is x   + x   + x   + x   + x   + x   + x   + x   + x  + x  + x  + x  + x  + x  +  Among other desirable properties  it has the property that it detects all bursts of length   or less and all bursts affecting an odd number of bits
It has been used widely since the s
However  this does not mean it is the best choice
Using an exhaustive computational search  Castagnoli et al
(   ) and Koopman (   ) found the best CRCs
These CRCs have a Hamming distance of  for typical message sizes  while the IEEE standard CRC-  has a Hamming distance of only
Although the calculation required to compute the CRC may seem complicated  it is easy to compute and verify CRCs in hardware with simple shift register circuits (Peterson and Brown  )
In practice  this hardware is nearly always used
Dozens of networking standards include various CRCs  including virtually all LANs (
) and point-to-point links (
packets over SONET)  ELEMENTARY DATA LINK PROTOCOLS To introduce the subject of protocols  we will begin by looking at three protocols of increasing complexity
For interested readers  a simulator for these and subsequent protocols is available via the Web (see the preface)
Before we look at the protocols  it is useful to make explicit some of the assumptions underlying the model of communication
To start with  we assume that the physical layer  data link layer  and network layer are independent processes that communicate by passing messages back and forth
A common implementation is shown in Fig
The physical layer process and some of the data link layer process run on dedicate hardware called a NIC (Network Interface Card)
The rest of the link layer process and the network layer process run on the main CPU as part of the operating system  with the software for the link layer process often taking the form of a device driver
However  other implementations are also possible (
three processes offloaded to dedicated hardware called a network accelerator  or three processes running on the THE DATA LINK LAYER
main CPU on a software-defined ratio)
Actually  the preferred implementation changes from decade to decade with technology trade-offs
In any event  treating the three layers as separate processes makes the discussion conceptually cleaner and also serves to emphasize the independence of the layers
Network Cable (medium) PHY Link Link Application Network Interface Card (NIC) Driver Operating system Computer Figure  -
Implementation of the physical  data link  and network layers
Another key assumption is that machine A wants to send a long stream of data to machine B  using a reliable  connection-oriented service
Later  we will consider the case where B also wants to send data to A simultaneously
A is assumed to have an infinite supply of data ready to send and never has to wait for data to be produced
Instead  when Aâs data link layer asks for data  the network layer is always able to comply immediately
(This restriction  too  will be dropped later
) We also assume that machines do not crash
That is  these protocols deal with communication errors  but not the problems caused by computers crashing and rebooting
As far as the data link layer is concerned  the packet passed across the interface to it from the network layer is pure data  whose every bit is to be delivered to the destinationâs network layer
The fact that the destinationâs network layer may interpret part of the packet as a header is of no concern to the data link layer
When the data link layer accepts a packet  it encapsulates the packet in a frame by adding a data link header and trailer to it (see Fig
Thus  a frame consists of an embedded packet  some control information (in the header)  and a checksum (in the trailer)
The frame is then transmitted to the data link layer on the other machine
We will assume that there exist suitable library procedures to physical layer to send a frame and from physical layer to receive a frame
These procedures compute and append or check the checksum (which is usually done in hardware) so that we do not need to worry about it as part of the protocols we develop in this tion
They might use the CRC algorithm discussed in the previous tion  for example
Initially  the receiver has nothing to do
It just sits around waiting for something to happen
In the example protocols throughout this  ter we will indicate that the data link layer is waiting for something to happen by the procedure call   ELEMENTARY DATA LINK PROTOCOLS #define MAX PKT  /* determines packet size in bytes */ typedef enum {false  true} boolean; /* boolean type */ typedef unsigned int seq nr; /* sequence or ack numbers */ typedef struct {unsigned char data[MAX PKT];} packet; /* packet definition */ typedef enum {data  ack  nak} frame kind; /* frame kind definition */ typedef struct { /* frames are transported in this layer */ frame kind kind; /* what kind of frame is it? */ seq nr seq; /* sequence number */ seq nr ack; /* acknowledgement number */ packet info; /* the network layer packet */ } frame; /* Wait for an event to happen; return its type in event
*/ void wait for event(event type *event); /* Fetch a packet from the network layer for transmission on the channel
*/ void from network layer(packet *p); /* Deliver information from an inbound frame to the network layer
*/ void to network layer(packet *p); /* Go get an inbound frame from the physical layer and copy it to r
*/ void from physical layer(frame *r); /* Pass the frame to the physical layer for transmission
*/ void to physical layer(frame *s); /* Start the clock running and enable the timeout event
*/ void start timer(seq nr k); /* Stop the clock and disable the timeout event
*/ void stop timer(seq nr k); /* Start an auxiliary timer and enable the ack timeout event
*/ void start ack timer(void); /* Stop the auxiliary timer and disable the ack timeout event
*/ void stop ack timer(void); /* Allow the network layer to cause a network layer ready event
*/ void enable network layer(void); /* Forbid the network layer from causing a network layer ready event
*/ void disable network layer(void); /* Macro inc is expanded in-line: increment k circularly
*/ #define inc(k) if (k < MAX SEQ) k = k +  ; else k =  Figure  -
Some definitions needed in the protocols to follow
These definitions are located in the file
THE DATA LINK LAYER
wait for event(&event)
This procedure only returns when something has happened (
a frame has arrived)
Upon return  the variable event tells what happened
The set of possible events differs for the various protocols to be described and will be defined separately for each protocol
Note that in a more realistic situation  the data link layer will not sit in a tight loop waiting for an event  as we have suggested  but will receive an interrupt  which will cause it to stop whatever it was doing and go handle the incoming frame
Nevertheless  for simplicity we will ignore all the details of parallel activity within the data link layer and assume that it is dedicated full time to handling just our one channel
When a frame arrives at the receiver  the checksum is recomputed
If the checksum in the frame is incorrect (
there was a transmission error)  the data link layer is so informed (event = cksum err)
If the inbound frame arrived undamaged  the data link layer is also informed (event = frame arrival ) so that it can acquire the frame for inspection using from physical layer
As soon as the receiving data link layer has acquired an undamaged frame  it checks the control information in the header  and  if everything is all right  passes the packet portion to the network layer
Under no circumstances is a frame header ever given to a network layer
There is a good reason why the network layer must never be given any part of the frame header: to keep the network and data link protocols completely separate
As long as the network layer knows nothing at all about the data link protocol or the frame format  these things can be changed without requiring changes to the network layerâs software
This happens whenever a new NIC is installed in a computer
Providing a rigid interface between the network and data link layers greatly simplifies the design task because communication protocols in different layers can evolve independently
Figure  -  shows some declarations (in C) common to many of the protocols to be discussed later
Five data structures are defined there: boolean  seq nr  packet  frame kind  and frame
A boolean is an enumerated type and can take on the values true and false
A seq nr is a small integer used to number the frames so that we can tell them apart
These sequence numbers run from  up to and including MAX SEQ  which is defined in each protocol needing it
A packet is the unit of information exchanged between the network layer and the data link layer on the same machine  or between network layer peers
In our model it always contains MAX PKT bytes  but more realistically it would be of variable length
A frame is composed of four fields: kind  seq  ack  and info  the first three of which contain control information and the last of which may contain actual data to be transferred
These control fields are collectively called the frame header
The kind field tells whether there are any data in the frame  because some of the protocols distinguish frames containing only control information from those containing data as well
The seq and ack fields are used for sequence numbers and acknowledgements  respectively; their use will be described in more detail later
The info field of a data frame contains a single packet; the info field of a   ELEMENTARY DATA LINK PROTOCOLS control frame is not used
A more realistic implementation would use a variablelength info field  omitting it altogether for control frames
Again  it is important to understand the relationship between a packet and a frame
The network layer builds a packet by taking a message from the transport layer and adding the network layer header to it
This packet is passed to the data link layer for inclusion in the info field of an outgoing frame
When the frame arrives at the destination  the data link layer extracts the packet from the frame and passes the packet to the network layer
In this manner  the network layer can act as though machines can exchange packets directly
A number of procedures are also listed in Fig
These are library routines whose details are implementation dependent and whose inner workings will not concern us further in the following discussions
The procedure wait for event sits in a tight loop waiting for something to happen  as mentioned earlier
The procedures to network layer and from network layer are used by the data link layer to pass packets to the network layer and accept packets from the network layer  respectively
Note that from physical layer and to physical layer pass frames between the data link layer and the physical layer
In other words  to network layer and from network layer deal with the interface between layers  and   whereas from physical layer and to physical layer deal with the interface between layers  and
In most of the protocols  we assume that the channel is unreliable and loses entire frames upon occasion
To be able to recover from such calamities  the sending data link layer must start an internal timer or clock whenever it sends a frame
If no reply has been received within a certain predetermined time interval  the clock times out and the data link layer receives an interrupt signal
In our protocols this is handled by allowing the procedure wait for event to return event = timeout
The procedures start timer and stop timer turn the timer on and off  respectively
Timeout events are possible only when the timer is running and before stop timer is called
It is explicitly permitted to call start timer while the timer is running; such a call simply resets the clock to cause the next timeout after a full timer interval has elapsed (unless it is reset or turned off)
The procedures start ack timer and stop ack timer control an auxiliary timer used to generate acknowledgements under certain conditions
The procedures enable network layer and disable network layer are used in the more sophisticated protocols  where we no longer assume that the network layer always has packets to send
When the data link layer enables the network layer  the network layer is then permitted to interrupt when it has a packet to be sent
We indicate this with event = network layer ready
When the network layer is disabled  it may not cause such events
By being careful about when it enables and disables its network layer  the data link layer can prevent the network layer from swamping it with packets for which it has no buffer space
Frame sequence numbers are always in the range  to MAX SEQ (inclusive)  where MAX SEQ is different for the different protocols
It is frequently necessary THE DATA LINK LAYER
to advance a sequence number by  circularly (
MAX SEQ is followed by  )
The macro inc performs this incrementing
It has been defined as a macro because it is used in-line within the critical path
As we will see later  the factor limiting network performance is often protocol processing  so defining simple operations like this as macros does not affect the readability of the code but does improve performance
The declarations of Fig
To save space and to provide a convenient reference  they have been extracted and listed together  but conceptually they should be merged with the protocols themselves
In C  this merging is done by putting the definitions in a special header file  in this case    and using the #include facility of the C preprocessor to include them in the protocol files
A Utopian Simplex Protocol As an initial example we will consider a protocol that is as simple as it can be because it does not worry about the possibility of anything going wrong
Data are transmitted in one direction only
Both the transmitting and receiving network layers are always ready
Processing time can be ignored
Infinite buffer space is available
And best of all  the communication channel between the data link layers never damages or loses frames
This thoroughly unrealistic protocol  which we will nickname ââUtopia ââ is simply to show the basic structure on which we will build
Itâs implementation is shown in Fig
The protocol consists of two distinct procedures  a sender and a receiver
The sender runs in the data link layer of the source machine  and the receiver runs in the data link layer of the destination machine
No sequence numbers or acknowledgements are used here  so MAX SEQ is not needed
The only event type possible is frame arrival (
the arrival of an undamaged frame)
The sender is in an infinite while loop just pumping data out onto the line as fast as it can
The body of the loop consists of three actions: go fetch a packet from the (always obliging) network layer  construct an outbound frame using the variable s  and send the frame on its way
Only the info field of the frame is used by this protocol  because the other fields have to do with error and flow control and there are no errors or flow control restrictions here
The receiver is equally simple
Initially  it waits for something to happen  the only possibility being the arrival of an undamaged frame
Eventually  the frame arrives and the procedure wait for event returns  with event set to frame arrival (which is ignored anyway)
The call to from physical layer removes the newly arrived frame from the hardware buffer and puts it in the variable r  where the receiver code can get at it
Finally  the data portion is passed on to the network layer  and the data link layer settles back to wait for the next frame  effectively suspending itself until the frame arrives
ELEMENTARY DATA LINK PROTOCOLS /* Protocol  (Utopia) provides for data transmission in one direction only  from sender to receiver
The communication channel is assumed to be error free and the receiver is assumed to be able to process all the input infinitely quickly
Consequently  the sender just sits in a loop pumping data out onto the line as fast as it can
"*/ typedef enum {frame arrival} event type; #include "" "" void sender (void) { frame s; /* buffer for an outbound frame */ packet buffer; /* buffer for an outbound packet */ while (true) { from network layer(&buffer); /* go get something to send */   = buffer; /* copy it into s for transmission */ to physical layer(&s); /* send it on its way */ } /* Tomorrow  and tomorrow  and tomorrow  Creeps in this petty pace from day to day To the last syllable of recorded time"
â Macbeth  V  v */ } void receiver (void) { frame r; event type event; /* filled in by wait  but not used here */ while (true) { wait for event(&event); /* only possibility is frame arrival */ from physical layer(&r); /* go get the inbound frame */ to network layer(& ); /* pass the data to the network layer */ } } Figure  -
A utopian simplex protocol
The utopia protocol is unrealistic because it does not handle either flow control or error correction
Its processing is close to that of an unacknowledged connectionless service that relies on higher layers to solve these problems  though even an unacknowledged connectionless service would do some error detection
A Simplex Stop-and-Wait Protocol for an Error-Free Channel Now we will tackle the problem of preventing the sender from flooding the receiver with frames faster than the latter is able to process them
This situation can easily happen in practice so being able to prevent it is of great importance
THE DATA LINK LAYER
The communication channel is still assumed to be error free  however  and the data traffic is still simplex
One solution is to build the receiver to be powerful enough to process a continuous stream of back-to-back frames (or  equivalently  define the link layer to be slow enough that the receiver can keep up)
It must have sufficient buffering and processing abilities to run at the line rate and must be able to pass the frames that are received to the network layer quickly enough
However  this is a worst-case solution
It requires dedicated hardware and can be wasteful of resources if the utilization of the link is mostly low
Moreover  it just shifts the problem of dealing with a sender that is too fast elsewhere; in this case to the network layer
A more general solution to this problem is to have the receiver provide feedback to the sender
After having passed a packet to its network layer  the receiver sends a little dummy frame back to the sender which  in effect  gives the sender permission to transmit the next frame
After having sent a frame  the sender is required by the protocol to bide its time until the little dummy (
acknowledgement) frame arrives
This delay is a simple example of a flow control protocol
Protocols in which the sender sends one frame and then waits for an acknowledgement before proceeding are called stop-and-wait
Figure  -  gives an example of a simplex stop-and-wait protocol
Although data traffic in this example is simplex  going only from the sender to the receiver  frames do travel in both directions
Consequently  the communication channel between the two data link layers needs to be capable of bidirectional information transfer
However  this protocol entails a strict alternation of flow: first the sender sends a frame  then the receiver sends a frame  then the sender sends another frame  then the receiver sends another one  and so on
A halfduplex physical channel would suffice here
As in protocol   the sender starts out by fetching a packet from the network layer  using it to construct a frame  and sending it on its way
But now  unlike in protocol   the sender must wait until an acknowledgement frame arrives before looping back and fetching the next packet from the network layer
The sending data link layer need not even inspect the incoming frame as there is only one possibility
The incoming frame is always an acknowledgement
The only difference between receiver  and receiver  is that after delivering a packet to the network layer  receiver  sends an acknowledgement frame back to the sender before entering the wait loop again
Because only the arrival of the frame back at the sender is important  not its contents  the receiver need not put any particular information in it
A Simplex Stop-and-Wait Protocol for a Noisy Channel Now let us consider the normal situation of a communication channel that makes errors
Frames may be either damaged or lost completely
However  we assume that if a frame is damaged in transit  the receiver hardware will detect this   ELEMENTARY DATA LINK PROTOCOLS /* Protocol  (Stop-and-wait) also provides for a one-directional flow of data from sender to receiver
The communication channel is once again assumed to be error free  as in protocol
However  this time the receiver has only a finite buffer capacity and a finite processing speed  so the protocol must explicitly prevent the sender from flooding the receiver with data faster than it can be handled
"*/ typedef enum {frame arrival} event type; #include "" "" void sender (void) { frame s; /* buffer for an outbound frame */ packet buffer; /* buffer for an outbound packet */ event type event; /* frame arrival is the only possibility */ while (true) { from network layer(&buffer); /* go get something to send */   = buffer; /* copy it into s for transmission */ to physical layer(&s); /* bye-bye little frame */ wait for event(&event); /* do not proceed until given the go ahead */ } } void receiver (void) { frame r  s; /* buffers for frames */ event type event; /* frame arrival is the only possibility */ while (true) { wait for event(&event); /* only possibility is frame arrival */ from physical layer(&r); /* go get the inbound frame */ to network layer(& ); /* pass the data to the network layer */ to physical layer(&s); /* send a dummy frame to awaken sender */ } } Figure  -"
A simplex stop-and-wait protocol
when it computes the checksum
If the frame is damaged in such a way that the checksum is nevertheless correctâan unlikely occurrenceâthis protocol (and all other protocols) can fail (
deliver an incorrect packet to the network layer)
At first glance it might seem that a variation of protocol  would work: adding a timer
The sender could send a frame  but the receiver would only send an acknowledgement frame if the data were correctly received
If a damaged frame arrived at the receiver  it would be discarded
After a while the sender would time out and send the frame again
This process would be repeated until the frame finally arrived intact
This scheme has a fatal flaw in it though
Think about the problem and try to discover what might go wrong before reading further
THE DATA LINK LAYER
To see what might go wrong  remember that the goal of the data link layer is to provide error-free  transparent communication between network layer processes
The network layer on machine A gives a series of packets to its data link layer  which must ensure that an identical series of packets is delivered to the network layer on machine B by its data link layer
In particular  the network layer on B has no way of knowing that a packet has been lost or duplicated  so the data link layer must guarantee that no combination of transmission errors  however unlikely  can cause a duplicate packet to be delivered to a network layer
Consider the following scenario:
The network layer on A gives packet  to its data link layer
The packet is correctly received at B and passed to the network layer on B
B sends an acknowledgement frame back to A The acknowledgement frame gets lost completely
It just never arrives at all
Life would be a great deal simpler if the channel mangled and lost only data frames and not control frames  but sad to say  the channel is not very discriminating The data link layer on A eventually times out
Not having received an acknowledgement  it (incorrectly) assumes that its data frame was lost or damaged and sends the frame containing packet  again The duplicate frame also arrives intact at the data link layer on B and is unwittingly passed to the network layer there
If A is sending a file to B  part of the file will be duplicated (
the copy of the file made by B will be incorrect and the error will not have been detected)
In other words  the protocol will fail
Clearly  what is needed is some way for the receiver to be able to distinguish a frame that it is seeing for the first time from a retransmission
The obvious way to achieve this is to have the sender put a sequence number in the header of each frame it sends
Then the receiver can check the sequence number of each arriving frame to see if it is a new frame or a duplicate to be discarded
Since the protocol must be correct and the sequence number field in the header is likely to be small to use the link efficiently  the question arises: what is the minimum number of bits needed for the sequence number? The header might provide  bit  a few bits   byte  or multiple bytes for a sequence number depending on the protocol
The important point is that it must carry sequence numbers that are large enough for the protocol to work correctly  or it is not much of a protocol
The only ambiguity in this protocol is between a frame  m  and its direct successor  m +
If frame m is lost or damaged  the receiver will not acknowledge it  so the sender will keep trying to send it
Once it has been correctly received  the receiver will send an acknowledgement to the sender
It is here that the potential   ELEMENTARY DATA LINK PROTOCOLS trouble crops up
Depending upon whether the acknowledgement frame gets back to the sender correctly or not  the sender may try to send m or m +
At the sender  the event that triggers the transmission of frame m +  is the arrival of an acknowledgement for frame m
But this situation implies that m â  has been correctly received  and furthermore that its acknowledgement has also been correctly received by the sender
Otherwise  the sender would not have begun with m  let alone have been considering m +
As a consequence  the only ambiguity is between a frame and its immediate predecessor or successor  not between the predecessor and successor themselves
A  -bit sequence number (  or  ) is therefore sufficient
At each instant of time  the receiver expects a particular sequence number next
When a frame containing the correct sequence number arrives  it is accepted and passed to the network layer  then acknowledged
Then the expected sequence number is incremented modulo  (
becomes  and  becomes  )
Any arriving frame containing the wrong sequence number is rejected as a duplicate
However  the last valid acknowledgement is repeated so that the sender can eventually discover that the frame has been received
An example of this kind of protocol is shown in Fig
Protocols in which the sender waits for a positive acknowledgement before advancing to the next data item are often called ARQ (Automatic Repeat reQuest) or PAR (Positive Acknowledgement with Retransmission)
Like protocol   this one also transmits data only in one direction
Protocol  differs from its predecessors in that both sender and receiver have a variable whose value is remembered while the data link layer is in the wait state
The sender remembers the sequence number of the next frame to send in next frame to send; the receiver remembers the sequence number of the next frame expected in frame expected
Each protocol has a short initialization phase before entering the infinite loop
After transmitting a frame  the sender starts the timer running
If it was already running  it will be reset to allow another full timer interval
The interval should be chosen to allow enough time for the frame to get to the receiver  for the receiver to process it in the worst case  and for the acknowledgement frame to propagate back to the sender
Only when that interval has elapsed is it safe to assume that either the transmitted frame or its acknowledgement has been lost  and to send a duplicate
If the timeout interval is set too short  the sender will transmit unnecessary frames
While these extra frames will not affect the correctness of the protocol  they will hurt performance
After transmitting a frame and starting the timer  the sender waits for something exciting to happen
Only three possibilities exist: an acknowledgement frame arrives undamaged  a damaged acknowledgement frame staggers in  or the timer expires
If a valid acknowledgement comes in  the sender fetches the next packet from its network layer and puts it in the buffer  overwriting the previous packet
It also advances the sequence number
If a damaged frame arrives or the THE DATA LINK LAYER
timer expires  neither the buffer nor the sequence number is changed so that a duplicate can be sent
In all cases  the contents of the buffer (either the next packet or a duplicate) are then sent
When a valid frame arrives at the receiver  its sequence number is checked to see if it is a duplicate
If not  it is accepted  passed to the network layer  and an acknowledgement is generated
Duplicates and damaged frames are not passed to the network layer  but they do cause the last correctly received frame to be acknowledged to signal the sender to advance to the next frame or retransmit a damaged frame  SLIDING WINDOW PROTOCOLS In the previous protocols  data frames were transmitted in one direction only
In most practical situations  there is a need to transmit data in both directions
One way of achieving full-duplex data transmission is to run two instances of one of the previous protocols  each using a separate link for simplex data traffic (in different directions)
Each link is then comprised of a ââforwardââ channel (for data) and a ââreverseââ channel (for acknowledgements)
In both cases the capacity of the reverse channel is almost entirely wasted
A better idea is to use the same link for data in both directions
After all  in protocols  and  it was already being used to transmit frames both ways  and the reverse channel normally has the same capacity as the forward channel
In this model the data frames from A to B are intermixed with the acknowledgement frames from A to B
By looking at the kind field in the header of an incoming frame  the receiver can tell whether the frame is data or an acknowledgement
Although interleaving data and control frames on the same link is a big improvement over having two separate physical links  yet another improvement is possible
When a data frame arrives  instead of immediately sending a separate control frame  the receiver restrains itself and waits until the network layer passes it the next packet
The acknowledgement is attached to the outgoing data frame (using the ack field in the frame header)
In effect  the acknowledgement gets a free ride on the next outgoing data frame
The technique of temporarily delaying outgoing acknowledgements so that they can be hooked onto the next outgoing data frame is known as piggybacking
The principal advantage of using piggybacking over having distinct acknowledgement frames is a better use of the available channel bandwidth
The ack field in the frame header costs only a few bits  whereas a separate frame would need a header  the acknowledgement  and a checksum
In addition  fewer frames sent generally means a lighter processing load at the receiver
In the next protocol to be examined  the piggyback field costs only  bit in the frame header
It rarely costs more than a few bits
However  piggybacking introduces a complication not present with separate acknowledgements
How long should the data link layer wait for a packet onto   SLIDING WINDOW PROTOCOLS /* Protocol  (PAR) allows unidirectional data flow over an unreliable channel
"*/ #define MAX SEQ  /* must be  for protocol  */ typedef enum {frame arrival  cksum err  timeout} event type; #include "" "" void sender (void) { seq nr next frame to send; /* seq number of next outgoing frame */ frame s; /* scratch variable */ packet buffer; /* buffer for an outbound packet */ event type event; next frame to send =  ; /* initialize outbound sequence numbers */ from network layer(&buffer); /* fetch first packet */ while (true) {   = buffer; /* construct a frame for transmission */   = next frame to send; /* insert sequence number in frame */ to physical layer(&s); /* send it on its way */ start timer( ); /* if answer takes too long  time out */ wait for event(&event); /* frame arrival  cksum err  timeout */ if (event == frame arrival) { from physical layer(&s); /* get the acknowledgement */ if (  == next frame to send) { stop timer( ); /* turn the timer off */ from network layer(&buffer); /* get the next one to send */ inc(next frame to send); /* invert next frame to send */ } } } } void receiver (void) { seq nr frame expected; frame r  s; event type event; frame expected =  ; while (true) { wait for event(&event); /* possibilities: frame arrival  cksum err */ if (event == frame arrival) { /* a valid frame has arrived */ from physical layer(&r); /* go get the newly arrived frame */ if (  == frame expected) { /* this is what we have been waiting for */ to network layer(& ); /* pass the data to the network layer */ inc(frame expected); /* next time expect the other sequence nr */ }   =  â frame expected; /* tell which frame is being acked */ to physical layer(&s); /* send acknowledgement */ } } } Figure  -"
A positive acknowledgement with retransmission protocol
THE DATA LINK LAYER
which to piggyback the acknowledgement? If the data link layer waits longer than the senderâs timeout period  the frame will be retransmitted  defeating the whole purpose of having acknowledgements
If the data link layer were an oracle and could foretell the future  it would know when the next network layer packet was going to come in and could decide either to wait for it or send a separate acknowledgement immediately  depending on how long the projected wait was going to be
Of course  the data link layer cannot foretell the future  so it must resort to some ad hoc scheme  such as waiting a fixed number of millionds
If a new packet arrives quickly  the acknowledgement is piggybacked onto it
Otherwise  if no new packet has arrived by the end of this time period  the data link layer just sends a separate acknowledgement frame
The next three protocols are bidirectional protocols that belong to a class called sliding window protocols
The three differ among themselves in terms of efficiency  complexity  and buffer requirements  as discussed later
In these  as in all sliding window protocols  each outbound frame contains a sequence number  ranging from  up to some maximum
The maximum is usually  n â  so the sequence number fits exactly in an n-bit field
The stop-and-wait sliding window protocol uses n =   restricting the sequence numbers to  and   but more sophisticated versions can use an arbitrary n
The essence of all sliding window protocols is that at any instant of time  the sender maintains a set of sequence numbers corresponding to frames it is permitted to send
These frames are said to fall within the sending window
Similarly  the receiver also maintains a receiving window corresponding to the set of frames it is permitted to accept
The senderâs window and the receiverâs window need not have the same lower and upper limits or even have the same size
In some protocols they are fixed in size  but in others they can grow or shrink over the course of time as frames are sent and received
Although these protocols give the data link layer more freedom about the order in which it may send and receive frames  we have definitely not dropped the requirement that the protocol must deliver packets to the destination network layer in the same order they were passed to the data link layer on the sending machine
Nor have we changed the requirement that the physical communication channel is ââwire-like ââ that is  it must deliver all frames in the order sent
The sequence numbers within the senderâs window represent frames that have been sent or can be sent but are as yet not acknowledged
Whenever a new packet arrives from the network layer  it is given the next highest sequence number  and the upper edge of the window is advanced by one
When an acknowledgement comes in  the lower edge is advanced by one
In this way the window continuously maintains a list of unacknowledged frames
Figure  -  shows an example
Since frames currently within the senderâs window may ultimately be lost or damaged in transit  the sender must keep all of these frames in its memory for possible retransmission
Thus  if the maximum window size is n  the sender needs n buffers to hold the unacknowledged frames
If the window ever grows to its   SLIDING WINDOW PROTOCOLS Sender Receiver                    (a) (b) (c) (d) Figure  -
A sliding window of size   with a  -bit sequence number
(a) Initially
(b) After the first frame has been sent
(c) After the first frame has been received
(d) After the first acknowledgement has been received
maximum size  the sending data link layer must forcibly shut off the network layer until another buffer becomes free
The receiving data link layerâs window corresponds to the frames it may accept
Any frame falling within the window is put in the receiverâs buffer
When a frame whose sequence number is equal to the lower edge of the window is received  it is passed to the network layer and the window is rotated by one
Any frame falling outside the window is discarded
In all of these cases  a subsequent acknowledgement is generated so that the sender may work out how to proceed
Note that a window size of  means that the data link layer only accepts frames in order  but for larger windows this is not so
The network layer  in contrast  is always fed data in the proper order  regardless of the data link layerâs window size
Figure  -  shows an example with a maximum window size of
Initially  no frames are outstanding  so the lower and upper edges of the senderâs window are equal  but as time goes on  the situation progresses as shown
Unlike the senderâs window  the receiverâs window always remains at its initial size  rotating as the next frame is accepted and delivered to the network layer
A One-Bit Sliding Window Protocol Before tackling the general case  let us examine a sliding window protocol with a window size of
Such a protocol uses stop-and-wait since the sender transmits a frame and waits for its acknowledgement before sending the next one
THE DATA LINK LAYER
Figure  -  depicts such a protocol
Like the others  it starts out by defining some variables
Next frame to send tells which frame the sender is trying to send
Similarly  frame expected tells which frame the receiver is expecting
In both cases   and  are the only possibilities
/* Protocol  (Sliding window) is bidirectional
"*/ #define MAX SEQ  /* must be  for protocol  */ typedef enum {frame arrival  cksum err  timeout} event type; #include "" "" void protocol  (void) { seq nr next frame to send; /*  or  only */ seq nr frame expected; /*  or  only */ frame r  s; /* scratch variables */ packet buffer; /* current packet being sent */ event type event; next frame to send =  ; /* next frame on the outbound stream */ frame expected =  ; /* frame expected next */ from network layer(&buffer); /* fetch a packet from the network layer */   = buffer; /* prepare to send the initial frame */   = next frame to send; /* insert sequence number into frame */   =  â frame expected; /* piggybacked ack */ to physical layer(&s); /* transmit the frame */ start timer( ); /* start the timer running */ while (true) { wait for event(&event); /* frame arrival  cksum err  or timeout */ if (event == frame arrival) { /* a frame has arrived undamaged */ from physical layer(&r); /* go get it */ if (  == frame expected) { /* handle inbound frame stream */ to network layer(& ); /* pass packet to network layer */ inc(frame expected); /* invert seq number expected next */ } if (  == next frame to send) { /* handle outbound frame stream */ stop timer( ); /* turn the timer off */ from network layer(&buffer); /* fetch new pkt from network layer */ inc(next frame to send); /* invert senderâs sequence number */ } }   = buffer; /* construct outbound frame */   = next frame to send; /* insert sequence number into it */   =  â frame expected; /* seq number of last received frame */ to physical layer(&s); /* transmit a frame */ start timer( ); /* start the timer running */ } } Figure  -"
A  -bit sliding window protocol
SLIDING WINDOW PROTOCOLS Under normal circumstances  one of the two data link layers goes first and transmits the first frame
In other words  only one of the data link layer programs should contain the to physical layer and start timer procedure calls outside the main loop
The starting machine fetches the first packet from its network layer  builds a frame from it  and sends it
When this (or any) frame arrives  the receiving data link layer checks to see if it is a duplicate  just as in protocol
If the frame is the one expected  it is passed to the network layer and the receiverâs window is slid up
The acknowledgement field contains the number of the last frame received without error
If this number agrees with the sequence number of the frame the sender is trying to send  the sender knows it is done with the frame stored in buffer and can fetch the next packet from its network layer
If the sequence number disagrees  it must continue trying to send the same frame
Whenever a frame is received  a frame is also sent back
Now let us examine protocol  to see how resilient it is to pathological scenarios
Assume that computer A is trying to send its frame  to computer B and that B is trying to send its frame  to A
Suppose that A sends a frame to B  but Aâs timeout interval is a little too short
Consequently  A may time out repeatedly  sending a series of identical frames  all with seq =  and ack =
When the first valid frame arrives at computer B  it will be accepted and frame expected will be set to a value of
All the subsequent frames received will be rejected because B is now expecting frames with sequence number   not
Furthermore  since all the duplicates will have ack =  and B is still waiting for an acknowledgement of   B will not go and fetch a new packet from its network layer
After every rejected duplicate comes in  B will send A a frame containing seq =  and ack =
Eventually  one of these will arrive correctly at A  causing A to begin sending the next packet
No combination of lost frames or premature timeouts can cause the protocol to deliver duplicate packets to either network layer  to skip a packet  or to deadlock
The protocol is correct
However  to show how subtle protocol interactions can be  we note that a peculiar situation arises if both sides simultaneously send an initial packet
This synchronization difficulty is illustrated by Fig
In part (a)  the normal operation of the protocol is shown
In (b) the peculiarity is illustrated
If B waits for Aâs first frame before sending one of its own  the sequence is as shown in (a)  and every frame is accepted
However  if A and B simultaneously initiate communication  their first frames cross  and the data link layers then get into situation (b)
In (a) each frame arrival brings a new packet for the network layer; there are no duplicates
In (b) half of the frames contain duplicates  even though there are no transmission errors
Similar situations can occur as a result of premature timeouts  even when one side clearly starts first
In fact  if multiple premature timeouts occur  frames may be sent three or more times  wasting valuable bandwidth
THE DATA LINK LAYER
A sends (  A ) A gets (  B )* A sends (  A ) B gets (  A )* B sends (  B ) B gets (  A )* B sends (  B ) B gets (  A )* B sends (  B ) B gets (  A )* B sends (  B ) A gets (  B )* A sends (  A ) A gets (  B )* A sends (  A ) A sends (  A ) A gets (  B )* A sends (  A ) B gets (  A ) B sends (  B ) B sends (  B ) B gets (  A )* B sends (  B ) B gets (  A )* B sends (  B ) B gets (  A ) B sends (  B ) A gets (  B ) A sends (  A ) A gets (  B )* A sends (  A ) Time (a) (b) Figure  -
Two scenarios for protocol
(a) Normal case
(b) Abnormal case
The notation is (seq  ack  packet number)
An asterisk indicates where a network layer accepts a packet
A Protocol Using Go-Back-N Until now we have made the tacit assumption that the transmission time required for a frame to arrive at the receiver plus the transmission time for the acknowledgement to come back is negligible
Sometimes this assumption is clearly false
In these situations the long round-trip time can have important implications for the efficiency of the bandwidth utilization
As an example  consider a  -kbps satellite channel with a   -m round-trip propagation delay
Let us imagine trying to use protocol  to send -bit frames via the satellite
At t =  the sender starts sending the first frame
At t =   m the frame has been completely sent
Not until t = m has the frame fully arrived at the receiver  and not until t = m has the acknowledgement arrived back at the sender  under the best of circumstances (of no waiting in the receiver and a short acknowledgement frame)
This means that the sender was blocked   /   or  % of the time
In other words  only  % of the available bandwidth was used
Clearly  the combination of a long transit time  high bandwidth  and short frame length is disastrous in terms of efficiency
The problem described here can be viewed as a consequence of the rule requiring a sender to wait for an acknowledgement before sending another frame
If we relax that restriction  much better efficiency can be achieved
Basically  the solution lies in allowing the sender to transmit up to w frames before blocking  instead of just
With a large enough choice of w the sender will be able to continuously transmit frames since the acknowledgements will arrive for previous frames before the window becomes full  preventing the sender from blocking
SLIDING WINDOW PROTOCOLS To find an appropriate value for w we need to know how many frames can fit inside the channel as they propagate from sender to receiver
This capacity is determined by the bandwidth in bits/ multiplied by the one-way transit time  or the bandwidth-delay product of the link
We can divide this quantity by the number of bits in a frame to express it as a number of frames
Call this quantity BD
Then w should be set to  BD +
Twice the bandwidth-delay is the number of frames that can be outstanding if the sender continuously sends frames when the round-trip time to receive an acknowledgement is considered
The ââ+ ââ is because an acknowledgement frame will not be sent until after a complete frame is received
For the example link with a bandwidth of   kbps and a one-way transit time of m  the bandwidth-delay product is
frames of  bits each
BD +  is then   frames
Assume the sender begins sending frame  as before and sends a new frame every   m
By the time it has finished sending   frames  at t = m  the acknowledgement for frame  will have just arrived
Thereafter  acknowledgements will arrive every   m  so the sender will always get permission to continue just when it needs it
From then onwards or   unacknowledged frames will always be outstanding
Put in other terms  the senderâs maximum window size is
For smaller window sizes  the utilization of the link will be less than   % since the sender will be blocked sometimes
We can write the utilization as the fraction of time that the sender is not blocked: link utilization â¤  +  BD w This value is an upper bound because it does not allow for any frame processing time and treats the acknowledgement frame as having zero length  since it is usually short
The equation shows the need for having a large window w whenever the bandwidth-delay product is large
If the delay is high  the sender will rapidly exhaust its window even for a moderate bandwidth  as in the satellite example
If the bandwidth is high  even for a moderate delay the sender will exhaust its window quickly unless it has a large window (
a  -Gbps link with  -m delay holds  megabit)
With stop-and-wait for which w =   if there is even one frameâs worth of propagation delay the efficiency will be less than  %
This technique of keeping multiple frames in flight is an example of pipelining
Pipelining frames over an unreliable communication channel raises some serious issues
First  what happens if a frame in the middle of a long stream is damaged or lost? Large numbers of succeeding frames will arrive at the receiver before the sender even finds out that anything is wrong
When a damaged frame arrives at the receiver  it obviously should be discarded  but what should the receiver do with all the correct frames following it? Remember that the receiving data link layer is obligated to hand packets to the network layer in sequence
THE DATA LINK LAYER
Two basic approaches are available for dealing with errors in the presence of pipelining  both of which are shown in Fig
E D D D D D D      Timeout interval Error Frames discarded by data link layer Frames buffered by data link layer Ack  Ack  Time (a) (b)    E        Error Ack  Ack  Nak    Ack  Ack Ack  Ack  Ack  Ack   Ack   Ack   Ack   Ack  Ack  Ack  Ack  Ack  Ack  Figure  -
Pipelining and error recovery
Effect of an error when (a) receiverâs window size is  and (b) receiverâs window size is large
One option  called go-back-n  is for the receiver simply to discard all subsequent frames  sending no acknowledgements for the discarded frames
This strategy corresponds to a receive window of size
In other words  the data link layer refuses to accept any frame except the next one it must give to the network layer
If the senderâs window fills up before the timer runs out  the pipeline will begin to empty
Eventually  the sender will time out and retransmit all unacknowledged frames in order  starting with the damaged or lost one
This approach can waste a lot of bandwidth if the error rate is high
Frames  and  are correctly received and acknowledged
Frame   however  is damaged or lost
The sender  unaware of this problem  continues to send frames until the timer for frame  expires
Then it backs up to frame  and starts over with it  sending  etc
all over again
The other general strategy for handling errors when frames are pipelined is called selective repeat
When it is used  a bad frame that is received is discarded  but any good frames received after it are accepted and buffered
When the sender times out  only the oldest unacknowledged frame is retransmitted
If that frame   SLIDING WINDOW PROTOCOLS arrives correctly  the receiver can deliver to the network layer  in sequence  all the frames it has buffered
Selective repeat corresponds to a receiver window larger than
This approach can require large amounts of data link layer memory if the window is large
Selective repeat is often combined with having the receiver send a negative acknowledgement (NAK) when it detects an error  for example  when it receives a checksum error or a frame out of sequence
NAKs stimulate retransmission before the corresponding timer expires and thus improve performance
When frame  arrives at the receiver  the data link layer there notices that it has missed a frame  so it sends back a NAK for  but buffers
When frames  and  arrive  they  too  are buffered by the data link layer instead of being passed to the network layer
Eventually  the NAK  gets back to the sender  which immediately resends frame
When that arrives  the data link layer now has  and  and can pass all of them to the network layer in the correct order
It can also acknowledge all frames up to and including   as shown in the figure
If the NAK should get lost  eventually the sender will time out for frame  and send it (and only it) of its own accord  but that may be a quite a while later
These two alternative approaches are trade-offs between efficient use of bandwidth and data link layer buffer space
Depending on which resource is scarcer  one or the other can be used
Figure  -  shows a go-back-n protocol in which the receiving data link layer only accepts frames in order; frames following an error are discarded
In this protocol  for the first time we have dropped the assumption that the network layer always has an infinite supply of packets to send
When the network layer has a packet it wants to send  it can cause a network layer ready event to happen
However  to enforce the flow control limit on the sender window or the number of unacknowledged frames that may be outstanding at any time  the data link layer must be able to keep the network layer from bothering it with more work
The library procedures enable network layer and disable network layer do this job
The maximum number of frames that may be outstanding at any instant is not the same as the size of the sequence number space
For go-back-n  MAX SEQ frames may be outstanding at any instant  even though there are MAX SEQ +  distinct sequence numbers (which are
We will see an even tighter restriction for the next protocol  selective repeat
To see why this restriction is required  consider the following scenario with MAX SEQ =  :
The sender sends frames  through   A piggybacked acknowledgement for  comes back to the sender The sender sends another eight frames  again with sequence numbers  through   Now another piggybacked acknowledgement for frame  comes in
THE DATA LINK LAYER
/* Protocol  (Go-back-n) allows multiple outstanding frames
The sender may transmit up to MAX SEQ frames without waiting for an ack
In addition  unlike in the previous protocols  the network layer is not assumed to have a new packet all the time
Instead  the network layer causes a network layer ready event when there is a packet to send
"*/ #define MAX SEQ  typedef enum {frame arrival  cksum err  timeout  network layer ready} event type; #include "" "" static boolean between(seq nr a  seq nr b  seq nr c) { /* Return true if a <= b < c circularly; false otherwise"
*/ if (((a <= b) && (b < c)) || ((c < a) && (a <= b)) || ((b < c) && (c < a))) return(true); else return(false); } static void send data(seq nr frame nr  seq nr frame expected  packet buffer[ ]) { /* Construct and send a data frame
*/ frame s; /* scratch variable */   = buffer[frame nr]; /* insert packet into frame */   = frame nr; /* insert sequence number into frame */   = (frame expected + MAX SEQ) % (MAX SEQ +  ); /* piggyback ack */ to physical layer(&s); /* transmit the frame */ start timer(frame nr); /* start the timer running */ } void protocol (void) { seq nr next frame to send; /* MAX SEQ >  ; used for outbound stream */ seq nr ack expected; /* oldest frame as yet unacknowledged */ seq nr frame expected; /* next frame expected on inbound stream */ frame r; /* scratch variable */ packet buffer[MAX SEQ +  ]; /* buffers for the outbound stream */ seq nr nbuffered; /* number of output buffers currently in use */ seq nr i; /* used to index into the buffer array */ event type event; enable network layer(); /* allow network layer ready events */ ack expected =  ; /* next ack expected inbound */ next frame to send =  ; /* next frame going out */ frame expected =  ; /* number of frame expected inbound */ nbuffered =  ; /* initially no packets are buffered */ while (true) { wait for event(&event); /* four possibilities: see event type above */   SLIDING WINDOW PROTOCOLS switch(event) { case network layer ready: /* the network layer has a packet to send */ /* Accept  save  and transmit a new frame
*/ from network layer(&buffer[next frame to send]); /* fetch new packet */ nbuffered = nbuffered +  ; /* expand the senderâs window */ send data(next frame to send  frame expected  buffer);/* transmit the frame */ inc(next frame to send); /* advance senderâs upper window edge */ break; case frame arrival: /* a data or control frame has arrived */ from physical layer(&r); /* get incoming frame from physical layer */ if (  == frame expected) { /* Frames are accepted only in order
*/ to network layer(& ); /* pass packet to network layer */ inc(frame expected); /* advance lower edge of receiverâs window */ } /* Ack n implies n â   n â   etc
Check for this
*/ while (between(ack expected     next frame to send)) { /* Handle piggybacked ack
*/ nbuffered = nbuffered â  ; /* one frame fewer buffered */ stop timer(ack expected); /* frame arrived intact; stop timer */ inc(ack expected); /* contract senderâs window */ } break; case cksum err: break; /* just ignore bad frames */ case timeout: /* trouble; retransmit all outstanding frames */ next frame to send = ack expected; /* start retransmitting here */ for (i =  ; i <= nbuffered; i++) { send data(next frame to send  frame expected  buffer);/* resend frame */ inc(next frame to send); /* prepare to send the next one */ } } if (nbuffered < MAX SEQ) enable network layer(); else disable network layer(); } } Figure  -
A sliding window protocol using go-back-n
The question is this: did all eight frames belonging to the ond batch arrive successfully  or did all eight get lost (counting discards following an error as lost)? In both cases the receiver would be sending frame  as the acknowledgement
THE DATA LINK LAYER
The sender has no way of telling
For this reason the maximum number of outstanding frames must be restricted to MAX SEQ
Although protocol  does not buffer the frames arriving after an error  it does not escape the problem of buffering altogether
Since a sender may have to retransmit all the unacknowledged frames at a future time  it must hang on to all transmitted frames until it knows for sure that they have been accepted by the receiver
When an acknowledgement comes in for frame n  frames n â   n â   and so on are also automatically acknowledged
This type of acknowledgement is called a cumulative acknowledgement
This property is especially important when some of the previous acknowledgement-bearing frames were lost or garbled
Whenever any acknowledgement comes in  the data link layer checks to see if any buffers can now be released
If buffers can be released (
there is some room available in the window)  a previously blocked network layer can now be allowed to cause more network layer ready events
For this protocol  we assume that there is always reverse traffic on which to piggyback acknowledgements
Protocol  does not need this assumption since it sends back one frame every time it receives a frame  even if it has already sent that frame
In the next protocol we will solve the problem of one-way traffic in an elegant way
Because protocol  has multiple outstanding frames  it logically needs multiple timers  one per outstanding frame
Each frame times out independently of all the other ones
However  all of these timers can easily be simulated in software using a single hardware clock that causes interrupts periodically
The pending timeouts form a linked list  with each node of the list containing the number of clock ticks until the timer expires  the frame being timed  and a pointer to the next node
Real time Pointer to next timeout Frame being timed Ticks to go (a) (b) Figure  -
Simulation of multiple timers in software
(a) The queued timeouts
(b) The situation after the first timeout has expired
As an illustration of how the timers could be implemented  consider the example of Fig
Assume that the clock ticks once every  m
Initially    SLIDING WINDOW PROTOCOLS the real time is  :  :
; three timeouts are pending  at  :  :
and  :  :   Every time the hardware clock ticks  the real time is updated and the tick counter at the head of the list is decremented
When the tick counter becomes zero  a timeout is caused and the node is removed from the list  as shown in Fig
Although this organization requires the list to be scanned when start timer or stop timer is called  it does not require much work per tick
In protocol   both of these routines have been given a parameter indicating which frame is to be timed
A Protocol Using Selective Repeat The go-back-n protocol works well if errors are rare  but if the line is poor it wastes a lot of bandwidth on retransmitted frames
An alternative strategy  the selective repeat protocol  is to allow the receiver to accept and buffer the frames following a damaged or lost one
In this protocol  both sender and receiver maintain a window of outstanding and acceptable sequence numbers  respectively
The senderâs window size starts out at  and grows to some predefined maximum
The receiverâs window  in contrast  is always fixed in size and equal to the predetermined maximum
The receiver has a buffer reserved for each sequence number within its fixed window
Associated with each buffer is a bit (arrived ) telling whether the buffer is full or empty
Whenever a frame arrives  its sequence number is checked by the function between to see if it falls within the window
If so and if it has not already been received  it is accepted and stored
This action is taken without regard to whether or not the frame contains the next packet expected by the network layer
Of course  it must be kept within the data link layer and not passed to the network layer until all the lower-numbered frames have already been delivered to the network layer in the correct order
A protocol using this algorithm is given in Fig
Nonsequential receive introduces further constraints on frame sequence numbers compared to protocols in which frames are only accepted in order
We can illustrate the trouble most easily with an example
Suppose that we have a  -bit sequence number  so that the sender is permitted to transmit up to seven frames before being required to wait for an acknowledgement
Initially  the senderâs and receiverâs windows are as shown in Fig
The sender now transmits frames  through
The receiverâs window allows it to accept any frame with a sequence number between  and  inclusive
All seven frames arrive correctly  so the receiver acknowledges them and advances its window to allow receipt of     or   as shown in Fig
All seven buffers are marked empty
It is at this point that disaster strikes in the form of a lightning bolt hitting the telephone pole and wiping out all the acknowledgements
The protocol should operate correctly despite this disaster
The sender eventually times out and retransmits frame
When this frame arrives at the receiver  a check is made to see if it falls within the receiverâs window
Unfortunately  in Fig
/* Protocol  (Selective repeat) accepts frames out of order but passes packets to the network layer in order
Associated with each outstanding frame is a timer
When the timer expires  only that frame is retransmitted  not all the outstanding frames  as in protocol
"*/ #define MAX SEQ  /* should be  Ën â  */ #define NR BUFS ((MAX SEQ +  )/ ) typedef enum {frame arrival  cksum err  timeout  network layer ready  ack timeout} event type; #include "" "" boolean no nak = true; /* no nak has been sent yet */ seq nr oldest frame = MAX SEQ +  ; /* initial value is only for the simulator */ static boolean between(seq nr a  seq nr b  seq nr c) { /* Same as between in protocol   but shorter and more obscure"
*/ return ((a <= b) && (b < c)) || ((c < a) && (a <= b)) || ((b < c) && (c < a)); } static void send frame(frame kind fk  seq nr frame nr  seq nr frame expected  packet buffer[ ]) { /* Construct and send a data  ack  or nak frame
*/ frame s; /* scratch variable */   = fk; /* kind == data  ack  or nak */ if (fk == data)   = buffer[frame nr % NR BUFS];   = frame nr; /* only meaningful for data frames */   = (frame expected + MAX SEQ) % (MAX SEQ +  ); if (fk == nak) no nak = false; /* one nak per frame  please */ to physical layer(&s); /* transmit the frame */ if (fk == data) start timer(frame nr % NR BUFS); stop ack timer(); /* no need for separate ack frame */ } void protocol (void) { seq nr ack expected; /* lower edge of senderâs window */ seq nr next frame to send; /* upper edge of senderâs window +  */ seq nr frame expected; /* lower edge of receiverâs window */ seq nr too far; /* upper edge of receiverâs window +  */ int i; /* index into buffer pool */ frame r; /* scratch variable */ packet out buf[NR BUFS]; /* buffers for the outbound stream */ packet in buf[NR BUFS]; /* buffers for the inbound stream */ boolean arrived[NR BUFS]; /* inbound bit map */ seq nr nbuffered; /* how many output buffers currently used */ event type event; enable network layer(); /* initialize */ ack expected =  ; /* next ack expected on the inbound stream */ next frame to send =  ; /* number of next outgoing frame */ frame expected =  ; too far = NR BUFS; nbuffered =  ; /* initially no packets are buffered */ for (i =  ; i < NR BUFS; i++) arrived[i] = false;   SLIDING WINDOW PROTOCOLS while (true) { wait for event(&event); /* five possibilities: see event type above */ switch(event) { case network layer ready: /* accept  save  and transmit a new frame */ nbuffered = nbuffered +  ; /* expand the window */ from network layer(&out buf[next frame to send % NR BUFS]); /* fetch new packet */ send frame(data  next frame to send  frame expected  out buf);/* transmit the frame */ inc(next frame to send); /* advance upper window edge */ break; case frame arrival: /* a data or control frame has arrived */ from physical layer(&r); /* fetch incoming frame from physical layer */ if (  == data) { /* An undamaged frame has arrived
*/ if ((  != frame expected) && no nak) send frame(nak frame expected  out buf); else start ack timer(); if (between(frame expected   too far) && (arrived[ %NR BUFS]==false)) { /* Frames may be accepted in any order
*/ arrived[  % NR BUFS] = true; /* mark buffer as full */ in buf[  % NR BUFS] =  ; /* insert data into buffer */ while (arrived[frame expected % NR BUFS]) { /* Pass frames and advance window
*/ to network layer(&in buf[frame expected % NR BUFS]); no nak = true; arrived[frame expected % NR BUFS] = false; inc(frame expected); /* advance lower edge of receiverâs window */ inc(too far); /* advance upper edge of receiverâs window */ start ack timer(); /* to see if a separate ack is needed */ } } } if(( ==nak) && between(ack expected ( + )%(MAX SEQ+ ) next frame to send)) send frame(data  ( + ) % (MAX SEQ +  )  frame expected  out buf); while (between(ack expected     next frame to send)) { nbuffered = nbuffered â  ; /* handle piggybacked ack */ stop timer(ack expected % NR BUFS); /* frame arrived intact */ inc(ack expected); /* advance lower edge of senderâs window */ } break; case cksum err: if (no nak) send frame(nak frame expected  out buf); /* damaged frame */ break; case timeout: send frame(data  oldest frame  frame expected  out buf); /* we timed out */ break; case ack timeout: send frame(ack  frame expected  out buf); /* ack timer expired; send ack */ } if (nbuffered < NR BUFS) enable network layer(); else disable network layer(); } } Figure  -
A sliding window protocol using selective repeat
THE DATA LINK LAYER
within the new window  so it is accepted as a new frame
The receiver also sends a (piggybacked) acknowledgement for frame   since  through  have been received
The sender is happy to learn that all its transmitted frames did actually arrive correctly  so it advances its window and immediately sends frames     and
Frame  will be accepted by the receiver and its packet will be passed directly to the network layer
Immediately thereafter  the receiving data link layer checks to see if it has a valid frame  already  discovers that it does  and passes the old buffered packet to the network layer as if it were a new packet
Consequently  the network layer gets an incorrect packet  and the protocol fails
The essence of the problem is that after the receiver advanced its window  the new range of valid sequence numbers overlapped the old one
Consequently  the following batch of frames might be either duplicates (if all the acknowledgements were lost) or new ones (if all the acknowledgements were received)
The poor receiver has no way of distinguishing these two cases
The way out of this dilemma lies in making sure that after the receiver has advanced its window there is no overlap with the original window
To ensure that there is no overlap  the maximum window size should be at most half the range of the sequence numbers
This situation is shown in Fig
-  (c) and Fig
With  bits  the sequence numbers range from  to
Only four unacknowledged frames should be outstanding at any instant
That way  if the receiver has just accepted frames  through  and advanced its window to permit acceptance of frames  through   it can unambiguously tell if subsequent frames are retransmissions (  through  ) or new ones (  through  )
In general  the window size for protocol  will be (MAX SEQ +  )/
An interesting question is: how many buffers must the receiver have? Under no conditions will it ever accept frames whose sequence numbers are below the lower edge of the window or frames whose sequence numbers are above the upper edge of the window
Consequently  the number of buffers needed is equal to the window size  not to the range of sequence numbers
In the preceding example of a  -bit sequence number  four buffers  numbered  through   are needed
When frame i arrives  it is put in buffer i mod
Notice that although i and (i +  ) mod  are ââcompetingââ for the same buffer  they are never within the window at the same time  because that would imply a window size of at least
For the same reason  the number of timers needed is equal to the number of buffers  not to the size of the sequence space
Effectively  a timer is associated with each buffer
When the timer runs out  the contents of the buffer are retransmitted
Protocol  also relaxes the implicit assumption that the channel is heavily loaded
We made this assumption in protocol  when we relied on frames being sent in the reverse direction on which to piggyback acknowledgements
If the reverse traffic is light  the acknowledgements may be held up for a long period of time  which can cause problems
In the extreme  if there is a lot of traffic in one   SLIDING WINDOW PROTOCOLS Sender Receiver                    (a) (b) (c) (d) Figure  -
(a) Initial situation with a window of size
(b) After  frames have been sent and received but not acknowledged
(c) Initial situation with a window size of
(d) After  frames have been sent and received but not acknowledged
direction and no traffic in the other direction  the protocol will block when the sender window reaches its maximum
To relax this assumption  an auxiliary timer is started by start ack timer after an in-sequence data frame arrives
If no reverse traffic has presented itself before this timer expires  a separate acknowledgement frame is sent
An interrupt due to the auxiliary timer is called an ack timeout event
With this arrangement  traffic flow in only one direction is possible because the lack of reverse data frames onto which acknowledgements can be piggybacked is no longer an obstacle
Only one auxiliary timer exists  and if start ack timer is called while the timer is running  it has no effect
The timer is not reset or extended since its purpose is to provide some minimum rate of acknowledgements
It is essential that the timeout associated with the auxiliary timer be appreciably shorter than the timeout used for timing out data frames
This condition is required to ensure that a correctly received frame is acknowledged early enough that the frameâs retransmission timer does not expire and retransmit the frame
Protocol  uses a more efficient strategy than protocol  for dealing with errors
Whenever the receiver has reason to suspect that an error has occurred  it sends a negative acknowledgement (NAK) frame back to the sender
Such a frame is a request for retransmission of the frame specified in the NAK
In two cases  the receiver should be suspicious: when a damaged frame arrives or a frame other than the expected one arrives (potential lost frame)
To avoid making multiple requests for retransmission of the same lost frame  the receiver should keep track of whether a NAK has already been sent for a given frame
The variable no nak in protocol  is true if no NAK has been sent yet for frame expected
If the NAK gets mangled or lost  no real harm is done  since the sender will eventually time out and retransmit the missing frame anyway
If the wrong frame arrives after a NAK has been sent and lost  no nak will be true and the auxiliary timer will be started
When it expires  an ACK will be sent to resynchronize the sender to the receiverâs current status
THE DATA LINK LAYER
In some situations  the time required for a frame to propagate to the destination  be processed there  and have the acknowledgement come back is (nearly) constant
In these situations  the sender can adjust its timer to be ââtight ââ just slightly larger than the normal time interval expected between sending a frame and receiving its acknowledgement
NAKs are not useful in this case
However  in other situations the time can be highly variable
For example  if the reverse traffic is sporadic  the time before acknowledgement will be shorter when there is reverse traffic and longer when there is not
The sender is faced with the choice of either setting the interval to a small value (and risking unnecessary retransmissions)  or setting it to a large value (and going idle for a long period after an error)
Both choices waste bandwidth
In general  if the standard deviation of the acknowledgement interval is large compared to the interval itself  the timer is set ââlooseââ to be conservative
NAKs can then appreciably speed up retransmission of lost or damaged frames
Closely related to the matter of timeouts and NAKs is the question of determining which frame caused a timeout
In protocol   it is always ack expected  because it is always the oldest
In protocol   there is no trivial way to determine who timed out
Suppose that frames  through  have been transmitted  meaning that the list of outstanding frames is   in order from oldest to youngest
Now imagine that  times out   (a new frame) is transmitted   times out   times out  and  (another new frame) is transmitted
At this point the list of outstanding frames is  from oldest to youngest
If all inbound traffic (
acknowledgement- bearing frames) is lost for a while  the seven outstanding frames will time out in that order
To keep the example from getting even more complicated than it already is  we have not shown the timer administration
Instead  we just assume that the variable oldest frame is set upon timeout to indicate which frame timed out  EXAMPLE DATA LINK PROTOCOLS Within a single building  LANs are widely used for interconnection  but most wide-area network infrastructure is built up from point-to-point lines
we will look at LANs
Here we will examine the data link protocols found on point-to-point lines in the Internet in two common situations
The first situation is when packets are sent over SONET optical fiber links in wide-area networks
These links are widely used  for example  to connect routers in the different locations of an ISPâs network
The ond situation is for ADSL links running on the local loop of the telephone network at the edge of the Internet
These links connect millions of individuals and businesses to the Internet
The Internet needs point-to-point links for these uses  as well as dial-up modems  leased lines  and cable modems  and so on
A standard protocol called PPP   EXAMPLE DATA LINK PROTOCOLS (Point-to-Point Protocol) is used to send packets over these links
PPP is defined in RFC  and further elaborated in RFC  and other RFCs (Simpson  a  b)
SONET and ADSL links both apply PPP  but in different ways
Packet over SONET SONET  which we covered in
is the physical layer protocol that is most commonly used over the wide-area optical fiber links that make up the backbone of communications networks  including the telephone system
It provides a bitstream that runs at a well-defined rate  for example
Gbps for an OC-  link
This bitstream is organized as fixed-size byte payloads that recur every Î¼  whether or not there is user data to send
To carry packets across these links  some framing mechanism is needed to distinguish occasional packets from the continuous bitstream in which they are transported
PPP runs on IP routers to provide this mechanism  as shown in Fig
IP SONET PPP Optical fiber Router IP packet PPP frame SONET payload SONET payload (a) (b) IP SONET PPP Figure  -
Packet over SONET
(a) A protocol stack
(b) Frame relationships
PPP improves on an earlier  simpler protocol called SLIP (Serial Line Internet Protocol) and is used to handle error detection link configuration  support multiple protocols  permit authentication  and more
With a wide set of options  PPP provides three main features:
A framing method that unambiguously delineates the end of one frame and the start of the next one
The frame format also handles error detection A link control protocol for bringing lines up  testing them  negotiating options  and bringing them down again gracefully when they are no longer needed
This protocol is called LCP (Link Control Protocol) A way to negotiate network-layer options in a way that is independent of the network layer protocol to be used
The method chosen is to have a different NCP (Network Control Protocol) for each network layer supported
THE DATA LINK LAYER
The PPP frame format was chosen to closely resemble the frame format of HDLC (High-level Data Link Control)  a widely used instance of an earlier family of protocols  since there was no need to reinvent the wheel
The primary difference between PPP and HDLC is that PPP is byte oriented rather than bit oriented
In particular  PPP uses byte stuffing and all frames are an integral number of bytes
HDLC uses bit stuffing and allows frames of  say
There is a ond major difference in practice  however
HDLC provides reliable transmission with a sliding window  acknowledgements  and timeouts in the manner we have studied
PPP can also provide reliable transmission in noisy environments  such as wireless networks; the exact details are defined in RFC
However  this is rarely done in practice
Instead  an ââunnumbered modeââ is nearly always used in the Internet to provide connectionless unacknowledged service
The PPP frame format is shown in Fig
All PPP frames begin with the standard HDLC flag byte of  x E (   )
The flag byte is stuffed if it occurs within the Payload field using the escape byte  x D
The following byte is the escaped byte XORed with  x   which flips the  th bit
For example  x D  x E is the escape sequence for the flag byte  x E
This means the start and end of frames can be searched for simply by scanning for the byte  x E since it will not occur elsewhere
The destuffing rule when receiving a frame is to look for  x D  remove it  and XOR the following byte with  x
Also  only one flag byte is needed between frames
Multiple flag bytes can be used to fill the link when there are no frames to be sent
After the start-of-frame flag byte comes the Address field
This field is always set to the binary value  to indicate that all stations are to accept the frame
Using this value avoids the issue of having to assign data link addresses
Flag  Flag  Address  Control Protocol  Payload Checksum Bytes    or  Variable  or Figure  -
The PPP full frame format for unnumbered mode operation
The Address field is followed by the Control field  the default value of which is
This value indicates an unnumbered frame
Since the Address and Control fields are always constant in the default configuration  LCP provides the necessary mechanism for the two parties to negotiate an option to omit them altogether and save  bytes per frame
The fourth PPP field is the Protocol field
Its job is to tell what kind of packet is in the Payload field
Codes starting with a  bit are defined for IP version   IP version   and other network layer protocols that might be used  such as IPX and   EXAMPLE DATA LINK PROTOCOLS AppleTalk
Codes starting with a  bit are used for PPP configuration protocols  including LCP and a different NCP for each network layer protocol supported
The default size of the Protocol field is  bytes  but it can be negotiated down to  byte using LCP
The designers were perhaps overly cautious in thinking that someday there might be more than protocols in use
The Payload field is variable length  up to some negotiated maximum
If the length is not negotiated using LCP during line setup  a default length of  bytes is used
Padding may follow the payload if it is needed
After the Payload field comes the Checksum field  which is normally  bytes  but a  -byte checksum can be negotiated
The  -byte checksum is in fact the same  -bit CRC whose generator polynomial is given at the end of
The  - byte checksum is also an industry-standard CRC
PPP is a framing mechanism that can carry the packets of multiple protocols over many types of physical layers
To use PPP over SONET  the choices to make are spelled out in RFC  (Malis and Simpson  )
A  -byte checksum is used  since this is the primary means of detecting transmission errors over the physical  link  and network layers
It is recommended that the Address  Control  and Protocol fields not be compressed  since SONET links already run at relatively high rates
There is also one unusual feature
The PPP payload is scrambled (as described in
) before it is inserted into the SONET payload
Scrambling XORs the payload with a long pseudorandom sequence before it is transmitted
The issue is that the SONET bitstream needs frequent bit transitions for synchronization
These transitions come naturally with the variation in voice signals  but in data communication the user chooses the information that is sent and might send a packet with a long run of  s
With scrambling  the likelihood of a user being able to cause problems by sending a long run of  s is made extremely low
Before PPP frames can be carried over SONET lines  the PPP link must be established and configured
The phases that the link goes through when it is brought up  used  and taken down again are shown in Fig
The link starts in the DEAD state  which means that there is no connection at the physical layer
When a physical layer connection is established  the link moves to ESTABLISH
At this point  the PPP peers exchange a series of LCP packets  each carried in the Payload field of a PPP frame  to select the PPP options for the link from the possibilities mentioned above
The initiating peer proposes options  and the responding peer either accepts or rejects them  in whole or part
The responder can also make alternative proposals
If LCP option negotiation is successful  the link reaches the AUTHENTICATE state
Now the two parties can check each otherâs identities  if desired
If authentication is successful  the NETWORK state is entered and a series of NCP packets are sent to configure the network layer
It is difficult to generalize about the NCP protocols because each one is specific to some network layer protocol and allows configuration requests to be made that are specific to that protocol
THE DATA LINK LAYER
DEAD NETWORK TERMINATE OPEN ESTABLISH AUTHENTICATE Carrier detected Both sides agree on options Authentication successful NCP configuration Carrier dropped Failed Failed Done Figure  -
State diagram for bringing a PPP link up and down
For IP  for example  the assignment of IP addresses to both ends of the link is the most important possibility
Once OPEN is reached  data transport can take place
It is in this state that IP packets are carried in PPP frames across the SONET line
When data transport is finished  the link moves into the TERMINATE state  and from there it moves back to the DEAD state when the physical layer connection is dropped
ADSL (Asymmetric Digital Subscriber Loop) ADSL connects millions of home subscribers to the Internet at megabit/ rates over the same telephone local loop that is used for plain old telephone service
we described how a device called a DSL modem is added on the home side
It sends bits over the local loop to a device called a DSLAM (DSL Access Multiplexer)  pronounced ââdee-slam ââ in the telephone companyâs local office
Now we will explore in more detail how packets are carried over ADSL links
The overall picture for the protocols and devices used with ADSL is shown in Fig
Different protocols are deployed in different networks  so we have chosen to show the most popular scenario
Inside the home  a computer such as a PC sends IP packets to the DSL modem using a link layer like Ethernet
The DSL modem then sends the IP packets over the local loop to the DSLAM using the protocols that we are about to study
At the DSLAM (or a router connected to it depending on the implementation) the IP packets are extracted and enter an ISP network so that they may reach any destination on the Internet
The protocols shown over the ADSL link in Fig
They are based on a digital modulation scheme called   EXAMPLE DATA LINK PROTOCOLS AAL  ADSL Local loop ATM PPP DSLAM (with router) AAL  ADSL ATM PPP DSL modem PC Ethernet Internet Customerâs home ISPâs office Ethernet IP Link IP Figure  -
ADSL protocol stacks
orthogonal frequency division multiplexing (also known as discrete multitone)  as we saw in
Near the top of the stack  just below the IP network layer  is PPP
This protocol is the same PPP that we have just studied for packet over SONET transports
It works in the same way to establish and configure the link and carry IP packets
In between ADSL and PPP are ATM and AAL
These are new protocols that we have not seen before
ATM (Asynchronous Transfer Mode) was designed in the early s and launched with incredible hype
It promised a network technology that would solve the worldâs telecommunications problems by merging voice  data  cable television  telegraph  carrier pigeon  tin cans connected by strings  tom toms  and everything else into an integrated system that could do everything for everyone
This did not happen
In large part  the problems of ATM were similar to those we described concerning the OSI protocols  that is  bad timing  technology  implementation  and politics
Nevertheless  ATM was much more successful than OSI
While it has not taken over the world  it remains widely used in niches including broadband access lines such as DSL  and WAN links inside telephone networks
ATM is a link layer that is based on the transmission of fixed-length cells of information
The ââAsynchronousââ in its name means that the cells do not always need to be sent in the way that bits are continuously sent over synchronous lines  as in SONET
Cells only need to be sent when there is information to carry
ATM is a connection-oriented technology
Each cell carries a virtual circuit identifier in its header and devices use this identifier to forward cells along the paths of established connections
The cells are each   bytes long  consisting of a  -byte payload plus a  -byte header
By using small cells  ATM can flexibly divide the bandwidth of a physical layer link among different users in fine slices
This ability is useful when  for example  sending both voice and data over one link without having long data packets that would cause large variations in the delay of the voice samples
The unusual choice for the cell length (
compared to the more natural choice of a THE DATA LINK LAYER
power of  ) is an indication of just how political the design of ATM was
The  -byte size for the payload was a compromise to resolve a deadlock between Europe  which wanted  -byte cells  and the
which wanted  -byte cells
A brief overview of ATM is given by Siu and Jain (   )
To send data over an ATM network  it needs to be mapped into a sequence of cells
This mapping is done with an ATM adaptation layer in a process called segmentation and reassembly
Several adaptation layers have been defined for different services  ranging from periodic voice samples to packet data
The main one used for packet data is AAL  (ATM Adaptation Layer  )
An AAL  frame is shown in Fig
Instead of a header  it has a trailer that gives the length and has a  -byte CRC for error detection
Naturally  the CRC is the same one used for PPP and IEEE LANs like Ethernet
Wang and Crowcroft (   ) have shown that it is strong enough to detect nontraditional errors such as cell reordering
As well as a payload  the AAL  frame has padding
This rounds out the overall length to be a multiple of   bytes so that the frame can be evenly divided into cells
No addresses are needed on the frame as the virtual circuit identifier carried in each cell will get it to the right destination
PPP protocol PPP payload Pad Unused Length CRC Bytes  or to  AAL  trailer Variable AAL  payload Figure  -
AAL  frame carrying PPP data
Now that we have described ATM  we have only to describe how PPP makes use of ATM in the case of ADSL
It is done with yet another standard called PPPoA (PPP over ATM)
This standard is not really a protocol (so it does not appear in Fig
-  ) but more a specification of how to work with both PPP and AAL  frames
It is described in RFC  (Gross et al
Only the PPP protocol and payload fields are placed in the AAL  payload  as shown in Fig
The protocol field indicates to the DSLAM at the far end whether the payload is an IP packet or a packet from another protocol such as LCP
The far end knows that the cells contain PPP information because an ATM virtual circuit is set up for this purpose
Within the AAL  frame  PPP framing is not needed as it would serve no purpose; ATM and AAL  already provide the framing
More framing would be worthless
The PPP CRC is also not needed because AAL  already includes the very same CRC
This error detection mechanism supplements the ADSL physical layer coding of a Reed-Solomon code for error correction and a  -byte CRC for the detection of any remaining errors not otherwise caught
This scheme has a much more sophisticated error-recovery mechanism than when packets are sent over a SONET line because ADSL is a much noisier channel
SUMMARY The task of the data link layer is to convert the raw bit stream offered by the physical layer into a stream of frames for use by the network layer
The link layer can present this stream with varying levels of reliability  ranging from connectionless  unacknowledged service to reliable  connection-oriented service
Various framing methods are used  including byte count  byte stuffing  and bit stuffing
Data link protocols can provide error control to detect or correct damaged frames and to retransmit lost frames
To prevent a fast sender from overrunning a slow receiver  the data link protocol can also provide flow control
The sliding window mechanism is widely used to integrate error control and flow control in a simple way
When the window size is  packet  the protocol is stop-and-wait
Codes for error correction and detection add redundant information to messages by using a variety of mathematical techniques
Convolutional codes and Reed-Solomon codes are widely deployed for error correction  with low-density parity check codes increasing in popularity
The codes for error detection that are used in practice include cyclic redundancy checks and checksums
All these codes can be applied at the link layer  as well as at the physical layer and higher layers
We examined a series of protocols that provide a reliable link layer using acknowledgements and retransmissions  or ARQ (Automatic Repeat reQuest)  under more realistic assumptions
Starting from an error-free environment in which the receiver can handle any frame sent to it  we introduced flow control  followed by error control with sequence numbers and the stop-and-wait algorithm
Then we used the sliding window algorithm to allow bidirectional communication and introduce the concept of piggybacking
The last two protocols pipeline the transmission of multiple frames to prevent the sender from blocking on a link with a long propagation delay
The receiver can either discard all frames other than the next one in sequence  or buffer out-of-order frames and send negative acknowledgements for greater bandwidth efficiency
The former strategy is a go-back-n protocol  and the latter strategy is a selective repeat protocol
The Internet uses PPP as the main data link protocol over point-to-point lines
It provides a connectionless unacknowledged service  using flag bytes to delimit frames and a CRC for error detection
It is used to carry packets across a range of links  including SONET links in wide-area networks and ADSL links for the home
An upper-layer packet is split into   frames  each of which has an  % chance of arriving undamaged
If no error control is done by the data link protocol  how many times must the message be sent on average to get the entire thing through? THE DATA LINK LAYER
The following character encoding is used in a data link protocol: A:  B:  FLAG:  ESC:  Show the bit sequence transmitted (in binary) for the four-character frame A B ESC FLAG when each of the following framing methods is used: (a) Byte count
(b) Flag bytes with byte stuffing
(c) Starting and ending flag bytes with bit stuffing The following data fragment occurs in the middle of a data stream for which the bytestuffing algorithm described in the text is used: A B ESC C ESC FLAG FLAG D
What is the output after stuffing?
What is the maximum overhead in byte-stuffing algorithm?
One of your classmates  Scrooge  has pointed out that it is wasteful to end each frame with a flag byte and then begin the next one with a ond flag byte
One flag byte could do the job as well  and a byte saved is a byte earned
Do you agree?
A bit string     needs to be transmitted at the data link layer
What is the string actually transmitted after bit stuffing?
Can you think of any circumstances under which an open-loop protocol (
a Hamming code) might be preferable to the feedback-type protocols discussed throughout this  ter?
To provide more reliability than a single parity bit can give  an error-detecting coding scheme uses one parity bit for checking all the odd-numbered bits and a ond parity bit for all the even-numbered bits
What is the Hamming distance of this code?
Sixteen-bit messages are transmitted using a Hamming code
How many check bits are needed to ensure that the receiver can detect and correct single-bit errors? Show the bit pattern transmitted for the message
Assume that even parity is used in the Hamming code A  -bit Hamming code whose hexadecimal value is  xE F arrives at a receiver
What was the original value in hexadecimal? Assume that not more than  bit is in error One way of detecting errors is to transmit data as a block of n rows of k bits per row and add parity bits to each row and each column
The bitin the lower-right corner is a parity bit that checks its row and its column
Will this scheme detect all single errors? Double errors? Triple errors? Show that this scheme cannot detect some four-bit errors Suppose that data are transmitted in blocks of sizes  bits
What is the maximum error rate under which error detection and retransmission mechanism (  parity bit per block) is better than using Hamming code? Assume that bit errors are independent of one another and no bit error occurs during retransmission A block of bits with n rows and k columns uses horizontal and vertical parity bits for error detection
Suppose that exactly  bits are inverted due to transmission errors
Derive an expression for the probability that the error will be undetected  PROBLEMS
Using the convolutional coder of Fig
Suppose that a message    is transmitted using Internet Checksum ( -bit word)
What is the value of the checksum?
What is the remainder obtained by dividing x  + x  +  by the generator polynomial x  +  ?
A bit stream  is transmitted using the standard CRC method described in the text
The generator polynomial is x  +
Show the actual bit string transmitted
Suppose that the third bit from the left is inverted during transmission
Show that this error is detected at the receiverâs end
Give an example of bit errors in the bit string transmitted that will not be detected by the receiver A -bit message is sent that contains data bits and   CRC bits
CRC is computed using the IEEE standardized   -degree CRC polynomial
For each of the following  explain whether the errors during message transmission will be detected by the receiver: (a) There was a single-bit error
(b) There were two isolated bit errors
(c) There were   isolated bit errors
(d) There were   isolated bit errors
(e) There was a  -bit long burst error
(f) There was a  -bit long burst error In the discussion of ARQ protocol in tion    a scenario was outlined that resulted in the receiver accepting two copies of the same frame due to a loss of acknowledgement frame
Is it possible that a receiver may accept multiple copies of the same frame when none of the frames (message or acknowledgement) are lost?
A channel has a bit rate of  kbps and a propagation delay of   m
For what range of frame sizes does stop-and-wait give an efficiency of at least  %?
In protocol   is it possible for the sender to start the timer when it is already running? If so  how might this occur? If not  why is it impossible?
A -km-long T  trunk is used to transmit  -byte frames using protocol
If the propagation speed is  Î¼/km  how many bits should the sequence numbers be?
Imagine a sliding window protocol using so many bits for sequence numbers that wraparound never occurs
What relations must hold among the four window edges and the window size  which is constant and the same for both the sender and the receiver?
If the procedure between in protocol  checked for the condition a â¤ b â¤ c instead of the condition a â¤ b < c  would that have any effect on the protocolâs correctness or efficiency? Explain your answer In protocol   when a data frame arrives  a check is made to see if the sequence number differs from the one expected and no nak is true
If both conditions hold  a NAK is sent
Otherwise  the auxiliary timer is started
Suppose that the else clause were omitted
Would this change affect the protocolâs correctness? THE DATA LINK LAYER   Suppose that the three-statement while loop near the end of protocol  was removed from the code
Would this affect the correctness of the protocol or just the performance? Explain your answer The distance from earth to a distant planet is approximately  Ã  m
What is the channel utilization if a stop-and-wait protocol is used for frame transmission on a   Mbps point-to-point link? Assume that the frame size is   KB and the speed of light is  Ã m/s In the previous problem  suppose a sliding window protocol is used instead
For what send window size will the link utilization be   %? You may ignore the protocol processing times at the sender and the receiver In protocol   the code for frame arrival has a tion used for NAKs
This tion is invoked if the incoming frame is a NAK and another condition is met
Give a scenario where the presence of this other condition is essential Consider the operation of protocol  over a  -Mbps perfect (
error-free) line
The maximum frame size is  bits
New packets are generated  ond apart
The timeout interval is   m
If the special acknowledgement timer were eliminated  unnecessary timeouts would occur
How many times would the average message be transmitted?
In protocol   MAX SEQ =  n â
While this condition is obviously desirable to make efficient use of header bits  we have not demonstrated that it is essential
Does the protocol work correctly for MAX SEQ =   for example?
Frames of  bits are sent over a  -Mbps channel using a geostationary satellite whose propagation time from the earth is m
Acknowledgements are always piggybacked onto data frames
The headers are very short
Three-bit sequence numbers are used
What is the maximum achievable channel utilization for (a) Stop-and-wait? (b) Protocol  ? (c) Protocol  ?
Compute the fraction of the bandwidth that is wasted on overhead (headers and retransmissions) for protocol  on a heavily loaded  -kbps satellite channel with data frames consisting of   header and  data bits
Assume that the signal propagation time from the earth to the satellite is m
ACK frames never occur
NAK frames are   bits
The error rate for data frames is  %  and the error rate for NAK frames is negligible
The sequence numbers are  bits Consider an error-free  -kbps satellite channel used to send   -byte data frames in one direction  with very short acknowledgements coming back the other way
What is the maximum throughput for window sizes of  and   ? The earth-satellite propagation time is m A   -km-long cable runs at the T  data rate
The propagation speed in the cable is  /  the speed of light in vacuum
How many bits fit in the cable?
Give at least one reason why PPP uses byte stuffing instead of bit stuffing to prevent accidental flag bytes within the payload from causing confusion  PROBLEMS
What is the minimum overhead to send an IP packet using PPP? Count only the overhead introduced by PPP itself  not the IP header overhead
What is the maximum overhead?
A   -byte IP packet is transmitted over a local loop using ADSL protocol stack
How many ATM cells will be transmitted? Briefly describe their contents The goal of this lab exercise is to implement an error-detection mechanism using the standard CRC algorithm described in the text
Write two programs  generator and verifier
The generator program reads from standard input a line of ASCII text containing an n-bit message consisting of a string of  s and  s
The ond line is the kbit polynomial  also in ASCII
It outputs to standard output a line of ASCII text with n + k  s and  s representing the message to be transmitted
Then it outputs the polynomial  just as it read it in
The verifier program reads in the output of the generator program and outputs a message indicating whether it is correct or not
Finally  write a program  alter  that inverts  bit on the first line depending on its argument (the bit number counting the leftmost bit as  ) but copies the rest of the two lines correctly
By typing generator <file | verifier you should see that the message is correct  but by typing generator <file | alter arg | verifier you should get the error message
This page intentionally left blank  THE MEDIUM ACCESS CONTROL SUBLAYER Network links can be divided into two categories: those using point-to-point connections and those using broadcast channels
We studied point-to-point links in
; this  ter deals with broadcast links and their protocols
In any broadcast network  the key issue is how to determine who gets to use the channel when there is competition for it
To make this point  consider a conference call in which six people  on six different telephones  are all connected so that each one can hear and talk to all the others
It is very likely that when one of them stops speaking  two or more will start talking at once  leading to chaos
In a face-to-face meeting  chaos is avoided by external means
For example  at a meeting  people raise their hands to request permission to speak
When only a single channel is available  it is much harder to determine who should go next
Many protocols for solving the problem are known
They form the contents of this  ter
In the literature  broadcast channels are sometimes referred to as multiaccess channels or random access channels
The protocols used to determine who goes next on a multiaccess channel belong to a sublayer of the data link layer called the MAC (Medium Access Control) sublayer
The MAC sublayer is especially important in LANs  particularly wireless ones because wireless is naturally a broadcast channel
WANs  in contrast  use point-to-point links  except for satellite networks
Because multiaccess channels and LANs are so closely related  in this  ter we will discuss LANs in    THE MEDIUM ACCESS CONTROL SUBLAYER
general  including a few issues that are not strictly part of the MAC sublayer  but the main subject here will be control of the channel
Technically  the MAC sublayer is the bottom part of the data link layer  so logically we should have studied it before examining all the point-to-point protocols in   Nevertheless  for most people  it is easier to understand protocols involving multiple parties after two-party protocols are well understood
For that reason we have deviated slightly from a strict bottom-up order of presentation  THE CHANNEL ALLOCATION PROBLEM The central theme of this  ter is how to allocate a single broadcast channel among competing users
The channel might be a portion of the wireless spectrum in a geographic region  or a single wire or optical fiber to which multiple nodes are connected
It does not matter
In both cases  the channel connects each user to all other users and any user who makes full use of the channel interferes with other users who also wish to use the channel
We will first look at the shortcomings of static allocation schemes for bursty traffic
Then  we will lay out the key assumptions used to model the dynamic schemes that we examine in the following tions
Static Channel Allocation The traditional way of allocating a single channel  such as a telephone trunk  among multiple competing users is to chop up its capacity by using one of the multiplexing schemes we described in   such as FDM (Frequency Division Multiplexing)
If there are N users  the bandwidth is divided into N equal-sized portions  with each user being assigned one portion
Since each user has a private frequency band  there is now no interference among users
When there is only a small and constant number of users  each of which has a steady stream or a heavy load of traffic  this division is a simple and efficient allocation mechanism
A wireless example is FM radio stations
Each station gets a portion of the FM band and uses it most of the time to broadcast its signal
However  when the number of senders is large and varying or the traffic is bursty  FDM presents some problems
If the spectrum is cut up into N regions and fewer than N users are currently interested in communicating  a large piece of valuable spectrum will be wasted
And if more than N users want to communicate  some of them will be denied permission for lack of bandwidth  even if some of the users who have been assigned a frequency band hardly ever transmit or receive anything
Even assuming that the number of users could somehow be held constant at N  dividing the single available channel into some number of static subchannels is   THE CHANNEL ALLOCATION PROBLEM inherently inefficient
The basic problem is that when some users are quiescent  their bandwidth is simply lost
They are not using it  and no one else is allowed to use it either
A static allocation is a poor fit to most computer systems  in which data traffic is extremely bursty  often with peak traffic to mean traffic ratios of :
Consequently  most of the channels will be idle most of the time
The poor performance of static FDM can easily be seen with a simple queueing theory calculation
Let us start by finding the mean time delay  T  to send a frame onto a channel of capacity C bps
We assume that the frames arrive randomly with an average arrival rate of Î» frames/  and that the frames vary in length with an average length of  /Î¼ bits
With these parameters  the service rate of the channel is Î¼C frames/
A standard queueing theory result is T = Î¼C â Î»  (For the curious  this result is for an ââM/M/ ââ queue
It requires that the randomness of the times between frame arrivals and the frame lengths follow an exponential distribution  or equivalently be the result of a Poisson process
) In our example  if C is Mbps  the mean frame length  /Î¼  is   bits  and the frame arrival rate  Î»  is  frames/  then T = Î¼
Note that if we ignored the queueing delay and just asked how long it takes to send a  - bit frame on a   -Mbps network  we would get the (incorrect) answer of Î¼
That result only holds when there is no contention for the channel
Now let us divide the single channel into N independent subchannels  each with capacity C /N bps
The mean input rate on each of the subchannels will now be Î»/N
Recomputing T  we get TN = Î¼(C /N) â (Î»/N)  = Î¼C â Î» N = NT ( - ) The mean delay for the divided channel is N times worse than if all the frames were somehow magically arranged orderly in a big central queue
This same result says that a bank lobby full of ATM machines is better off having a single queue feeding all the machines than a separate queue in front of each machine
Precisely the same arguments that apply to FDM also apply to other ways of statically dividing the channel
If we were to use time division multiplexing (TDM) and allocate each user every Nth time slot  if a user does not use the allocated slot  it would just lie fallow
The same would hold if we split up the networks physically
Using our previous example again  if we were to replace the   -Mbps network with   networks of   Mbps each and statically allocate each user to one of them  the mean delay would jump from Î¼ to  m
Since none of the traditional static channel allocation methods work well at all with bursty traffic  we will now explore dynamic methods
THE MEDIUM ACCESS CONTROL SUBLAYER
Assumptions for Dynamic Channel Allocation Before we get to the first of the many channel allocation methods in this  ter  it is worthwhile to carefully formulate the allocation problem
Underlying all the work done in this area are the following five key assumptions:
Independent Traffic
The model consists of N independent stations (
computers  telephones)  each with a program or user that generates frames for transmission
The expected number of frames generated in an interval of length Ît is Î»Ît  where Î» is a constant (the arrival rate of new frames)
Once a frame has been generated  the station is blocked and does nothing until the frame has been successfully transmitted Single Channel
A single channel is available for all communication
All stations can transmit on it and all can receive from it
The stations are assumed to be equally capable  though protocols may assign them different roles (
priorities) Observable Collisions
If two frames are transmitted simultaneously  they overlap in time and the resulting signal is garbled
This event is called a collision
All stations can detect that a collision has occurred
A collided frame must be transmitted again later
No errors other than those generated by collisions occur Continuous or Slotted Time
Time may be assumed continuous  in which case frame transmission can begin at any instant
Alternatively  time may be slotted or divided into discrete intervals (called slots)
Frame transmissions must then begin at the start of a slot
A slot may contain  or more frames  corresponding to an idle slot  a successful transmission  or a collision  respectively Carrier Sense or No Carrier Sense
With the carrier sense assumption  stations can tell if the channel is in use before trying to use it
No station will attempt to use the channel while it is sensed as busy
If there is no carrier sense  stations cannot sense the channel before trying to use it
They just go ahead and transmit
Only later can they determine whether the transmission was successful
Some discussion of these assumptions is in order
The first one says that frame arrivals are independent  both across stations and at a particular station  and that frames are generated unpredictably but at a constant rate
Actually  this assumption is not a particularly good model of network traffic  as it is well known that packets come in bursts over a range of time scales (Paxson and Floyd  ; and Leland et al
Nonetheless  Poisson models  as they are frequently called  are useful because they are mathematically tractable
They help us analyze   THE CHANNEL ALLOCATION PROBLEM protocols to understand roughly how performance changes over an operating range and how it compares with other designs
The single-channel assumption is the heart of the model
No external ways to communicate exist
Stations cannot raise their hands to request that the teacher call on them  so we will have to come up with better solutions
The remaining three assumptions depend on the engineering of the system  and we will say which assumptions hold when we examine a particular protocol
The collision assumption is basic
Stations need some way to detect collisions if they are to retransmit frames rather than let them be lost
For wired channels  node hardware can be designed to detect collisions when they occur
The stations can then terminate their transmissions prematurely to avoid wasting capacity
This detection is much harder for wireless channels  so collisions are usually inferred after the fact by the lack of an expected acknowledgement frame
It is also possible for some frames involved in a collision to be successfully received  depending on the details of the signals and the receiving hardware
However  this situation is not the common case  so we will assume that all frames involved in a collision are lost
We will also see protocols that are designed to prevent collisions from occurring in the first place
The reason for the two alternative assumptions about time is that slotted time can be used to improve performance
However  it requires the stations to follow a master clock or synchronize their actions with each other to divide time into discrete intervals
Hence  it is not always available
We will discuss and analyze systems with both kinds of time
For a given system  only one of them holds
Similarly  a network may have carrier sensing or not have it
Wired networks will generally have carrier sense
Wireless networks cannot always use it effectively because not every station may be within radio range of every other station
Similarly  carrier sense will not be available in other settings in which a station cannot communicate directly with other stations  for example a cable modem in which stations must communicate via the cable headend
Note that the word ââcarrierââ in this sense refers to a signal on the channel and has nothing to do with the common carriers (
telephone companies) that date back to the days of the Pony Express
To avoid any misunderstanding  it is worth noting that no multiaccess protocol guarantees reliable delivery
Even in the absence of collisions  the receiver may have copied some of the frame incorrectly for various reasons
Other parts of the link layer or higher layers provide reliability  MULTIPLE ACCESS PROTOCOLS Many algorithms for allocating a multiple access channel are known
In the following tions  we will study a small sample of the more interesting ones and give some examples of how they are commonly used in practice
THE MEDIUM ACCESS CONTROL SUBLAYER
ALOHA The story of our first MAC starts out in pristine Hawaii in the early s
In this case  ââpristineââ can be interpreted as âânot having a working telephone system
ââ This did not make life more pleasant for researcher Norman Abramson and his colleagues at the University of Hawaii who were trying to connect users on remote islands to the main computer in Honolulu
Stringing their own cables under the Pacific Ocean was not in the cards  so they looked for a different solution
The one they found used short-range radios  with each user terminal sharing the same upstream frequency to send frames to the central computer
It included a simple and elegant method to solve the channel allocation problem
Their work has been extended by many researchers since then (Schwartz and Abramson  )
Although Abramsonâs work  called the ALOHA system  used groundbased radio broadcasting  the basic idea is applicable to any system in which uncoordinated users are competing for the use of a single shared channel
We will discuss two versions of ALOHA here: pure and slotted
They differ with respect to whether time is continuous  as in the pure version  or divided into discrete slots into which all frames must fit
Pure ALOHA The basic idea of an ALOHA system is simple: let users transmit whenever they have data to be sent
There will be collisions  of course  and the colliding frames will be damaged
Senders need some way to find out if this is the case
In the ALOHA system  after each station has sent its frame to the central computer  this computer rebroadcasts the frame to all of the stations
A sending station can thus listen for the broadcast from the hub to see if its frame has gotten through
In other systems  such as wired LANs  the sender might be able to listen for collisions while transmitting
If the frame was destroyed  the sender just waits a random amount of time and sends it again
The waiting time must be random or the same frames will collide over and over  in lockstep
Systems in which multiple users share a common channel in a way that can lead to conflicts are known as contention systems
A sketch of frame generation in an ALOHA system is given in Fig
We have made the frames all the same length because the throughput of ALOHA systems is maximized by having a uniform frame size rather than by allowing variable- length frames
Whenever two frames try to occupy the channel at the same time  there will be a collision (as seen in Fig
- ) and both will be garbled
If the first bit of a new frame overlaps with just the last bit of a frame that has almost finished  both frames will be totally destroyed (
have incorrect checksums) and both will have to be retransmitted later
The checksum does not (and should not) distinguish between a total loss and a near miss
Bad is bad
MULTIPLE ACCESS PROTOCOLS User A B C D E Collision Time Collision Figure  -
In pure ALOHA  frames are transmitted at completely arbitrary times
An interesting question is: what is the efficiency of an ALOHA channel? In other words  what fraction of all transmitted frames escape collisions under these chaotic circumstances? Let us first consider an infinite collection of users typing at their terminals (stations)
A user is always in one of two states: typing or waiting
Initially  all users are in the typing state
When a line is finished  the user stops typing  waiting for a response
The station then transmits a frame containing the line over the shared channel to the central computer and checks the channel to see if it was successful
If so  the user sees the reply and goes back to typing
If not  the user continues to wait while the station retransmits the frame over and over until it has been successfully sent
Let the ââframe timeââ denote the amount of time needed to transmit the standard  fixed-length frame (
the frame length divided by the bit rate)
At this point  we assume that the new frames generated by the stations are well modeled by a Poisson distribution with a mean of N frames per frame time
(The infinitepopulation assumption is needed to ensure that N does not decrease as users become blocked
) If N >   the user community is generating frames at a higher rate than the channel can handle  and nearly every frame will suffer a collision
For reasonable throughput  we would expect  < N <
In addition to the new frames  the stations also generate retransmissions of frames that previously suffered collisions
Let us further assume that the old and new frames combined are well modeled by a Poisson distribution  with mean of G frames per frame time
Clearly  G â¥ N
At low load (
N â¼â¼  )  there will be few collisions  hence few retransmissions  so G â¼â¼ N
At high load  there will be many collisions  so G > N
Under all loads  the throughput  S  is just the offered load  G  times the probability  P  of a transmission succeedingâthat is  S = GP  where P  is the probability that a frame does not suffer a collision
A frame will not suffer a collision if no other frames are sent within one frame time of its start  as shown in Fig
Under what conditions will the THE MEDIUM ACCESS CONTROL SUBLAYER
shaded frame arrive undamaged? Let t be the time required to send one frame
If any other user has generated a frame between time t  and t  + t  the end of that frame will collide with the beginning of the shaded one
In fact  the shaded frameâs fate was already sealed even before the first bit was sent  but since in pure ALOHA a station does not listen to the channel before transmitting  it has no way of knowing that another frame was already underway
Similarly  any other frame started between t  + t and t  +  t will bump into the end of the shaded frame
Collides with the start of the shaded frame Collides with the end of the shaded frame t t  t + t t +  t t +  t Time Vulnerable Figure  -
Vulnerable period for the shaded frame
The probability that k frames are generated during a given frame time  in which G frames are expected  is given by the Poisson distribution Pr[k ] = k! Gk e âG ( - ) so the probability of zero frames is just e âG
In an interval two frame times long  the mean number of frames generated is  G
The probability of no frames being initiated during the entire vulnerable period is thus given by P  = e â G
Using S = GP  we get S = Geâ G The relation between the offered traffic and the throughput is shown in Fig
The maximum throughput occurs at G =
with S =  / e  which is about   In other words  the best we can hope for is a channel utilization of  %
This result is not very encouraging  but with everyone transmitting at will  we could hardly have expected a   % success rate
Slotted ALOHA Soon after ALOHA came onto the scene  Roberts (   ) published a method for doubling the capacity of an ALOHA system
His proposal was to divide time into discrete intervals called slots  each interval corresponding to one frame
This   MULTIPLE ACCESS PROTOCOLS
G (attempts per packet time)    S (throughput per frame time) Slotted ALOHA: S = GeâG Pure ALOHA: S = Geâ G Figure  -
Throughput versus offered traffic for ALOHA systems
approach requires the users to agree on slot boundaries
One way to achieve synchronization would be to have one special station emit a pip at the start of each interval  like a clock
In Robertsâ method  which has come to be known as slotted ALOHAâin contrast to Abramsonâs pure ALOHAâa station is not permitted to send whenever the user types a line
Instead  it is required to wait for the beginning of the next slot
Thus  the continuous time ALOHA is turned into a discrete time one
This halves the vulnerable period
To see this  look at Fig
The probability of no other traffic during the same slot as our test frame is then e âG  which leads to S = GeâG ( - ) As you can see from Fig
-  slotted ALOHA peaks at G =   with a throughput of S =  /e or about
twice that of pure ALOHA
If the system is operating at G =   the probability of an empty slot is
The best we can hope for using slotted ALOHA is  % of the slots empty   % successes  and  % collisions
Operating at higher values of G reduces the number of empties but increases the number of collisions exponentially
To see how this rapid growth of collisions with G comes about  consider the transmission of a test frame
The probability that it will avoid a collision is e âG  which is the probability that all the other stations are silent in that slot
The probability of a collision is then just  â e âG
The probability of a transmission requiring exactly k attempts (
k â  collisions followed by one success) is Pk = e âG(  â e âG)k â  The expected number of transmissions  E  per line typed at a terminal is then E = k =  Î£ â kPk = k =  Î£ â ke âG(  â e âG)k â  = eG THE MEDIUM ACCESS CONTROL SUBLAYER
As a result of the exponential dependence of E upon G  small increases in the channel load can drastically reduce its performance
Slotted ALOHA is notable for a reason that may not be initially obvious
It was devised in the s  used in a few early experimental systems  then almost forgotten
When Internet access over the cable was invented  all of a sudden there was a problem of how to allocate a shared channel among multiple competing users
Slotted ALOHA was pulled out of the garbage can to save the day
Later  having multiple RFID tags talk to the same RFID reader presented another variation on the same problem
Slotted ALOHA  with a dash of other ideas mixed in  again came to the rescue
It has often happened that protocols that are perfectly valid fall into disuse for political reasons (
some big company wants everyone to do things its way) or due to ever-changing technology trends
Then  years later some clever person realizes that a long-discarded protocol solves his current problem
For this reason  in this  ter we will study a number of elegant protocols that are not currently in widespread use but might easily be used in future applications  provided that enough network designers are aware of them
Of course  we will also study many protocols that are in current use as well
Carrier Sense Multiple Access Protocols With slotted ALOHA  the best channel utilization that can be achieved is  /e
This low result is hardly surprising  since with stations transmitting at will  without knowing what the other stations are doing there are bound to be many collisions
In LANs  however  it is often possible for stations to detect what other stations are doing  and thus adapt their behavior accordingly
These networks can achieve a much better utilization than  /e
In this tion  we will discuss some protocols for improving performance
Protocols in which stations listen for a carrier (
a transmission) and act accordingly are called carrier sense protocols
A number of them have been proposed  and they were long ago analyzed in detail
For example  see Kleinrock and Tobagi (   )
Below we will look at several versions of carrier sense protocols
Persistent and Nonpersistent CSMA The first carrier sense protocol that we will study here is called  -persistent CSMA (Carrier Sense Multiple Access)
That is a bit of a mouthful for the simplest CSMA scheme
When a station has data to send  it first listens to the channel to see if anyone else is transmitting at that moment
If the channel is idle  the stations sends its data
Otherwise  if the channel is busy  the station just waits until it becomes idle
Then the station transmits a frame
If a collision occurs  the   MULTIPLE ACCESS PROTOCOLS station waits a random amount of time and starts all over again
The protocol is called  -persistent because the station transmits with a probability of  when it finds the channel idle
You might expect that this scheme avoids collisions except for the rare case of simultaneous sends  but it in fact it does not
If two stations become ready in the middle of a third stationâs transmission  both will wait politely until the transmission ends  and then both will begin transmitting exactly simultaneously  resulting in a collision
If they were not so impatient  there would be fewer collisions
More subtly  the propagation delay has an important effect on collisions
There is a chance that just after a station begins sending  another station will become ready to send and sense the channel
If the first stationâs signal has not yet reached the ond one  the latter will sense an idle channel and will also begin sending  resulting in a collision
This chance depends on the number of frames that fit on the channel  or the bandwidth-delay product of the channel
If only a tiny fraction of a frame fits on the channel  which is the case in most LANs since the propagation delay is small  the chance of a collision happening is small
The larger the bandwidth-delay product  the more important this effect becomes  and the worse the performance of the protocol
Even so  this protocol has better performance than pure ALOHA because both stations have the decency to desist from interfering with the third stationâs frame
Exactly the same holds for slotted ALOHA
A ond carrier sense protocol is nonpersistent CSMA
In this protocol  a conscious attempt is made to be less greedy than in the previous one
As before  a station senses the channel when it wants to send a frame  and if no one else is sending  the station begins doing so itself
However  if the channel is already in use  the station does not continually sense it for the purpose of seizing it immediately upon detecting the end of the previous transmission
Instead  it waits a random period of time and then repeats the algorithm
Consequently  this algorithm leads to better channel utilization but longer delays than  -persistent CSMA
The last protocol is p-persistent CSMA
It applies to slotted channels and works as follows
When a station becomes ready to send  it senses the channel
If it is idle  it transmits with a probability p
With a probability q =  â p  it defers until the next slot
If that slot is also idle  it either transmits or defers again  with probabilities p and q
This process is repeated until either the frame has been transmitted or another station has begun transmitting
In the latter case  the unlucky station acts as if there had been a collision (
it waits a random time and starts again)
If the station initially senses that the channel is busy  it waits until the next slot and applies the above algorithm
uses a refinement of p-persistent CSMA that we will discuss in
Figure  -  shows the computed throughput versus offered traffic for all three protocols  as well as for pure and slotted ALOHA
THE MEDIUM ACCESS CONTROL SUBLAYER
S (throughput per packet time) G (attempts per packet time) Pure ALOHA Slotted ALOHA  -persistent CSMA
-persistent CSMA Figure  -
Comparison of the channel utilization versus load for various random access protocols
CSMA with Collision Detection Persistent and nonpersistent CSMA protocols are definitely an improvement over ALOHA because they ensure that no station begins to transmit while the channel is busy
However  if two stations sense the channel to be idle and begin transmitting simultaneously  their signals will still collide
Another improvement is for the stations to quickly detect the collision and abruptly stop transmitting  (rather than finishing them) since they are irretrievably garbled anyway
This strategy saves time and bandwidth
This protocol  known as CSMA/CD (CSMA with Collision Detection)  is the basis of the classic Ethernet LAN  so it is worth devoting some time to looking at it in detail
It is important to realize that collision detection is an analog process
The stationâs hardware must listen to the channel while it is transmitting
If the signal it reads back is different from the signal it is putting out  it knows that a collision is occurring
The implications are that a received signal must not be tiny compared to the transmitted signal (which is difficult for wireless  as received signals may be  times weaker than transmitted signals) and that the modulation must be chosen to allow collisions to be detected (
a collision of two  - volt signals may well be impossible to detect)
CSMA/CD  as well as many other LAN protocols  uses the conceptual model of Fig
At the point marked t   a station has finished transmitting its frame
Any other station having a frame to send may now attempt to do so
If two or more stations decide to transmit simultaneously  there will be a collision
If a station detects a collision  it aborts its transmission  waits a random period of time  and then tries again (assuming that no other station has started transmitting in the   MULTIPLE ACCESS PROTOCOLS meantime)
Therefore  our model for CSMA/CD will consist of alternating contention and transmission periods  with idle periods occurring when all stations are quiet (
for lack of work)
Contention slots Contention period Transmission period Idle period to Frame Frame Frame Frame Time Figure  -
CSMA/CD can be in contention  transmission  or idle state
Now let us look at the details of the contention algorithm
Suppose that two stations both begin transmitting at exactly time t
How long will it take them to realize that they have collided? The answer is vital to determining the length of the contention period and hence what the delay and throughput will be
The minimum time to detect the collision is just the time it takes the signal to propagate from one station to the other
Based on this information  you might think that a station that has not heard a collision for a time equal to the full cable propagation time after starting its transmission can be sure it has seized the cable
By ââseized ââ we mean that all other stations know it is transmitting and will not interfere
This conclusion is wrong
Consider the following worst-case scenario
Let the time for a signal to propagate between the two farthest stations be Ï
At t   one station begins transmitting
At t  + Ï â Îµ  an instant before the signal arrives at the most distant station  that station also begins transmitting
Of course  it detects the collision almost instantly and stops  but the little noise burst caused by the collision does not get back to the original station until time  Ï â Îµ
In other words  in the worst case a station cannot be sure that it has seized the channel until it has transmitted for  Ï without hearing a collision
With this understanding  we can think of CSMA/CD contention as a slotted ALOHA system with a slot width of  Ï
On a  -km long coaxial cable  Ïâ¼â¼  Î¼
The difference for CSMA/CD compared to slotted ALOHA is that slots in which only one station transmits (
in which the channel is seized) are followed by the rest of a frame
This difference will greatly improve performance if the frame time is much longer than the propagation time
Collision-Free Protocols Although collisions do not occur with CSMA/CD once a station has unambiguously captured the channel  they can still occur during the contention period
These collisions adversely affect the system performance  especially when the THE MEDIUM ACCESS CONTROL SUBLAYER
bandwidth-delay product is large  such as when the cable is long (
large Ï) and the frames are short
Not only do collisions reduce bandwidth  but they make the time to send a frame variable  which is not a good fit for real-time traffic such as voice over IP
CSMA/CD is also not universally applicable
In this tion  we will examine some protocols that resolve the contention for the channel without any collisions at all  not even during the contention period
Most of these protocols are not currently used in major systems  but in a rapidly changing field  having some protocols with excellent properties available for future systems is often a good thing
In the protocols to be described  we assume that there are exactly N stations  each programmed with a unique address from  to N â
It does not matter that some stations may be inactive part of the time
We also assume that propagation delay is negligible
The basic question remains: which station gets the channel after a successful transmission? We continue using the model of Fig
A Bit-Map Protocol In our first collision-free protocol  the basic bit-map method  each contention period consists of exactly N slots
If station  has a frame to send  it transmits a  bit during the slot
No other station is allowed to transmit during this slot
Regardless of what station  does  station  gets the opportunity to transmit a  bit during slot   but only if it has a frame queued
In general  station j may announce that it has a frame to send by inserting a  bit into slot j
After all N slots have passed by  each station has complete knowledge of which stations wish to transmit
At that point  they begin transmitting frames in numerical order (see Fig
Contention slots  Frames  Contention slots  d Figure  -
The basic bit-map protocol
Since everyone agrees on who goes next  there will never be any collisions
After the last ready station has transmitted its frame  an event all stations can easily monitor  another N-bit contention period is begun
If a station becomes ready just after its bit slot has passed by  it is out of luck and must remain silent until every station has had a chance and the bit map has come around again
MULTIPLE ACCESS PROTOCOLS Protocols like this in which the desire to transmit is broadcast before the actual transmission are called reservation protocols because they reserve channel ownership in advance and prevent collisions
Let us briefly analyze the performance of this protocol
For convenience  we will measure time in units of the contention bit slot  with data frames consisting of d time units
Under conditions of low load  the bit map will simply be repeated over and over  for lack of data frames
Consider the situation from the point of view of a low-numbered station  such as  or
Typically  when it becomes ready to send  the ââcurrentââ slot will be somewhere in the middle of the bit map
On average  the station will have to wait N /  slots for the current scan to finish and another full N slots for the following scan to run to completion before it may begin transmitting
The prospects for high-numbered stations are brighter
Generally  these will only have to wait half a scan (N /  bit slots) before starting to transmit
Highnumbered stations rarely have to wait for the next scan
Since low-numbered stations must wait on average
N slots and high-numbered stations must wait on average
N slots  the mean for all stations is N slots
The channel efficiency at low load is easy to compute
The overhead per frame is N bits and the amount of data is d bits  for an efficiency of d /(d + N)
At high load  when all the stations have something to send all the time  the Nbit contention period is prorated over N frames  yielding an overhead of only  bit per frame  or an efficiency of d /(d +  )
The mean delay for a frame is equal to the sum of the time it queues inside its station  plus an additional (N â  )d + N once it gets to the head of its internal queue
This interval is how long it takes to wait for all other stations to have their turn sending a frame and another bitmap
Token Passing The essence of the bit-map protocol is that it lets every station transmit a frame in turn in a predefined order
Another way to accomplish the same thing is to pass a small message called a token from one station to the next in the same predefined order
The token represents permission to send
If a station has a frame queued for transmission when it receives the token  it can send that frame before it passes the token to the next station
If it has no queued frame  it simply passes the token
In a token ring protocol  the topology of the network is used to define the order in which stations send
The stations are connected one to the next in a single ring
Passing the token to the next station then simply consists of receiving the token in from one direction and transmitting it out in the other direction  as seen in Fig
Frames are also transmitted in the direction of the token
This way they will circulate around the ring and reach whichever station is the destination
However  to stop the frame circulating indefinitely (like the token)  some station needs THE MEDIUM ACCESS CONTROL SUBLAYER
to remove it from the ring
This station may be either the one that originally sent the frame  after it has gone through a complete cycle  or the station that was the intended recipient of the frame
Direction of transmission Station Token Figure  -
Token ring
Note that we do not need a physical ring to implement token passing
The channel connecting the stations might instead be a single long bus
Each station then uses the bus to send the token to the next station in the predefined sequence
Possession of the token allows a station to use the bus to send one frame  as before
This protocol is called token bus
The performance of token passing is similar to that of the bit-map protocol  though the contention slots and frames of one cycle are now intermingled
After sending a frame  each station must wait for all N stations (including itself) to send the token to their neighbors and the other N â  stations to send a frame  if they have one
A subtle difference is that  since all positions in the cycle are equivalent  there is no bias for low- or high-numbered stations
For token ring  each station is also sending the token only as far as its neighboring station before the protocol takes the next step
Each token does not need to propagate to all stations before the protocol advances to the next step
Token rings have cropped up as MAC protocols with some consistency
An early token ring protocol (called ââToken Ringââ and standardized as IEEE
) was popular in the s as an alternative to classic Ethernet
In the s  a much faster token ring called FDDI (Fiber Distributed Data Interface) was beaten out by switched Ethernet
In the s  a token ring called RPR (Resilient Packet Ring) was defined as IEEE
to standardize the mix of metropolitan area rings in use by ISPs
We wonder what the s will have to offer
Binary Countdown A problem with the basic bit-map protocol  and by extension token passing  is that the overhead is  bit per station  so it does not scale well to networks with thousands of stations
We can do better than that by using binary station addresses with a channel that combines transmissions
A station wanting to use the   MULTIPLE ACCESS PROTOCOLS channel now broadcasts its address as a binary bit string  starting with the highorder bit
All addresses are assumed to be the same length
The bits in each address position from different stations are BOOLEAN ORed together by the channel when they are sent at the same time
We will call this protocol binary countdown
It was used in Datakit (Fraser  )
It implicitly assumes that the transmission delays are negligible so that all stations see asserted bits essentially instantaneously
To avoid conflicts  an arbitration rule must be applied: as soon as a station sees that a high-order bit position that is  in its address has been overwritten with a   it gives up
For example  if stations     and  are all trying to get the channel  in the first bit time the stations transmit  and   respectively
These are ORed together to form a
Stations  and  see the  and know that a higher-numbered station is competing for the channel  so they give up for the current round
Stations  and  continue
The next bit is   and both stations continue
The next bit is   so station  gives up
The winner is station  because it has the highest address
After winning the bidding  it may now transmit a frame  after which another bidding cycle starts
The protocol is illustrated in Fig
It has the property that higher- numbered stations have a higher priority than lower-numbered stations  which may be either good or bad  depending on the context
â â â    Bit time  â â â  â    Result    Stations  and  see this  and give up Station  sees this  and gives up Figure  -
The binary countdown protocol
A dash indicates silence
The channel efficiency of this method is d /(d + log  N)
If  however  the frame format has been cleverly chosen so that the senderâs address is the first field in the frame  even these log  N bits are not wasted  and the efficiency is   %
Binary countdown is an example of a simple  elegant  and efficient protocol that is waiting to be rediscovered
Hopefully  it will find a new home some day
THE MEDIUM ACCESS CONTROL SUBLAYER
Limited-Contention Protocols We have now considered two basic strategies for channel acquisition in a broadcast network: contention  as in CSMA  and collision-free protocols
Each strategy can be rated as to how well it does with respect to the two important performance measures  delay at low load and channel efficiency at high load
Under conditions of light load  contention (
pure or slotted ALOHA) is preferable due to its low delay (since collisions are rare)
As the load increases  contention becomes increasingly less attractive because the overhead associated with channel arbitration becomes greater
Just the reverse is true for the collision-free protocols
At low load  they have relatively high delay but as the load increases  the channel efficiency improves (since the overheads are fixed)
Obviously  it would be nice if we could combine the best properties of the contention and collision-free protocols  arriving at a new protocol that used contention at low load to provide low delay  but used a collision-free technique at high load to provide good channel efficiency
Such protocols  which we will call limited-contention protocols  do in fact exist  and will conclude our study of carrier sense networks
Up to now  the only contention protocols we have studied have been symmetric
That is  each station attempts to acquire the channel with some probability  p  with all stations using the same p
Interestingly enough  the overall system performance can sometimes be improved by using a protocol that assigns different probabilities to different stations
Before looking at the asymmetric protocols  let us quickly review the performance of the symmetric case
Suppose that k stations are contending for channel access
Each has a probability p of transmitting during each slot
The probability that some station successfully acquires the channel during a given slot is the probability that any one station transmits  with probability p  and all other k â  stations defer  each with probability  â p
This value is kp(  â p)k â
To find the optimal value of p  we differentiate with respect to p  set the result to zero  and solve for p
Doing so  we find that the best value of p is  /k
Substituting p =  /k  we get Pr[success with optimal p] = â§âªâ© k k â  â«âªâ­ k â  ( - ) This probability is plotted in Fig
For small numbers of stations  the chances of success are good  but as soon as the number of stations reaches even five  the probability has dropped close to its asymptotic value of  /e
The limited-contention protocols do precisely that
They first divide the stations into (not necessarily disjoint) groups
Only the members of group  are permitted   MULTIPLE ACCESS PROTOCOLS          Probability of success Number of ready stations Figure  -
Acquisition probability for a symmetric contention channel
to compete for slot
If one of them succeeds  it acquires the channel and transmits its frame
If the slot lies fallow or if there is a collision  the members of group  contend for slot   etc
By making an appropriate division of stations into groups  the amount of contention for each slot can be reduced  thus operating each slot near the left end of Fig
The trick is how to assign stations to slots
Before looking at the general case  let us consider some special cases
At one extreme  each group has but one member
Such an assignment guarantees that there will never be collisions because at most one station is contending for any given slot
We have seen such protocols before (
binary countdown)
The next special case is to assign two stations per group
The probability that both will try to transmit during a slot is p  which for a small p is negligible
As more and more stations are assigned to the same slot  the probability of a collision grows  but the length of the bit-map scan needed to give everyone a chance shrinks
The limiting case is a single group containing all stations (
slotted ALOHA)
What we need is a way to assign stations to slots dynamically  with many stations per slot when the load is low and few (or even just one) station per slot when the load is high
The Adaptive Tree Walk Protocol One particularly simple way of performing the necessary assignment is to use the algorithm devised by the
Army for testing soldiers for syphilis during World War II (Dorfman  )
In short  the Army took a blood sample from N soldiers
A portion of each sample was poured into a single test tube
This mixed sample was then tested for antibodies
If none were found  all the soldiers in the group were declared healthy
If antibodies were present  two new mixed samples THE MEDIUM ACCESS CONTROL SUBLAYER
were prepared  one from soldiers  through N/  and one from the rest
The process was repeated recursively until the infected soldiers were determined
For the computerized version of this algorithm (Capetanakis  )  it is convenient to think of the stations as the leaves of a binary tree  as illustrated in Fig
In the first contention slot following a successful frame transmission  slot   all stations are permitted to try to acquire the channel
If one of them does so  fine
If there is a collision  then during slot  only those stations falling under node  in the tree may compete
If one of them acquires the channel  the slot following the frame is reserved for those stations under node
If  on the other hand  two or more stations under node  want to transmit  there will be a collision during slot   in which case it is node  âs turn during slot
A B C D E F G H Stations Figure  -
The tree for eight stations
In essence  if a collision occurs during slot   the entire tree is searched  depth first  to locate all ready stations
Each bit slot is associated with some particular node in the tree
If a collision occurs  the search continues recursively with the nodeâs left and right children
If a bit slot is idle or if only one station transmits in it  the searching of its node can stop because all ready stations have been located
(Were there more than one  there would have been a collision
) When the load on the system is heavy  it is hardly worth the effort to dedicate slot  to node  because that makes sense only in the unlikely event that precisely one station has a frame to send
Similarly  one could argue that nodes  and  should be skipped as well for the same reason
Put in more general terms  at what level in the tree should the search begin? Clearly  the heavier the load  the farther down the tree the search should begin
We will assume that each station has a good estimate of the number of ready stations  q  for example  from monitoring recent traffic
To proceed  let us number the levels of the tree from the top  with node  in Fig
Notice that each node at level i   MULTIPLE ACCESS PROTOCOLS has a fraction  âi of the stations below it
If the q ready stations are uniformly distributed  the expected number of them below a specific node at level i is just  âiq
Intuitively  we would expect the optimal level to begin searching the tree to be the one at which the mean number of contending stations per slot is   that is  the level at which  âiq =
Solving this equation  we find that i = log  q
Numerous improvements to the basic algorithm have been discovered and are discussed in some detail by Bertsekas and Gallager (   )
For example  consider the case of stations G and H being the only ones wanting to transmit
At node  a collision will occur  so  will be tried and discovered idle
It is pointless to probe node  since it is guaranteed to have a collision (we know that two or more stations under  are ready and none of them are under   so they must all be under  )
The probe of  can be skipped and  tried next
When this probe also turns up nothing   can be skipped and node G tried next
Wireless LAN Protocols A system of laptop computers that communicate by radio can be regarded as a wireless LAN  as we discussed in
Such a LAN is an example of a broadcast channel
It also has somewhat different properties than a wired LAN  which leads to different MAC protocols
In this tion  we will examine some of these protocols
In   we will look at
(WiFi) in detail
A common configuration for a wireless LAN is an office building with access points (APs) strategically placed around the building
The APs are wired together using copper or fiber and provide connectivity to the stations that talk to them
If the transmission power of the APs and laptops is adjusted to have a range of tens of meters  nearby rooms become like a single cell and the entire building becomes like the cellular telephony systems we studied in
except that each cell only has one channel
This channel is shared by all the stations in the cell  including the AP
It typically provides megabit/ bandwidths  up to Mbps
We have already remarked that wireless systems cannot normally detect a collision while it is occurring
The received signal at a station may be tiny  perhaps a million times fainter than the signal that is being transmitted
Finding it is like looking for a ripple on the ocean
Instead  acknowledgements are used to discover collisions and other errors after the fact
There is an even more important difference between wireless LANs and wired LANs
A station on a wireless LAN may not be able to transmit frames to or receive frames from all other stations because of the limited radio range of the stations
In wired LANs  when one station sends a frame  all other stations receive it
The absence of this property in wireless LANs causes a variety of complications
We will make the simplifying assumption that each radio transmitter has some fixed range  represented by a circular coverage region within which another station can sense and receive the stationâs transmission
It is important to realize that THE MEDIUM ACCESS CONTROL SUBLAYER
in practice coverage regions are not nearly so regular because the propagation of radio signals depends on the environment
Walls and other obstacles that attenuate and reflect signals may cause the range to differ markedly in different directions
But a simple circular model will do for our purposes
A naive approach to using a wireless LAN might be to try CSMA: just listen for other transmissions and only transmit if no one else is doing so
The trouble is  this protocol is not really a good way to think about wireless because what matters for reception is interference at the receiver  not at the sender
To see the nature of the problem  consider Fig
For our purposes  it does not matter which are APs and which are laptops
The radio range is such that A and B are within each otherâs range and can potentially interfere with one another
C can also potentially interfere with both B and D  but not with A
Radio range (a) (b) Radio range A B C D A B C D Figure  -
A wireless LAN
(a) A and C are hidden terminals when transmitting to B
(b) B and C are exposed terminals when transmitting to A and D
First consider what happens when A and C transmit to B  as depicted in Fig
If A sends and then C immediately senses the medium  it will not hear A because A is out of range
Thus C will falsely conclude that it can transmit to B
If C does start transmitting  it will interfere at B  wiping out the frame from A
(We assume here that no CDMA-type scheme is used to provide multiple channels  so collisions garble the signal and destroy both frames
) We want a MAC protocol that will prevent this kind of collision from happening because it wastes bandwidth
The problem of a station not being able to detect a potential competitor for the medium because the competitor is too far away is called the hidden terminal problem
Now let us look at a different situation: B transmitting to A at the same time that C wants to transmit to D  as shown in Fig
If C senses the medium  it will hear a transmission and falsely conclude that it may not send to D (shown as a dashed line)
In fact  such a transmission would cause bad reception only in the zone between B and C  where neither of the intended receivers is located
We want a MAC protocol that prevents this kind of deferral from happening because it wastes bandwidth
The problem is called the exposed terminal problem
The difficulty is that  before starting a transmission  a station really wants to know whether there is radio activity around the receiver
CSMA merely tells it   MULTIPLE ACCESS PROTOCOLS whether there is activity near the transmitter by sensing the carrier
With a wire  all signals propagate to all stations  so this distinction does not exist
However  only one transmission can then take place at once anywhere in the system
In a system based on short-range radio waves  multiple transmissions can occur simultaneously if they all have different destinations and these destinations are out of range of one another
We want this concurrency to happen as the cell gets larger and larger  in the same way that people at a party should not wait for everyone in the room to go silent before they talk; multiple conversations can take place at once in a large room as long as they are not directed to the same location
An early and influential protocol that tackles these problems for wireless LANs is MACA (Multiple Access with Collision Avoidance) (Karn  )
The basic idea behind it is for the sender to stimulate the receiver into outputting a short frame  so stations nearby can detect this transmission and avoid transmitting for the duration of the upcoming (large) data frame
This technique is used instead of carrier sense
MACA is illustrated in Fig
Let us see how A sends a frame to B
A starts by sending an RTS (Request To Send) frame to B  as shown in Fig
This short frame (  bytes) contains the length of the data frame that will eventually follow
Then B replies with a CTS (Clear To Send) frame  as shown in Fig
The CTS frame contains the data length (copied from the RTS frame)
Upon receipt of the CTS frame  A begins transmission
(a) (b) Range of A's transmitter A RTS E C B D A CTS E C B D Range of B's transmitter Figure  -
The MACA protocol
(a) A sending an RTS to B
(b) B responding with a CTS to A
Now let us see how stations overhearing either of these frames react
Any station hearing the RTS is clearly close to A and must remain silent long enough for the CTS to be transmitted back to A without conflict
Any station hearing the CTS is clearly close to B and must remain silent during the upcoming data transmission  whose length it can tell by examining the CTS frame
THE MEDIUM ACCESS CONTROL SUBLAYER
-   C is within range of A but not within range of B
Therefore  it hears the RTS from A but not the CTS from B
As long as it does not interfere with the CTS  it is free to transmit while the data frame is being sent
In contrast  D is within range of B but not A
It does not hear the RTS but does hear the CTS
Hearing the CTS tips it off that it is close to a station that is about to receive a frame  so it defers sending anything until that frame is expected to be finished
Station E hears both control messages and  like D  must be silent until the data frame is complete
Despite these precautions  collisions can still occur
For example  B and C could both send RTS frames to A at the same time
These will collide and be lost
In the event of a collision  an unsuccessful transmitter (
one that does not hear a CTS within the expected time interval) waits a random amount of time and tries again later  ETHERNET We have now finished our discussion of channel allocation protocols in the abstract  so it is time to see how these principles apply to real systems
Many of the designs for personal  local  and metropolitan area networks have been standardized under the name of IEEE
A few have survived but many have not  as we saw in Fig
Some people who believe in reincarnation think that Charles Darwin came back as a member of the IEEE Standards Association to weed out the unfit
The most important of the survivors are
(Ethernet) and
(wireless LAN)
Bluetooth (wireless PAN) is widely deployed but has now been standardized outside of    With
(wireless MAN)  it is too early to tell
Please consult the  th edition of this book to find out
We will begin our study of real systems with Ethernet  probably the most ubiquitous kind of computer network in the world
Two kinds of Ethernet exist: classic Ethernet  which solves the multiple access problem using the techniques we have studied in this  ter; and switched Ethernet  in which devices called switches are used to connect different computers
It is important to note that  while they are both referred to as Ethernet  they are quite different
Classic Ethernet is the original form and ran at rates from  to   Mbps
Switched Ethernet is what Ethernet has become and runs at   and   Mbps  in forms called fast Ethernet  gigabit Ethernet  and   gigabit Ethernet
In practice  only switched Ethernet is used nowadays
We will discuss these historical forms of Ethernet in chronological order showing how they developed
Since Ethernet and IEEE
are identical except for a minor difference (which we will discuss shortly)  many people use the terms ââEthernetââ and ââIEEE
ââ interchangeably
We will do so  too
For more information about Ethernet  see Spurgeon (   )
ETHERNET    Classic Ethernet Physical Layer The story of Ethernet starts about the same time as that of ALOHA  when a student named Bob Metcalfe got his bachelorâs degree at
and then moved up the river to get his
at Harvard
During his studies  he was exposed to Abramsonâs work
He became so interested in it that after graduating from Harvard  he decided to spend the summer in Hawaii working with Abramson before starting work at Xerox PARC (Palo Alto Research Center)
When he got to PARC  he saw that the researchers there had designed and built what would later be called personal computers
But the machines were isolated
Using his knowledge of Abramsonâs work  he  together with his colleague David Boggs  designed and implemented the first local area network (Metcalfe and Boggs  )
It used a single long  thick coaxial cable and ran at  Mbps
They called the system Ethernet after the luminiferous ether  through which electromagnetic radiation was once thought to propagate
(When the  th-century British physicist James Clerk Maxwell discovered that electromagnetic radiation could be described by a wave equation  scientists assumed that space must be filled with some ethereal medium in which the radiation was propagating
Only after the famous Michelson-Morley experiment in  did physicists discover that electromagnetic radiation could propagate in a vacuum
) The Xerox Ethernet was so successful that DEC  Intel  and Xerox drew up a standard in  for a  -Mbps Ethernet  called the DIX standard
With a minor change  the DIX standard became the IEEE
standard in
Unfortunately for Xerox  it already had a history of making seminal inventions (such as the personal computer) and then failing to commercialize on them  a story told in Fumbling the Future (Smith and Alexander  )
When Xerox showed little interest in doing anything with Ethernet other than helping standardize it  Metcalfe formed his own company  Com  to sell Ethernet adapters for PCs
It sold many millions of them
Classic Ethernet snaked around the building as a single long cable to which all the computers were attached
This architecture is shown in Fig
The first variety  popularly called thick Ethernet  resembled a yellow garden hose  with markings every
meters to show where to attach computers
standard did not actually require the cable to be yellow  but it did suggest it
) It was succeeded by thin Ethernet  which bent more easily and made connections using industry-standard BNC connectors
Thin Ethernet was much cheaper and easier to install  but it could run for only meters per segment (instead of m with thick Ethernet)  each of which could handle only   machines (instead of   )
Each version of Ethernet has a maximum cable length per segment (
unamplified length) over which the signal will propagate
To allow larger networks  multiple cables can be connected by repeaters
A repeater is a physical layer device that receives  amplifies (
regenerates)  and retransmits signals in both directions
As far as the software is concerned  a series of cable segments THE MEDIUM ACCESS CONTROL SUBLAYER
Ether Transceiver Interface cable Figure  -
Architecture of classic Ethernet
connected by repeaters is no different from a single cable (except for a small amount of delay introduced by the repeaters)
Over each of these cables  information was sent using the Manchester encoding we studied in
An Ethernet could contain multiple cable segments and multiple repeaters  but no two transceivers could be more than
km apart and no path between any two transceivers could traverse more than four repeaters
The reason for this restriction was so that the MAC protocol  which we will look at next  would work correctly
Classic Ethernet MAC Sublayer Protocol The format used to send frames is shown in Fig
First comes a Preamble of  bytes  each containing the bit pattern  (with the exception of the last byte  in which the last  bits are set to  )
This last byte is called the Start of Frame delimiter for    The Manchester encoding of this pattern produces a  -MHz square wave for
Î¼ to allow the receiverâs clock to synchronize with the senderâs
The last two  bits tell the receiver that the rest of the frame is about to start
(a) Preamble Bytes Type Data Pad Checksum Destination address Source address  -  - (b) Preamble Length Data Pad Checksum Destination address Source address Figure  -
Frame formats
(a) Ethernet (DIX)
(b) IEEE    Next come two addresses  one for the destination and one for the source
They are each  bytes long
The first transmitted bit of the destination address is a  for   ETHERNET ordinary addresses and a  for group addresses
Group addresses allow multiple stations to listen to a single address
When a frame is sent to a group address  all the stations in the group receive it
Sending to a group of stations is called multicasting
The special address consisting of all  bits is reserved for broadcasting
A frame containing all  s in the destination field is accepted by all stations on the network
Multicasting is more selective  but it involves group management to define which stations are in the group
Conversely  broadcasting does not differentiate between stations at all  so it does not require any group management
An interesting feature of station source addresses is that they are globally unique  assigned centrally by IEEE to ensure that no two stations anywhere in the world have the same address
The idea is that any station can uniquely address any other station by just giving the right  -bit number
To do this  the first  bytes of the address field are used for an OUI (Organizationally Unique Identifier)
Values for this field are assigned by IEEE and indicate a manufacturer
Manufacturers are assigned blocks of addresses
The manufacturer assigns the last  bytes of the address and programs the complete address into the NIC before it is sold
Next comes the Type or Length field  depending on whether the frame is Ethernet or IEEE    Ethernet uses a Type field to tell the receiver what to do with the frame
Multiple network-layer protocols may be in use at the same time on the same machine  so when an Ethernet frame arrives  the operating system has to know which one to hand the frame to
The Type field specifies which process to give the frame to
For example  a type code of  x means that the data contains an IPv  packet
in its wisdom  decided that this field would carry the length of the frame  since the Ethernet length was determined by looking inside the dataâa layering violation if ever there was one
Of course  this meant there was no way for the receiver to figure out what to do with an incoming frame
That problem was handled by the addition of another header for the LLC (Logical Link Control) protocol within the data
It uses  bytes to convey the  bytes of protocol type information
Unfortunately  by the time
was published  so much hardware and software for DIX Ethernet was already in use that few manufacturers and users were enthusiastic about repackaging the Type and Length fields
In  IEEE threw in the towel and said that both ways were fine with it
Fortunately  all the Type fields in use before  had values greater than  then well established as the maximum data size
Now the rule is that any number there less than or equal to  x   (   ) can be interpreted as Length  and any number greater than  x   can be interpreted as Type
Now IEEE can maintain that everyone is using its standard and everybody else can keep on doing what they were already doing (not bothering with LLC) without feeling guilty about it
Next come the data  up to  bytes
This limit was chosen somewhat arbitrarily at the time the Ethernet standard was cast in stone  mostly based on the fact THE MEDIUM ACCESS CONTROL SUBLAYER
that a transceiver needs enough RAM to hold an entire frame and RAM was expensive in
A larger upper limit would have meant more RAM  and hence a more expensive transceiver
In addition to there being a maximum frame length  there is also a minimum frame length
While a data field of  bytes is sometimes useful  it causes a problem
When a transceiver detects a collision  it truncates the current frame  which means that stray bits and pieces of frames appear on the cable all the time
To make it easier to distinguish valid frames from garbage  Ethernet requires that valid frames must be at least   bytes long  from destination address to checksum  including both
If the data portion of a frame is less than   bytes  the Pad field is used to fill out the frame to the minimum size
Another (and more important) reason for having a minimum length frame is to prevent a station from completing the transmission of a short frame before the first bit has even reached the far end of the cable  where it may collide with another frame
This problem is illustrated in Fig
At time   station A  at one end of the network  sends off a frame
Let us call the propagation time for this frame to reach the other end Ï
Just before the frame gets to the other end (
at time Ï â Îµ)  the most distant station  B  starts transmitting
When B detects that it is receiving more power than it is putting out  it knows that a collision has occurred  so it aborts its transmission and generates a  -bit noise burst to warn all other stations
In other words  it jams the ether to make sure the sender does not miss the collision
At about time  Ï  the sender sees the noise burst and aborts its transmission  too
It then waits a random time before trying again
Packet starts A at time  B A B Packet almost at B at Collision at time A B Noise burst gets back to A at  A B (a) (b) (c) (d) Figure  -
Collision detection can take as long as  Ï
If a station tries to transmit a very short frame  it is conceivable that a collision will occur  but the transmission will have completed before the noise burst gets back to the station at  Ï
The sender will then incorrectly conclude that the frame was successfully sent
To prevent this situation from occurring  all frames must take more than  Ï to send so that the transmission is still taking place when   ETHERNET the noise burst gets back to the sender
For a  -Mbps LAN with a maximum length of  meters and four repeaters (from the
specification)  the round-trip time (including time to propagate through the four repeaters) has been determined to be nearly   Î¼ in the worst case
Therefore  the shortest allowed frame must take at least this long to transmit
At   Mbps  a bit takes n  so bits is the smallest frame that is guaranteed to work
To add some margin of safety  this number was rounded up to bits or   bytes
The final field is the Checksum
It is a  -bit CRC of the kind we studied in
In fact  it is defined exactly by the generator polynomial we gave there  which popped up for PPP  ADSL  and other links too
This CRC is an errordetecting code that is used to determine if the bits of the frame have been received correctly
It just does error detection  with the frame dropped if an error is detected
CSMA/CD with Binary Exponential Backoff Classic Ethernet uses the  -persistent CSMA/CD algorithm that we studied in
This descriptor just means that stations sense the medium when they have a frame to send and send the frame as soon as the medium becomes idle
They monitor the channel for collisions as they send
If there is a collision  they abort the transmission with a short jam signal and retransmit after a random interval
Let us now see how the random interval is determined when a collision occurs  as it is a new method
The model is still that of Fig
After a collision  time is divided into discrete slots whose length is equal to the worst-case roundtrip propagation time on the ether ( Ï)
To accommodate the longest path allowed by Ethernet  the slot time has been set to bit times  or
After the first collision  each station waits either  or  slot times at random before trying again
If two stations collide and each one picks the same random number  they will collide again
After the ond collision  each one picks either  or  at random and waits that number of slot times
If a third collision occurs (the probability of this happening is
)  the next time the number of slots to wait is chosen at random from the interval  to   â
In general  after i collisions  a random number between  and  i â  is chosen  and that number of slots is skipped
However  after   collisions have been reached  the randomization interval is frozen at a maximum of  slots
After   collisions  the controller throws in the towel and reports failure back to the computer
Further recovery is up to higher layers
This algorithm  called binary exponential backoff  was chosen to dynamically adapt to the number of stations trying to send
If the randomization interval for all collisions were  the chance of two stations colliding for a ond time would be negligible  but the average wait after a collision would be hundreds of slot times  introducing significant delay
On the other hand  if each station always THE MEDIUM ACCESS CONTROL SUBLAYER
delayed for either  or  slots  then if stations ever tried to send at once they would collide over and over until   of them picked  and the remaining station picked
This might take years
By having the randomization interval grow exponentially as more and more conutive collisions occur  the algorithm ensures a low delay when only a few stations collide but also ensures that the collisions are resolved in a reasonable interval when many stations collide
Truncating the backoff at  keeps the bound from growing too large
If there is no collision  the sender assumes that the frame was probably successfully delivered
That is  neither CSMA/CD nor Ethernet provides acknowledgements
This choice is appropriate for wired and optical fiber channels that have low error rates
Any errors that do occur must then be detected by the CRC and recovered by higher layers
For wireless channels that have more errors  we will see that acknowledgements are used
Ethernet Performance Now let us briefly examine the performance of classic Ethernet under conditions of heavy and constant load  that is  with k stations always ready to transmit
A rigorous analysis of the binary exponential backoff algorithm is complicated
Instead  we will follow Metcalfe and Boggs (   ) and assume a constant retransmission probability in each slot
If each station transmits during a contention slot with probability p  the probability A that some station acquires the channel in that slot is A = kp(  â p)k â  ( - ) A is maximized when p =  /k  with A â  /e as k ââ
The probability that the contention interval has exactly j slots in it is A(  â A)j â   so the mean number of slots per contention is given by j =  Î£ â jA(  â A)j â  = A  Since each slot has a duration  Ï  the mean contention interval  w  is  Ï/A
Assuming optimal p  the mean number of contention slots is never more than e  so w is at most  Ïe â¼â¼
If the mean frame takes P  to transmit  when many stations have frames to send  Channel efficiency = P +  Ï/A P ( - ) Here we see where the maximum cable distance between any two stations enters into the performance figures
The longer the cable  the longer the contention interval  which is why the Ethernet standard specifies a maximum cable length
ETHERNET It is instructive to formulate Eq
( - ) in terms of the frame length  F  the network bandwidth  B  the cable length  L  and the speed of signal propagation  c  for the optimal case of e contention slots per frame
With P = F/B  Eq
( - ) becomes Channel efficiency =  +  BLe /cF  ( - ) When the ond term in the denominator is large  network efficiency will be low
More specifically  increasing network bandwidth or distance (the BL product) reduces efficiency for a given frame size
Unfortunately  much research on network hardware is aimed precisely at increasing this product
People want high bandwidth over long distances (fiber optic MANs  for example)  yet classic Ethernet implemented in this manner is not the best system for these applications
We will see other ways of implementing Ethernet in the next tion
-   the channel efficiency is plotted versus the number of ready stations for  Ï =
Î¼ and a data rate of   Mbps  using Eq
With a  - byte slot time  it is not surprising that  -byte frames are not efficient
On the other hand  with -byte frames and an asymptotic value of e  -byte slots per contention interval  the contention period is bytes long and the efficiency is  %
This result is much better than the  % efficiency of slotted ALOHA
Number of stations trying to send Channel efficiency     -byte frames   -byte frames   -byte frames   -byte frames  -byte frames Figure  -
Efficiency of Ethernet at   Mbps with   -bit slot times
It is probably worth mentioning that there has been a large amount of theoretical performance analysis of Ethernet (and other networks)
Most of the results should be taken with a grain (or better yet  a metric ton) of salt  for two reasons
THE MEDIUM ACCESS CONTROL SUBLAYER
First  virtually all of the theoretical work assumes Poisson traffic
As researchers have begun looking at real data  it now appears that network traffic is rarely Poisson
Instead  it is self-similar or bursty over a range of time scales (Paxson and Floyd  ; and Leland et al
What this means is that averaging over long periods of time does not smooth out the traffic
As well as using questionable models  many of the analyses focus on the ââinterestingââ performance cases of abnormally high load
Boggs et al
(   ) showed by experimentation that Ethernet works well in reality  even at moderately high load
Switched Ethernet Ethernet soon began to evolve away from the single long cable architecture of classic Ethernet
The problems associated with finding breaks or loose connections drove it toward a different kind of wiring pattern  in which each station has a dedicated cable running to a central hub
A hub simply connects all the attached wires electrically  as if they were soldered together
This configuration is shown in Fig
Port Line Hub Switch (a) (b) Port Line Figure  -
(b) Switch
The wires were telephone company twisted pairs  since most office buildings were already wired this way and normally plenty of spares were available
This reuse was a win  but it did reduce the maximum cable run from the hub to meters (   meters if high quality Category  twisted pairs were used)
Adding or removing a station is simpler in this configuration  and cable breaks can be detected easily
With the advantages of being able to use existing wiring and ease of maintenance  twisted-pair hubs quickly became the dominant form of Ethernet
However  hubs do not increase capacity because they are logically equivalent to the single long cable of classic Ethernet
As more and more stations are added  each station gets a decreasing share of the fixed capacity
Eventually  the LAN will saturate
One way out is to go to a higher speed  say  from   Mbps to Mbps   Gbps  or even higher speeds
But with the growth of multimedia and powerful servers  even a  -Gbps Ethernet can become saturated
ETHERNET Fortunately  there is an another way to deal with increased load: switched Ethernet
The heart of this system is a switch containing a high-speed backplane that connects all of the ports  as shown in Fig
From the outside  a switch looks just like a hub
They are both boxes  typically with  to   ports  each with a standard RJ-  connector for a twisted-pair cable
Each cable connects the switch or hub to a single computer  as shown in Fig
A switch has the same advantages as a hub  too
It is easy to add or remove a new station by plugging or unplugging a wire  and it is easy to find most faults since a flaky cable or port will usually affect just one station
There is still a shared component that can failâthe switch itselfâbut if all stations lose connectivity the IT folks know what to do to fix the problem: replace the whole switch
Switch Twisted pair Switch ports Hub Figure  -
An Ethernet switch
Inside the switch  however  something very different is happening
Switches only output frames to the ports for which those frames are destined
When a switch port receives an Ethernet frame from a station  the switch checks the Ethernet addresses to see which port the frame is destined for
This step requires the switch to be able to work out which ports correspond to which addresses  a process that we will describe in   when we get to the general case of switches connected to other switches
For now  just assume that the switch knows the frameâs destination port
The switch then forwards the frame over its high-speed backplane to the destination port
The backplane typically runs at many Gbps  using a proprietary protocol that does not need to be standardized because it is entirely hidden inside the switch
The destination port then transmits the frame on the wire so that it reaches the intended station
None of the other ports even knows the frame exists
What happens if more than one of the stations or ports wants to send a frame at the same time? Again  switches differ from hubs
In a hub  all stations are in the same collision domain
They must use the CSMA/CD algorithm to schedule their transmissions
In a switch  each port is its own independent collision domain
In the common case that the cable is full duplex  both the station and the port can send a frame on the cable at the same time  without worrying about other ports and stations
Collisions are now impossible and CSMA/CD is not needed
However  if the cable is half duplex  the station and the port must contend for transmission with CSMA/CD in the usual way
THE MEDIUM ACCESS CONTROL SUBLAYER
A switch improves performance over a hub in two ways
First  since there are no collisions  the capacity is used more efficiently
ond  and more importantly  with a switch multiple frames can be sent simultaneously (by different stations)
These frames will reach the switch ports and travel over the switchâs backplane to be output on the proper ports
However  since two frames might be sent to the same output port at the same time  the switch must have buffering so that it can temporarily queue an input frame until it can be transmitted to the output port
Overall  these improvements give a large performance win that is not possible with a hub
The total system throughput can often be increased by an order of magnitude  depending on the number of ports and traffic patterns
The change in the ports on which frames are output also has urity benefits
Most LAN interfaces have a promiscuous mode  in which all frames are given to each computer  not just those addressed to it
With a hub  every computer that is attached can see the traffic sent between all of the other computers
Spies and busybodies love this feature
With a switch  traffic is forwarded only to the ports where it is destined
This restriction provides better isolation so that traffic will not easily escape and fall into the wrong hands
However  it is better to encrypt traffic if urity is really needed
Because the switch just expects standard Ethernet frames on each input port  it is possible to use some of the ports as concentrators
As frames arrive at the hub  they contend for the ether in the usual way  including collisions and binary backoff
Successful frames make it through the hub to the switch and are treated there like any other incoming frames
The switch does not know they had to fight their way in
Once in the switch  they are sent to the correct output line over the high-speed backplane
It is also possible that the correct destination was one on the lines attached to the hub  in which case the frame has already been delivered so the switch just drops it
Hubs are simpler and cheaper than switches  but due to falling switch prices they have become an endangered species
Modern networks largely use switched Ethernet
Nevertheless  legacy hubs still exist
Fast Ethernet At the same time that switches were becoming popular  the speed of  -Mbps Ethernet was coming under pressure
At first Mbps seemed like heaven  just as cable modems seemed like heaven to the users of telephone modems
But the novelty wore off quickly
As a kind of corollary to Parkinsonâs Law (ââWork expands to fill the time available for its completionââ)  it seemed that data expanded to fill the bandwidth available for their transmission
Many installations needed more bandwidth and thus had numerous  -Mbps LANs connected by a maze of repeaters  hubs  and switches  although to the network managers it sometimes felt that they were being held together by bubble   ETHERNET gum and chicken wire
But even with Ethernet switches  the maximum bandwidth of a single computer was limited by the cable that connected it to the switch port
It was in this environment that IEEE reconvened the
committee in  with instructions to come up with a faster LAN
One proposal was to keep
exactly as it was  but just make it go faster
Another proposal was to redo it totally and give it lots of new features  such as real-time traffic and digitized voice  but just keep the old name (for marketing reasons)
After some wrangling  the committee decided to keep
the way it was  and just make it go faster
This strategy would get the job done before the technology changed and avoid unforeseen problems with a brand new design
The new design would also be backwardcompatible with existing Ethernet LANs
The people behind the losing proposal did what any self-respecting computer-industry people would have done under these circumstances: they stomped off and formed their own committee and standardized their LAN anyway (eventually as
It flopped miserably
The work was done quickly (by standards committeesâ norms)  and the result
u  was approved by IEEE in June
Technically
u is not a new standard  but an addendum to the existing
standard (to emphasize its backward compatibility)
This strategy is used a lot
Since practically everyone calls it fast Ethernet  rather than
u  we will do that  too
The basic idea behind fast Ethernet was simple: keep all the old frame formats  interfaces  and procedural rules  but reduce the bit time from n to   n
Technically  it would have been possible to copy  -Mbps classic Ethernet and still detect collisions on time by just reducing the maximum cable length by a factor of
However  the advantages of twisted-pair wiring were so overwhelming that fast Ethernet is based entirely on this design
Thus  all fast Ethernet systems use hubs and switches; multidrop cables with vampire taps or BNC connectors are not permitted
Nevertheless  some choices still had to be made  the most important being which wire types to support
One contender was Category  twisted pair
The argument for it was that practically every office in the Western world had at least four Category  (or better) twisted pairs running from it to a telephone wiring closet within meters
Sometimes two such cables existed
Thus  using Category  twisted pair would make it possible to wire up desktop computers using fast Ethernet without having to rewire the building  an enormous advantage for many organizations
The main disadvantage of a Category  twisted pair is its inability to carry Mbps over meters  the maximum computer-to-hub distance specified for  -Mbps hubs
In contrast  Category  twisted pair wiring can handle m easily  and fiber can go much farther
The compromise chosen was to allow all three possibilities  as shown in Fig
The Category  UTP scheme  called   Base-T  used a signaling speed of   MHz  only  % faster than standard Ethernetâs   MHz
(Remember that THE MEDIUM ACCESS CONTROL SUBLAYER
Name Cable Max
segment Advantages   Base-T  Twisted pair m Uses category  UTP   Base-TX Twisted pair m Full duplex at Mbps (Cat  UTP)   Base-FX Fiber optics  m Full duplex at Mbps; long runs Figure  -
The original fast Ethernet cabling
Manchester encoding  discussed in   requires two clock periods for each of the   million bits sent each ond
) However  to achieve the necessary bit rate Base-T  requires four twisted pairs
Of the four pairs  one is always to the hub  one is always from the hub  and the other two are switchable to the current transmission direction
To get Mbps out of the three twisted pairs in the transmission direction  a fairly involved scheme is used on each twisted pair
It involves sending ternary digits with three different voltage levels
This scheme is not likely to win any prizes for elegance  and we will skip the details
However  since standard telephone wiring for decades has had four twisted pairs per cable  most offices are able to use the existing wiring plant
Of course  it means giving up your office telephone  but that is surely a small price to pay for faster email
Base-T  fell by the wayside as many office buildings were rewired with Category  UTP for   Base-TX Ethernet  which came to dominate the market
This design is simpler because the wires can handle clock rates of MHz
Only two twisted pairs per station are used  one to the hub and one from it
Neither straight binary coding (
NRZ) nor Manchester coding is used
Instead  the  B/ B encoding we described in
data bits are encoded as  signal bits and sent at MHz to provide Mbps
This scheme is simple but has sufficient transitions for synchronization and uses the bandwidth of the wire relatively well
The   Base-TX system is full duplex; stations can transmit at Mbps on one twisted pair and receive at Mbps on another twisted pair at the same time
The last option Base-FX  uses two strands of multimode fiber  one for each direction  so it  too  can run full duplex with Mbps in each direction
In this setup  the distance between a station and the switch can be up to  km
Fast Ethernet allows interconnection by either hubs or switches
To ensure that the CSMA/CD algorithm continues to work  the relationship between the minimum frame size and maximum cable length must be maintained as the network speed goes up from   Mbps to Mbps
So  either the minimum frame size of   bytes must go up or the maximum cable length of  m must come down  proportionally
The easy choice was for the maximum distance between any two stations to come down by a factor of since a hub with   -m cables falls within this new maximum already
However  -km   Base-FX cables are   ETHERNET too long to permit a   -Mbps hub with the normal Ethernet collision algorithm
These cables must instead be connected to a switch and operate in a full-duplex mode so that there are no collisions
Users quickly started to deploy fast Ethernet  but they were not about to throw away  -Mbps Ethernet cards on older computers
As a consequence  virtually all fast Ethernet switches can handle a mix of  -Mbps and   -Mbps stations
To make upgrading easy  the standard itself provides a mechanism called autonegotiation that lets two stations automatically negotiate the optimum speed (  or Mbps) and duplexity (half or full)
It works well most of the time but is known to lead to duplex mismatch problems when one end of the link autonegotiates but the other end does not and is set to full-duplex mode (Shalunov and Carlson  )
Most Ethernet products use this feature to configure themselves
Gigabit Ethernet The ink was barely dry on the fast Ethernet standard when the committee began working on a yet faster Ethernet  quickly dubbed gigabit Ethernet
IEEE ratified the most popular form as
Below we will discuss some of the key features of gigabit Ethernet
More information is given by Spurgeon (   )
The committeeâs goals for gigabit Ethernet were essentially the same as the committeeâs goals for fast Ethernet: increase performance tenfold while maintaining compatibility with all existing Ethernet standards
In particular  gigabit Ethernet had to offer unacknowledged datagram service with both unicast and broadcast  use the same  -bit addressing scheme already in use  and maintain the same frame format  including the minimum and maximum frame sizes
The final standard met all these goals
Like fast Ethernet  all configurations of gigabit Ethernet use point-to-point links
In the simplest configuration  illustrated in Fig
The more common case  however  uses a switch or a hub connected to multiple computers and possibly additional switches or hubs  as shown in Fig
In both configurations  each individual Ethernet cable has exactly two devices on it  no more and no fewer
Also like fast Ethernet  gigabit Ethernet supports two different modes of operation: full-duplex mode and half-duplex mode
The âânormalââ mode is fullduplex mode  which allows traffic in both directions at the same time
This mode is used when there is a central switch connected to computers (or other switches) on the periphery
In this configuration  all lines are buffered so each computer and switch is free to send frames whenever it wants to
The sender does not have to sense the channel to see if anybody else is using it because contention is impossible
On the line between a computer and a switch  the computer is the only possible sender to the switch  and the transmission will succeed even if the switch is currently sending a frame to the computer (because the line is full duplex)
Since THE MEDIUM ACCESS CONTROL SUBLAYER
Switch or hub Ethernet (a) (b) Ethernet Computer Figure  -
(a) A two-station Ethernet
(b) A multistation Ethernet
no contention is possible  the CSMA/CD protocol is not used  so the maximum length of the cable is determined by signal strength issues rather than by how long it takes for a noise burst to propagate back to the sender in the worst case
Switch\%es are free to mix and match speeds
Autonegotiation is supported just as in fast Ethernet  only now the choice is among  and  Mbps
The other mode of operation  half-duplex  is used when the computers are connected to a hub rather than a switch
A hub does not buffer incoming frames
Instead  it electrically connects all the lines internally  simulating the multidrop cable used in classic Ethernet
In this mode  collisions are possible  so the standard CSMA/CD protocol is required
Because a  -byte frame (the shortest allowed) can now be transmitted times faster than in classic Ethernet  the maximum cable length must be times less  or   meters  to maintain the essential property that the sender is still transmitting when the noise burst gets back to it  even in the worst case
With a -meter-long cable  the sender of a  -byte frame at  Gbps would be long finished before the frame got even a tenth of the way to the other end  let alone to the end and back
This length restriction was painful enough that two features were added to the standard to increase the maximum cable length to meters  which is probably enough for most offices
The first feature  called carrier extension  essentially tells the hardware to add its own padding after the normal frame to extend the frame to bytes
Since this padding is added by the sending hardware and removed by the receiving hardware  the software is unaware of it  meaning that no changes are needed to existing software
The downside is that using bytes worth of bandwidth to transmit   bytes of user data (the payload of a  -byte frame) has a line efficiency of only  %
The ond feature  called frame bursting  allows a sender to transmit a concatenated sequence of multiple frames in a single transmission
If the total burst is less than bytes  the hardware pads it again
If enough frames are waiting for transmission  this scheme is very efficient and preferred over carrier extension
ETHERNET In all fairness  it is hard to imagine an organization buying modern computers with gigabit Ethernet cards and then connecting them with an old-fashioned hub to simulate classic Ethernet with all its collisions
Gigabit Ethernet interfaces and switches used to be expensive  but their prices fell rapidly as sales volumes picked up
Still  backward compatibility is sacred in the computer industry  so the committee was required to put it in
Today  most computers ship with an Ethernet interface that is capable of  - -  and -Mbps operation and compatible with all of them
Gigabit Ethernet supports both copper and fiber cabling  as listed in Fig
Signaling at or near  Gbps requires encoding and sending a bit every nanoond
This trick was initially accomplished with short  shielded copper cables (the Base-CX version) and optical fibers
For the optical fibers  two wavelengths are permitted and result in two different versions:
microns (short  for Base-SX) and
microns (long  for Base-LX)
Name Cable Max
segment Advantages Base-SX Fiber optics m Multimode fiber (
microns) Base-LX Fiber optics  m Single (  Î¼) or multimode (
Î¼) Base-CX  Pairs of STP   m Shielded twisted pair Base-T  Pairs of UTP m Standard category  UTP Figure  -
Gigabit Ethernet cabling
Signaling at the short wavelength can be achieved with cheaper LEDs
It is used with multimode fiber and is useful for connections within a building  as it can run up to m for  -micron fiber
Signaling at the long wavelength requires more expensive lasers
On the other hand  when combined with singlemode (  -micron) fiber  the cable length can be up to  km
This limit allows long distance connections between buildings  such as for a campus backbone  as a dedicated point-to-point link
Later variations of the standard allowed even longer links over single-mode fiber
To send bits over these versions of gigabit Ethernet  the  B/  B encoding we described in   was borrowed from another networking technology called Fibre Channel
That scheme encodes  bits of data into  -bit codewords that are sent over the wire or fiber  hence the name  B/  B
The codewords were chosen so that they could be balanced (
have the same number of  s and  s) with sufficient transitions for clock recovery
Sending the coded bits with NRZ requires a signaling bandwidth of  % more than that required for the uncoded bits  a big improvement over the   % expansion of Manchester coding
However  all of these options required new copper or fiber cables to support the faster signaling
None of them made use of the large amount of Category  UTP that had been installed along with fast Ethernet
Within a year  Base-T THE MEDIUM ACCESS CONTROL SUBLAYER
came along to fill this gap  and it has been the most popular form of gigabit Ethernet ever since
People apparently dislike rewiring their buildings
More complicated signaling is needed to make Ethernet run at  Mbps over Category  wires
To start  all four twisted pairs in the cable are used  and each pair is used in both directions at the same time by using digital signal processing to separate signals
Over each wire  five voltage levels that carry  bits are used for signaling at Msymbols/
The mapping to produce the symbols from the bits is not straightforward
It involves scrambling  for transitions  followed by an error correcting code in which four values are embedded into five signal levels
A speed of  Gbps is quite fast
For example  if a receiver is busy with some other task for even  m and does not empty the input buffer on some line  up to  frames may have accumulated in that gap
Also  when a computer on a gigabit Ethernet is shipping data down the line to a computer on a classic Ethernet  buffer overruns are very likely
As a consequence of these two observations  gigabit Ethernet supports flow control
The mechanism consists of one end sending a special control frame to the other end telling it to pause for some period of time
These PAUSE control frames are normal Ethernet frames containing a type of  x
Pauses are given in units of the minimum frame time
For gigabit Ethernet  the time unit is n  allowing for pauses as long as
There is one more extension that was introduced along with gigabit Ethernet
Jumbo frames allow for frames to be longer than  bytes  usually up to  KB
This extension is proprietary
It is not recognized by the standard because if it is used then Ethernet is no longer compatible with earlier versions  but most vendors support it anyway
The rationale is that  bytes is a short unit at gigabit speeds
By manipulating larger blocks of information  the frame rate can be decreased  along with the processing associated with it  such as interrupting the processor to say that a frame has arrived  or splitting up and recombining messages that were too long to fit in one Ethernet frame
IEEE told them to start on  -gigabit Ethernet
This work followed much the same pattern as the previous Ethernet standards  with standards for fiber and shielded copper cable appearing first in  and  followed by the standard for copper twisted pair in
Gbps is a truly prodigious speed  x faster than the original Ethernet
Where could it be needed? The answer is inside data centers and exchanges to connect high-end routers  switches  and servers  as well as in long-distance  high bandwidth trunks between offices that are enabling entire metropolitan area networks based on Ethernet and fiber
The long distance connections use optical fiber  while the short connections may use copper or fiber
ETHERNET All versions of  -gigabit Ethernet support only full-duplex operation
CSMA/CD is no longer part of the design  and the standards concentrate on the details of physical layers that can run at very high speed
Compatibility still matters  though  so  -gigabit Ethernet interfaces autonegotiate and fall back to the highest speed supported by both ends of the line
The main kinds of  -gigabit Ethernet are listed in Fig
Multimode fiber with the
Î¼ (short) wavelength is used for medium distances  and singlemode fiber at
Î¼ (long) and
Î¼ (extended) is used for long distances
GBase-ER can run for distances of   km  making it suitable for wide area applications
All of these versions send a serial stream of information that is produced by scrambling the data bits  then encoding them with a  B/  B code
This encoding has less overhead than an  B/  B code
Name Cable Max
segment Advantages  GBase-SR Fiber optics Up to m Multimode fiber (
Î¼)  GBase-LR Fiber optics   km Single-mode fiber (
Î¼)  GBase-ER Fiber optics   km Single-mode fiber (
Î¼)  GBase-CX   Pairs of twinax   m Twinaxial copper  GBase-T  Pairs of UTP m Category  a UTP Figure  -
The first copper version defined   GBase-CX  uses a cable with four pairs of twinaxial copper wiring
Each pair uses  B/  B coding and runs at
Gsymbols/ond to reach   Gbps
This version is cheaper than fiber and was early to market  but it remains to be seen whether it will be beat out in the long run by  -gigabit Ethernet over more garden variety twisted pair wiring
GBase-T is the version that uses UTP cables
While it calls for Category  a wiring  for shorter runs  it can use lower categories (including Category  ) to allow some reuse of installed cabling
Not surprisingly  the physical layer is quite involved to reach   Gbps over twisted pair
We will only sketch some of the high-level details
Each of the four twisted pairs is used to send  Mbps in both directions
This speed is reached using a signaling rate of Msymbols/ with symbols that use   voltage levels
The symbols are produced by scrambling the data  protecting it with a LDPC (Low Density Parity Check) code  and further coding for error correction
committee has already moved on
At the end of  IEEE created a group to standardize Ethernet operating at   Gbps and Gbps
This upgrade will let Ethernet compete in very high-performance settings  including long-distance connections in backbone networks and short connections over the equipment backplanes
The standard is not yet complete  but proprietary products are already available
THE MEDIUM ACCESS CONTROL SUBLAYER
Retrospective on Ethernet Ethernet has been around for over   years and has no serious competitors in sight  so it is likely to be around for many years to come
Few CPU architectures  operating systems  or programming languages have been king of the mountain for three decades going on strong
Clearly  Ethernet did something right
What? Probably the main reason for its longevity is that Ethernet is simple and flexible
In practice  simple translates into reliable  cheap  and easy to maintain
Once the hub and switch architecture was adopted  failures became extremely rare
People hesitate to replace something that works perfectly all the time  especially when they know that an awful lot of things in the computer industry work very poorly  so that many so-called ââupgradesââ are worse than what they replaced
Simple also translates into cheap
Twisted-pair wiring is relatively inexpensive as are the hardware components
They may start out expensive when there is a transition  for example  new gigabit Ethernet NICs or switches  but they are merely additions to a well established network (not a replacement of it) and the prices fall quickly as the sales volume picks up
Ethernet is easy to maintain
There is no software to install (other than the drivers) and not much in the way of configuration tables to manage (and get wrong)
Also  adding new hosts is as simple as just plugging them in
Another point is that Ethernet interworks easily with TCP/IP  which has become dominant
IP is a connectionless protocol  so it fits perfectly with Ethernet  which is also connectionless
IP fits much less well with connection-oriented alternatives such as ATM
This mismatch definitely hurt ATMâs chances
Lastly  and perhaps most importantly  Ethernet has been able to evolve in certain crucial ways
Speeds have gone up by several orders of magnitude and hubs and switches have been introduced  but these changes have not required changing the software and have often allowed the existing cabling to be reused for a time
When a network salesman shows up at a large installation and says ââI have this fantastic new network for you
All you have to do is throw out all your hardware and rewrite all your software ââ he has a problem
Many alternative technologies that you have probably not even heard of were faster than Ethernet when they were introduced
As well as ATM  this list includes FDDI (Fiber Distributed Data Interface) and Fibre Channel â  two ringbased optical LANs
Both were incompatible with Ethernet
Neither one made it
They were too complicated  which led to complex chips and high prices
The lesson that should have been learned here was KISS (Keep It Simple  Stupid)
Eventually  Ethernet caught up with them in terms of speed  often by borrowing some of their technology  for example  the  B/ B coding from FDDI and the  B/  B coding from Fibre Channel
Then they had no advantages left and quietly died off or fell into specialized roles
â  It is called ââFibre Channelââ and not ââFiber Channelââ because the document editor was British
ETHERNET It looks like Ethernet will continue to expand in its applications for some time
Much effort is being put into carrier-grade Ethernet to let network providers offer Ethernet-based services to their customers for metropolitan and wide area networks (Fouli and Maler  )
This application carries Ethernet frames long distances over fiber and calls for better management features to help operators offer reliable  high-quality services
Very high speed networks are also finding uses in backplanes connecting components in large routers or servers
Both of these uses are in addition to that of sending frames between computers in offices  WIRELESS LANS Wireless LANs are increasingly popular  and homes  offices  cafes  libraries  airports  zoos  and other public places are being outfitted with them to connect computers  PDAs  and smart phones to the Internet
Wireless LANs can also be used to let two or more nearby computers communicate without using the Internet
The main wireless LAN standard is    We gave some background information on it in
Now it is time to take a closer look at the technology
In the following tions  we will look at the protocol stack  physical-layer radio transmission techniques  the MAC sublayer protocol  the frame structure  and the services provided
For more information about
see Gast (   )
To get the truth from the mouth of the horse  consult the published standard  IEEE
Architecture and Protocol Stack
networks can be used in two modes
The most popular mode is to connect clients  such as laptops and smart phones  to another network  such as a company intranet or the Internet
This mode is shown in Fig
In infrastructure mode  each client is associated with an AP (Access Point) that is in turn connected to the other network
The client sends and receives its packets via the AP
Several access points may be connected together  typically by a wired network called a distribution system  to form an extended
In this case  clients can send frames to other clients via their APs
The other mode  shown in Fig
This mode is a collection of computers that are associated so that they can directly send frames to each other
There is no access point
Since Internet access is the killer application for wireless  ad hoc networks are not very popular
Now we will look at the protocols
All the protocols  including
and Ethernet  have a certain commonality of structure
A partial view of the
protocol stack is given in Fig
The stack is the same for clients and THE MEDIUM ACCESS CONTROL SUBLAYER
(a) (b) Access To network point Client Figure  -
architecture
(a) Infrastructure mode
(b) Ad-hoc mode
The physical layer corresponds fairly well to the OSI physical layer  but the data link layer in all the protocols is split into two or more sublayers
the MAC (Medium Access Control) sublayer determines how the channel is allocated  that is  who gets to transmit next
Above it is the LLC (Logical Link Control) sublayer  whose job it is to hide the differences between the different variants and make them indistinguishable as far as the network layer is concerned
This could have been a significant responsibility  but these days the LLC is a glue layer that identifies the protocol (
IP) that is carried within an
(legacy) Frequency hopping and infrared
b Spread spectrum
n MIMO OFDM Logical link layer Release date: â    Upper layers Data link layer Physical layer MAC sublayer Figure  -
Part of the
protocol stack
Several transmission techniques have been added to the physical layer as
has evolved since it first appeared in
Two of the initial techniques  infrared in the manner of television remote controls and frequency hopping in the
The third initial technique  direct sequence spread spectrum at  or  Mbps in the
It is now known as
WIRELESS LANS To give wireless junkies a much-wanted speed boost  new transmission techniques based on the OFDM (Orthogonal Frequency Division Multiplexing) scheme we described in
were introduced in  and
The first is called
a and uses a different frequency band   GHz
The ond stuck with
GHz and compatibility
It is called
Both give rates up to   Mbps
Most recently  transmission techniques that simultaneously use multiple antennas at the transmitter and receiver for a speed boost were finalized as
n in Oct With four antennas and wider channels  the
standard now defines rates up to a startling Mbps
We will now examine each of these transmission techniques briefly
We will only cover those that are in use  however  skipping the legacy
transmission methods
Technically  these belong to the physical layer and should have been examined in
but since they are so closely tied to LANs in general and the
LAN in particular  we treat them here instead
Physical Layer Each of the transmission techniques makes it possible to send a MAC frame over the air from one station to another
They differ  however  in the technology used and speeds achievable
A detailed discussion of these technologies is far beyond the scope of this book  but a few words on each one will relate the techniques to the material we covered in   and will provide interested readers with the key terms to search for elsewhere for more information
All of the
techniques use short-range radios to transmit signals in either the
These bands have the advantage of being unlicensed and hence freely available to any transmitter willing to meet some restrictions  such as radiated power of at most  W (though   mW is more typical for wireless LAN radios)
Unfortunately  this fact is also known to the manufacturers of garage door openers  cordless phones  microwave ovens  and countless other devices  all of which compete with laptops for the same spectrum
All of the transmission methods also define multiple rates
The idea is that different rates can be used depending on the current conditions
If the wireless signal is weak  a low rate can be used
If the signal is clear  the highest rate can be used
This adjustment is called rate adaptation
Since the rates vary by a factor of   or more  good rate adaptation is important for good performance
Of course  since it is not needed for interoperability  the standards do not say how rate adaptation should be done
The first transmission method we shall look at is
It is a spread-spectrum method that supports rates of
and   Mbps  though in practice the operating rate is nearly always   Mbps
It is similar to the CDMA system we THE MEDIUM ACCESS CONTROL SUBLAYER
examined in   except that there is only one spreading code that is shared by all users
Spreading is used to satisfy the FCC requirement that power be spread over the ISM band
The spreading sequence used by
b is a Barker sequence
It has the property that its autocorrelation is low except when the sequences are aligned
This property allows a receiver to lock onto the start of a transmission
To send at a rate of  Mbps  the Barker sequence is used with BPSK modulation to send  bit per   chips
The chips are transmitted at a rate of   Mchips/
To send at  Mbps  it is used with QPSK modulation to send  bits per   chips
The higher rates are different
These rates use a technique called CCK (Complementary Code Keying) to construct codes instead of the Barker sequence
Next we come to
a  which supports rates up to   Mbps in the  -GHz ISM band
You might have expected that
a to come before
b  but that was not the case
Although the
a group was set up first  the
b standard was approved first and its product got to market well ahead of the
a products  partly because of the difficulty of operating in the higher  -GHz band
a method is based on OFDM (Orthogonal Frequency Division Multiplexing) because OFDM uses the spectrum efficiently and resists wireless signal degradations such as multipath
Bits are sent over   subcarriers in parallel carrying data and  used for synchronization
Each symbol lasts  Î¼s and sends  or  bits
The bits are coded for error correction with a binary convolutional code first  so only  /   /  or  /  of the bits are not redundant
With different combinations
a can run at eight different rates  ranging from  to   Mbps
These rates are significantly faster than
b rates  and there is less interference in the  -GHz band
b has a range that is about seven times greater than that of
a  which is more important in many situations
Even with the greater range  the
b people had no intention of letting this upstart win the speed championship
Fortunately  in May  the FCC dropped its long-standing rule requiring all wireless communications equipment operating in the ISM bands in the
to use spread spectrum  so it got to work on
g  which was approved by IEEE in
It copies the OFDM modulation methods of
a but operates in the narrow
It offers the same rates as
a (  to   Mbps) plus of course compatibility with any
b devices that happen to be nearby
All of these different choices can be confusing for customers  so it is common for products to support
a/b/g in a single NIC
Not content to stop there  the IEEE committee began work on a high-throughput physical layer called
It was ratified in
The goal for
n was throughput of at least Mbps after all the wireless overheads were removed
This goal called for a raw speed increase of at least a factor of four
To make it happen  the committee doubled the channels from   MHz to   MHz and   WIRELESS LANS reduced framing overheads by allowing a group of frames to be sent together
More significantly  however
n uses up to four antennas to transmit up to four streams of information at the same time
The signals of the streams interfere at the receiver  but they can be separated using MIMO (Multiple Input Multiple Output) communications techniques
The use of multiple antennas gives a large speed boost  or better range and reliability instead
MIMO  like OFDM  is one of those clever communications ideas that is changing wireless designs and which we are all likely to hear a lot about in the future
For a brief introduction to multiple antennas in
see Halperin et al
MAC Sublayer Protocol Let us now return from the land of electrical engineering to the land of computer science
MAC sublayer protocol is quite different from that of Ethernet  due to two factors that are fundamental to wireless communication
First  radios are nearly always half duplex  meaning that they cannot transmit and listen for noise bursts at the same time on a single frequency
The received signal can easily be a million times weaker than the transmitted signal  so it cannot be heard at the same time
With Ethernet  a station just waits until the ether goes silent and then starts transmitting
If it does not receive a noise burst back while transmitting the first   bytes  the frame has almost assuredly been delivered correctly
With wireless  this collision detection mechanism does not work
tries to avoid collisions with a protocol called CSMA/CA (CSMA with Collision Avoidance)
This protocol is conceptually similar to Ethernetâs CSMA/CD  with channel sensing before sending and exponential back off after collisions
However  a station that has a frame to send starts with a random backoff (except in the case that it has not used the channel recently and the channel is idle)
It does not wait for a collision
The number of slots to backoff is chosen in the range  to  say in the case of the OFDM physical layer
The station waits until the channel is idle  by sensing that there is no signal for a short period of time (called the DIFS  as we explain below)  and counts down idle slots  pausing when frames are sent
It sends its frame when the counter reaches
If the frame gets through  the destination immediately sends a short acknowledgement
Lack of an acknowledgement is inferred to indicate an error  whether a collision or otherwise
In this case  the sender doubles the backoff period and tries again  continuing with exponential backoff as in Ethernet until the frame has been successfully transmitted or the maximum number of retransmissions has been reached
An example timeline is shown in Fig
Station A is the first to send a frame
While A is sending  stations B and C become ready to send
They see that the channel is busy and wait for it to become idle
Shortly after A receives an acknowledgement  the channel goes idle
However  rather than sending a frame right away and colliding  B and C both perform a backoff
C picks a short backoff  THE MEDIUM ACCESS CONTROL SUBLAYER
and thus sends first
B pauses its countdown while it senses that C is using the channel  and resumes after C has received an acknowledgement
B soon completes its backoff and sends its frame
Station A B C Time Data Wait for idle Backoff Rest of backoff Ack A sends to D B ready to send D acks A C sends to D D acks C B sends to D D acks B Data Ack Data Ack Wait for idle Wait for idle Backoff C ready to send Figure  -
Sending a frame with CSMA/CA
Compared to Ethernet  there are two main differences
First  starting backoffs early helps to avoid collisions
This avoidance is worthwhile because collisions are expensive  as the entire frame is transmitted even if one occurs
ond  acknowledgements are used to infer collisions because collisions cannot be detected
This mode of operation is called DCF (Distributed Coordination Function) because each station acts independently  without any kind of central control
The standard also includes an optional mode of operation called PCF (Point Coordination Function) in which the access point controls all activity in its cell  just like a cellular base station
However  PCF is not used in practice because there is normally no way to prevent stations in another nearby network from transmitting competing traffic
The ond problem is that the transmission ranges of different stations may be different
With a wire  the system is engineered so that all stations can hear each other
With the complexities of RF propagation this situation does not hold for wireless stations
Consequently  situations such as the hidden terminal problem mentioned earlier and illustrated again in Fig
Since not all stations are within radio range of each other  transmissions going on in one part of a cell may not be received elsewhere in the same cell
In this example  station C is transmitting to station B
If A senses the channel  it will not hear anything and will falsely conclude that it may now start transmitting to B
This decision leads to a collision
The inverse situation is the exposed terminal problem  illustrated in Fig
Here  B wants to send to C  so it listens to the channel
When it hears a   WIRELESS LANS Range of C's radio A B C (a) A C Range of A's radio B (b) A wants to send to B but cannot hear that B is busy B wants to send to C but mistakenly thinks the transmission will fail C is transmitting A is transmitting Figure  -
(a) The hidden terminal problem
(b) The exposed terminal problem
transmission  it falsely concludes that it may not send to C  even though A may in fact be transmitting to D (not shown)
This decision wastes a transmission opportunity
To reduce ambiguities about which station is sending
defines channel sensing to consist of both physical sensing and virtual sensing
Physical sensing simply checks the medium to see if there is a valid signal
With virtual sensing  each station keeps a logical record of when the channel is in use by tracking the NAV (Network Allocation Vector)
Each frame carries a NAV field that says how long the sequence of which this frame is part will take to complete
Stations that overhear this frame know that the channel will be busy for the period indicated by the NAV  regardless of whether they can sense a physical signal
For example  the NAV of a data frame includes the time needed to send an acknowledgement
All stations that hear the data frame will defer during the acknowledgement period  whether or not they can hear the acknowledgement
An optional RTS/CTS mechanism uses the NAV to prevent terminals from sending frames at the same time as hidden terminals
It is shown in Fig
In this example  A wants to send to B
C is a station within range of A (and possibly within range of B  but that does not matter)
D is a station within range of B but not within range of A
The protocol starts when A decides it wants to send data to B
A begins by sending an RTS frame to B to request permission to send it a frame
If B receives this request  it answers with a CTS frame to indicate that the channel is clear to send
Upon receipt of the CTS  A sends its frame and starts an ACK timer
Upon correct receipt of the data frame  B responds with an ACK frame  completing the exchange
If Aâs ACK timer expires before the ACK gets back to it  it is treated as a collision and the whole protocol is run again after a backoff
THE MEDIUM ACCESS CONTROL SUBLAYER
A RTS Data B CTS ACK C D NAV NAV Time Figure  -
Virtual channel sensing using CSMA/CA
Now let us consider this exchange from the viewpoints of C and D
C is within range of A  so it may receive the RTS frame
If it does  it realizes that someone is going to send data soon
From the information provided in the RTS request  it can estimate how long the sequence will take  including the final ACK
So  for the good of all  it desists from transmitting anything until the exchange is completed
It does so by updating its record of the NAV to indicate that the channel is busy  as shown in Fig
D does not hear the RTS  but it does hear the CTS  so it also updates its NAV
Note that the NAV signals are not transmitted; they are just internal reminders to keep quiet for a certain period of time
However  while RTS/CTS sounds good in theory  it is one of those designs that has proved to be of little value in practice
Several reasons why it is seldom used are known
It does not help for short frames (which are sent in place of the RTS) or for the AP (which everyone can hear  by definition)
For other situations  it only slows down operation
RTS/CTS in
is a little different than in the MACA protocol we saw in
because everyone hearing the RTS or CTS remains quiet for the duration to allow the ACK to get through without collision
Because of this  it does not help with exposed terminals as MACA did  only with hidden terminals
Most often there are few hidden terminals  and CSMA/CA already helps them by slowing down stations that transmit unsuccessfully  whatever the cause  to make it more likely that transmissions will succeed
CSMA/CA with physical and virtual sensing is the core of the
However  there are several other mechanisms that have been developed to go with it
Each of these mechanisms was driven by the needs of real operation  so we will look at them briefly
The first need we will look at is reliability
In contrast to wired networks  wireless networks are noisy and unreliable  in no small part due to interference from other kinds of devices  such as microwave ovens  which also use the unlicensed ISM bands
The use of acknowledgements and retransmissions is of little help if the probability of getting a frame through is small in the first place
WIRELESS LANS The main strategy that is used to increase successful transmissions is to lower the transmission rate
Slower rates use more robust modulations that are more likely to be received correctly for a given signal-to-noise ratio
If too many frames are lost  a station can lower the rate
If frames are delivered with little loss  a station can occasionally test a higher rate to see if it should be used
Another strategy to improve the chance of the frame getting through undamaged is to send shorter frames
If the probability of any bit being in error is p  the probability of an n-bit frame being received entirely correctly is (  â p)n
For example  for p =  â  the probability of receiving a full Ethernet frame (  bits) correctly is less than  %
Most frames will be lost
But if the frames are only a third as long ( bits) two thirds of them will be received correctly
Now most frames will get through and fewer retransmissions will be needed
Shorter frames can be implemented by reducing the maximum size of the message that is accepted from the network layer
Alternatively
allows frames to be split into smaller pieces  called fragments  each with its own checksum
The fragment size is not fixed by the standard  but is a parameter that can be adjusted by the AP
The fragments are individually numbered and acknowledged using a stop-and-wait protocol (
the sender may not transmit fragment k +  until it has received the acknowledgement for fragment k)
Once the channel has been acquired  multiple fragments are sent as a burst
They go one after the other with an acknowledgement (and possibly retransmissions) in between  until either the whole frame has been successfully sent or the transmission time reaches the maximum allowed
The NAV mechanism keeps other stations quiet only until the next acknowledgement  but another mechanism (see below) is used to allow a burst of fragments to be sent without other stations sending a frame in the middle
The ond need we will discuss is saving power
Battery life is always an issue with mobile wireless devices
standard pays attention to the issue of power management so that clients need not waste power when they have neither information to send nor to receive
The basic mechanism for saving power builds on beacon frames
Beacons are periodic broadcasts by the AP (
The frames advertise the presence of the AP to clients and carry system parameters  such as the identifier of the AP  the time  how long until the next beacon  and urity settings
Clients can set a power-management bit in frames that they send to the AP to tell it that they are entering power-save mode
In this mode  the client can doze and the AP will buffer traffic intended for it
To check for incoming traffic  the client wakes up for every beacon  and checks a traffic map that is sent as part of the beacon
This map tells the client if there is buffered traffic
If so  the client sends a poll message to the AP  which then sends the buffered traffic
The client can then go back to sleep until the next beacon is sent
Another power-saving mechanism  called APSD (Automatic Power Save Delivery)  was also added to
With this new mechanism  the AP buffers frames and sends them to a client just after the client sends frames to the THE MEDIUM ACCESS CONTROL SUBLAYER
The client can then go to sleep until it has more traffic to send (and receive)
This mechanism works well for applications such as VoIP that have frequent traffic in both directions
For example  a VoIP wireless phone might use it to send and receive frames every   m  much more frequently than the beacon interval of m  while dozing in between
The third and last need we will examine is quality of service
When the VoIP traffic in the preceding example competes with peer-to-peer traffic  the VoIP traffic will suffer
It will be delayed due to contention with the high-bandwidth peer-to-peer traffic  even though the VoIP bandwidth is low
These delays are likely to degrade the voice calls
To prevent this degradation  we would like to let the VoIP traffic go ahead of the peer-to-peer traffic  as it is of higher priority
has a clever mechanism to provide this kind of quality of service that was introduced as set of extensions under the name
It works by extending CSMA/CA with carefully defined intervals between frames
After a frame has been sent  a certain amount of idle time is required before any station may send a frame to check that the channel is no longer in use
The trick is to define different time intervals for different kinds of frames
Five intervals are depicted in Fig
The interval between regular data frames is called the DIFS (DCF InterFrame Spacing)
Any station may attempt to acquire the channel to send a new frame after the medium has been idle for DIFS
The usual contention rules apply  and binary exponential backoff may be needed if a collision occurs
The shortest interval is SIFS (Short InterFrame Spacing)
It is used to allow the parties in a single dialog the chance to go first
Examples include letting the receiver send an ACK  other control frame sequences like RTS and CTS  or letting a sender transmit a burst of fragments
Sending the next fragment after waiting only SIFS is what prevents another station from jumping in with a frame in the middle of the exchange
ACK SIFS AIFS  DIFS EIFS AIFS  Control frame or next fragment may be sent here High-priority frame here Regular DCF frame here Low-priority frame here Bad frame recovery done Time Figure  -
Interframe spacing in    The two AIFS (Arbitration InterFrame Space) intervals show examples of two different priority levels
The short interval  AIFS  is smaller than DIFS but longer than SIFS
It can be used by the AP to move voice or other high-priority   WIRELESS LANS traffic to the head of the line
The AP will wait for a shorter interval before it sends the voice traffic  and thus send it before regular traffic
The long interval  AIFS  is larger than DIFS
It is used for background traffic that can be deferred until after regular traffic
The AP will wait for a longer interval before it sends this traffic  giving regular traffic the opportunity to transmit first
The complete quality of service mechanism defines four different priority levels that have different backoff parameters as well as different idle parameters
The last time interval  EIFS (Extended InterFrame Spacing)  is used only by a station that has just received a bad or unknown frame  to report the problem
The idea is that since the receiver may have no idea of what is going on  it should wait a while to avoid interfering with an ongoing dialog between two stations
A further part of the quality of service extensions is the notion of a TXOP or transmission opportunity
The original CSMA/CA mechanism let stations send one frame at a time
This design was fine until the range of rates increased
a/g  one station might be sending at  Mbps and another station be sending at   Mbps
They each get to send one frame  but the  -Mbps station takes nine times as long (ignoring fixed overheads) as the  -Mbps station to send its frame
This disparity has the unfortunate side effect of slowing down a fast sender who is competing with a slow sender to roughly the rate of the slow sender
For example  again ignoring fixed overheads  when sending alone the  -Mbps and  -Mbps senders will get their own rates  but when sending together they will both get
Mbps on average
It is a stiff penalty for the fast sender
This issue is known as the rate anomaly (Heusse et al
With transmission opportunities  each station gets an equal amount of airtime  not an equal number of frames
Stations that send at a higher rate for their airtime will get higher throughput
In our example  when sending together the  -Mbps and  -Mbps senders will now get  Mbps and   Mbps  respectively
Frame Structure The
standard defines three different classes of frames in the air: data  control  and management
Each of these has a header with a variety of fields used within the MAC sublayer
In addition  there are some headers used by the physical layer  but these mostly deal with the modulation techniques used  so we will not discuss them here
We will look at the format of the data frame as an example
It is shown in Fig
First comes the Frame control field  which is made up of   subfields
The first of these is the Protocol version  set to
It is there to allow future versions of
to operate at the same time in the same cell
Then come the Type (data  control  or management) and Subtype fields (
RTS or CTS)
For a regular data frame (without quality of service)  they are set to   and  in binary
The To DS and From DS bits are set to indicate whether the frame is going to or coming from the network connected to the APs  which is called the distribution THE MEDIUM ACCESS CONTROL SUBLAYER
The More fragments bit means that more fragments will follow
The Retry bit marks a retransmission of a frame sent earlier
The Power management bit indicates that the sender is going into power-save mode
The More data bit indicates that the sender has additional frames for the receiver
The Protected Frame bit indicates that the frame body has been encrypted for urity
We will discuss urity briefly in the next tion
Finally  the Order bit tells the receiver that the higher layer expects the sequence of frames to arrive strictly in order
Bytes   â Address  Sequence Duration (recipient) Data Frame control Check sequence    Address  (transmitter) Address  Subtype =  Type =   Version =   To DS From DS More frag
More data Protected Order Bits  Figure  -
Format of the
data frame
The ond field of the data frame  the Duration field  tells how long the frame and its acknowledgement will occupy the channel  measured in microonds
It is present in all types of frames  including control frames  and is what stations use to manage the NAV mechanism
Next come addresses
Data frames sent to or from an AP have three addresses  all in standard IEEE format
The first address is the receiver  and the ond address is the transmitter
They are obviously needed  but what is the third address for? Remember that the AP is simply a relay point for frames as they travel between a client and another point on the network  perhaps a distant client or a portal to the Internet
The third address gives this distant endpoint
The Sequence field numbers frames so that duplicates can be detected
Of the   bits available   identify the fragment and   carry a number that is advanced with each new transmission
The Data field contains the payload  up to  bytes
The first bytes of this payload are in a format known as LLC (Logical Link Control)
This layer is the glue that identifies the higher-layer protocol (
IP) to which the payloads should be passed
Last comes the Frame check sequence  which is the same  -bit CRC we saw in
and elsewhere
Management frames have the same format as data frames  plus a format for the data portion that varies with the subtype (
parameters in beacon frames)
Control frames are short
Like all frames  they have the Frame control  Duration  and Frame check sequence fields
However  they may have only one address and no data portion
Most of the key information is conveyed with the Subtype field (
ACK  RTS and CTS)
WIRELESS LANS    Services The
standard defines the services that the clients  the access points  and the network connecting them must be a conformant wireless LAN
These services cluster into several groups
The association service is used by mobile stations to connect themselves to APs
Typically  it is used just after a station moves within radio range of the AP
Upon arrival  the station learns the identity and capabilities of the AP  either from beacon frames or by directly asking the AP
The capabilities include the data rates supported  urity arrangements  power-saving capabilities  quality of service support  and more
The station sends a request to associate with the AP
The AP may accept or reject the request
Reassociation lets a station change its preferred AP
This facility is useful for mobile stations moving from one AP to another AP in the same extended
LAN  like a handover in the cellular network
If it is used correctly  no data will be lost as a consequence of the handover
like Ethernet  is just a best-effort service
) Either the station or the AP may also disassociate  breaking their relationship
A station should use this service before shutting down or leaving the network
The AP may use it before going down for maintenance
Stations must also authenticate before they can send frames via the AP  but authentication is handled in different ways depending on the choice of urity scheme
network is ââopen ââ anyone is allowed to use it
Otherwise  credentials are needed to authenticate
The recommended scheme  called WPA  (WiFi Protected Access  )  implements urity as defined in the
i standard
(Plain WPA is an interim scheme that implements a subset of
We will skip it and go straight to the complete scheme
) With WPA  the AP can talk to an authentication server that has a username and password database to determine if the station is allowed to access the network
Alternatively a pre-shared key  which is a fancy name for a network password  may be configured
Several frames are exchanged between the station and the AP with a challenge and response that lets the station prove it has the right credentials
This exchange happens after association
The scheme that was used before WPA is called WEP (Wired Equivalent Privacy)
For this scheme  authentication with a preshared key happens before association
However  its use is discouraged because of design flaws that make WEP easy to compromise
The first practical demonstration that WEP was broken came when Adam Stubblefield was a summer intern at AT&T (Stubblefield et al
He was able to code up and test an attack in one week  much of which was spent getting permission from management to buy the WiFi cards needed for experiments
Software to crack WEP passwords is now freely available
Once frames reach the AP  the distribution service determines how to route them
If the destination is local to the AP  the frames can be sent out directly over the air
Otherwise  they will have to be forwarded over the wired network
The THE MEDIUM ACCESS CONTROL SUBLAYER
integration service handles any translation that is needed for a frame to be sent outside the
LAN  or to arrive from outside the
The common case here is connecting the wireless LAN to the Internet
Data transmission is what it is all about  so
naturally provides a data delivery service
This service lets stations transmit and receive data using the protocols we described earlier in this  ter
is modeled on Ethernet and transmission over Ethernet is not guaranteed to be   % reliable  transmission over
is not guaranteed to be reliable either
Higher layers must deal with detecting and correcting errors
Wireless is a broadcast signal
For information sent over a wireless LAN to be kept confidential  it must be encrypted
This goal is accomplished with a privacy service that manages the details of encryption and decryption
The encryption algorithm for WPA  is based on AES (Advanced Encryption Standard)  a
government standard approved in
The keys that are used for encryption are determined during the authentication procedure
To handle traffic with different priorities  there is a QOS traffic scheduling service
It uses the protocols we described to give voice and video traffic preferential treatment compared to best-effort and background traffic
A companion service also provides higher-layer timer synchronization
This lets stations coordinate their actions  which may be useful for media processing
Finally  there are two services that help stations manage their use of the spectrum
The transmit power control service gives stations the information they need to meet regulatory limits on transmit power that vary from region to region
The dynamic frequency selection service give stations the information they need to avoid transmitting on frequencies in the  -GHz band that are being used for radar in the proximity
With these services
provides a rich set of functionality for connecting nearby mobile clients to the Internet
It has been a huge success  and the standard has repeatedly been amended to add more functionality
For a perspective on where the standard has been and where it is heading  see Hiertz et al
(   )  BROADBAND WIRELESS We have been indoors too long
Let us go outdoors  where there is quite a bit of interesting networking over the so-called ââlast mile
ââ With the deregulation of the telephone systems in many countries  competitors to the entrenched telephone companies are now often allowed to offer local voice and high-speed Internet service
There is certainly plenty of demand
The problem is that running fiber or coax to millions of homes and businesses is prohibitively expensive
What is a competitor to do? The answer is broadband wireless
Erecting a big antenna on a hill just outside of town is much easier and cheaper than digging many trenches and stringing   BROADBAND WIRELESS cables
Thus  companies have begun to experiment with providing multimegabit wireless communication services for voice  Internet  movies on demand  etc
To stimulate the market  IEEE formed a group to standardize a broadband wireless metropolitan area network
The next number available in the numbering space was
so the standard got this number
Informally the technology is called WiMAX (Worldwide Interoperability for Microwave Access)
We will use the terms
and WiMAX interchangeably
standard was approved in December
Early versions provided a wireless local loop between fixed points with a line of sight to each other
This design soon changed to make WiMAX a more competitive alternative to cable and DSL for Internet access
By January
had been revised to support non-line-of-sight links by using OFDM technology at frequencies between  GHz and   GHz
This change made deployment much easier  though stations were still fixed locations
The rise of  G cellular networks posed a threat by promising high data rates and mobility
In response
was enhanced again to allow mobility at vehicular speeds by December
Mobile broadband Internet access is the target of the current standard  IEEE
Like the other standards
was heavily influenced by the OSI model  including the (sub)layers  terminology  service primitives  and more
Unfortunately  also like OSI  it is fairly complicated
In fact  the WiMAX Forum was created to define interoperable subsets of the standard for commercial offerings
In the following tions  we will give a brief description of some of the highlights of the common forms of
air interface  but this treatment is far from complete and leaves out many details
For additional information about WiMAX and broadband wireless in general  see Andrews et al
Comparison of
and  G At this point you may be thinking: why devise a new standard? Why not just use
or  G? In fact  WiMAX combines aspects of both
and  G  making it more like a  G technology
WiMAX is all about wirelessly connecting devices to the Internet at megabit/ speeds  instead of using cable or DSL
The devices may be mobile  or at least portable
WiMAX did not start by adding low-rate data on the side of voice-like cellular networks;
was designed to carry IP packets over the air and to connect to an IP-based wired network with a minimum of fuss
The packets may carry peer-to-peer traffic  VoIP calls  or streaming media to support a range of applications
it is based on OFDM technology to ensure good performance in spite of wireless signal degradations such as multipath fading  and on MIMO technology to achieve high levels of throughput
However  WiMAX is more like  G (and thus unlike
) in several key respects
The key technical problem is to achieve high capacity by the efficient use of spectrum  so that a large number of subscribers in a coverage area can all get THE MEDIUM ACCESS CONTROL SUBLAYER
high throughput
The typical distances are at least   times larger than for an
Consequently  WiMAX base stations are more powerful than
Access Points (APs)
To handle weaker signals over larger distances  the base station uses more power and better antennas  and it performs more processing to handle errors
To maximize throughput  transmissions are carefully scheduled by the base station for each particular subscriber; spectrum use is not left to chance with CSMA/CA  which may waste capacity with collisions
Licensed spectrum is the expected case for WiMAX  typically around
GHz in the
The whole system is substantially more optimized than    This complexity is worth it  considering the large amount of money involved for licensed spectrum
the result is a managed and reliable service with good support for quality of service
With all of these features
most closely resembles the  G cellular networks that are now being standardized under the name LTE (Long Term Evolution)
While  G cellular networks are based on CDMA and support voice and data  G cellular networks will be based on OFDM with MIMO  and they will target data  with voice as just one application
It looks like WiMAX and  G are on a collision course in terms of technology and applications
Perhaps this convergence is unsurprising  given that the Internet is the killer application and OFDM and MIMO are the best-known technologies for efficiently using the spectrum
Architecture and Protocol Stack The
architecture is shown in Fig
Base stations connect directly to the providerâs backbone network  which is in turn connected to the Internet
The base stations communicate with stations over the wireless air interface
Two kinds of stations exist
Subscriber stations remain in a fixed location  for example  broadband Internet access for homes
Mobile stations can receive service while they are moving  for example  a car equipped with WiMAX
protocol stack that is used across the air interface is shown in Fig
The general structure is similar to that of the other networks  but with more sublayers
The bottom layer deals with transmission  and here we have shown only the popular offerings of
fixed and mobile WiMAX
There is a different physical layer for each offering
Both layers operate in licensed spectrum below   GHz and use OFDM  but in different ways
Above the physical layer  the data link layer consists of three sublayers
The bottom one deals with privacy and urity  which is far more crucial for public outdoor networks than for private indoor networks
It manages encryption  decryption  and key management
Next comes the MAC common sublayer part
This part is where the main protocols  such as channel management  are located
The model here is that the base station completely controls the system
It can schedule the downlink (
base to subscriber) channels very efficiently and plays a major role in managing   BROADBAND WIRELESS Base station Mobile stations Subscriber stations Backbone network (to Internet) Air interface Figure  -
architecture
âFixed WiMAXâ OFDM (
a) âMobile WiMAXâ Scalable OFDMA (
e) Service specific convergence sublayer Release date:  Upper layers Data link layer Physical layer MAC common sublayer urity sublayer IP  for example Figure  -
protocol stack
the uplink (
subscriber to base) channels as well
An unusual feature of this MAC sublayer is that  unlike those of the other protocols  it is completely connection oriented  in order to provide quality of service guarantees for telephony and multimedia communication
The service-specific convergence sublayer takes the place of the logical link sublayer in the other protocols
Its function is to provide an interface to the network layer
Different convergence layers are defined to integrate seamlessly with different upper layers
The important choice is IP  though the standard defines mappings for protocols such as Ethernet and ATM too
Since IP is connectionless and the
MAC sublayer is connection-oriented  this layer must map between addresses and connections
THE MEDIUM ACCESS CONTROL SUBLAYER
Physical Layer Most WiMAX deployments use licensed spectrum around either
As with  G  finding available spectrum is a key problem
To help  the
standard is designed for flexibility
It allows operation from  GHz to   GHz
Channels of different sizes are supported  for example
MHz for fixed WiMAX and from
MHz to   MHz for mobile WiMAX
Transmissions are sent over these channels with OFDM  the technique we described in
Compared to
OFDM design is optimized to make the most out of licensed spectrum and wide area transmissions
The channel is divided into more subcarriers with a longer symbol duration to tolerate larger wireless signal degradations; WiMAX parameters are around   times larger than comparable
parameters
For example  in mobile WiMAX there are subcarriers for a  -MHz channel and the time to send a symbol on each subcarrier is roughly Î¼
Symbols on each subcarrier are sent with QPSK  QAM-   or QAM-   modulation schemes we described in
When the mobile or subscriber station is near the base station and the received signal has a high signal-to-noise ratio (SNR)  QAM-  can be used to send  bits per symbol
To reach distant stations with a low SNR  QPSK can be used to deliver  bits per symbol
The data is first coded for error correction with the convolutional coding (or better schemes) that we described in
This coding is common on noisy channels to tolerate some bit errors without needing to send retransmissions
In fact  the modulation and coding methods should sound familiar by now as they are used for many networks we have studied  including
cable  and DSL
The net result is that a base station can support up to
Mbps of downlink traffic and
Mbps of uplink traffic per  -MHz channel and pair of antennas
One thing the designers of
did not like was a certain aspect of the way GSM and DAMPS work
Both of those systems use equal frequency bands for upstream and downstream traffic
That is  they implicitly assume there is as much upstream traffic as downstream traffic
For voice  traffic is symmetric for the most part  but for Internet access (and certainly Web surfing) there is often more downstream traffic than upstream traffic
The ratio is often  :   :  or more:
So  the designers chose a flexible scheme for dividing the channel between stations  called OFDMA (Orthogonal Frequency Division Multiple Access)
With OFDMA  different sets of subcarriers can be assigned to different stations  so that more than one station can send or receive at once
If this were
all subcarriers would be used by one station to send at any given moment
The added flexibility in how bandwidth is assigned can increase performance because a given subcarrier might be faded at one receiver due to multipath effects but clear at another
Subcarriers can be assigned to the stations that can use them best
As well as having asymmetric traffic  stations usually alternate between sending and receiving
This method is called TDD (Time Division Duplex)
The   BROADBAND WIRELESS alternative method  in which a station sends and receives at the same time (on different subcarrier frequencies)  is called FDD (Frequency Division Duplex)
WiMAX allows both methods  but TDD is preferred because it is easier to implement and more flexible
Guard Ranging Burst Burst Burst Burst Burst Burst Burst Burst Downlink map Uplink map Preamble Time Subcarrier Downlink Uplink Next frame Last frame Figure  -
Frame structure for OFDMA with time division duplexing
It starts with a preamble to synchronize all stations  followed by downlink transmissions from the base station
First  the base station sends maps that tell all stations how the downlink and uplink subcarriers are assigned over the frame
The base station controls the maps  so it can allocate different amounts of bandwidth to stations from frame to frame depending on the needs of each station
Next  the base station sends bursts of traffic to different subscriber and mobile stations on the subcarriers at the times given in the map
The downlink transmissions end with a guard time for stations to switch from receiving to transmitting
Finally  the subscriber and mobile stations send their bursts of traffic to the base station in the uplink positions that were reserved for them in the map
One of these uplink bursts is reserved for ranging  which is the process by which new stations adjust their timing and request initial bandwidth to connect to the base station
Since no connection is set up at this stage  new stations just transmit and hope there is no collision
MAC Sublayer Protocol The data link layer is divided into three sublayers  as we saw in Fig
Since we will not study cryptography until
it is difficult to explain now how the urity sublayer works
Suffice it to say that encryption is used to keep ret all data transmitted
Only the frame payloads are encrypted; the headers THE MEDIUM ACCESS CONTROL SUBLAYER
This property means that a snooper can see who is talking to whom but cannot tell what they are saying to each other
If you already know something about cryptography  what follows is a oneparagraph explanation of the urity sublayer
If you know nothing about cryptography  you are not likely to find the next paragraph terribly enlightening (but you might consider rereading it after finishing
When a subscriber connects to a base station  they perform mutual authentication with RSA public-key cryptography using X
certificates
The payloads themselves are encrypted using a symmetric-key system  either AES (Rijndael) or DES with cipher block chaining
Integrity checking uses SHA-
Now that was not so bad  was it? Let us now look at the MAC common sublayer part
The MAC sublayer is connection-oriented and point-to-multipoint  which means that one base station communicates with multiple subscriber stations
Much of this design is borrowed from cable modems  in which one cable headend controls the transmissions of multiple cable modems at the customer premises
The downlink direction is fairly straightforward
The base station controls the physical-layer bursts that are used to send information to the different subscriber stations
The MAC sublayer simply packs its frames into this structure
To reduce overhead  there are several different options
For example  MAC frames may be sent individually  or packed back-to-back into a group
The uplink channel is more complicated since there are competing subscribers that need access to it
Its allocation is tied closely to the quality of service issue
Four classes of service are defined  as follows:
Constant bit rate service Real-time variable bit rate service Non-real-time variable bit rate service Best-effort service
All service in
is connection-oriented
Each connection gets one of these service classes  determined when the connection is set up
This design is different from that of
or Ethernet  which are connectionless in the MAC sublayer
Constant bit rate service is intended for transmitting uncompressed voice
This service needs to send a predetermined amount of data at predetermined time intervals
It is accommodated by dedicating certain bursts to each connection of this type
Once the bandwidth has been allocated  the bursts are available automatically  without the need to ask for each one
Real-time variable bit rate service is for compressed multimedia and other soft real-time applications in which the amount of bandwidth needed at each instant may vary
It is accommodated by the base station polling the subscriber at a fixed interval to ask how much bandwidth is needed this time
BROADBAND WIRELESS Non-real-time variable bit rate service is for heavy transmissions that are not real time  such as large file transfers
For this service  the base station polls the subscriber often  but not at rigidly prescribed time intervals
Connections with this service can also use best-effort service  described next  to request bandwidth
Best-effort service is for everything else
No polling is done and the subscriber must contend for bandwidth with other best-effort subscribers
Requests for bandwidth are sent in bursts marked in the uplink map as available for contention
If a request is successful  its success will be noted in the next downlink map
If it is not successful  the unsuccessful subscriber have to try again later
To minimize collisions  the Ethernet binary exponential backoff algorithm is used
Frame Structure All MAC frames begin with a generic header
The header is followed by an optional payload and an optional checksum (CRC)  as illustrated in Fig
The payload is not needed in control frames  for example  those requesting channel slots
The checksum is (surprisingly) also optional  due to the error correction in the physical layer and the fact that no attempt is ever made to retransmit realtime frames
If no retransmissions will be attempted  why even bother with a checksum? But if there is a checksum  it is the standard IEEE CRC  and acknowledgements and retransmissions are used for reliability
Bits (a) (b)  Type Length Type Bytes needed E EK C C I Connection ID Header Data CRC CRC Connection ID Header CRC        Bits Figure  -
(a) A generic frame
(b) A bandwidth request frame
A quick rundown of the header fields of Fig
The EC bit tells whether the payload is encrypted
The Type field identifies the frame type  mostly telling whether packing and fragmentation are present
The CI field indicates the presence or absence of the final checksum
The EK field tells which of the encryption keys is being used (if any)
The Length field gives the complete length of the frame  including the header
The Connection identifier tells which connection this frame belongs to
Finally  the Header CRC field is a checksum over the header only  using the polynomial x  + x  + x +
protocol has many kinds of frames
An example of a different type of frame  one that is used to request bandwidth  is shown in Fig
It THE MEDIUM ACCESS CONTROL SUBLAYER
starts with a  bit instead of a  bit and is otherwise similar to the generic header except that the ond and third bytes form a  -bit number telling how much bandwidth is needed to carry the specified number of bytes
Bandwidth request frames do not carry a payload or full-frame CRC
A great deal more could be said about
but this is not the place to say it
For more information  please consult the IEEE
Ericsson company became interested in connecting its mobile phones to other devices (
laptops) without cables
Together with four other companies (IBM  Intel  Nokia  and Toshiba)  it formed a SIG (Special Interest Group
consortium) in  to develop a wireless standard for interconnecting computing and communication devices and accessories using short-range  low-power  inexpensive wireless radios
The project was named Bluetooth  after Harald Blaatand (Bluetooth) II (  â  )  a Viking king who unified (
conquered) Denmark and Norway  also without cables
was released in July  and since then the SIG has never looked back
All manner of consumer electronic devices now use Bluetooth  from mobile phones and laptops to headsets  printers  keyboards  mice  gameboxes  watches  music players  navigation units  and more
The Bluetooth protocols let these devices find and connect to each other  an act called pairing  and urely transfer data
The protocols have evolved over the past decade  too
After the initial protocols stabilized  higher data rates were added to Bluetooth
release in  Bluetooth can be used for device pairing in combination with
for high-throughput data transfer
release in December  specified low-power operation
That will be handy for people who do not want to change the batteries regularly in all of those devices around the house
We will cover the main aspects of Bluetooth below
Bluetooth Architecture Let us start our study of the Bluetooth system with a quick overview of what it contains and what it is intended to do
The basic unit of a Bluetooth system is a piconet  which consists of a master node and up to seven active slave nodes within a distance of   meters
Multiple piconets can exist in the same (large) room and can even be connected via a bridge node that takes part in multiple piconets  as in Fig
An interconnected collection of piconets is called a scatternet
In addition to the seven active slave nodes in a piconet  there can be up to parked nodes in the net
These are devices that the master has switched to a lowpower state to reduce the drain on their batteries
In parked state  a device cannot   BLUETOOTH S S S S S S S S S S S M M Bridge slave Parked slave Piconet  Piconet  Active slave Figure  -
Two piconets can be connected to form a scatternet
do anything except respond to an activation or beacon signal from the master
Two intermediate power states  hold and sniff  also exist  but these will not concern us here
The reason for the master/slave design is that the designers intended to facilitate the implementation of complete Bluetooth chips for under $
The consequence of this decision is that the slaves are fairly dumb  basically just doing whatever the master tells them to do
At its heart  a piconet is a centralized TDM system  with the master controlling the clock and determining which device gets to communicate in which time slot
All communication is between the master and a slave; direct slave-slave communication is not possible
Bluetooth Applications Most network protocols just provide channels between communicating entities and let application designers figure out what they want to use them for
For example
does not specify whether users should use their notebook computers for reading email  surfing the Web  or something else
In contrast  the Bluetooth SIG specifies particular applications to be supported and provides different protocol stacks for each one
At the time of writing  there are   applications  which are called profiles
Unfortunately  this approach leads to a very large amount of complexity
We will omit the complexity here but will briefly look at the profiles to see more clearly what the Bluetooth SIG is trying to accomplish
Six of the profiles are for different uses of audio and video
For example  the intercom profile allows two telephones to connect as walkie-talkies
The headset and hands-free profiles both provide voice communication between a headset and its base station  as might be used for hands-free telephony while driving a car
THE MEDIUM ACCESS CONTROL SUBLAYER
Other profiles are for streaming stereo-quality audio and video  say  from a portable music player to headphones  or from a digital camera to a TV
The human interface device profile is for connecting keyboards and mice to computers
Other profiles let a mobile phone or other computer receive images from a camera or send images to a printer
Perhaps of more interest is a profile to use a mobile phone as a remote control for a (Bluetooth-enabled) TV
Still other profiles enable networking
The personal area network profile lets Bluetooth devices form an ad hoc network or remotely access another network  such as an
LAN  via an access point
The dial-up networking profile was actually the original motivation for the whole project
It allows a notebook computer to connect to a mobile phone containing a built-in modem without using wires
Profiles for higher-layer information exchange have also been defined
The synchronization profile is intended for loading data into a mobile phone when it leaves home and collecting data from it when it returns
We will skip the rest of the profiles  except to mention that some profiles serve as building blocks on which the above profiles are built
The generic access profile  on which all of the other profiles are built  provides a way to establish and maintain ure links (channels) between the master and the slaves
The other generic profiles define the basics of object exchange and audio and video transport
Utility profiles are used widely for functions such as emulating a serial line  which is especially useful for many legacy applications
Was it really necessary to spell out all these applications in detail and provide different protocol stacks for each one? Probably not  but there were a number of different working groups that devised different parts of the standard  and each one just focused on its specific problem and generated its own profile
Think of this as Conwayâs Law in action
(In the April  issue of Datamation magazine  Melvin Conway observed that if you assign n people to write a compiler  you will get an n-pass compiler  or more generally  the software structure mirrors the structure of the group that produced it
) It would probably have been possible to get away with two protocol stacks instead of one for file transfer and one for streaming real-time communication
The Bluetooth Protocol Stack The Bluetooth standard has many protocols grouped loosely into the layers shown in Fig
The first observation to make is that the structure does not follow the OSI model  the TCP/IP model  the model  or any other model
The bottom layer is the physical radio layer  which corresponds fairly well to the physical layer in the OSI and models
It deals with radio transmission and modulation
Many of the concerns here have to do with the goal of making the system inexpensive so that it can become a mass-market item
BLUETOOTH Host-controller interface Upper layers Datalink layer Physical Radio layer Link control (Baseband) Link manager L CAP Service RFcomm discovery Applications
Profile Profile Profile Figure  -
The Bluetooth protocol architecture
The link control (or baseband) layer is somewhat analogous to the MAC sublayer but also includes elements of the physical layer
It deals with how the master controls time slots and how these slots are grouped into frames
Next come two protocols that use the link control protocol
The link manager handles the establishment of logical channels between devices  including power management  pairing and encryption  and quality of service
It lies below the host controller interface line
This interface is a convenience for implementation: typically  the protocols below the line will be implemented on a Bluetooth chip  and the protocols above the line will be implemented on the Bluetooth device that hosts the chip
The link protocol above the line is L CAP (Logical Link Control Adaptation Protocol)
It frames variable-length messages and provides reliability if needed
Many protocols use L CAP  such as the two utility protocols that are shown
The service discovery protocol is used to locate services within the network
The RFcomm (Radio Frequency communication) protocol emulates the standard serial port found on PCs for connecting the keyboard  mouse  and modem  among other devices
The top layer is where the applications are located
The profiles are represented by vertical boxes because they each define a slice of the protocol stack for a particular purpose
Specific profiles  such as the headset profile  usually contain only those protocols needed by that application and no others
For example  profiles may include L CAP if they have packets to send but skip L CAP if they have only a steady flow of audio samples
In the following tions  we will examine the Bluetooth radio layer and various link protocols  since these roughly correspond to the physical and MAC sublayers in the other procotol stacks we have studied
THE MEDIUM ACCESS CONTROL SUBLAYER
The Bluetooth Radio Layer The radio layer moves the bits from master to slave  or vice versa
It is a low-power system with a range of   meters operating in the same
To coexist with other networks using the ISM band  frequency hopping spread spectrum is used
There can be up to  hops/ over slots with a dwell time of Î¼
All the nodes in a piconet hop frequencies simultaneously  following the slot timing and pseudorandom hop sequence dictated by the master
Unfortunately  it turned out that early versions of Bluetooth and
interfered enough to ruin each otherâs transmissions
Some companies responded by banning Bluetooth altogether  but eventually a technical solution was devised
The solution is for Bluetooth to adapt its hop sequence to exclude channels on which there are other RF signals
This process reduces the harmful interference
It is called adaptive frequency hopping
Three forms of modulation are used to send bits on a channel
The basic scheme is to use frequency shift keying to send a  -bit symbol every microond  giving a gross data rate of  Mbps
Enhanced rates were introduced with the
version of Bluetooth
These rates use phase shift keying to send either  or  bits per symbol  for gross data rates of  or  Mbps
The enhanced rates are only used in the data portion of frames
The Bluetooth Link Layers The link control (or baseband) layer is the closest thing Bluetooth has to a MAC sublayer
It turns the raw bit stream into frames and defines some key formats
In the simplest form  the master in each piconet defines a series of   - Î¼ time slots  with the masterâs transmissions starting in the even slots and the slavesâ transmissions starting in the odd ones
This scheme is traditional time division multiplexing  with the master getting half the slots and the slaves sharing the other half
Frames can be  or  slots long
Each frame has an overhead of bits for an access code and header  plus a settling time of   â   Î¼ per hop to allow the inexpensive radio circuits to become stable
The payload of the frame can be encrypted for confidentiality with a key that is chosen when the master and slave connect
Hops only happen between frames  not during a frame
The result is that a  -slot frame is much more efficient than a  -slot frame because the overhead is constant but more data is sent
The link manager protocol sets up logical channels  called links  to carry frames between the master and a slave device that have discovered each other
A pairing procedure is followed to make sure that the two devices are allowed to communicate before the link is used
The old pairing method is that both devices must be configured with the same four-digit PIN (Personal Identification Number)
The matching PIN is how each device would know that it was connecting to   BLUETOOTH the right remote device
However  unimaginative users and devices default to PINs such as ââ   ââ and ââ   ââ meant that this method provided very little urity in practice
The new ure simple pairing method enables users to confirm that both devices are displaying the same passkey  or to observe the passkey on one device and enter it into the ond device
This method is more ure because users do not have to choose or set a PIN
They merely confirm a longer  device-generated passkey
Of course  it cannot be used on some devices with limited input/output  such as a hands-free headset
Once pairing is complete  the link manager protocol sets up the links
Two main kinds of links exist to carry user data
The first is the SCO (Synchronous Connection Oriented) link
It is used for real-time data  such as telephone connections
This type of link is allocated a fixed slot in each direction
A slave may have up to three SCO links with its master
Each SCO link can transmit one  -bps PCM audio channel
Due to the time-critical nature of SCO links  frames sent over them are never retransmitted
Instead  forward error correction can be used to increase reliability
The other kind is the ACL (Asynchronous ConnectionLess) link
This type of link is used for packet-switched data that is available at irregular intervals
ACL traffic is delivered on a best-effort basis
No guarantees are given
Frames can be lost and may have to be retransmitted
A slave may have only one ACL link to its master
The data sent over ACL links come from the L CAP layer
This layer has four major functions
First  it accepts packets of up to   KB from the upper layers and breaks them into frames for transmission
At the far end  the frames are reassembled into packets
ond  it handles the multiplexing and demultiplexing of multiple packet sources
When a packet has been reassembled  the L CAP layer determines which upper-layer protocol to hand it to  for example  RFcomm or service discovery
Third  L CAP handles error control and retransmission
It detects errors and resends packets that were not acknowledged
Finally  L CAP enforces quality of service requirements between multiple links
The Bluetooth Frame Structure Bluetooth defines several frame formats  the most important of which is shown in two forms in Fig
It begins with an access code that usually identifies the master so that slaves within radio range of two masters can tell which traffic is for them
Next comes a  -bit header containing typical MAC sublayer fields
If the frame is sent at the basic rate  the data field comes next
It has up to  bits for a five-slot transmission
For a single time slot  the format is the same except that the data field is bits
If the frame is sent at the enhanced rate  the data portion may have up to two or three times as many bits because each symbol carries  or  bits instead of  THE MEDIUM ACCESS CONTROL SUBLAYER
Repeated  times Bits â Access code Data (at  X rate)   Header (a) Basic rate data frame  top Access code Header Guard/Sync Data (at  X or  X rate) Trailer Bits  â  (b) Enhanced rate data frame  bottom  x micro slots Addr Type F A S CRC  Figure  -
Typical Bluetooth data frame at (a) basic and (b) enhanced  data rates
These data are preceded by a guard field and a synchronization pattern that is used to switch to the faster data rate
That is  the access code and header are carried at the basic rate and only the data portion is carried at the faster rate
Enhanced-rate frames end with a short trailer
Let us take a quick look at the common header
The Address field identifies which of the eight active devices the frame is intended for
The Type field identifies the frame type (ACL  SCO  poll  or null)  the type of error correction used in the data field  and how many slots long the frame is
The Flow bit is asserted by a slave when its buffer is full and cannot receive any more data
This bit enables a primitive form of flow control
The Acknowledgement bit is used to piggyback an ACK onto a frame
The Sequence bit is used to number the frames to detect retransmissions
The protocol is stop-and-wait  so  bit is enough
Then comes the  -bit header Checksum
The entire  -bit header is repeated three times to form the  -bit header shown in Fig
On the receiving side  a simple circuit examines all three copies of each bit
If all three are the same  the bit is accepted
If not  the majority opinion wins
Thus bits of transmission capacity are used to send   bits of header
The reason is that to reliably send data in a noisy environment using cheap  low-powered (
mW) devices with little computing capacity  a great deal of redundancy is needed
Various formats are used for the data field for ACL and SCO frames
The basic-rate SCO frames are a simple example to study: the data field is always bits
Three variants are defined  permitting  or bits of actual payload  with the rest being used for error correction
In the most reliable version (  -bit payload)  the contents are just repeated three times  the same as the header
We can work out the capacity with this frame as follows
Since the slave may use only the odd slots  it gets slots/  just as the master does
With an  -bit   BLUETOOTH payload  the channel capacity from the slave is   bps as is the channel capacity from the master
This capacity is exactly enough for a single full-duplex PCM voice channel (which is why a hop rate of  hops/ was chosen)
That is  despite a raw bandwidth of  Mbps  a single full-duplex uncompressed voice channel can completely saturate the piconet
The efficiency of  % is the result of spending  % of the capacity on settling time   % on headers  and  % on repetition coding
This shortcoming highlights the value of the enhanced rates and frames of more than a single slot
There is much more to be said about Bluetooth  but no more space to say it here
For the curious  the Bluetooth
specification contains all the details  RFID We have looked at MAC designs from LANs up to MANs and down to PANs
As a last example  we will study a category of low-end wireless devices that people may not recognize as forming a computer network: the RFID (Radio Frequency IDentification) tags and readers that we described in
RFID technology takes many forms  used in smartcards  implants for pets  passports  library books  and more
The form that we will look at was developed in the quest for an EPC (Electronic Product Code) that started with the Auto-ID Center at the Massachusetts Institute of Technology in
An EPC is a replacement for a barcode that can carry a larger amount of information and is electronically readable over distances up to   m  even when it is not visible
It is different technology than  for example  the RFID used in passports which must be placed quite close to a reader to perform a transaction
The ability to communicate over a distance makes EPCs more relevant to our studies
EPCglobal was formed in  to commercialize the RFID technology developed by the Auto-ID Center
The effort got a boost in  when Walmart required its top suppliers to label all shipments with RFID tags
Widespread deployment has been hampered by the difficulty of competing with cheap printed barcodes  but new uses  such as in drivers licenses  are now growing
We will describe the ond generation of this technology  which is informally called EPC Gen  (EPCglobal  )
EPC Gen  Architecture The architecture of an EPC Gen  RFID network is shown in Fig
It has two key components: tags and readers
RFID tags are small  inexpensive devices that have a unique  -bit EPC identifier and a small amount of memory that can be read and written by the RFID reader
The memory might be used to record the location history of an item  for example  as it moves through the supply chain
THE MEDIUM ACCESS CONTROL SUBLAYER
Often  the tags look like stickers that can be placed on  for example  pairs of jeans on the shelves in a store
Most of the sticker is taken up by an antenna that is printed onto it
A tiny dot in the middle is the RFID integrated circuit
Alternatively  the RFID tags can be integrated into an object  such as a driverâs license
In both cases  the tags have no battery and they must gather power from the radio transmissions of a nearby RFID reader to run
This kind of tag is called a ââClass  ââ tag to distinguish it from more capable tags that have batteries
RFID reader RFID tag Backscatter signal Reader signal Figure  -
RFID architecture
The readers are the intelligence in the system  analogous to base stations and access points in cellular and WiFi networks
Readers are much more powerful than tags
They have their own power sources  often have multiple antennas  and are in charge of when tags send and receive messages
As there will commonly be multiple tags within the reading range  the readers must solve the multiple access problem
There may be multiple readers that can contend with each other in the same area  too
The main job of the reader is to inventory the tags in the neighborhood  that is  to discover the identifiers of the nearby tags
The inventory is accomplished with the physical layer protocol and the tag-identification protocol that are outlined in the following tions
EPC Gen  Physical Layer The physical layer defines how bits are sent between the RFID reader and tags
Much of it uses methods for sending wireless signals that we have seen previously
transmissions are sent in the unlicensed   â   MHz ISM band
This band falls in the UHF (Ultra High Frequency) range  so the tags are referred to as UHF RFID tags
The reader performs frequency hopping at least every m to spread its signal across the channel  to limit interference and satisfy regulatory requirements
The reader and tags use forms of ASK (Amplitude Shift Keying) modulation that we described in
to encode bits
They take turns to send bits  so the link is half duplex
RFID There are two main differences from other physical layers that we have studied
The first is that the reader is always transmitting a signal  regardless of whether it is the reader or tag that is communicating
Naturally  the reader transmits a signal to send bits to tags
For the tags to send bits to the reader  the reader transmits a fixed carrier signal that carries no bits
The tags harvest this signal to get the power they need to run; otherwise  a tag would not be able to transmit in the first place
To send data  a tag changes whether it is reflecting the signal from the reader  like a radar signal bouncing off a target  or absorbing it
This method is called backscatter
It differs from all the other wireless situations we have seen so far  in which the sender and receiver never both transmit at the same time
Backscatter is a low-energy way for the tag to create a weak signal of its own that shows up at the reader
For the reader to decode the incoming signal  it must filter out the outgoing signal that it is transmitting
Because the tag signal is weak  tags can only send bits to the reader at a low rate  and tags cannot receive or even sense transmissions from other tags
The ond difference is that very simple forms of modulation are used so that they can be implemented on a tag that runs on very little power and costs only a few cents to make
To send data to the tags  the reader uses two amplitude levels
Bits are determined to be either a  or a   depending on how long the reader waits before a low-power period
The tag measures the time between low-power periods and compares this time to a reference measured during a preamble
As shown in Fig
Tag responses consist of the tag alternating its backscatter state at fixed intervals to create a series of pulses in the signal
Anywhere from one to eight pulse periods can be used to encode each  or   depending on the need for reliability
s have fewer transitions than  s  as is shown with an example of two-pulse period coding in Fig
Time Power Reader â â Reader â â Tag â â Tag â â Backscatter Figure  -
Reader and tag backscatter signals
EPC Gen  Tag Identification Layer To inventory the nearby tags  the reader needs to receive a message from each tag that gives the identifier for the tag
This situation is a multiple access problem for which the number of tags is unknown in the general case
The reader might THE MEDIUM ACCESS CONTROL SUBLAYER
broadcast a query to ask all tags to send their identifiers
However  tags that replied right away would then collide in much the same way as stations on a classic Ethernet
We have seen many ways of tackling the multiple access problem in this  ter
The closest protocol for the current situation  in which the tags cannot hear each othersâ transmissions  is slotted ALOHA  one of the earliest protocols we studied
This protocol is adapted for use in Gen  RFID
The sequence of messages used to identify a tag is shown in Fig
In the first slot (slot  )  the reader sends a Query message to start the process
Each QRepeat message advances to the next slot
The reader also tells the tags the range of slots over which to randomize transmissions
Using a range is necessary because the reader synchronizes tags when it starts the process; unlike stations on an Ethernet  tags do not wake up with a message at a time of their choosing
Time RFID tag Query (slot  ) RN  (slot  ) EPC identifier
QRepeat (slot ) Ack QRepeat (slot  ) QRepeat (slot N) QRepeat (slot  ) RFID reader Figure  -
Example message exchange to identify a tag
Tags pick a random slot in which to reply
However  tags do not send their identifiers when they first reply
Instead  a tag sends a short  -bit random number in an RN  message
If there is no collision  the reader receives this message and sends an ACK message of its own
At this stage  the tag has acquired the slot and sends its EPC identifier
The reason for this exchange is that EPC identifiers are long  so collisions on these messages would be expensive
Instead  a short exchange is used to test whether the tag can safely use the slot to send its identifier
Once its identifier has been successfully transmitted  the tag temporarily stops responding to new Query messages so that all the remaining tags can be identified
RFID A key problem is for the reader to adjust the number of slots to avoid collisions  but without using so many slots that performance suffers
This adjustment is analogous to binary exponential backoff in Ethernet
If the reader sees too many slots with no responses or too many slots with collisions  it can send a QAdjust message to decrease or increase the range of slots over which the tags are responding
The RFID reader can perform other operations on the tags
For example  it can select a subset of tags before running an inventory  allowing it to collect responses from  say  tagged jeans but not tagged shirts
The reader can also write data to tags as they are identified
This feature could be used to record the point of sale or other relevant information
Tag Identification Message Formats The format of the Query message is shown in Fig
The message is compact because the downlink rates are limited  from   kbps up to kbps
The Command field carries the code  to identify the message as a Query
Physical parameters Bits  Command  DR M TR Sel Session Target Q CRC Tag selection Figure  -
Format of the Query message
The next flags  DR  M  and TR  determine the physical layer parameters for reader transmissions and tag responses
For example  the response rate may be set to between  kbps and kbps
We will skip over the details of these flags
Then come three fields  Sel  Session  and Target  that select the tags to respond
As well as the readers being able to select a subset of identifiers  the tags keep track of up to four concurrent sessions and whether they have been identified in those sessions
In this way  multiple readers can operate in overlapping coverage areas by using different sessions
Next is the most important parameter for this command  Q
This field defines the range of slots over which tags will respond  from  to  Qâ
Finally  there is a CRC to protect the message fields
At  bits  it is shorter than most CRCs we have seen  but the Query message is much shorter than most packets too
Tag-to-reader messages are simpler
Since the reader is in control  it knows what message to expect in response to each of its transmissions
The tag responses simply carry data  such as the EPC identifier
THE MEDIUM ACCESS CONTROL SUBLAYER
Originally the tags were just for identification purposes
However  they have grown over time to resemble very small computers
Some research tags have sensors and are able to run small programs to gather and process data (Sample et al
One vision for this technology is the ââInternet of thingsââ that connects objects in the physical world to the Internet (Welbourne et al
; and Gershenfeld et al
)  DATA LINK LAYER SWITCHING Many organizations have multiple LANs and wish to connect them
Would it not be convenient if we could just join the LANs together to make a larger LAN? In fact  we can do this when the connections are made with devices called bridges
The Ethernet switches we described in
are a modern name for bridges; they provide functionality that goes beyond classic Ethernet and Ethernet hubs to make it easy to join multiple LANs into a larger and faster network
We shall use the terms ââbridgeââ and ââswitchââ interchangeably
Bridges operate in the data link layer  so they examine the data link layer addresses to forward frames
Since they are not supposed to examine the payload field of the frames they forward  they can handle IP packets as well as other kinds of packets  such as AppleTalk packets
In contrast  routers examine the addresses in packets and route based on them  so they only work with the protocols that they were designed to handle
In this tion  we will look at how bridges work and are used to join multiple physical LANs into a single logical LAN
We will also look at how to do the reverse and treat one physical LAN as multiple logical LANs  called VLANs (Virtual LANs)
Both technologies provide useful flexibility for managing networks
For a comprehensive treatment of bridges  switches  and related topics  see Seifert and Edwards (   ) and Perlman (   )
Uses of Bridges Before getting into the technology of bridges  let us take a look at some common situations in which bridges are used
We will mention three reasons why a single organization may end up with multiple LANs
First  many university and corporate departments have their own LANs to connect their own personal computers  servers  and devices such as printers
Since the goals of the various departments differ  different departments may set up different LANs  without regard to what other departments are doing
Sooner or later  though  there is a need for interaction  so bridges are needed
In this example  multiple LANs come into existence due to the autonomy of their owners
DATA LINK LAYER SWITCHING ond  the organization may be geographically spread over several buildings separated by considerable distances
It may be cheaper to have separate LANs in each building and connect them with bridges and a few long-distance fiber optic links than to run all the cables to a single central switch
Even if laying the cables is easy to do  there are limits on their lengths (
m for twisted-pair gigabit Ethernet)
The network would not work for longer cables due to the excessive signal attenuation or round-trip delay
The only solution is to partition the LAN and install bridges to join the pieces to increase the total physical distance that can be covered
Third  it may be necessary to split what is logically a single LAN into separate LANs (connected by bridges) to accommodate the load
At many large universities  for example  thousands of workstations are available for student and faculty computing
Companies may also have thousands of employees
The scale of this system precludes putting all the workstations on a single LANâthere are more computers than ports on any Ethernet hub and more stations than allowed on a single classic Ethernet
Even if it were possible to wire all the workstations together  putting more stations on an Ethernet hub or classic Ethernet would not add capacity
All of the stations share the same  fixed amount of bandwidth
The more stations there are  the less average bandwidth per station
However  two separate LANs have twice the capacity of a single LAN
Bridges let the LANs be joined together while keeping this capacity
The key is not to send traffic onto ports where it is not needed  so that each LAN can run at full speed
This behavior also increases reliability  since on a single LAN a defective node that keeps outputting a continuous stream of garbage can clog up the entire LAN
By deciding what to forward and what not to forward  bridges act like fire doors in a building  preventing a single node that has gone berserk from bringing down the entire system
To make these benefits easily available  ideally bridges should be completely transparent
It should be possible to go out and buy bridges  plug the LAN cables into the bridges  and have everything work perfectly  instantly
There should be no hardware changes required  no software changes required  no setting of address switches  no downloading of routing tables or parameters  nothing at all
Just plug in the cables and walk away
Furthermore  the operation of the existing LANs should not be affected by the bridges at all
As far as the stations are concerned  there should be no observable difference whether or not they are part of a bridged LAN
It should be as easy to move stations around the bridged LAN as it is to move them around a single LAN
Surprisingly enough  it is actually possible to create bridges that are transparent
Two algorithms are used: a backward learning algorithm to stop traffic being sent where it is not needed; and a spanning tree algorithm to break loops that may be formed when switches are cabled together willy-nilly
Let us now take a look at these algorithms in turn to learn how this magic is accomplished
THE MEDIUM ACCESS CONTROL SUBLAYER
Learning Bridges The topology of two LANs bridged together is shown in Fig
On the left-hand side  two multidrop LANs  such as classic Ethernets  are joined by a special stationâthe bridgeâthat sits on both LANs
On the right-hand side  LANs with point-to-point cables  including one hub  are joined together
The bridges are the devices to which the stations and hub are attached
If the LAN technology is Ethernet  the bridges are better known as Ethernet switches
(a) (b) A D Bridge B  Port B C E G F C Bridge B  B  A B G D H  Port    F E Hub Figure  -
(a) Bridge connecting two multidrop LANs
(b) Bridges (and a hub) connecting seven point-to-point stations
Bridges were developed when classic Ethernets were in use  so they are often shown in topologies with multidrop cables  as in Fig
However  all the topologies that are encountered today are comprised of point-to-point cables and switches
The bridges work the same way in both settings
All of the stations attached to the same port on a bridge belong to the same collision domain  and this is different than the collision domain for other ports
If there is more than one station  as in a classic Ethernet  a hub  or a half-duplex link  the CSMA/CD protocol is used to send frames
There is a difference  however  in how the bridged LANs are built
To bridge multidrop LANs  a bridge is added as a new station on each of the multidrop LANs  as in Fig
To bridge point-to-point LANs  the hubs are either connected to a bridge or  preferably  replaced with a bridge to increase performance
Different kinds of cables can also be attached to one bridge
For example  the cable connecting bridge B  to bridge B  in Fig
This arrangement is useful for bridging LANs in different buildings
Now let us consider what happens inside the bridges
Each bridge operates in promiscuous mode  that is  it accepts every frame transmitted by the stations   DATA LINK LAYER SWITCHING attached to each of its ports
The bridge must decide whether to forward or discard each frame  and  if the former  on which port to output the frame
This decision is made by using the destination address
As an example  consider the topology of Fig
If station A sends a frame to station B  bridge B  will receive the frame on port
This frame can be immediately discarded without further ado because it is already on the correct port
However  in the topology of Fig
Bridge B  will receive the frame on port  and output it on port
Bridge B  will then receive the frame on its port  and output it on its port
A simple way to implement this scheme is to have a big (hash) table inside the bridge
The table can list each possible destination and which output port it belongs on
For example  in Fig
That  in fact  more forwarding will happen later when the frame hits B  is not of interest to B
When the bridges are first plugged in  all the hash tables are empty
None of the bridges know where any of the destinations are  so they use a flooding algorithm: every incoming frame for an unknown destination is output on all the ports to which the bridge is connected except the one it arrived on
As time goes on  the bridges learn where destinations are
Once a destination is known  frames destined for it are put only on the proper port; they are not flooded
The algorithm used by the bridges is backward learning
As mentioned above  the bridges operate in promiscuous mode  so they see every frame sent on any of their ports
By looking at the source addresses  they can tell which machines are accessible on which ports
For example  if bridge B  in Fig
-  (b) sees a frame on port  coming from C  it knows that C must be reachable via port   so it makes an entry in its hash table
Any subsequent frame addressed to C coming in to B  on any other port will be forwarded to port
The topology can change as machines and bridges are powered up and down and moved around
To handle dynamic topologies  whenever a hash table entry is made  the arrival time of the frame is noted in the entry
Whenever a frame whose source is already in the table arrives  its entry is updated with the current time
Thus  the time associated with every entry tells the last time a frame from that machine was seen
Periodically  a process in the bridge scans the hash table and purges all entries more than a few minutes old
In this way  if a computer is unplugged from its LAN  moved around the building  and plugged in again somewhere else  within a few minutes it will be back in normal operation  without any manual intervention
This algorithm also means that if a machine is quiet for a few minutes  any traffic sent to it will have to be flooded until it next sends a frame itself
The routing procedure for an incoming frame depends on the port it arrives on (the source port) and the address to which it is destined (the destination address)
The procedure is as follows
THE MEDIUM ACCESS CONTROL SUBLAYER
If the port for the destination address is the same as the source port  discard the frame If the port for the destination address and the source port are different  forward the frame on to the destination port If the destination port is unknown  use flooding and send the frame on all ports except the source port
You might wonder whether the first case can occur with point-to-point links
The answer is that it can occur if hubs are used to connect a group of computers to a bridge
An example is shown in Fig
If E sends a frame to F  the hub will relay it to B  as well as to F
That is what hubs doâthey wire all ports together so that a frame input on one port is simply output on all other ports
The frame will arrive at B  on port   which is already the right output port to reach the destination
Bridge B  need only discard the frame
As each frame arrives  this algorithm must be applied  so it is usually implemented with special-purpose VLSI chips
The chips do the lookup and update the table entry  all in a few microonds
Because bridges only look at the MAC addresses to decide how to forward frames  it is possible to start forwarding as soon as the destination header field has come in  before the rest of the frame has arrived (provided the output line is available  of course)
This design reduces the latency of passing through the bridge  as well as the number of frames that the bridge must be able to buffer
It is referred to as cut-through switching or wormhole routing and is usually handled in hardware
We can look at the operation of a bridge in terms of protocol stacks to understand what it means to be a link layer device
Consider a frame sent from station A to station D in the configuration of Fig
The frame will pass through one bridge
The protocol stack view of processing is shown in Fig
The packet comes from a higher layer and descends into the Ethernet MAC layer
It acquires an Ethernet header (and also a trailer  not shown in the figure)
This unit is passed to the physical layer  goes out over the cable  and is picked up by the bridge
In the bridge  the frame is passed up from the physical layer to the Ethernet MAC layer
This layer has extended processing compared to the Ethernet MAC layer at a station
It passes the frame to a relay  still within the MAC layer
The bridge relay function uses only the Ethernet MAC header to determine how to handle the frame
In this case  it passes the frame to the Ethernet MAC layer of the port used to reach station D  and the frame continues on its way
In the general case  relays at a given layer can rewrite the headers for that layer
VLANs will provide an example shortly
In no case should the bridge look inside the frame and learn that it is carrying an IP packet; that is irrelevant to the   DATA LINK LAYER SWITCHING Eth Eth Packet Packet Packet Relay Network Ethernet MAC Physical Bridge Station A Station D Wire Wire Eth Eth Packet Packet Packet Eth Packet Eth Packet Eth Packet Eth Packet Figure  -
Protocol processing at a bridge
bridge processing and would violate protocol layering
Also note that a bridge with k ports will have k instances of MAC and physical layers
The value of k is  for our simple example
Spanning Tree Bridges To increase reliability  redundant links can be used between bridges
In the example of Fig
This design ensures that if one link is cut  the network will not be partitioned into two sets of computers that cannot talk to each other
Frame F  Bridge B  A B  Redundant links F  F  F  F  Figure  -
Bridges with two parallel links
However  this redundancy introduces some additional problems because it creates loops in the topology
An example of these problems can be seen by looking at how a frame sent by A to a previously unobserved destination is handled in Fig
Each bridge follows the normal rule for handling unknown destinations  which is to flood the frame
Call the frame from A that reaches bridge B  frame F
The bridge sends copies of this frame out all of its other ports
We THE MEDIUM ACCESS CONTROL SUBLAYER
will only consider the bridge ports that connect B  to B  (though the frame will be sent out the other ports  too)
Since there are two links from B  to B  two copies of the frame will reach B
They are shown in Fig
Shortly thereafter  bridge B  receives these frames
However  it does not (and cannot) know that they are copies of the same frame  rather than two different frames sent one after the other
So bridge B  takes F  and sends copies of it out all the other ports  and it also takes F  and sends copies of it out all the other ports
This produces frames F  and F  that are sent along the two links back to B
Bridge B  then sees two new frames with unknown destinations and copies them again
This cycle goes on forever
The solution to this difficulty is for the bridges to communicate with each other and overlay the actual topology with a spanning tree that reaches every bridge
In effect  some potential connections between bridges are ignored in the interest of constructing a fictitious loop-free topology that is a subset of the actual topology
For example  in Fig
Each station connects to only one bridge
There are some redundant connections between the bridges so that frames will be forwarded in loops if all of the links are used
This topology can be thought of as a graph in which the bridges are the nodes and the point-to-point links are the edges
The graph can be reduced to a spanning tree  which has no cycles by definition  by dropping the links shown as dashed lines in Fig
Using this spanning tree  there is exactly one path from every station to every other station
Once the bridges have agreed on the spanning tree  all forwarding between stations follows the spanning tree
Since there is a unique path from each source to each destination  loops are impossible
Bridge Station B  B  B  B  B  Link that is not part of the spanning tree Root bridge Figure  -
A spanning tree connecting five bridges
The dashed lines are links that are not part of the spanning tree
To build the spanning tree  the bridges run a distributed algorithm
Each bridge periodically broadcasts a configuration message out all of its ports to its   DATA LINK LAYER SWITCHING neighbors and processes the messages it receives from other bridges  as described next
These messages are not forwarded  since their purpose is to build the tree  which can then be used for forwarding
The bridges must first choose one bridge to be the root of the spanning tree
To make this choice  they each include an identifier based on their MAC address in the configuration message  as well as the identifier of the bridge they believe to be the root
MAC addresses are installed by the manufacturer and guaranteed to be unique worldwide  which makes these identifiers convenient and unique
The bridges choose the bridge with the lowest identifier to be the root
After enough messages have been exchanged to spread the news  all bridges will agree on which bridge is the root
Next  a tree of shortest paths from the root to every bridge is constructed
Bridge B  can be reached in two hops  via either B  or B
To break this tie  the path via the bridge with the lowest identifier is chosen  so B  is reached via B
Bridge B  can be reached in two hops via B
To find these shortest paths  bridges include the distance from the root in their configuration messages
Each bridge remembers the shortest path it finds to the root
The bridges then turn off ports that are not part of the shortest path
Although the tree spans all the bridges  not all the links (or even bridges) are necessarily present in the tree
This happens because turning off the ports prunes some links from the network to prevent loops
Even after the spanning tree has been established  the algorithm continues to run during normal operation to automatically detect topology changes and update the tree
The algorithm for constructing the spanning tree was invented by Radia Perlman
Her job was to solve the problem of joining LANs without loops
She was given a week to do it  but she came up with the idea for the spanning tree algorithm in a day
Fortunately  this left her enough time to write it as a poem (Perlman  ): I think that I shall never see A graph more lovely than a tree
A tree whose crucial property Is loop-free connectivity
A tree which must be sure to span
So packets can reach every LAN
First the Root must be selected By ID it is elected
Least cost paths from Root are traced In the tree these paths are placed
A mesh is made by folks like me Then bridges find a spanning tree
THE MEDIUM ACCESS CONTROL SUBLAYER
The spanning tree algorithm was then standardized as IEEE
D and used for many years
In  it was revised to more rapidly find a new spanning tree after a topology change
For a detailed treatment of bridges  see Perlman (   )
Repeaters  Hubs  Bridges  Switches  Routers  and Gateways So far in this book  we have looked at a variety of ways to get frames and packets from one computer to another
We have mentioned repeaters  hubs  bridges  switches  routers  and gateways
All of these devices are in common use  but they all differ in subtle and not-so-subtle ways
Since there are so many of them  it is probably worth taking a look at them together to see what the similarities and differences are
The key to understanding these devices is to realize that they operate in different layers  as illustrated in Fig
The layer matters because different devices use different pieces of information to decide how to switch
In a typical scenario  the user generates some data to be sent to a remote machine
Those data are passed to the transport layer  which then adds a header (for example  a TCP header) and passes the resulting unit down to the network layer
The network layer adds its own header to form a network layer packet (
an IP packet)
Then the packet goes to the data link layer  which adds its own header and checksum (CRC) and gives the resulting frame to the physical layer for transmission  for example  over a LAN
Application layer Application gateway Transport layer Transport gateway Network layer Router Frame header Packet header TCP header Packet (supplied by network layer) Frame (built by data link layer) (a) (b) User data CRC Data link layer Bridge  switch Physical layer Repeater  hub Figure  -
(a) Which device is in which layer
(b) Frames  packets  and headers
Now let us look at the switching devices and see how they relate to the packets and frames
At the bottom  in the physical layer  we find the repeaters
These are analog devices that work with signals on the cables to which they are connected
A signal appearing on one cable is cleaned up  amplified  and put out on another cable
Repeaters do not understand frames  packets  or headers
They understand the symbols that encode bits as volts
Classic Ethernet  for example  was   DATA LINK LAYER SWITCHING designed to allow four repeaters that would boost the signal to extend the maximum cable length from meters to  meters
Next we come to the hubs
A hub has a number of input lines that it joins electrically
Frames arriving on any of the lines are sent out on all the others
If two frames arrive at the same time  they will collide  just as on a coaxial cable
All the lines coming into a hub must operate at the same speed
Hubs differ from repeaters in that they do not (usually) amplify the incoming signals and are designed for multiple input lines  but the differences are slight
Like repeaters  hubs are physical layer devices that do not examine the link layer addresses or use them in any way
Now let us move up to the data link layer  where we find bridges and switches
We just studied bridges at some length
A bridge connects two or more LANs
Like a hub  a modern bridge has multiple ports  usually enough for  to   input lines of a certain type
Unlike in a hub  each port is isolated to be its own collision domain; if the port has a full-duplex point-to-point line  the CSMA/CD algorithm is not needed
When a frame arrives  the bridge extracts the destination address from the frame header and looks it up in a table to see where to send the frame
For Ethernet  this address is the  -bit destination address shown in Fig
The bridge only outputs the frame on the port where it is needed and can forward multiple frames at the same time
Bridges offer much better performance than hubs  and the isolation between bridge ports also means that the input lines may run at different speeds  possibly even with different network types
A common example is a bridge with ports that connect to  - -  and -Mbps Ethernet
Buffering within the bridge is needed to accept a frame on one port and transmit the frame out on a different port
If frames come in faster than they can be retransmitted  the bridge may run out of buffer space and have to start discarding frames
For example  if a gigabit Ethernet is pouring bits into a  -Mbps Ethernet at top speed  the bridge will have to buffer them  hoping not to run out of memory
This problem still exists even if all the ports run at the same speed because more than one port may be sending frames to a given destination port
Bridges were originally intended to be able to join different kinds of LANs  for example  an Ethernet and a Token Ring LAN
However  this never worked well because of differences between the LANs
Different frame formats require copying and reformatting  which takes CPU time  requires a new checksum calculation  and introduces the possibility of undetected errors due to bad bits in the bridgeâs memory
Different maximum frame lengths are also a serious problem with no good solution
Basically  frames that are too large to be forwarded must be discarded
So much for transparency
Two other areas where LANs can differ are urity and quality of service
Some LANs have link-layer encryption  for example
and some do not  for example Ethernet
Some LANs have quality of service features such as priorities  for example
and some do not  for example Ethernet
Consequently  when THE MEDIUM ACCESS CONTROL SUBLAYER
a frame must travel between these LANs  the urity or quality of service expected by the sender may not be able to be provided
For all of these reasons  modern bridges usually work for one network type  and routers  which we will come to soon  are used instead to join networks of different types
Switches are modern bridges by another name
The differences are more to do with marketing than technical issues  but there are a few points worth knowing
Bridges were developed when classic Ethernet was in use  so they tend to join relatively few LANs and thus have relatively few ports
The term ââswitchââ is more popular nowadays
Also  modern installations all use point-to-point links  such as twisted-pair cables  so individual computers plug directly into a switch and thus the switch will tend to have many ports
Finally  ââswitchââ is also used as a general term
With a bridge  the functionality is clear
On the other hand  a switch may refer to an Ethernet switch or a completely different kind of device that makes forwarding decisions  such as a telephone switch
So far  we have seen repeaters and hubs  which are actually quite similar  as well as bridges and switches  which are even more similar to each other
Now we move up to routers  which are different from all of the above
When a packet comes into a router  the frame header and trailer are stripped off and the packet located in the frameâs payload field (shaded in Fig
-  ) is passed to the routing software
This software uses the packet header to choose an output line
For an IP packet  the packet header will contain a  -bit (IPv ) or   -bit (IPv ) address  but not a  -bit IEEE address
The routing software does not see the frame addresses and does not even know whether the packet came in on a LAN or a point-to-point line
We will study routers and routing in   Up another layer  we find transport gateways
These connect two computers that use different connection-oriented transport protocols
For example  suppose a computer using the connection-oriented TCP/IP protocol needs to talk to a computer using a different connection-oriented transport protocol called SCTP
The transport gateway can copy the packets from one connection to the other  reformatting them as need be
Finally  application gateways understand the format and contents of the data and can translate messages from one format to another
An email gateway could translate Internet messages into SMS messages for mobile phones  for example
Like ââswitch ââ ââgatewayââ is somewhat of a general term
It refers to a forwarding process that runs at a high layer
Virtual LANs In the early days of local area networking  thick yellow cables snaked through the cable ducts of many office buildings
Every computer they passed was plugged in
No thought was given to which computer belonged on which LAN
All the people in adjacent offices were put on the same LAN  whether they belonged together or not
Geography trumped corporate organization charts
DATA LINK LAYER SWITCHING With the advent of twisted pair and hubs in the s  all that changed
Buildings were rewired (at considerable expense) to rip out all the yellow garden hoses and install twisted pairs from every office to central wiring closets at the end of each corridor or in a central machine room  as illustrated in Fig
If the Vice President in Charge of Wiring was a visionary  Category  twisted pairs were installed; if he was a bean counter  the existing (Category  ) telephone wiring was used (only to be replaced a few years later  when fast Ethernet emerged)
Twisted pair to a hub Office Switch Hub Hub Corridor Cable duct Figure  -
A building with centralized wiring using hubs and a switch
Today  the cables have changed and hubs have become switches  but the wiring pattern is still the same
This pattern makes it possible to configure LANs logically rather than physically
For example  if a company wants k LANs  it could buy k switches
By carefully choosing which connectors to plug into which switches  the occupants of a LAN can be chosen in a way that makes organizational sense  without too much regard to geography
Does it matter who is on which LAN? After all  in nearly all organizations  all the LANs are interconnected
In short  yes  it often matters
Network administrators like to group users on LANs to reflect the organizational structure rather than the physical layout of the building  for a variety of reasons
One issue is urity
One LAN might host Web servers and other computers intended for public use
Another LAN might host computers containing the records of the Human Resources department that are not to be passed outside of the department
In such a situation  putting all the computers on a single LAN and not letting any of the servers be accessed from off the LAN makes sense
Management tends to frown when hearing that such an arrangement is impossible
THE MEDIUM ACCESS CONTROL SUBLAYER
A ond issue is load
Some LANs are more heavily used than others and it may be desirable to separate them
For example  if the folks in research are running all kinds of nifty experiments that sometimes get out of hand and saturate their LAN  the folks in management may not be enthusiastic about donating some of the capacity they were using for videoconferencing to help out
Then again  this might impress on management the need to install a faster network
A third issue is broadcast traffic
Bridges broadcast traffic when the location of the destination is unknown  and upper-layer protocols use broadcasting as well
For example  when a user wants to send a packet to an IP address x  how does it know which MAC address to put in the frame? We will study this question in
but briefly summarized  the answer is that it broadcasts a frame containing the question ââwho owns IP address x?ââ Then it waits for an answer
As the number of computers in a LAN grows  so does the number of broadcasts
Each broadcast consumes more of the LAN capacity than a regular frame because it is delivered to every computer on the LAN
By keeping LANs no larger than they need to be  the impact of broadcast traffic is reduced
Related to broadcasts is the problem that once in a while a network interface will break down or be misconfigured and begin generating an endless stream of broadcast frames
If the network is really unlucky  some of these frames will elicit responses that lead to ever more traffic
The result of this broadcast storm is that ( ) the entire LAN capacity is occupied by these frames  and ( ) all the machines on all the interconnected LANs are crippled just processing and discarding all the frames being broadcast
At first it might appear that broadcast storms could be limited in scope by separating the LANs with bridges or switches  but if the goal is to achieve transparency (
a machine can be moved to a different LAN across the bridge without anyone noticing it)  then bridges have to forward broadcast frames
Having seen why companies might want multiple LANs with restricted scopes  let us get back to the problem of decoupling the logical topology from the physical topology
Building a physical topology to reflect the organizational structure can add work and cost  even with centralized wiring and switches
For example  if two people in the same department work in different buildings  it may be easier to wire them to different switches that are part of different LANs
Even if this is not the case  a user might be shifted within the company from one department to another without changing offices  or might change offices without changing departments
This might result in the user being on the wrong LAN until an administrator changes the userâs connector from one switch to another
Furthermore  the number of computers that belong to different departments may not be a good match for the number of ports on switches; some departments may be too small and others so big that they require multiple switches
This results in wasted switch ports that are not used
In many companies  organizational changes occur all the time  meaning that system administrators spend a lot of time pulling out plugs and pushing them back   DATA LINK LAYER SWITCHING in somewhere else
Also  in some cases  the change cannot be made at all because the twisted pair from the userâs machine is too far from the correct switch (
in the wrong building)  or the available switch ports are on the wrong LAN
In response to customer requests for more flexibility  network vendors began working on a way to rewire buildings entirely in software
The resulting concept is called a VLAN (Virtual LAN)
It has been standardized by the IEEE committee and is now widely deployed in many organizations
Let us now take a look at it
For additional information about VLANs  see Seifert and Edwards (   )
VLANs are based on VLAN-aware switches
To set up a VLAN-based network  the network administrator decides how many VLANs there will be  which computers will be on which VLAN  and what the VLANs will be called
Often the VLANs are (informally) named by colors  since it is then possible to print color diagrams showing the physical layout of the machines  with the members of the red LAN in red  members of the green LAN in green  and so on
In this way  both the physical and logical layouts are visible in a single view
As an example  consider the bridged LAN of Fig
Machines from the gray VLAN are spread across two switches  including two machines that connect to a switch via a hub
Gray station B  B  Hub G W W GW G G G GW G G G G W W White station Gray port White port Gray and White port Bridge Figure  -
Two VLANs  gray and white  on a bridged LAN
To make the VLANs function correctly  configuration tables have to be set up in the bridges
These tables tell which VLANs are accessible via which ports
When a frame comes in from  say  the gray VLAN  it must be forwarded on all the ports marked with a G
This holds for ordinary (
unicast) traffic for which the bridges have not learned the location of the destination  as well as for multicast and broadcast traffic
Note that a port may be labeled with multiple VLAN colors
As an example  suppose that one of the gray stations plugged into bridge B  in Fig
Bridge B  will receive the frame and see that it came from a machine on the gray THE MEDIUM ACCESS CONTROL SUBLAYER
VLAN  so it will flood the frame on all ports labeled G (except the incoming port)
The frame will be sent to the five other gray stations attached to B  as well as over the link from B  to bridge B
At bridge B  the frame is similarly forwarded on all ports labeled G
This sends the frame to one further station and the hub (which will transmit the frame to all of its stations)
The hub has both labels because it connects to machines from both VLANs
The frame is not sent on other ports without G in the label because the bridge knows that there are no machines on the gray VLAN that can be reached via these ports
In our example  the frame is only sent from bridge B  to bridge B  because there are machines on the gray VLAN that are connected to B
Looking at the white VLAN  we can see that the bridge B  port that connects to bridge B  is not labeled W
This means that a frame on the white VLAN will not be forwarded from bridge B  to bridge B
This behavior is correct because no stations on the white VLAN are connected to B
Q Standard To implement this scheme  bridges need to know to which VLAN an incoming frame belongs
Without this information  for example  when bridge B  gets a frame from bridge B  in Fig
If we were designing a new type of LAN  it would be easy enough to just add a VLAN field in the header
But what to do about Ethernet  which is the dominant LAN  and did not have any spare fields lying around for the VLAN identifier? The IEEE committee had this problem thrown into its lap in
After much discussion  it did the unthinkable and changed the Ethernet header
The new format was published in IEEE standard
Q  issued in
The new format contains a VLAN tag; we will examine it shortly
Not surprisingly  changing something as well established as the Ethernet header was not entirely trivial
A few questions that come to mind are:
Need we throw out several hundred million existing Ethernet cards?
If not  who generates the new fields?
What happens to frames that are already the maximum size? Of course  the committee was (only too painfully) aware of these problems and had to come up with solutions  which it did
The key to the solution is to realize that the VLAN fields are only actually used by the bridges and switches and not by the user machines
Thus  in Fig
Also  to use VLANs  the bridges have to be VLAN aware
This fact makes the design feasible
DATA LINK LAYER SWITCHING As to throwing out all existing Ethernet cards  the answer is no
Remember that the
committee could not even get people to change the Type field into a Length field
You can imagine the reaction to an announcement that all existing Ethernet cards had to be thrown out
However  new Ethernet cards are
Q compliant and can correctly fill in the VLAN fields
Because there can be computers (and switches) that are not VLAN aware  the first VLAN-aware bridge to touch a frame adds VLAN fields and the last one down the road removes them
An example of a mixed topology is shown in Fig
In this figure  VLAN-aware computers generate tagged (  Q) frames directly  and further switching uses these tags
The shaded symbols are VLAN-aware and the empty ones are not
Legacy bridge and host B  B  B  Tagged frame B  B  B  VLAN-aware host and bridge Legacy frame Figure  -
Bridged LAN that is only partly VLAN aware
The shaded symbols are VLAN aware
The empty ones are not
Q  frames are colored depending on the port on which they are received
For this method to work  all machines on a port must belong to the same VLAN  which reduces flexibility
For example  in Fig
Additionally  the bridge can use the higher-layer protocol to select the color
In this way  frames arriving on a port might be placed in different VLANs depending on whether they carry IP packets or PPP frames
Other methods are possible  but they are not supported by
As one example  the MAC address can be used to select the VLAN color
This might be useful for frames coming in from a nearby
LAN in which laptops send frames via different ports as they move
One MAC address would then be mapped to a fixed VLAN regardless of which port it entered the LAN on
As to the problem of frames longer than  bytes
Q just raised the limit to  bytes
Luckily  only VLAN-aware computers and switches must support these longer frames
Now let us take a look at the
Q frame format
It is shown in Fig
The only change is the addition of a pair of  -byte fields
The first one is the THE MEDIUM ACCESS CONTROL SUBLAYER
VLAN protocol ID
It always has the value  x
Since this number is greater than  all Ethernet cards interpret it as a type rather than a length
What a legacy card does with such a frame is moot since such frames are not supposed to be sent to legacy cards
Length Data Pad Checksum Destination address Source address
Q Tag Length Data Pad VLAN protocol VLAN Identifier ID ( x   ) Pri CFI Checksum Destination address Source address Figure  -
(legacy) and
Q Ethernet frame formats
The ond  -byte field contains three subfields
The main one is the VLAN identifier  occupying the low-order   bits
This is what the whole thing is aboutâthe color of the VLAN to which the frame belongs
The  -bit Priority field has nothing to do with VLANs at all  but since changing the Ethernet header is a once-in-a-decade event taking three years and featuring a hundred people  why not put in some other good things while you are at it? This field makes it possible to distinguish hard real-time traffic from soft real-time traffic from timeinsensitive traffic in order to provide better quality of service over Ethernet
It is needed for voice over Ethernet (although in all fairness  IP has had a similar field for a quarter of a century and nobody ever used it)
The last field  CFI (Canonical format indicator)  should have been called the CEI (Corporate ego indicator)
It was originally intended to indicate the order of the bits in the MAC addresses (little-endian versus big-endian)  but that use got lost in other controversies
Its presence now indicates that the payload contains a freeze-dried
frame that is hoping to find another
LAN at the destination while being carried by Ethernet in between
This whole arrangement  of course  has nothing whatsoever to do with VLANs
But standardsâ committee politics are not unlike regular politics: if you vote for my bit  I will vote for your bit
As we mentioned above  when a tagged frame arrives at a VLAN-aware switch  the switch uses the VLAN identifier as an index into a table to find out which ports to send it on
But where does the table come from? If it is manually constructed  we are back to square zero: manual configuration of bridges
The beauty of the transparent bridge is that it is plug-and-play and does not require any manual configuration
It would be a terrible shame to lose that property
Fortunately  VLAN-aware bridges can also autoconfigure themselves based on observing the tags that come by
If a frame tagged as VLAN  comes in on port   DATA LINK LAYER SWITCHING   apparently some machine on port  is on VLAN
Q standard explains how to build the tables dynamically  mostly by referencing appropriate portions of the
D standard
Before leaving the subject of VLAN routing  it is worth making one last observation
Many people in the Internet and Ethernet worlds are fanatically in favor of connectionless networking and violently opposed to anything smacking of connections in the data link or network layers
Yet VLANs introduce something that is surprisingly similar to a connection
To use VLANs properly  each frame carries a new special identifier that is used as an index into a table inside the switch to look up where the frame is supposed to be sent
That is precisely what happens in connection-oriented networks
In connectionless networks  it is the destination address that is used for routing  not some kind of connection identifier
We will see more of this creeping connectionism in
SUMMARY Some networks have a single channel that is used for all communication
In these networks  the key design issue is the allocation of this channel among the competing stations wishing to use it
FDM and TDM are simple  efficient allocation schemes when the number of stations is small and fixed and the traffic is continuous
Both are widely used under these circumstances  for example  for dividing up the bandwidth on telephone trunks
However  when the number of stations is large and variable or the traffic is fairly burstyâthe common case in computer networksâFDM and TDM are poor choices
Numerous dynamic channel allocation algorithms have been devised
The ALOHA protocol  with and without slotting  is used in many derivatives in real systems  for example  cable modems and RFID
As an improvement when the state of the channel can be sensed  stations can avoid starting a transmission while another station is transmitting
This technique  carrier sensing  has led to a variety of CSMA protocols for LANs and MANs
It is the basis for classic Ethernet and
A class of protocols that eliminates contention altogether  or at least reduces it considerably  is well known
The bitmap protocol  topologies such as rings  and the binary countdown protocol completely eliminate contention
The tree walk protocol reduces it by dynamically dividing the stations into two disjoint groups of different sizes and allowing contention only within one group; ideally that group is chosen so that only one station is ready to send when it is permitted to do so
Wireless LANs have the added problems that it is difficult to sense colliding transmissions  and that the coverage regions of stations may differ
In the dominant wireless LAN  IEEE
stations use CSMA/CA to mitigate the first problem by leaving small gaps to avoid collisions
The stations can also use the RTS/CTS protocol to combat hidden terminals that arise because of the ond THE MEDIUM ACCESS CONTROL SUBLAYER
is commonly used to connect laptops and other devices to wireless access points  but it can also be used between devices
Any of several physical layers can be used  including multichannel FDM with and without multiple antennas  and spread spectrum
RFID readers and tags use a random access protocol to communicate identifiers
Other wireless PANs and MANs have different designs
The Bluetooth system connects headsets and many kinds of peripherals to computers without wires
provides a wide area wireless Internet data service for stationary and mobile computers
Both of these networks use a centralized  connection-oriented design in which the Bluetooth master and the WiMAX base station decide when each station may send or receive data
this design supports different quality of service for real-time traffic like telephone calls and interactive traffic like Web browsing
For Bluetooth  placing the complexity in the master leads to inexpensive slave devices
Ethernet is the dominant form of wired LAN
Classic Ethernet used CSMA/CD for channel allocation on a yellow cable the size of a garden hose that snaked from machine to machine
The architecture has changed as speeds have risen from   Mbps to   Gbps and continue to climb
Now  point-to-point links such as twisted pair are attached to hubs and switches
With modern switches and full-duplex links  there is no contention on the links and the switch can forward frames between different ports in parallel
With buildings full of LANs  a way is needed to interconnect them all
Plugand- play bridges are used for this purpose
The bridges are built with a backward learning algorithm and a spanning tree algorithm
Since this functionality is built into modern switches  the terms ââbridgeââ and ââswitchââ are used interchangeably
To help with the management of bridged LANs  VLANs let the physical topology be divided into different logical topologies
The VLAN standard  IEEE
Q  introduces a new format for Ethernet frames
For this problem  use a formula from this  ter  but first state the formula
Frames arrive randomly at a   -Mbps channel for transmission
If the channel is busy when a frame arrives  it waits its turn in a queue
Frame length is exponentially distributed with a mean of   bits/frame
For each of the following frame arrival rates  give the delay experienced by the average frame  including both queueing time and transmission time
(a)   frames/
(b) frames/
(c)  frames/  PROBLEMS
A group of N stations share a  -kbps pure ALOHA channel
Each station outputs a -bit frame on average once every   even if the previous one has not yet been sent (
the stations can buffer outgoing frames)
What is the maximum value of N?
Consider the delay of pure ALOHA versus slotted ALOHA at low load
Which one is less? Explain your answer A large population of ALOHA users manages to generate   requests/  including both originals and retransmissions
Time is slotted in units of   m
(a) What is the chance of success on the first attempt? (b) What is the probability of exactly k collisions and then a success? (c) What is the expected number of transmission attempts needed?
In an infinite-population slotted ALOHA system  the mean number of slots a station waits between a collision and a retransmission is
Plot the delay versus throughput curve for this system What is the length of a contention slot in CSMA/CD for (a) a  -km twin-lead cable (signal propagation speed is  % of the signal propagation speed in vacuum)?  and (b) a  -km multimode fiber optic cable (signal propagation speed is  % of the signal propagation speed in vacuum)?
How long does a station  s  have to wait in the worst case before it can start transmitting its frame over a LAN that uses the basic bit-map protocol?
In the binary countdown protocol  explain how a lower-numbered station may be starved from sending a packet Sixteen stations  numbered  through are contending for the use of a shared channel by using the adaptive tree walk protocol
If all the stations whose addresses are prime numbers suddenly become ready at once  how many bit slots are needed to resolve the contention?
Consider five wireless stations  A  B  C  D  and E
Station A can communicate with all other stations
B can communicate with A  C and E
C can communicate with A  B and D
D can communicate with A  C and E
E can communicate A  D and B
(a) When A is sending to B  what other communications are possible? (b) When B is sending to A  what other communications are possible? (c) When B is sending to C  what other communications are possible?
Six stations  A through F  communicate using the MACA protocol
Is it possible for two transmissions to take place simultaneously? Explain your answer A seven-story office building has   adjacent offices per floor
Each office contains a wall socket for a terminal in the front wall  so the sockets form a rectangular grid in the vertical plane  with a separation of  m between sockets  both horizontally and vertically
Assuming that it is feasible to run a straight cable between any pair of sockets  horizontally  vertically  or diagonally  how many meters of cable are needed to connect all sockets using (a) A star configuration with a single router in the middle? (b) A classic
LAN? THE MEDIUM ACCESS CONTROL SUBLAYER   What is the baud rate of classic  -Mbps Ethernet?
Sketch the Manchester encoding on a classic Ethernet for the bit stream   A  -km-long   -Mbps CSMA/CD LAN (not
) has a propagation speed of m/Î¼
Repeaters are not allowed in this system
Data frames are bits long  including   bits of header  checksum  and other overhead
The first bit slot after a successful transmission is reserved for the receiver to capture the channel in order to send a  -bit acknowledgement frame
What is the effective data rate  excluding overhead  assuming that there are no collisions?
Two CSMA/CD stations are each trying to transmit long (multiframe) files
After each frame is sent  they contend for the channel  using the binary exponential backoff algorithm
What is the probability that the contention ends on round k  and what is the mean number of rounds per contention period?
An IP packet to be transmitted by Ethernet is   bytes long  including all its headers
If LLC is not in use  is padding needed in the Ethernet frame  and if so  how many bytes?
Ethernet frames must be at least   bytes long to ensure that the transmitter is still going in the event of a collision at the far end of the cable
Fast Ethernet has the same  -byte minimum frame size but can get the bits out ten times faster
How is it possible to maintain the same minimum frame size?
Some books quote the maximum size of an Ethernet frame as  bytes instead of  bytes
Are they wrong? Explain your answer How many frames per ond can gigabit Ethernet handle? Think carefully and take into account all the relevant cases
Hint: the fact that it is gigabit Ethernet matters Name two networks that allow frames to be packed back-to-back
Why is this feature worth having?
-   four stations  A  B  C  and D  are shown
Which of the last two stations do you think is closest to A and why?
Give an example to show that the RTS/CTS in the
protocol is a little different than in the MACA protocol A wireless LAN with one AP has   client stations
Four stations have data rates of  Mbps  four stations have data rates of   Mbps  and the last two stations have data rates of   Mbps
What is the data rate experienced by each station when all ten stations are sending data together  and (a) TXOP is not used? (b) TXOP is used?
Suppose that an  -Mbps
b LAN is transmitting  -byte frames back-to-back over a radio channel with a bit error rate of  â
How many frames per ond will be damaged on average?
network has a channel width of   MHz
How many bits/ can be sent to a subscriber station?
Give two reasons why networks might use an error-correcting code instead of error detection and retransmission List two ways in which WiMAX is similar to
and two ways in which it is different from
Is there any reason why one device cannot be the master in both of them at the same time?
What is the maximum size of the data field for a  -slot Bluetooth frame at basic rate? Explain your answer Figure  -  shows several physical layer protocols
Which of these is closest to the Bluetooth physical layer protocol? What is the biggest difference between the two?
It is mentioned in tion    that the efficiency of a  -slot frame with repetition encoding is about  % at basic data rate
What will the efficiency be if a  -slot frame with repetition encoding is used at basic data rate instead?
Beacon frames in the frequency hopping spread spectrum variant of
contain the dwell time
Do you think the analogous beacon frames in Bluetooth also contain the dwell time? Discuss your answer Suppose that there are   RFID tags around an RFID reader
What is the best value of Q? How likely is it that one tag responds with no collision in a given slot?
List some of the urity concerns of an RFID system A switch designed for use with fast Ethernet has a backplane that can move   Gbps
How many frames/ can it handle in the worst case?
Briefly describe the difference between store-and-forward and cut-through switches Consider the extended LAN connected using bridges B  and B  in Fig
Suppose the hash tables in the two bridges are empty
List all ports on which a packet will be forwarded for the following sequence of data transmissions: (a) A sends a packet to C
(b) E sends a packet to F
(c) F sends a packet to E
(d) G sends a packet to E
(e) D sends a packet to A
(f) B sends a packet to F Store-and-forward switches have an advantage over cut-through switches with respect to damaged frames
Explain what it is It is mentioned in tion    that some bridges may not even be present in the spanning tree
Outline a scenario where a bridge may not be present in the spanning tree To make VLANs work  configuration tables are needed in the bridges
What if the VLANs of Fig
Would it be possible to use a legacy switch there? If so  how would that work? If not  why not?
Write a program to simulate the behavior of the CSMA/CD protocol over Ethernet when there are N stations ready to transmit while a frame is being transmitted
Your program should report the times when each station successfully starts sending its frame
Assume that a clock tick occurs once every slot time (
Î¼) and a collision detection and sending of a jamming sequence takes one slot time
All frames are the maximum length allowed
THE NETWORK LAYER The network layer is concerned with getting packets from the source all the way to the destination
Getting to the destination may require making many hops at intermediate routers along the way
This function clearly contrasts with that of the data link layer  which has the more modest goal of just moving frames from one end of a wire to the other
Thus  the network layer is the lowest layer that deals with end-to-end transmission
To achieve its goals  the network layer must know about the topology of the network (
the set of all routers and links) and choose appropriate paths through it  even for large networks
It must also take care when choosing routes to avoid overloading some of the communication lines and routers while leaving others idle
Finally  when the source and destination are in different networks  new problems occur
It is up to the network layer to deal with them
In this  ter we will study all these issues and illustrate them  primarily using the Internet and its network layer protocol  IP  NETWORK LAYER DESIGN ISSUES In the following tions  we will give an introduction to some of the issues that the designers of the network layer must grapple with
These issues include the service provided to the transport layer and the internal design of the network
THE NETWORK LAYER
Store-and-Forward Packet Switching Before starting to explain the details of the network layer  it is worth restating the context in which the network layer protocols operate
This context can be seen in Fig
The major components of the network are the ISPâs equipment (routers connected by transmission lines)  shown inside the shaded oval  and the customersâ equipment  shown outside the oval
Host H  is directly connected to one of the ISPâs routers  A  perhaps as a home computer that is plugged into a DSL modem
In contrast  H  is on a LAN  which might be an office Ethernet  with a router  F  owned and operated by the customer
This router has a leased line to the ISPâs equipment
We have shown F as being outside the oval because it does not belong to the ISP
For the purposes of this  ter  however  routers on customer premises are considered part of the ISP network because they run the same algorithms as the ISPâs routers (and our main concern here is algorithms)
D C B A E F Packet Process P  Host H  Router ISPâs equipment LAN H  P  Figure  -
The environment of the network layer protocols
This equipment is used as follows
A host with a packet to send transmits it to the nearest router  either on its own LAN or over a point-to-point link to the ISP
The packet is stored there until it has fully arrived and the link has finished its processing by verifying the checksum
Then it is forwarded to the next router along the path until it reaches the destination host  where it is delivered
This mechanism is store-and-forward packet switching  as we have seen in previous  ters
Services Provided to the Transport Layer The network layer provides services to the transport layer at the network layer/transport layer interface
An important question is precisely what kind of services the network layer provides to the transport layer
The services need to be carefully designed with the following goals in mind:   NETWORK LAYER DESIGN ISSUES
The services should be independent of the router technology The transport layer should be shielded from the number  type  and topology of the routers present The network addresses made available to the transport layer should use a uniform numbering plan  even across LANs and WANs
Given these goals  the designers of the network layer have a lot of freedom in writing detailed specifications of the services to be offered to the transport layer
This freedom often degenerates into a raging battle between two warring factions
The discussion centers on whether the network layer should provide connectionoriented service or connectionless service
One camp (represented by the Internet community) argues that the routersâ job is moving packets around and nothing else
In this view (based on   years of experience with a real computer network)  the network is inherently unreliable  no matter how it is designed
Therefore  the hosts should accept this fact and do error control (
error detection and correction) and flow control themselves
This viewpoint leads to the conclusion that the network service should be connectionless  with primitives SEND PACKET and RECEIVE PACKET and little else
In particular  no packet ordering and flow control should be done  because the hosts are going to do that anyway and there is usually little to be gained by doing it twice
This reasoning is an example of the end-to-end argument  a design principle that has been very influential in shaping the Internet (Saltzer et al
Furthermore  each packet must carry the full destination address  because each packet sent is carried independently of its predecessors  if any
The other camp (represented by the telephone companies) argues that the network should provide a reliable  connection-oriented service
They claim that years of successful experience with the worldwide telephone system is an excellent guide
In this view  quality of service is the dominant factor  and without connections in the network  quality of service is very difficult to achieve  especially for real-time traffic such as voice and video
Even after several decades  this controversy is still very much alive
Early  widely used data networks  such as X
in the s and its successor Frame Relay in the s  were connection-oriented
However  since the days of the ARPANET and the early Internet  connectionless network layers have grown tremendously in popularity
The IP protocol is now an ever-present symbol of success
It was undeterred by a connection-oriented technology called ATM that was developed to overthrow it in the s; instead  it is ATM that is now found in niche uses and IP that is taking over telephone networks
Under the covers  however  the Internet is evolving connection-oriented features as quality of service becomes more important
Two examples of connection-oriented technologies are MPLS (MultiProtocol Label Switching)  which we will describe in this  ter  and VLANs  which we saw in   Both technologies are widely used
THE NETWORK LAYER
Implementation of Connectionless Service Having looked at the two classes of service the network layer can provide to its users  it is time to see how this layer works inside
Two different organizations are possible  depending on the type of service offered
If connectionless service is offered  packets are injected into the network individually and routed independently of each other
No advance setup is needed
In this context  the packets are frequently called datagrams (in analogy with telegrams) and the network is called a datagram network
If connection-oriented service is used  a path from the source router all the way to the destination router must be established before any data packets can be sent
This connection is called a VC (virtual circuit)  in analogy with the physical circuits set up by the telephone system  and the network is called a virtual-circuit network
In this tion  we will examine datagram networks; in the next one  we will examine virtual-circuit networks
Let us now see how a datagram network works
Suppose that the process P  in Fig
It hands the message to the transport layer  with instructions to deliver it to process P  on host H
The transport layer code runs on H  typically within the operating system
It prepends a transport header to the front of the message and hands the result to the network layer  probably just another procedure within the operating system
Aâs table (initially) Aâs table (later) Câs table Eâs table Dest
Line D C B A E F Packet Process P  Host H  Router ISPâs equipment LAN H  P  A B B â C C D B E C F C A B B â C C D B E B F B A B A A C â D E E E F E A B D C C C D D E â F F Figure  -
Routing within a datagram network
Let us assume for this example that the message is four times longer than the maximum packet size  so the network layer has to break it into four packets     NETWORK LAYER DESIGN ISSUES   and   and send each of them in turn to router A using some point-to-point protocol  for example  PPP
At this point the ISP takes over
Every router has an internal table telling it where to send packets for each of the possible destinations
Each table entry is a pair consisting of a destination and the outgoing line to use for that destination
Only directly connected lines can be used
For example  in Fig
Aâs initial routing table is shown in the figure under the label ââinitially
ââ At A  packets  and  are stored briefly  having arrived on the incoming link and had their checksums verified
Then each packet is forwarded according to Aâs table  onto the outgoing link to C within a new frame
Packet  is then forwarded to E and then to F
When it gets to F  it is sent within a frame over the LAN to H
Packets  and  follow the same route
However  something different happens to packet
When it gets to A it is sent to router B  even though it is also destined for F
For some reason  A decided to send packet  via a different route than that of the first three packets
Perhaps it has learned of a traffic jam somewhere along the ACE path and updated its routing table  as shown under the label ââlater
ââ The algorithm that manages the tables and makes the routing decisions is called the routing algorithm
Routing algorithms are one of the main topics we will study in this  ter
There are several different kinds of them  as we will see
IP (Internet Protocol)  which is the basis for the entire Internet  is the dominant example of a connectionless network service
Each packet carries a destination IP address that routers use to individually forward each packet
The addresses are   bits in IPv  packets and bits in IPv  packets
We will describe IP in much detail later in this  ter
Implementation of Connection-Oriented Service For connection-oriented service  we need a virtual-circuit network
Let us see how that works
The idea behind virtual circuits is to avoid having to choose a new route for every packet sent  as in Fig
Instead  when a connection is established  a route from the source machine to the destination machine is chosen as part of the connection setup and stored in tables inside the routers
That route is used for all traffic flowing over the connection  exactly the same way that the telephone system works
When the connection is released  the virtual circuit is also terminated
With connection-oriented service  each packet carries an identifier telling which virtual circuit it belongs to
As an example  consider the situation shown in Fig
Here  host H  has established connection  with host H
This connection is remembered as the first entry in each of the routing tables
The first line of Aâs table says that if a packet THE NETWORK LAYER
bearing connection identifier  comes in from H  it is to be sent to router C and given connection identifier
Similarly  the first entry at C routes the packet to E  also with connection identifier
Aâs table In Out D C B E F Packet Router ISPâs equipment LAN H  P  H  H  Process P  A Host H  P  H  C C Câs table A A E E Eâs table C C F F Figure  -
Routing within a virtual-circuit network
Now let us consider what happens if H  also wants to establish a connection to H
It chooses connection identifier  (because it is initiating the connection and this is its only connection) and tells the network to establish the virtual circuit
This leads to the ond row in the tables
Note that we have a conflict here because although A can easily distinguish connection  packets from H  from connection  packets from H  C cannot do this
For this reason  A assigns a different connection identifier to the outgoing traffic for the ond connection
Avoiding conflicts of this kind is why routers need the ability to replace connection identifiers in outgoing packets
In some contexts  this process is called label switching
An example of a connection-oriented network service is MPLS (MultiProtocol Label Switching)
It is used within ISP networks in the Internet  with IP packets wrapped in an MPLS header having a  -bit connection identifier or label
MPLS is often hidden from customers  with the ISP establishing long-term connections for large amounts of traffic  but it is increasingly being used to help when quality of service is important but also with other ISP traffic management tasks
We will have more to say about MPLS later in this  ter
NETWORK LAYER DESIGN ISSUES    Comparison of Virtual-Circuit and Datagram Networks Both virtual circuits and datagrams have their supporters and their detractors
We will now attempt to summarize both sets of arguments
The major issues are listed in Fig
Issue Datagram network Virtual-circuit network Circuit setup Not needed Required Addressing Each packet contains the full source and destination address Each packet contains a short VC number State information Routers do not hold state information about connections Each VC requires router table space per connection Routing Each packet is routed independently Route chosen when VC is set up; all packets follow it Effect of router failures None  except for packets lost during the crash All VCs that passed through the failed router are terminated Quality of service Difficult Easy if enough resources can be allocated in advance for each VC Congestion control Difficult Easy if enough resources can be allocated in advance for each VC Figure  -
Comparison of datagram and virtual-circuit networks
Inside the network  several trade-offs exist between virtual circuits and datagrams
One trade-off is setup time versus address parsing time
Using virtual circuits requires a setup phase  which takes time and consumes resources
However  once this price is paid  figuring out what to do with a data packet in a virtual-circuit network is easy: the router just uses the circuit number to index into a table to find out where the packet goes
In a datagram network  no setup is needed but a more complicated lookup procedure is required to locate the entry for the destination
A related issue is that the destination addresses used in datagram networks are longer than circuit numbers used in virtual-circuit networks because they have a global meaning
If the packets tend to be fairly short  including a full destination address in every packet may represent a significant amount of overhead  and hence a waste of bandwidth
Yet another issue is the amount of table space required in router memory
A datagram network needs to have an entry for every possible destination  whereas a virtual-circuit network just needs an entry for each virtual circuit
However  this THE NETWORK LAYER
advantage is somewhat illusory since connection setup packets have to be routed too  and they use destination addresses  the same as datagrams do
Virtual circuits have some advantages in guaranteeing quality of service and avoiding congestion within the network because resources (
buffers  bandwidth  and CPU cycles) can be reserved in advance  when the connection is established
Once the packets start arriving  the necessary bandwidth and router capacity will be there
With a datagram network  congestion avoidance is more difficult
For transaction processing systems (
stores calling up to verify credit card purchases)  the overhead required to set up and clear a virtual circuit may easily dwarf the use of the circuit
If the majority of the traffic is expected to be of this kind  the use of virtual circuits inside the network makes little sense
On the other hand  for long-running uses such as VPN traffic between two corporate offices  permanent virtual circuits (that are set up manually and last for months or years) may be useful
Virtual circuits also have a vulnerability problem
If a router crashes and loses its memory  even if it comes back up a ond later  all the virtual circuits passing through it will have to be aborted
In contrast  if a datagram router goes down  only those users whose packets were queued in the router at the time need suffer (and probably not even then since the sender is likely to retransmit them shortly)
The loss of a communication line is fatal to virtual circuits using it  but can easily be compensated for if datagrams are used
Datagrams also allow the routers to balance the traffic throughout the network  since routes can be changed partway through a long sequence of packet transmissions  ROUTING ALGORITHMS The main function of the network layer is routing packets from the source machine to the destination machine
In most networks  packets will require multiple hops to make the journey
The only notable exception is for broadcast networks  but even here routing is an issue if the source and destination are not on the same network segment
The algorithms that choose the routes and the data structures that they use are a major area of network layer design
The routing algorithm is that part of the network layer software responsible for deciding which output line an incoming packet should be transmitted on
If the network uses datagrams internally  this decision must be made anew for every arriving data packet since the best route may have changed since last time
If the network uses virtual circuits internally  routing decisions are made only when a new virtual circuit is being set up
Thereafter  data packets just follow the already established route
The latter case is sometimes called session routing because a route remains in force for an entire session (
while logged in over a VPN)
ROUTING ALGORITHMS It is sometimes useful to make a distinction between routing  which is making the decision which routes to use  and forwarding  which is what happens when a packet arrives
One can think of a router as having two processes inside it
One of them handles each packet as it arrives  looking up the outgoing line to use for it in the routing tables
This process is forwarding
The other process is responsible for filling in and updating the routing tables
That is where the routing algorithm comes into play
Regardless of whether routes are chosen independently for each packet sent or only when new connections are established  certain properties are desirable in a routing algorithm: correctness  simplicity  robustness  stability  fairness  and efficiency
Correctness and simplicity hardly require comment  but the need for robustness may be less obvious at first
Once a major network comes on the air  it may be expected to run continuously for years without system-wide failures
During that period there will be hardware and software failures of all kinds
Hosts  routers  and lines will fail repeatedly  and the topology will change many times
The routing algorithm should be able to cope with changes in the topology and traffic without requiring all jobs in all hosts to be aborted
Imagine the havoc if the network needed to be rebooted every time some router crashed! Stability is also an important goal for the routing algorithm
There exist routing algorithms that never converge to a fixed set of paths  no matter how long they run
A stable algorithm reaches equilibrium and stays there
It should converge quickly too  since communication may be disrupted until the routing algorithm has reached equilibrium
Fairness and efficiency may sound obviousâsurely no reasonable person would oppose themâbut as it turns out  they are often contradictory goals
As a simple example of this conflict  look at Fig
Suppose that there is enough traffic between A and Aâ²  between B and Bâ²  and between C and Câ² to saturate the horizontal links
To maximize the total flow  the X to Xâ² traffic should be shut off altogether
Unfortunately  X and Xâ² may not see it that way
Evidently  some compromise between global efficiency and fairness to individual connections is needed
Before we can even attempt to find trade-offs between fairness and efficiency  we must decide what it is we seek to optimize
Minimizing the mean packet delay is an obvious candidate to send traffic through the network effectively  but so is maximizing total network throughput
Furthermore  these two goals are also in conflict  since operating any queueing system near capacity implies a long queueing delay
As a compromise  many networks attempt to minimize the distance a packet must travel  or simply reduce the number of hops a packet must make
Either choice tends to improve the delay and also reduce the amount of bandwidth consumed per packet  which tends to improve the overall network throughput as well
Routing algorithms can be grouped into two major classes: nonadaptive and adaptive
Nonadaptive algorithms do not base their routing decisions on any THE NETWORK LAYER
X Xâ² A B C A' B' C' Figure  -
Network with a conflict between fairness and efficiency
measurements or estimates of the current topology and traffic
Instead  the choice of the route to use to get from I to J (for all I and J) is computed in advance  offline  and downloaded to the routers when the network is booted
This procedure is sometimes called static routing
Because it does not respond to failures  static routing is mostly useful for situations in which the routing choice is clear
For example  router F in Fig
Adaptive algorithms  in contrast  change their routing decisions to reflect changes in the topology  and sometimes changes in the traffic as well
These dynamic routing algorithms differ in where they get their information (
locally  from adjacent routers  or from all routers)  when they change the routes (
when the topology changes  or every ÎT onds as the load changes)  and what metric is used for optimization (
distance  number of hops  or estimated transit time)
In the following tions  we will discuss a variety of routing algorithms
The algorithms cover delivery models besides sending a packet from a source to a destination
Sometimes the goal is to send the packet to multiple  all  or one of a set of destinations
All of the routing algorithms we describe here make decisions based on the topology; we defer the possibility of decisions based on the traffic levels to       The Optimality Principle Before we get into specific algorithms  it may be helpful to note that one can make a general statement about optimal routes without regard to network topology or traffic
This statement is known as the optimality principle (Bellman  )
It states that if router J is on the optimal path from router I to router K    ROUTING ALGORITHMS then the optimal path from J to K also falls along the same route
To see this  call the part of the route from I to J r  and the rest of the route r
If a route better than r  existed from J to K  it could be concatenated with r  to improve the route from I to K  contradicting our statement that r  r  is optimal
As a direct consequence of the optimality principle  we can see that the set of optimal routes from all sources to a given destination form a tree rooted at the destination
Such a tree is called a sink tree and is illustrated in Fig
The goal of all routing algorithms is to discover and use the sink trees for all routers
B A F D E C J N O I H G L M K (a) B A F D E C J N O I H G L M K (b) Figure  -
(a) A network
(b) A sink tree for router B
Note that a sink tree is not necessarily unique; other trees with the same path lengths may exist
If we allow all of the possible paths to be chosen  the tree becomes a more general structure called a DAG (Directed Acyclic Graph)
DAGs have no loops
We will use sink trees as a convenient shorthand for both cases
Both cases also depend on the technical assumption that the paths do not interfere with each other so  for example  a traffic jam on one path will not cause another path to divert
Since a sink tree is indeed a tree  it does not contain any loops  so each packet will be delivered within a finite and bounded number of hops
In practice  life is not quite this easy
Links and routers can go down and come back up during operation  so different routers may have different ideas about the current topology
Also  we have quietly finessed the issue of whether each router has to individually acquire the information on which to base its sink tree computation or whether this information is collected by some other means
We will come back to these issues shortly
Nevertheless  the optimality principle and the sink tree provide a benchmark against which other routing algorithms can be measured
THE NETWORK LAYER
Shortest Path Algorithm Let us begin our study of routing algorithms with a simple technique for computing optimal paths given a complete picture of the network
These paths are the ones that we want a distributed routing algorithm to find  even though not all routers may know all of the details of the network
The idea is to build a graph of the network  with each node of the graph representing a router and each edge of the graph representing a communication line  or link
To choose a route between a given pair of routers  the algorithm just finds the shortest path between them on the graph
The concept of a shortest path deserves some explanation
One way of measuring path length is the number of hops
Using this metric  the paths ABC and ABE in Fig
Another metric is the geographic distance in kilometers  in which case ABC is clearly much longer than ABE (assuming the figure is drawn to scale)
A  D G  (a) F (â  â) D(â â) B  C  H  E  F  A (c) B (  A) C (  B) H (â  â) E (  B) G (  A) A F (  E) D (â â) (e) B (  A) C (  B) H (  G) E (  B) G (  E) A F (  E) D (â â) (f) B (  A) C (  B) H (  F) E (  B) G (  E) A F (  E) D (â  ) (d) B (  A) C (  B) H (â  â) E (  B) G (  E) A F (â  â) D (â  â) (b) B (  A) C (â  â) H (â  â) E (â  â) G (  A) Figure  -
The first six steps used in computing the shortest path from A to D
The arrows indicate the working node
ROUTING ALGORITHMS However  many other metrics besides hops and physical distance are also possible
For example  each edge could be labeled with the mean delay of a standard test packet  as measured by hourly runs
With this graph labeling  the shortest path is the fastest path rather than the path with the fewest edges or kilometers
In the general case  the labels on the edges could be computed as a function of the distance  bandwidth  average traffic  communication cost  measured delay  and other factors
By changing the weighting function  the algorithm would then compute the ââshortestââ path measured according to any one of a number of criteria or to a combination of criteria
Several algorithms for computing the shortest path between two nodes of a graph are known
This one is due to Dijkstra (   ) and finds the shortest paths between a source and all destinations in the network
Each node is labeled (in parentheses) with its distance from the source node along the best known path
The distances must be non-negative  as they will be if they are based on real quantities like bandwidth and delay
Initially  no paths are known  so all nodes are labeled with infinity
As the algorithm proceeds and paths are found  the labels may change  reflecting better paths
A label may be either tentative or permanent
Initially  all labels are tentative
When it is discovered that a label represents the shortest possible path from the source to that node  it is made permanent and never changed thereafter
To illustrate how the labeling algorithm works  look at the weighted  undirected graph of Fig
We want to find the shortest path from A to D
We start out by marking node A as permanent  indicated by a filled-in circle
Then we examine  in turn  each of the nodes adjacent to A (the working node)  relabeling each one with the distance to A
Whenever a node is relabeled  we also label it with the node from which the probe was made so that we can reconstruct the final path later
If the network had more than one shortest path from A to D and we wanted to find all of them  we would need to remember all of the probe nodes that could reach a node with the same distance
Having examined each of the nodes adjacent to A  we examine all the tentatively labeled nodes in the whole graph and make the one with the smallest label permanent  as shown in Fig
This one becomes the new working node
We now start at B and examine all nodes adjacent to it
If the sum of the label on B and the distance from B to the node being considered is less than the label on that node  we have a shorter path  so the node is relabeled
After all the nodes adjacent to the working node have been inspected and the tentative labels changed if possible  the entire graph is searched for the tentatively labeled node with the smallest value
This node is made permanent and becomes the working node for the next round
Figure  -  shows the first six steps of the algorithm
To see why the algorithm works  look at Fig
At this point we have just made E permanent
Suppose that there were a shorter path than ABE  say THE NETWORK LAYER
AXYZE (for some X and Y)
There are two possibilities: either node Z has already been made permanent  or it has not been
If it has  then E has already been probed (on the round following the one when Z was made permanent)  so the AXYZE path has not escaped our attention and thus cannot be a shorter path
Now consider the case where Z is still tentatively labeled
If the label at Z is greater than or equal to that at E  then AXYZE cannot be a shorter path than ABE
If the label is less than that of E  then Z and not E will become permanent first  allowing E to be probed from Z
This algorithm is given in Fig
The global variables n and dist describe the graph and are initialized before shortest path is called
The only difference between the program and the algorithm described above is that in Fig
Since the shortest paths from t to s in an undirected graph are the same as the shortest paths from s to t  it does not matter at which end we begin
The reason for searching backward is that each node is labeled with its predecessor rather than its successor
When the final path is copied into the output variable  path  the path is thus reversed
The two reversal effects cancel  and the answer is produced in the correct order
Flooding When a routing algorithm is implemented  each router must make decisions based on local knowledge  not the complete picture of the network
A simple local technique is flooding  in which every incoming packet is sent out on every outgoing line except the one it arrived on
Flooding obviously generates vast numbers of duplicate packets  in fact  an infinite number unless some measures are taken to damp the process
One such measure is to have a hop counter contained in the header of each packet that is decremented at each hop  with the packet being discarded when the counter reaches zero
Ideally  the hop counter should be initialized to the length of the path from source to destination
If the sender does not know how long the path is  it can initialize the counter to the worst case  namely  the full diameter of the network
Flooding with a hop count can produce an exponential number of duplicate packets as the hop count grows and routers duplicate packets they have seen before
A better technique for damming the flood is to have routers keep track of which packets have been flooded  to avoid sending them out a ond time
One way to achieve this goal is to have the source router put a sequence number in each packet it receives from its hosts
Each router then needs a list per source router telling which sequence numbers originating at that source have already been seen
If an incoming packet is on the list  it is not flooded
ROUTING ALGORITHMS #define MAX NODES  /* maximum number of nodes */ #define INFINITY   /* a number larger than every maximum path */ int n  dist[MAX NODES][MAX NODES]; /* dist[i][j] is the distance from i to j */ void shortest path(int s  int t  int path[]) { struct state { /* the path being worked on */ int predecessor; /* previous node */ int length; /* length from source to this node */ enum {permanent  tentative} label; /* label state */ } state[MAX NODES]; int i  k  min; struct state *p; for (p = &state[ ]; p < &state[n]; p++) { /* initialize state */ p->predecessor = â ; p->length = INFINITY; p->label = tentative; } state[t] =  ; state[t] = permanent; k = t; /* k is the initial working node */ do { /* Is there a better path from k? */ for (i =  ; i < n; i++) /* this graph has n nodes */ if (dist[k][i] !=  && state[i] == tentative) { if (state[k] + dist[k][i] < state[i]) { state[i] = k; state[i] = state[k] + dist[k][i]; } } /* Find the tentatively labeled node with the smallest label
*/ k =  ; min = INFINITY; for (i =  ; i < n; i++) if (state[i] == tentative && state[i] < min) { min = state[i]; k = i; } state[k] = permanent; } while (k != s); /* Copy the path into the output array
*/ i =  ; k = s; do {path[i++] = k; k = state[k]; } while (k >=  ); } Figure  -
Dijkstraâs algorithm to compute the shortest path through a graph
To prevent the list from growing without bound  each list should be augmented by a counter  k  meaning that all sequence numbers through k have been seen
When a packet comes in  it is easy to check if the packet has already been THE NETWORK LAYER
flooded (by comparing its sequence number to k; if so  it is discarded
Furthermore  the full list below k is not needed  since k effectively summarizes it
Flooding is not practical for sending most packets  but it does have some important uses
First  it ensures that a packet is delivered to every node in the network
This may be wasteful if there is a single destination that needs the packet  but it is effective for broadcasting information
In wireless networks  all messages transmitted by a station can be received by all other stations within its radio range  which is  in fact  flooding  and some algorithms utilize this property
ond  flooding is tremendously robust
Even if large numbers of routers are blown to bits (
in a military network located in a war zone)  flooding will find a path if one exists  to get a packet to its destination
Flooding also requires little in the way of setup
The routers only need to know their neighbors
This means that flooding can be used as a building block for other routing algorithms that are more efficient but need more in the way of setup
Flooding can also be used as a metric against which other routing algorithms can be compared
Flooding always chooses the shortest path because it chooses every possible path in parallel
Consequently  no other algorithm can produce a shorter delay (if we ignore the overhead generated by the flooding process itself)
Distance Vector Routing Computer networks generally use dynamic routing algorithms that are more complex than flooding  but more efficient because they find shortest paths for the current topology
Two dynamic algorithms in particular  distance vector routing and link state routing  are the most popular
In this tion  we will look at the former algorithm
In the following tion  we will study the latter algorithm
A distance vector routing algorithm operates by having each router maintain a table (
a vector) giving the best known distance to each destination and which link to use to get there
These tables are updated by exchanging information with the neighbors
Eventually  every router knows the best link to reach each destination
The distance vector routing algorithm is sometimes called by other names  most commonly the distributed Bellman-Ford routing algorithm  after the researchers who developed it (Bellman  ; and Ford and Fulkerson  )
It was the original ARPANET routing algorithm and was also used in the Internet under the name RIP
In distance vector routing  each router maintains a routing table indexed by  and containing one entry for each router in the network
This entry has two parts: the preferred outgoing line to use for that destination and an estimate of the distance to that destination
The distance might be measured as the number of hops or using another metric  as we discussed for computing shortest paths
The router is assumed to know the ââdistanceââ to each of its neighbors
If the metric is hops  the distance is just one hop
If the metric is propagation delay  the   ROUTING ALGORITHMS router can measure it directly with special ECHO packets that the receiver just timestamps and sends back as fast as it can
As an example  assume that delay is used as a metric and that the router knows the delay to each of its neighbors
Once every T m  each router sends to each neighbor a list of its estimated delays to each destination
It also receives a similar list from each neighbor
Imagine that one of these tables has just come in from neighbor X  with Xi being Xâs estimate of how long it takes to get to router i
If the router knows that the delay to X is m m  it also knows that it can reach router i via X in Xi + m m
By performing this calculation for each neighbor  a router can find out which estimate seems the best and use that estimate and the corresponding link in its new routing table
Note that the old routing table is not used in the calculation
This updating process is illustrated in Fig
Part (a) shows a network
The first four columns of part (b) show the delay vectors received from the neighbors of router J
A claims to have a  -m delay to B  a  -m delay to C  a  - m delay to D  etc
Suppose that J has measured or estimated its delay to its neighbors  A  I  H  and K  as   and  m  respectively
(a) A B C D E I J K L F G H Router                        A A I H I I H H I â K K To A I H K Line New estimated delay from J A B C DE F G H I J K L JA JI JH JK delay delay delay delay is is is is  New routing table for J Vectors received from J's four neighbors (b) Figure  -
(a) A network
(b) Input from A  I  H  K  and the new routing table for J
Consider how J computes its new route to router G
It knows that it can get to A in  m  and furthermore A claims to be able to get to G in   m  so J knows it can count on a delay of   m to G if it forwards packets bound for G THE NETWORK LAYER
Similarly  it computes the delay to G via I  H  and K as   (  +  ) (  +  )  and   (  +  ) m  respectively
The best of these values is so it makes an entry in its routing table that the delay to G is   m and that the route to use is via H
The same calculation is performed for all the other destinations  with the new routing table shown in the last column of the figure
The Count-to-Infinity Problem The settling of routes to best paths across the network is called convergence
Distance vector routing is useful as a simple technique by which routers can collectively compute shortest paths  but it has a serious drawback in practice: although it converges to the correct answer  it may do so slowly
In particular  it reacts rapidly to good news  but leisurely to bad news
Consider a router whose best route to destination X is long
If  on the next exchange  neighbor A suddenly reports a short delay to X  the router just switches over to using the line to A to send traffic to X
In one vector exchange  the good news is processed
To see how fast good news propagates  consider the five-node (linear) network of Fig
Suppose A is down initially and all the other routers know this
In other words  they have all recorded the delay to A as infinity
A B C D E â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢    Initially After  exchange After  exchanges After  exchanges After  exchanges A B C D E    â¢ â¢ â¢ â¢         Initially After  exchange After  exchanges After  exchanges After  exchanges After  exchanges After  exchanges
(a) (b) Figure  -
The count-to-infinity problem
When A comes up  the other routers learn about it via the vector exchanges
For simplicity  we will assume that there is a gigantic gong somewhere that is struck periodically to initiate a vector exchange at all routers simultaneously
At the time of the first exchange  B learns that its left-hand neighbor has zero delay to A
B now makes an entry in its routing table indicating that A is one hop away to the left
All the other routers still think that A is down
At this point  the routing table entries for A are as shown in the ond row of Fig
On the next   ROUTING ALGORITHMS exchange  C learns that B has a path of length  to A  so it updates its routing table to indicate a path of length   but D and E do not hear the good news until later
Clearly  the good news is spreading at the rate of one hop per exchange
In a network whose longest path is of length N hops  within N exchanges everyone will know about newly revived links and routers
Now let us consider the situation of Fig
Routers B  C  D  and E have distances to A of  and  hops  respectively
Suddenly  either A goes down or the link between A and B is cut (which is effectively the same thing from Bâs point of view)
At the first packet exchange  B does not hear anything from A
Fortunately  C says ââDo not worry; I have a path to A of length
ââ Little does B suspect that Câs path runs through B itself
For all B knows  C might have ten links all with separate paths to A of length
As a result  B thinks it can reach A via C  with a path length of
D and E do not update their entries for A on the first exchange
On the ond exchange  C notices that each of its neighbors claims to have a path to A of length
It picks one of them at random and makes its new distance to A   as shown in the third row of Fig
Subsequent exchanges produce the history shown in the rest of Fig
From this figure  it should be clear why bad news travels slowly: no router ever has a value more than one higher than the minimum of all its neighbors
Gradually  all routers work their way up to infinity  but the number of exchanges required depends on the numerical value used for infinity
For this reason  it is wise to set infinity to the longest path plus
Not entirely surprisingly  this problem is known as the count-to-infinity problem
There have been many attempts to solve it  for example  preventing routers from advertising their best paths back to the neighbors from which they heard them with the split horizon with poisoned reverse rule discussed in RFC
However  none of these heuristics work well in practice despite the colorful names
The core of the problem is that when X tells Y that it has a path somewhere  Y has no way of knowing whether it itself is on the path
Link State Routing Distance vector routing was used in the ARPANET until  when it was replaced by link state routing
The primary problem that caused its demise was that the algorithm often took too long to converge after the network topology changed (due to the count-to-infinity problem)
Consequently  it was replaced by an entirely new algorithm  now called link state routing
Variants of link state routing called IS-IS and OSPF are the routing algorithms that are most widely used inside large networks and the Internet today
The idea behind link state routing is fairly simple and can be stated as five parts
Each router must do the following things to make it work: THE NETWORK LAYER
Discover its neighbors and learn their network addresses Set the distance or cost metric to each of its neighbors Construct a packet telling all it has just learned Send this packet to and receive packets from all other routers Compute the shortest path to every other router
In effect  the complete topology is distributed to every router
Then Dijkstraâs algorithm can be run at each router to find the shortest path to every other router
Below we will consider each of these five steps in more detail
Learning about the Neighbors When a router is booted  its first task is to learn who its neighbors are
It accomplishes this goal by sending a special HELLO packet on each point-to-point line
The router on the other end is expected to send back a reply giving its name
These names must be globally unique because when a distant router later hears that three routers are all connected to F  it is essential that it can determine whether all three mean the same F
When two or more routers are connected by a broadcast link (
a switch  ring  or classic Ethernet)  the situation is slightly more complicated
-  (a) illustrates a broadcast LAN to which three routers  A  C  and F  are directly connected
Each of these routers is connected to one or more additional routers  as shown
Router A B C D E C D E H I F G G H F I N A B LAN (a) (b) Figure  -
(a) Nine routers and a broadcast LAN
(b) A graph model of (a)
The broadcast LAN provides connectivity between each pair of attached routers
However  modeling the LAN as many point-to-point links increases the size   ROUTING ALGORITHMS of the topology and leads to wasteful messages
A better way to model the LAN is to consider it as a node itself  as shown in Fig
Here  we have introduced a new  artificial node  N  to which A  C  and F are connected
One designated router on the LAN is selected to play the role of N in the routing protocol
The fact that it is possible to go from A to C on the LAN is represented by the path ANC here
Setting Link Costs The link state routing algorithm requires each link to have a distance or cost metric for finding shortest paths
The cost to reach neighbors can be set automatically  or configured by the network operator
A common choice is to make the cost inversely proportional to the bandwidth of the link
For example  -Gbps Ethernet may have a cost of  and   -Mbps Ethernet a cost of
This makes higher-capacity paths better choices
If the network is geographically spread out  the delay of the links may be factored into the cost so that paths over shorter links are better choices
The most direct way to determine this delay is to send over the line a special ECHO packet that the other side is required to send back immediately
By measuring the round-trip time and dividing it by two  the sending router can get a reasonable estimate of the delay
Building Link State Packets Once the information needed for the exchange has been collected  the next step is for each router to build a packet containing all the data
The packet starts with the identity of the sender  followed by a sequence number and age (to be described later) and a list of neighbors
The cost to each neighbor is also given
An example network is presented in Fig
The corresponding link state packets for all six routers are shown in Fig
B C E F A D    (a) A Seq
Age B C D E F B  E  Seq
Age A  C  Seq
Age B  D  Seq
Age C  F  Seq
Age A  C  Seq
Age B  D  F  E  F  E  Link State Packets (b) Figure  -
(a) A network
(b) The link state packets for this network
THE NETWORK LAYER
Building the link state packets is easy
The hard part is determining when to build them
One possibility is to build them periodically  that is  at regular intervals
Another possibility is to build them when some significant event occurs  such as a line or neighbor going down or coming back up again or changing its properties appreciably
Distributing the Link State Packets The trickiest part of the algorithm is distributing the link state packets
All of the routers must get all of the link state packets quickly and reliably
If different routers are using different versions of the topology  the routes they compute can have inconsistencies such as loops  unreachable machines  and other problems
First  we will describe the basic distribution algorithm
After that we will give some refinements
The fundamental idea is to use flooding to distribute the link state packets to all routers
To keep the flood in check  each packet contains a sequence number that is incremented for each new packet sent
Routers keep track of all the (source router  sequence) pairs they see
When a new link state packet comes in  it is checked against the list of packets already seen
If it is new  it is forwarded on all lines except the one it arrived on
If it is a duplicate  it is discarded
If a packet with a sequence number lower than the highest one seen so far ever arrives  it is rejected as being obsolete as the router has more recent data
This algorithm has a few problems  but they are manageable
First  if the sequence numbers wrap around  confusion will reign
The solution here is to use a  -bit sequence number
With one link state packet per ond  it would take years to wrap around  so this possibility can be ignored
ond  if a router ever crashes  it will lose track of its sequence number
If it starts again at   the next packet it sends will be rejected as a duplicate
Third  if a sequence number is ever corrupted and   is received instead of  (a  -bit error)  packets  through   will be rejected as obsolete  since the current sequence number will be thought to be
The solution to all these problems is to include the age of each packet after the sequence number and decrement it once per ond
When the age hits zero  the information from that router is discarded
Normally  a new packet comes in  say  every     so router information only times out when a router is down (or six conutive packets have been lost  an unlikely event)
The Age field is also decremented by each router during the initial flooding process  to make sure no packet can get lost and live for an indefinite period of time (a packet whose age is zero is discarded)
Some refinements to this algorithm make it more robust
When a link state packet comes in to a router for flooding  it is not queued for transmission immediately
Instead  it is put in a holding area to wait a short while in case more links are coming up or going down
If another link state packet from the same source comes in before the first packet is transmitted  their sequence numbers are   ROUTING ALGORITHMS compared
If they are equal  the duplicate is discarded
If they are different  the older one is thrown out
To guard against errors on the links  all link state packets are acknowledged
The data structure used by router B for the network shown in Fig
Each row here corresponds to a recently arrived  but as yet not fully processed  link state packet
The table records where the packet originated  its sequence number and age  and the data
In addition  there are send and acknowledgement flags for each of Bâs three links (to A  C  and F  respectively)
The send flags mean that the packet must be sent on the indicated link
The acknowledgement flags mean that it must be acknowledged there
D     C     E     F     A     Source Seq
Age A C F A C F Data Send flags ACK flags Figure  -
The packet buffer for router B in Fig
-   the link state packet from A arrives directly  so it must be sent to C and F and acknowledged to A  as indicated by the flag bits
Similarly  the packet from F has to be forwarded to A and C and acknowledged to F
However  the situation with the third packet  from E  is different
It arrives twice  once via EAB and once via EFB
Consequently  it has to be sent only to C but must be acknowledged to both A and F  as indicated by the bits
If a duplicate arrives while the original is still in the buffer  bits have to be changed
For example  if a copy of Câs state arrives from F before the fourth entry in the table has been forwarded  the six bits will be changed to   to indicate that the packet must be acknowledged to F but not sent there
Computing the New Routes Once a router has accumulated a full set of link state packets  it can construct the entire network graph because every link is represented
Every link is  in fact  represented twice  once for each direction
The different directions may even have different costs
The shortest-path computations may then find different paths from router A to B than from router B to A
Now Dijkstraâs algorithm can be run locally to construct the shortest paths to all possible destinations
The results of this algorithm tell the router which link to THE NETWORK LAYER
use to reach each destination
This information is installed in the routing tables  and normal operation is resumed
Compared to distance vector routing  link state routing requires more memory and computation
For a network with n routers  each of which has k neighbors  the memory required to store the input data is proportional to kn  which is at least as large as a routing table listing all the destinations
Also  the computation time grows faster than kn  even with the most efficient data structures  an issue in large networks
Nevertheless  in many practical situations  link state routing works well because it does not suffer from slow convergence problems
Link state routing is widely used in actual networks  so a few words about some example protocols are in order
Many ISPs use the IS-IS (Intermediate System-Intermediate System) link state protocol (Oran  )
It was designed for an early network called DECnet  later adopted by ISO for use with the OSI protocols and then modified to handle other protocols as well  most notably  IP
OSPF (Open Shortest Path First) is the other main link state protocol
It was designed by IETF several years after IS-IS and adopted many of the innovations designed for IS-IS
These innovations include a self-stabilizing method of flooding link state updates  the concept of a designated router on a LAN  and the method of computing and supporting path splitting and multiple metrics
As a consequence  there is very little difference between IS-IS and OSPF
The most important difference is that IS-IS can carry information about multiple network layer protocols at the same time (
IP  IPX  and AppleTalk)
OSPF does not have this feature  and it is an advantage in large multiprotocol environments
We will go over OSPF in
A general comment on routing algorithms is also in order
Link state  distance vector  and other algorithms rely on processing at all the routers to compute routes
Problems with the hardware or software at even a small number of routers can wreak havoc across the network
For example  if a router claims to have a link it does not have or forgets a link it does have  the network graph will be incorrect
If a router fails to forward packets or corrupts them while forwarding them  the route will not work as expected
Finally  if it runs out of memory or does the routing calculation wrong  bad things will happen
As the network grows into the range of tens or hundreds of thousands of nodes  the probability of some router failing occasionally becomes nonnegligible
The trick is to try to arrange to limit the damage when the inevitable happens
Perlman (   ) discusses these problems and their possible solutions in detail
Hierarchical Routing As networks grow in size  the router routing tables grow proportionally
Not only is router memory consumed by ever-increasing tables  but more CPU time is needed to scan them and more bandwidth is needed to send status reports about them
At a certain point  the network may grow to the point where it is no longer   ROUTING ALGORITHMS feasible for every router to have an entry for every other router  so the routing will have to be done hierarchically  as it is in the telephone network
When hierarchical routing is used  the routers are divided into what we will call regions
Each router knows all the details about how to route packets to destinations within its own region but knows nothing about the internal structure of other regions
When different networks are interconnected  it is natural to regard each one as a separate region to free the routers in one network from having to know the topological structure of the other ones
For huge networks  a two-level hierarchy may be insufficient; it may be necessary to group the regions into clusters  the clusters into zones  the zones into groups  and so on  until we run out of names for aggregations
As an example of a multilevel hierarchy  consider how a packet might be routed from Berkeley  California  to Malindi  Kenya
The Berkeley router would know the detailed topology within California but would send all out-of-state traffic to the Los Angeles router
The Los Angeles router would be able to route traffic directly to other domestic routers but would send all foreign traffic to New York
The New York router would be programmed to direct all traffic to the router in the destination country responsible for handling foreign traffic  say  in Nairobi
Finally  the packet would work its way down the tree in Kenya until it got to Malindi
Figure  -  gives a quantitative example of routing in a two-level hierarchy with five regions
The full routing table for router  A has   entries  as shown in Fig
When routing is done hierarchically  as in Fig
-  (c)  there are entries for all the local routers  as before  but all other regions are condensed into a single router  so all traffic for region  goes via the  B- A line  but the rest of the remote traffic goes via the  C- B line
Hierarchical routing has reduced the table from   to  entries
As the ratio of the number of regions to the number of routers per region grows  the savings in table space increase
Unfortunately  these gains in space are not free
There is a penalty to be paid: increased path length
For example  the best route from  A to  C is via region   but with hierarchical routing all traffic to region  goes via region   because that is better for most destinations in region
When a single network becomes very large  an interesting question is ââhow many levels should the hierarchy have?ââ For example  consider a network with routers
If there is no hierarchy  each router needs routing table entries
If the network is partitioned into   regions of   routers each  each router needs   local entries plus   remote entries for a total of   entries
If a three-level hierarchy is chosen  with  clusters each containing  regions of   routers  each router needs   entries for local routers   entries for routing to other regions within its own cluster  and  entries for distant clusters  for a total of   entries
Kamoun and Kleinrock (   ) discovered that the optimal number of levels for an N router network is ln N  requiring a total of e ln N entries per router
They have also shown that the increase in effective mean path length caused by hierarchical routing is sufficiently small that it is usually acceptable
THE NETWORK LAYER
Region  Region  Region  Region  Region   B  A  C  A  B  C  B  C  A  E  D  D  A  B  C  A  B  B   C   B   B   B   B   C   C   C   C   C   C   C   B   C   C   A â â  C  A  B  C  D  A  B  A  B  C  A  B  C  D  E  B Dest
Line Hops Full table for  A  A  C  B Dest
Line Hops Hierarchical table for  A  B   C   B   C   C   C  â â (a) (b) (c) Figure  -
Hierarchical routing
Broadcast Routing In some applications  hosts need to send messages to many or all other hosts
For example  a service distributing weather reports  stock market updates  or live radio programs might work best by sending to all machines and letting those that are interested read the data
Sending a packet to all destinations simultaneously is called broadcasting
Various methods have been proposed for doing it
One broadcasting method that requires no special features from the network is for the source to simply send a distinct packet to each destination
Not only is the method wasteful of bandwidth and slow  but it also requires the source to have a complete list of all destinations
This method is not desirable in practice  even though it is widely applicable
An improvement is multidestination routing  in which each packet contains either a list of destinations or a bit map indicating the desired destinations
When a packet arrives at a router  the router checks all the destinations to determine the set of output lines that will be needed
(An output line is needed if it is the best route to at least one of the destinations
) The router generates a new copy of the packet for each output line to be used and includes in each packet only those destinations that are to use the line
In effect  the destination set is partitioned among   ROUTING ALGORITHMS the output lines
After a sufficient number of hops  each packet will carry only one destination like a normal packet
Multidestination routing is like using separately addressed packets  except that when several packets must follow the same route  one of them pays full fare and the rest ride free
The network bandwidth is therefore used more efficiently
However  this scheme still requires the source to know all the destinations  plus it is as much work for a router to determine where to send one multidestination packet as it is for multiple distinct packets
We have already seen a better broadcast routing technique: flooding
When implemented with a sequence number per source  flooding uses links efficiently with a decision rule at routers that is relatively simple
Although flooding is illsuited for ordinary point-to-point communication  it rates serious consideration for broadcasting
However  it turns out that we can do better still once the shortest path routes for regular packets have been computed
The idea for reverse path forwarding is elegant and remarkably simple once it has been pointed out (Dalal and Metcalfe  )
When a broadcast packet arrives at a router  the router checks to see if the packet arrived on the link that is normally used for sending packets toward the source of the broadcast
If so  there is an excellent chance that the broadcast packet itself followed the best route from the router and is therefore the first copy to arrive at the router
This being the case  the router forwards copies of it onto all links except the one it arrived on
If  however  the broadcast packet arrived on a link other than the preferred one for reaching the source  the packet is discarded as a likely duplicate
I F H J N A D E K G O M O C G D N H B L L B A E H B C D F J G O M K L N I (a) A B C D G J O F I E H K L M N (b) (c) E K H Figure  -
Reverse path forwarding
(a) A network
(b) A sink tree
(c) The tree built by reverse path forwarding
An example of reverse path forwarding is shown in Fig
Part (a) shows a network  part (b) shows a sink tree for router I of that network  and part (c) shows how the reverse path algorithm works
On the first hop  I sends packets to F  H  J  and N  as indicated by the ond row of the tree
Each of these packets arrives on the preferred path to I (assuming that the preferred path falls along the sink tree) and is so indicated by a circle around the letter
On the ond hop  THE NETWORK LAYER
eight packets are generated  two by each of the routers that received a packet on the first hop
As it turns out  all eight of these arrive at previously unvisited routers  and five of these arrive along the preferred line
Of the six packets generated on the third hop  only three arrive on the preferred path (at C  E  and K); the others are duplicates
After five hops and   packets  the broadcasting terminates  compared with four hops and   packets had the sink tree been followed exactly
The principal advantage of reverse path forwarding is that it is efficient while being easy to implement
It sends the broadcast packet over each link only once in each direction  just as in flooding  yet it requires only that routers know how to reach all destinations  without needing to remember sequence numbers (or use other mechanisms to stop the flood) or list all destinations in the packet
Our last broadcast algorithm improves on the behavior of reverse path forwarding
It makes explicit use of the sink treeâor any other convenient spanning treeâfor the router initiating the broadcast
A spanning tree is a subset of the network that includes all the routers but contains no loops
Sink trees are spanning trees
If each router knows which of its lines belong to the spanning tree  it can copy an incoming broadcast packet onto all the spanning tree lines except the one it arrived on
This method makes excellent use of bandwidth  generating the absolute minimum number of packets necessary to do the job
The only problem is that each router must have knowledge of some spanning tree for the method to be applicable
Sometimes this information is available (
with link state routing  all routers know the complete topology  so they can compute a spanning tree) but sometimes it is not (
with distance vector routing)
Multicast Routing Some applications  such as a multiplayer game or live video of a sports event streamed to many viewing locations  send packets to multiple receivers
Unless the group is very small  sending a distinct packet to each receiver is expensive
On the other hand  broadcasting a packet is wasteful if the group consists of  say  machines on a million-node network  so that most receivers are not interested in the message (or worse yet  they are definitely interested but are not supposed to see it)
Thus  we need a way to send messages to well-defined groups that are numerically large in size but small compared to the network as a whole
Sending a message to such a group is called multicasting  and the routing algorithm used is called multicast routing
All multicasting schemes require some way to create and destroy groups and to identify which routers are members of a group
How these tasks are accomplished is not of concern to the routing algorithm
For now  we will assume that each group is identified by a multicast address and that routers know the groups to which they belong
We will revisit group membership when we describe the network layer of the Internet in
ROUTING ALGORITHMS Multicast routing schemes build on the broadcast routing schemes we have already studied  sending packets along spanning trees to deliver the packets to the members of the group while making efficient use of bandwidth
However  the best spanning tree to use depends on whether the group is dense  with receivers scattered over most of the network  or sparse  with much of the network not belonging to the group
In this tion we will consider both cases
If the group is dense  broadcast is a good start because it efficiently gets the packet to all parts of the network
But broadcast will reach some routers that are not members of the group  which is wasteful
The solution explored by Deering and Cheriton (   ) is to prune the broadcast spanning tree by removing links that do not lead to members
The result is an efficient multicast spanning tree
As an example  consider the two groups   and   in the network shown in Fig
Some routers are attached to hosts that belong to one or both of these groups  as indicated in the figure
A spanning tree for the leftmost router is shown in Fig
This tree can be used for broadcast but is overkill for multicast  as can be seen from the two pruned versions that are shown next
-  (c)  all the links that do not lead to hosts that are members of group  have been removed
The result is the multicast spanning tree for the leftmost router to send to group
Packets are forwarded only along this spanning tree  which is more efficient than the broadcast tree because there are  links instead of
It is efficient too  with only five links this time
It also shows that different multicast groups have different spanning trees
Various ways of pruning the spanning tree are possible
The simplest one can be used if link state routing is used and each router is aware of the complete topology  including which hosts belong to which groups
Each router can then construct its own pruned spanning tree for each sender to the group in question by constructing a sink tree for the sender as usual and then removing all links that do not connect group members to the sink node
MOSPF (Multicast OSPF) is an example of a link state protocol that works in this way (Moy  )
With distance vector routing  a different pruning strategy can be followed
The basic algorithm is reverse path forwarding
However  whenever a router with no hosts interested in a particular group and no connections to other routers receives a multicast message for that group  it responds with a PRUNE message  telling the neighbor that sent the message not to send it any more multicasts from the sender for that group
When a router with no group members among its own hosts has received such messages on all the lines to which it sends the multicast  it  too  can respond with a PRUNE message
In this way  the spanning tree is recursively pruned
DVMRP (Distance Vector Multicast Routing Protocol) is an example of a multicast routing protocol that works this way (Waitzman et al
Pruning results in efficient spanning trees that use only the links that are actually needed to reach members of the group
One potential disadvantage is that it is lots of work for routers  especially for large networks
Suppose that a network THE NETWORK LAYER
(a) (b) (c) (d) Figure  -
(a) A network
(b) A spanning tree for the leftmost router
(c) A multicast tree for group
(d) A multicast tree for group
has n groups  each with an average of m nodes
At each router and for each group  m pruned spanning trees must be stored  for a total of mn trees
For example  Fig
-  (c) gives the spanning tree for the leftmost router to send to group
The spanning tree for the rightmost router to send to group  (not shown) will look quite different  as packets will head directly for group members rather than via the left side of the graph
This in turn means that routers must forward packets destined to group  in different directions depending on which node is sending to the group
When many large groups with many senders exist  considerable storage is needed to store all the trees
An alternative design uses core-based trees to compute a single spanning tree for the group (Ballardie et al
All of the routers agree on a root (called the core or rendezvous point) and build the tree by sending a packet from each member to the root
The tree is the union of the paths traced by these packets
To send to this group  a sender sends a packet to the core
When the packet reaches the core  it is forwarded down the tree
This is shown in Fig
As a performance optimization  packets destined for the group do not need to reach the core before they are multicast
As soon as a packet reaches the   ROUTING ALGORITHMS tree  it can be forwarded up toward the root  as well as down all the other branches
This is the case for the sender at the top of Fig
Core Core Sender Sender (a) (b) Figure  -
(a) Core-based tree for group
(b) Sending to group
Having a shared tree is not optimal for all sources
For example  in Fig
The inefficiency depends on where the core and senders are located  but often it is reasonable when the core is in the middle of the senders
When there is only a single sender  as in a video that is streamed to a group  using the sender as the core is optimal
Also of note is that shared trees can be a major savings in storage costs  messages sent  and computation
Each router has to keep only one tree per group  instead of m trees
Further  routers that are not part of the tree do no work at all to support the group
For this reason  shared tree approaches like core-based trees are used for multicasting to sparse groups in the Internet as part of popular protocols such as PIM (Protocol Independent Multicast) (Fenner et al
Anycast Routing So far  we have covered delivery models in which a source sends to a single destination (called unicast)  to all destinations (called broadcast)  and to a group of destinations (called multicast)
Another delivery model  called anycast is sometimes also useful
In anycast  a packet is delivered to the nearest member of a group (Partridge et al
Schemes that find these paths are called anycast routing
Why would we want anycast? Sometimes nodes provide a service  such as time of day or content distribution for which it is getting the right information all that matters  not the node that is contacted; any node will do
For example  anycast is used in the Internet as part of DNS  as we will see in   Luckily  we will not have to devise new routing schemes for anycast because regular distance vector and link state routing can produce anycast routes
Suppose THE NETWORK LAYER
we want to anycast to the members of group
They will all be given the address ââ  ââ instead of different addresses
Distance vector routing will distribute vectors as usual  and nodes will choose the shortest path to destination
This will result in nodes sending to the nearest instance of destination
The routes are shown in Fig
This procedure works because the routing protocol does not realize that there are multiple instances of destination
That is  it believes that all the instances of node  are the same node  as in the topology shown in Fig
(a) (b) Figure  -
(a) Anycast routes to group
(b) Topology seen by the routing protocol
This procedure works for link state routing as well  although there is the added consideration that the routing protocol must not find seemingly short paths that pass through node
This would result in jumps through hyperspace  since the instances of node  are really nodes located in different parts of the network
However  link state protocols already make this distinction between routers and hosts
We glossed over this fact earlier because it was not needed for our discussion
Routing for Mobile Hosts Millions of people use computers while on the go  from truly mobile situations with wireless devices in moving cars  to nomadic situations in which laptop computers are used in a series of different locations
We will use the term mobile hosts to mean either category  as distinct from stationary hosts that never move
Increasingly  people want to stay connected wherever in the world they may be  as easily as if they were at home
These mobile hosts introduce a new complication: to route a packet to a mobile host  the network first has to find it
The model of the world that we will consider is one in which all hosts are assumed to have a permanent home location that never changes
Each hosts also has a permanent home address that can be used to determine its home location  analogous to the way the telephone number  -  -   indicates the United States (country code  ) and Manhattan (  )
The routing goal in systems with   ROUTING ALGORITHMS mobile hosts is to make it possible to send packets to mobile hosts using their fixed home addresses and have the packets efficiently reach them wherever they may be
The trick  of course  is to find them
Some discussion of this model is in order
A different model would be to recompute routes as the mobile host moves and the topology changes
We could then simply use the routing schemes described earlier in this tion
However  with a growing number of mobile hosts  this model would soon lead to the entire network endlessly computing new routes
Using the home addresses greatly reduces this burden
Another alternative would be to provide mobility above the network layer  which is what typically happens with laptops today
When they are moved to new Internet locations  laptops acquire new network addresses
There is no association between the old and new addresses; the network does not know that they belonged to the same laptop
In this model  a laptop can be used to browse the Web  but other hosts cannot send packets to it (for example  for an incoming call)  without building a higher layer location service  for example  signing into Skype again after moving
Moreover  connections cannot be maintained while the host is moving; new connections must be started up instead
Network-layer mobility is useful to fix these problems
The basic idea used for mobile routing in the Internet and cellular networks is for the mobile host to tell a host at the home location where it is now
This host  which acts on behalf of the mobile host  is called the home agent
Once it knows where the mobile host is currently located  it can forward packets so that they are delivered
A sender in the northwest city of Seattle wants to send a packet to a host normally located across the United States in New York
The case of interest to us is when the mobile host is not at home
Instead  it is temporarily in San Diego
The mobile host in San Diego must acquire a local network address before it can use the network
This happens in the normal way that hosts obtain network addresses; we will cover how this works for the Internet later in this  ter
The local address is called a care of address
Once the mobile host has this address  it can tell its home agent where it is now
It does this by sending a registration message to the home agent (step  ) with the care of address
The message is shown with a dashed line in Fig
Next  the sender sends a data packet to the mobile host using its permanent address (step  )
This packet is routed by the network to the hostâs home location because that is where the home address belongs
In New York  the home agent intercepts this packet because the mobile host is away from home
It then wraps or encapsulates the packet with a new header and sends this bundle to the care of address (step  )
This mechanism is called tunneling
It is very important in the Internet so we will look at it in more detail later
THE NETWORK LAYER
Mobile host at care of address  : Tunnel to care of address  : Register care of address  : Send to home address Home agent at home address Sender  : Reply  : Tunnel to sender to care of address Figure  -
Packet routing for mobile hosts
When the encapsulated packet arrives at the care of address  the mobile host unwraps it and retrieves the packet from the sender
The mobile host then sends its reply packet directly to the sender (step  )
The overall route is called triangle routing because it may be circuitous if the remote location is far from the home location
As part of step   the sender may learn the current care of address
Subsequent packets can be routed directly to the mobile host by tunneling them to the care of address (step  )  bypassing the home location entirely
If connectivity is lost for any reason as the mobile moves  the home address can always be used to reach the mobile
An important aspect that we have omitted from this description is urity
In general  when a host or router gets a message of the form ââStarting right now  please send all of Stephanyâs mail to me ââ it might have a couple of questions about whom it is talking to and whether this is a good idea
urity information is included in the messages so that their validity can be checked with cryptographic protocols that we will study in   There are many variations on mobile routing
The scheme above is modeled on IPv  mobility  the form of mobility used in the Internet (Johnson et al
) and as part of IP-based cellular networks such as UMTS
We showed the sender to be a stationary node for simplicity  but the designs let both nodes be mobile hosts
Alternatively  the host may be part of a mobile network  for example a computer in a plane
Extensions of the basic scheme support mobile networks with no work on the part of the hosts (Devarapalli et al
Some schemes make use of a foreign (
remote) agent  similar to the home agent but at the foreign location  or analogous to the VLR (Visitor Location Register) in cellular networks
However  in more recent schemes  the foreign agent is not needed; mobile hosts act as their own foreign agents
In either case  knowledge of the temporary location of the mobile host is limited to a small number of   ROUTING ALGORITHMS hosts (
the mobile  home agent  and senders) so that the many routers in a large network do not need to recompute routes
For more information about mobile routing  see also Perkins (  ) and Snoeren and Balakrishnan (   )
Routing in Ad Hoc Networks We have now seen how to do routing when the hosts are mobile but the routers are fixed
An even more extreme case is one in which the routers themselves are mobile
Among the possibilities are emergency workers at an earthquake site  military vehicles on a battlefield  a fleet of ships at sea  or a gathering of people with laptop computers in an area lacking    In all these cases  and others  each node communicates wirelessly and acts as both a host and a router
Networks of nodes that just happen to be near each other are called ad hoc networks or MANETs (Mobile Ad hoc NETworks)
Let us now examine them briefly
More information can be found in Perkins (   )
What makes ad hoc networks different from wired networks is that the topology is suddenly tossed out the window
Nodes can come and go or appear in new places at the drop of a bit
With a wired network  if a router has a valid path to some destination  that path continues to be valid barring failures  which are hopefully rare
With an ad hoc network  the topology may be changing all the time  so the desirability and even the validity of paths can change spontaneously without warning
Needless to say  these circumstances make routing in ad hoc networks more challenging than routing in their fixed counterparts
Many  many routing algorithms for ad hoc networks have been proposed
However  since ad hoc networks have been little used in practice compared to mobile networks  it is unclear which of these protocols are most useful
As an example  we will look at one of the most popular routing algorithms  AODV (Ad hoc On-demand Distance Vector) (Perkins and Royer  )
It is a relative of the distance vector algorithm that has been adapted to work in a mobile environment  in which nodes often have limited bandwidth and battery lifetimes
Let us now see how it discovers and maintains routes
Route Discovery In AODV  routes to a destination are discovered on demand  that is  only when a somebody wants to send a packet to that destination
This saves much work that would otherwise be wasted when the topology changes before the route is used
At any instant  the topology of an ad hoc network can be described by a graph of connected nodes
Two nodes are connected (
have an arc between them in the graph) if they can communicate directly using their radios
A basic but adequate model that is sufficient for our purposes is that each node can communicate with all other nodes that lie within its coverage circle
Real networks are THE NETWORK LAYER
more complicated  with buildings  hills  and other obstacles that block communication  and nodes for which A is connected to B but B is not connected to A because A has a more powerful transmitter than B
However  for simplicity  we will assume all connections are symmetric
To describe the algorithm  consider the newly formed ad hoc network of Fig
Suppose that a process at node A wants to send a packet to node I
The AODV algorithm maintains a distance vector table at each node  keyed by destination  giving information about that destination  including the neighbor to which to send packets to reach the destination
First  A looks in its table and does not find an entry for I
It now has to discover a route to I
This property of discovering routes only when they are needed is what makes this algorithm ââon demand
ââ A B C Range of Aâs broadcast A D B C E F F H I H I G G E E D D B C B C A A G H I F D E G H I (a) (b) (c) (d) F Figure  -
(a) Range of Aâs broadcast
(b) After B and D receive it
(c) After C  F  and G receive it
(d) After E  H  and I receive it
The shaded nodes are new recipients
The dashed lines show possible reverse routes
The solid lines show the discovered route
To locate I  A constructs a ROUTE REQUEST packet and broadcasts it using flooding  as described in
The transmission from A reaches B and D  as illustrated in Fig
Each node rebroadcasts the request  which continues to reach nodes F  G  and C in Fig
-  (c) and nodes H  E  and I in Fig
A sequence number set at the source is used to weed out duplicates during the flood
For example  D discards the transmission from B in Fig
-  (c) because it has already forwarded the request
Eventually  the request reaches node I  which constructs a ROUTE REPLY packet
This packet is unicast to the sender along the reverse of the path followed by the request
For this to work  each intermediate node must remember the node that sent it the request
The arrows in Fig
-  (b)â(d) show the reverse route information that is stored
Each intermediate node also increments a hop count as it forwards the reply
This tells the nodes how far they are from the destination
The replies tell each intermediate node which neighbor to use to reach the destination: it is the node that sent them the reply
Intermediate nodes G and D put the   ROUTING ALGORITHMS best route they hear into their routing tables as they process the reply
When the reply reaches A  a new route  ADGI  has been created
In a large network  the algorithm generates many broadcasts  even for destinations that are close by
To reduce overhead  the scope of the broadcasts is limited using the IP packetâs Time to live field
This field is initialized by the sender and decremented on each hop
If it hits   the packet is discarded instead of being broadcast
The route discovery process is then modified as follows
To locate a destination  the sender broadcasts a ROUTE REQUEST packet with Time to live set to
If no response comes back within a reasonable time  another one is sent  this time with Time to live set to
Subsequent attempts use  etc
In this way  the search is first attempted locally  then in increasingly wider rings
Route Maintenance Because nodes can move or be switched off  the topology can change spontaneously
For example  in Fig
The algorithm needs to be able to deal with this
Periodically  each node broadcasts a Hello message
Each of its neighbors is expected to respond to it
If no response is forthcoming  the broadcaster knows that that neighbor has moved out of range or failed and is no longer connected to it
Similarly  if it tries to send a packet to a neighbor that does not respond  it learns that the neighbor is no longer available
This information is used to purge routes that no longer work
For each possible destination  each node  N  keeps track of its active neighbors that have fed it a packet for that destination during the last ÎT onds
When any of Nâs neighbors becomes unreachable  it checks its routing table to see which destinations have routes using the now-gone neighbor
For each of these routes  the active neighbors are informed that their route via N is now invalid and must be purged from their routing tables
In our example  D purges its entries for G and I from its routing table and notifies A  which purges its entry for I
In the general case  the active neighbors tell their active neighbors  and so on  recursively  until all routes depending on the now-gone node are purged from all routing tables
At this stage  the invalid routes have been purged from the network  and senders can find new  valid routes by using the discovery mechanism that we described
However  there is a complication
Recall that distance vector protocols can suffer from slow convergence or count-to-infinity problems after a topology change in which they confuse old  invalid routes with new  valid routes
To ensure rapid convergence  routes include a sequence number that is controlled by the destination
The destination sequence number is like a logical clock
The destination increments it every time that it sends a fresh ROUTE REPLY
Senders ask for a fresh route by including in the ROUTE REQUEST the destination sequence number of the last route they used  which will either be the sequence number of the route that was just purged  or  as an initial value
The THE NETWORK LAYER
request will be broadcast until a route with a higher sequence number is found
Intermediate nodes store the routes that have a higher sequence number  or the fewest hops for the current sequence number
In the spirit of an on demand protocol  intermediate nodes only store the routes that are in use
Other route information learned during broadcasts is timed out after a short delay
Discovering and storing only the routes that are used helps to save bandwidth and battery life compared to a standard distance vector protocol that periodically broadcasts updates
So far  we have considered only a single route  from A to I
To further save resources  route discovery and maintenance are shared when routes overlap
For instance  if B also wants to send packets to I  it will perform route discovery
However  in this case the request will first reach D  which already has a route to I
Node D can then generate a reply to tell B the route without any additional work being required
There are many other ad hoc routing schemes
Another well-known on demand scheme is DSR (Dynamic Source Routing) (Johnson et al
A different strategy based on geography is explored by GPSR (Greedy Perimeter Stateless Routing) (Karp and Kung  )
If all nodes know their geographic positions  forwarding to a destination can proceed without route computation by simply heading in the right direction and circling back to escape any dead ends
Which protocols win out will depend on the kinds of ad hoc networks that prove useful in practice  CONGESTION CONTROL ALGORITHMS Too many packets present in (a part of) the network causes packet delay and loss that degrades performance
This situation is called congestion
The network and transport layers share the responsibility for handling congestion
Since congestion occurs within the network  it is the network layer that directly experiences it and must ultimately determine what to do with the excess packets
However  the most effective way to control congestion is to reduce the load that the transport layer is placing on the network
This requires the network and transport layers to work together
In this  ter we will look at the network aspects of congestion
we will complete the topic by covering the transport aspects of congestion
Figure  -  depicts the onset of congestion
When the number of packets hosts send into the network is well within its carrying capacity  the number delivered is proportional to the number sent
If twice as many are sent  twice as many are delivered
However  as the offered load approaches the carrying capacity  bursts of traffic occasionally fill up the buffers inside routers and some packets are lost
These lost packets consume some of the capacity  so the number of delivered packets falls below the ideal curve
The network is now congested
CONGESTION CONTROL ALGORITHMS Ideal Goodput (packets/) Desirable response Capacity of the network Congestion collapse Offered load (packet/) Onset of congestion Figure  -
With too much traffic  performance drops sharply
Unless the network is well designed  it may experience a congestion collapse  in which performance plummets as the offered load increases beyond the capacity
This can happen because packets can be sufficiently delayed inside the network that they are no longer useful when they leave the network
For example  in the early Internet  the time a packet spent waiting for a backlog of packets ahead of it to be sent over a slow  -kbps link could reach the maximum time it was allowed to remain in the network
It then had to be thrown away
A different failure mode occurs when senders retransmit packets that are greatly delayed  thinking that they have been lost
In this case  copies of the same packet will be delivered by the network  again wasting its capacity
To capture these factors  the y-axis of Fig
We would like to design networks that avoid congestion where possible and do not suffer from congestion collapse if they do become congested
Unfortunately  congestion cannot wholly be avoided
If all of a sudden  streams of packets begin arriving on three or four input lines and all need the same output line  a queue will build up
If there is insufficient memory to hold all of them  packets will be lost
Adding more memory may help up to a point  but Nagle (   ) realized that if routers have an infinite amount of memory  congestion gets worse  not better
This is because by the time packets get to the front of the queue  they have already timed out (repeatedly) and duplicates have been sent
This makes matters worse  not betterâit leads to congestion collapse
Low-bandwidth links or routers that process packets more slowly than the line rate can also become congested
In this case  the situation can be improved by directing some of the traffic away from the bottleneck to other parts of the network
Eventually  however  all regions of the network will be congested
In this situation  there is no alternative but to shed load or build a faster network
It is worth pointing out the difference between congestion control and flow control  as the relationship is a very subtle one
Congestion control has to do with THE NETWORK LAYER
making sure the network is able to carry the offered traffic
It is a global issue  involving the behavior of all the hosts and routers
Flow control  in contrast  relates to the traffic between a particular sender and a particular receiver
Its job is to make sure that a fast sender cannot continually transmit data faster than the receiver is able to absorb it
To see the difference between these two concepts  consider a network made up of   -Gbps fiber optic links on which a supercomputer is trying to force feed a large file to a personal computer that is capable of handling only  Gbps
Although there is no congestion (the network itself is not in trouble)  flow control is needed to force the supercomputer to stop frequently to give the personal computer a chance to breathe
At the other extreme  consider a network with  -Mbps lines and  large computers  half of which are trying to transfer files at kbps to the other half
Here  the problem is not that of fast senders overpowering slow receivers  but that the total offered traffic exceeds what the network can handle
The reason congestion control and flow control are often confused is that the best way to handle both problems is to get the host to slow down
Thus  a host can get a ââslow downââ message either because the receiver cannot handle the load or because the network cannot handle it
We will come back to this point in   We will start our study of congestion control by looking at the approaches that can be used at different time scales
Then we will look at approaches to preventing congestion from occurring in the first place  followed by approaches for coping with it once it has set in
Approaches to Congestion Control The presence of congestion means that the load is (temporarily) greater than the resources (in a part of the network) can handle
Two solutions come to mind: increase the resources or decrease the load
As shown in Fig
Traffic-aware routing Network provisioning Traffic throttling Admission control Load shedding Slower (Preventative) Faster (Reactive) Figure  -
Timescales of approaches to congestion control
The most basic way to avoid congestion is to build a network that is well matched to the traffic that it carries
If there is a low-bandwidth link on the path along which most traffic is directed  congestion is likely
Sometimes resources   CONGESTION CONTROL ALGORITHMS can be added dynamically when there is serious congestion  for example  turning on spare routers or enabling lines that are normally used only as backups (to make the system fault tolerant) or purchasing bandwidth on the open market
More often  links and routers that are regularly heavily utilized are upgraded at the earliest opportunity
This is called provisioning and happens on a time scale of months  driven by long-term traffic trends
To make the most of the existing network capacity  routes can be tailored to traffic patterns that change during the day as network users wake and sleep in different time zones
For example  routes may be changed to shift traffic away from heavily used paths by changing the shortest path weights
Some local radio stations have helicopters flying around their cities to report on road congestion to make it possible for their mobile listeners to route their packets (cars) around hotspots
This is called traffic-aware routing
Splitting traffic across multiple paths is also helpful
However  sometimes it is not possible to increase capacity
The only way then to beat back the congestion is to decrease the load
In a virtual-circuit network  new connections can be refused if they would cause the network to become congested
This is called admission control
At a finer granularity  when congestion is imminent the network can deliver feedback to the sources whose traffic flows are responsible for the problem
The network can request these sources to throttle their traffic  or it can slow down the traffic itself
Two difficulties with this approach are how to identify the onset of congestion  and how to inform the source that needs to slow down
To tackle the first issue  routers can monitor the average load  queueing delay  or packet loss
In all cases  rising numbers indicate growing congestion
To tackle the ond issue  routers must participate in a feedback loop with the sources
For a scheme to work correctly  the time scale must be adjusted carefully
If every time two packets arrive in a row  a router yells STOP and every time a router is idle for   Î¼  it yells GO  the system will oscillate wildly and never converge
On the other hand  if it waits   minutes to make sure before saying anything  the congestion-control mechanism will react too sluggishly to be of any use
Delivering timely feedback is a nontrivial matter
An added concern is having routers send more messages when the network is already congested
Finally  when all else fails  the network is forced to discard packets that it cannot deliver
The general name for this is load shedding
A good policy for choosing which packets to discard can help to prevent congestion collapse
Traffic-Aware Routing The first approach we will examine is traffic-aware routing
The routing schemes we looked at in
used fixed link weights
These schemes adapted to changes in topology  but not to changes in load
The goal in taking load into THE NETWORK LAYER
account when computing routes is to shift traffic away from hotspots that will be the first places in the network to experience congestion
The most direct way to do this is to set the link weight to be a function of the (fixed) link bandwidth and propagation delay plus the (variable) measured load or average queuing delay
Least-weight paths will then favor paths that are more lightly loaded  all else being equal
Traffic-aware routing was used in the early Internet according to this model (Khanna and Zinky  )
However  there is a peril
Consider the network of Fig
Suppose that most of the traffic between East and West is using link CF  and  as a result  this link is heavily loaded with long delays
Including queueing delay in the weight used for the shortest path calculation will make EI more attractive
After the new routing tables have been installed  most of the East-West traffic will now go over EI  loading this link
Consequently  in the next update  CF will appear to be the shortest path
As a result  the routing tables may oscillate wildly  leading to erratic routing and many potential problems
West East B A D E C F G H J I Figure  -
A network in which the East and West parts are connected by two links
If load is ignored and only bandwidth and propagation delay are considered  this problem does not occur
Attempts to include load but change weights within a narrow range only slow down routing oscillations
Two techniques can contribute to a successful solution
The first is multipath routing  in which there can be multiple paths from a source to a destination
In our example this means that the traffic can be spread across both of the East to West links
The ond one is for the routing scheme to shift traffic across routes slowly enough that it is able to converge  as in the scheme of Gallagher (   )
Given these difficulties  in the Internet routing protocols do not generally adjust their routes depending on the load
Instead  adjustments are made outside the routing protocol by slowly changing its inputs
This is called traffic engineering
CONGESTION CONTROL ALGORITHMS    Admission Control One technique that is widely used in virtual-circuit networks to keep congestion at bay is admission control
The idea is simple: do not set up a new virtual circuit unless the network can carry the added traffic without becoming congested
Thus  attempts to set up a virtual circuit may fail
This is better than the alternative  as letting more people in when the network is busy just makes matters worse
By analogy  in the telephone system  when a switch gets overloaded it practices admission control by not giving dial tones
The trick with this approach is working out when a new virtual circuit will lead to congestion
The task is straightforward in the telephone network because of the fixed bandwidth of calls (  kbps for uncompressed audio)
However  virtual circuits in computer networks come in all shapes and sizes
Thus  the circuit must come with some characterization of its traffic if we are to apply admission control
Traffic is often described in terms of its rate and shape
The problem of how to describe it in a simple yet meaningful way is difficult because traffic is typically burstyâthe average rate is only half the story
For example  traffic that varies while browsing the Web is more difficult to handle than a streaming movie with the same long-term throughput because the bursts of Web traffic are more likely to congest routers in the network
A commonly used descriptor that captures this effect is the leaky bucket or token bucket
A leaky bucket has two parameters that bound the average rate and the instantaneous burst size of traffic
Since leaky buckets are widely used for quality of service  we will go over them in detail in
Armed with traffic descriptions  the network can decide whether to admit the new virtual circuit
One possibility is for the network to reserve enough capacity along the paths of each of its virtual circuits that congestion will not occur
In this case  the traffic description is a service agreement for what the network will guarantee its users
We have prevented congestion but veered into the related topic of quality of service a little too early; we will return to it in the next tion
Even without making guarantees  the network can use traffic descriptions for admission control
The task is then to estimate how many circuits will fit within the carrying capacity of the network without congestion
Suppose that virtual circuits that may blast traffic at rates up to   Mbps all pass through the same   - Mbps physical link
How many circuits should be admitted? Clearly circuits can be admitted without risking congestion  but this is wasteful in the normal case since it may rarely happen that all   are transmitting full blast at the same time
In real networks  measurements of past behavior that capture the statistics of transmissions can be used to estimate the number of circuits to admit  to trade better performance for acceptable risk
Admission control can also be combined with traffic-aware routing by considering routes around traffic hotspots as part of the setup procedure
For example  THE NETWORK LAYER
consider the network illustrated in Fig
A Congestion Virtual circuit Congestion B A B (a) (b) Figure  -
(a) A congested network
(b) The portion of the network that is not congested
A virtual circuit from A to B is also shown
Suppose that a host attached to router A wants to set up a connection to a host attached to router B
Normally  this connection would pass through one of the congested routers
To avoid this situation  we can redraw the network as shown in Fig
The dashed line shows a possible route for the virtual circuit that avoids the congested routers
Shaikh et al
(   ) give a design for this kind of load-sensitive routing
Traffic Throttling In the Internet and many other computer networks  senders adjust their transmissions to send as much traffic as the network can readily deliver
In this setting  the network aims to operate just before the onset of congestion
When congestion is imminent  it must tell the senders to throttle back their transmissions and slow down
This feedback is business as usual rather than an exceptional situation
The term congestion avoidance is sometimes used to contrast this operating point with the one in which the network has become (overly) congested
Let us now look at some approaches to throttling traffic that can be used in both datagram networks and virtual-circuit networks
Each approach must solve two problems
First  routers must determine when congestion is approaching  ideally before it has arrived
To do so  each router can continuously monitor the resources it is using
Three possibilities are the utilization of the output links  the buffering of queued packets inside the router  and the number of packets that are lost due to insufficient buffering
Of these possibilities  the ond one is the most useful
Averages of utilization do not directly account for the burstiness of   CONGESTION CONTROL ALGORITHMS most trafficâa utilization of  % may be low for smooth traffic and too high for highly variable traffic
Counts of packet losses come too late
Congestion has already set in by the time that packets are lost
The queueing delay inside routers directly captures any congestion experienced by packets
It should be low most of time  but will jump when there is a burst of traffic that generates a backlog
To maintain a good estimate of the queueing delay  d  a sample of the instantaneous queue length  s  can be made periodically and d updated according to dnew = Î±dold + (  â Î±)s where the constant Î± determines how fast the router forgets recent history
This is called an EWMA (Exponentially Weighted Moving Average)
It smoothes out fluctuations and is equivalent to a low-pass filter
Whenever d moves above the threshold  the router notes the onset of congestion
The ond problem is that routers must deliver timely feedback to the senders that are causing the congestion
Congestion is experienced in the network  but relieving congestion requires action on behalf of the senders that are using the network
To deliver feedback  the router must identify the appropriate senders
It must then warn them carefully  without sending many more packets into the already congested network
Different schemes use different feedback mechanisms  as we will now describe
Choke Packets The most direct way to notify a sender of congestion is to tell it directly
In this approach  the router selects a congested packet and sends a choke packet back to the source host  giving it the destination found in the packet
The original packet may be tagged (a header bit is turned on) so that it will not generate any more choke packets farther along the path and then forwarded in the usual way
To avoid increasing load on the network during a time of congestion  the router may only send choke packets at a low rate
When the source host gets the choke packet  it is required to reduce the traffic sent to the specified destination  for example  by  %
In a datagram network  simply picking packets at random when there is congestion is likely to cause choke packets to be sent to fast senders  because they will have the most packets in the queue
The feedback implicit in this protocol can help prevent congestion yet not throttle any sender unless it causes trouble
For the same reason  it is likely that multiple choke packets will be sent to a given host and destination
The host should ignore these additional chokes for the fixed time interval until its reduction in traffic takes effect
After that period  further choke packets indicate that the network is still congested
An example of a choke packet used in the early Internet is the SOURCEQUENCH message (Postel  )
It never caught on  though  partly because the THE NETWORK LAYER
circumstances in which it was generated and the effect it had were not clearly specified
The modern Internet uses an alternative notification design that we will describe next
Explicit Congestion Notification Instead of generating additional packets to warn of congestion  a router can tag any packet it forwards (by setting a bit in the packetâs header) to signal that it is experiencing congestion
When the network delivers the packet  the destination can note that there is congestion and inform the sender when it sends a reply packet
The sender can then throttle its transmissions as before
This design is called ECN (Explicit Congestion Notification) and is used in the Internet (Ramakrishnan et al
It is a refinement of early congestion signaling protocols  notably the binary feedback scheme of Ramakrishnan and Jain (   ) that was used in the DECNET architecture
Two bits in the IP packet header are used to record whether the packet has experienced congestion
Packets are unmarked when they are sent  as illustrated in Fig
If any of the routers they pass through is congested  that router will then mark the packet as having experienced congestion as it is forwarded
The destination will then echo any marks back to the sender as an explicit congestion signal in its next reply packet
This is shown with a dashed line in the figure to indicate that it happens above the IP level (
The sender must then throttle its transmissions  as in the case of choke packets
Congestion signal Host Marked packet Host Packet Congested router Figure  -
Explicit congestion notification Hop-by-Hop Backpressure At high speeds or over long distances  many new packets may be transmitted after congestion has been signaled because of the delay before the signal takes effect
Consider  for example  a host in San Francisco (router A in Fig
-  ) that is sending traffic to a host in New York (router D in Fig
-  ) at the OC-  speed of Mbps
If the New York host begins to run out of buffers  it will take about   m for a choke packet to get back to San Francisco to tell it to slow down
An ECN indication will take even longer because it is delivered via the destination
Choke packet propagation is illustrated as the ond  third  and fourth steps in   CONGESTION CONTROL ALGORITHMS Fig
In those   m  another
megabits will have been sent
Even if the host in San Francisco completely shuts down immediately  the
megabits in the pipe will continue to pour in and have to be dealt with
Only in the seventh diagram in Fig
An alternative approach is to have the choke packet take effect at every hop it passes through  as shown in the sequence of Fig
Here  as soon as the choke packet reaches F  F is required to reduce the flow to D
Doing so will require F to devote more buffers to the connection  since the source is still sending away at full blast  but it gives D immediate relief  like a headache remedy in a television commercial
In the next step  the choke packet reaches E  which tells E to reduce the flow to F
This action puts a greater demand on Eâs buffers but gives F immediate relief
Finally  the choke packet reaches A and the flow genuinely slows down
The net effect of this hop-by-hop scheme is to provide quick relief at the point of congestion  at the price of using up more buffers upstream
In this way  congestion can be nipped in the bud without losing any packets
The idea is discussed in detail by Mishra et al
Load Shedding When none of the above methods make the congestion disappear  routers can bring out the heavy artillery: load shedding
Load shedding is a fancy way of saying that when routers are being inundated by packets that they cannot handle  they just throw them away
The term comes from the world of electrical power generation  where it refers to the practice of utilities intentionally blacking out certain areas to save the entire grid from collapsing on hot summer days when the demand for electricity greatly exceeds the supply
The key question for a router drowning in packets is which packets to drop
The preferred choice may depend on the type of applications that use the network
For a file transfer  an old packet is worth more than a new one
This is because dropping packet  and keeping packets  through for example  will only force the receiver to do more work to buffer data that it cannot yet use
In contrast  for real-time media  a new packet is worth more than an old one
This is because packets become useless if they are delayed and miss the time at which they must be played out to the user
The former policy (old is better than new) is often called wine and the latter (new is better than old) is often called milk because most people would rather drink new milk and old wine than the alternative
More intelligent load shedding requires cooperation from the senders
An example is packets that carry routing information
These packets are more important than regular data packets because they establish routes; if they are lost  the network may lose connectivity
Another example is that algorithms for compressing video  like MPEG  periodically transmit an entire frame and then send subsequent THE NETWORK LAYER
(a) (b) Choke Choke B C A D E F Choke Reduced flow Flow is still at maximum rate Flow is reduced B C A D E F Heavy flow Choke Choke Choke Reduced flow Figure  -
(a) A choke packet that affects only the source
(b) A choke packet that affects each hop it passes through
CONGESTION CONTROL ALGORITHMS frames as differences from the last full frame
In this case  dropping a packet that is part of a difference is preferable to dropping one that is part of a full frame because future packets depend on the full frame
To implement an intelligent discard policy  applications must mark their packets to indicate to the network how important they are
Then  when packets have to be discarded  routers can first drop packets from the least important class  then the next most important class  and so on
Of course  unless there is some significant incentive to avoid marking every packet as VERY IMPORTANTâNEVER  EVER DISCARD  nobody will do it
Often accounting and money are used to discourage frivolous marking
For example  the network might let senders send faster than the service they purchased allows if they mark excess packets as low priority
Such a strategy is actually not a bad idea because it makes more efficient use of idle resources  allowing hosts to use them as long as nobody else is interested  but without establishing a right to them when times get tough
Random Early Detection Dealing with congestion when it first starts is more effective than letting it gum up the works and then trying to deal with it
This observation leads to an interesting twist on load shedding  which is to discard packets before all the buffer space is really exhausted
The motivation for this idea is that most Internet hosts do not yet get congestion signals from routers in the form of ECN
Instead  the only reliable indication of congestion that hosts get from the network is packet loss
After all  it is difficult to build a router that does not drop packets when it is overloaded
Transport protocols such as TCP are thus hardwired to react to loss as congestion  slowing down the source in response
The reasoning behind this logic is that TCP was designed for wired networks and wired networks are very reliable  so lost packets are mostly due to buffer overruns rather than transmission errors
Wireless links must recover transmission errors at the link layer (so they are not seen at the network layer) to work well with TCP
This situation can be exploited to help reduce congestion
By having routers drop packets early  before the situation has become hopeless  there is time for the source to take action before it is too late
A popular algorithm for doing this is called RED (Random Early Detection) (Floyd and Jacobson  )
To determine when to start discarding  routers maintain a running average of their queue lengths
When the average queue length on some link exceeds a threshold  the link is said to be congested and a small fraction of the packets are dropped at random
Picking packets at random makes it more likely that the fastest senders will see a packet drop; this is the best option since the router cannot tell which source is causing the most trouble in a datagram network
The affected sender will notice the loss when there is no acknowledgement  and then the transport protocol THE NETWORK LAYER
will slow down
The lost packet is thus delivering the same message as a choke packet  but implicitly  without the router sending any explicit signal
RED routers improve performance compared to routers that drop packets only when their buffers are full  though they may require tuning to work well
For example  the ideal number of packets to drop depends on how many senders need to be notified of congestion
However  ECN is the preferred option if it is available
It works in exactly the same manner  but delivers a congestion signal explicitly rather than as a loss; RED is used when hosts cannot receive explicit signals  QUALITY OF SERVICE The techniques we looked at in the previous tions are designed to reduce congestion and improve network performance
However  there are applications (and customers) that demand stronger performance guarantees from the network than ââthe best that could be done under the circumstances
ââ Multimedia applications in particular  often need a minimum throughput and maximum latency to work
In this tion  we will continue our study of network performance  but now with a sharper focus on ways to provide quality of service that is matched to application needs
This is an area in which the Internet is undergoing a long-term upgrade
An easy solution to provide good quality of service is to build a network with enough capacity for whatever traffic will be thrown at it
The name for this solution is overprovisioning
The resulting network will carry application traffic without significant loss and  assuming a decent routing scheme  will deliver packets with low latency
Performance doesnât get any better than this
To some extent  the telephone system is overprovisioned because it is rare to pick up a telephone and not get a dial tone instantly
There is simply so much capacity available that demand can almost always be met
The trouble with this solution is that it is expensive
It is basically solving a problem by throwing money at it
Quality of service mechanisms let a network with less capacity meet application requirements just as well at a lower cost
Moreover  overprovisioning is based on expected traffic
All bets are off if the traffic pattern changes too much
With quality of service mechanisms  the network can honor the performance guarantees that it makes even when traffic spikes  at the cost of turning down some requests
Four issues must be addressed to ensure quality of service:
What applications need from the network How to regulate the traffic that enters the network How to reserve resources at routers to guarantee performance Whether the network can safely accept more traffic
QUALITY OF SERVICE No single technique deals efficiently with all these issues
Instead  a variety of techniques have been developed for use at the network (and transport) layer
Practical quality-of-service solutions combine multiple techniques
To this end  we will describe two versions of quality of service for the Internet called Integrated Services and Differentiated Services
Application Requirements A stream of packets from a source to a destination is called a flow (Clark  )
A flow might be all the packets of a connection in a connection-oriented network  or all the packets sent from one process to another process in a connectionless network
The needs of each flow can be characterized by four primary parameters: bandwidth  delay  jitter  and loss
Together  these determine the QoS (Quality of Service) the flow requires
Several common applications and the stringency of their network requirements are listed in Fig
Note that network requirements are less demanding than application requirements in those cases that the application can improve on the service provided by the network
In particular  networks do not need to be lossless for reliable file transfer  and they do not need to deliver packets with identical delays for audio and video playout
Some amount of loss can be repaired with retransmissions  and some amount of jitter can be smoothed by buffering packets at the receiver
However  there is nothing applications can do to remedy the situation if the network provides too little bandwidth or too much delay
Application Bandwidth Delay Jitter Loss Email Low Low Low Medium File sharing High Low Low Medium Web access Medium Medium Low Medium Remote login Low Medium Medium Medium Audio on demand Low Low High Low Video on demand High Low High Low Telephony Low High High Low Videoconferencing High High High Low Figure  -
Stringency of applicationsâ quality-of-service requirements
The applications differ in their bandwidth needs  with email  audio in all forms  and remote login not needing much  but file sharing and video in all forms needing a great deal
More interesting are the delay requirements
File transfer applications  including email and video  are not delay sensitive
If all packets are delayed uniformly by a few onds  no harm is done
Interactive applications  such as Web THE NETWORK LAYER
surfing and remote login  are more delay sensitive
Real-time applications  such as telephony and videoconferencing  have strict delay requirements
If all the words in a telephone call are each delayed by too long  the users will find the connection unacceptable
On the other hand  playing audio or video files from a server does not require low delay
The variation (
standard deviation) in the delay or packet arrival times is called jitter
The first three applications in Fig
Remote login is somewhat sensitive to that  since updates on the screen will appear in little bursts if the connection suffers much jitter
Video and especially audio are extremely sensitive to jitter
If a user is watching a video over the network and the frames are all delayed by exactly
onds  no harm is done
But if the transmission time varies randomly between  and  onds  the result will be terrible unless the application hides the jitter
For audio  a jitter of even a few millionds is clearly audible
The first four applications have more stringent requirements on loss than audio and video because all bits must be delivered correctly
This goal is usually achieved with retransmissions of packets that are lost in the network by the transport layer
This is wasted work; it would be better if the network refused packets it was likely to lose in the first place
Audio and video applications can tolerate some lost packets without retransmission because people do not notice short pauses or occasional skipped frames
To accommodate a variety of applications  networks may support different categories of QoS
An influential example comes from ATM networks  which were once part of a grand vision for networking but have since become a niche technology
They support:
Constant bit rate (
telephony) Real-time variable bit rate (
compressed videoconferencing) Non-real-time variable bit rate (
watching a movie on demand) Available bit rate (
file transfer)
These categories are also useful for other purposes and other networks
Constant bit rate is an attempt to simulate a wire by providing a uniform bandwidth and a uniform delay
Variable bit rate occurs when video is compressed  with some frames compressing more than others
Sending a frame with a lot of detail in it may require sending many bits  whereas a shot of a white wall may compress extremely well
Movies on demand are not actually real time because a few onds of video can easily be buffered at the receiver before playback starts  so jitter on the network merely causes the amount of stored-but-not-played video to vary
Available bit rate is for applications such as email that are not sensitive to delay or jitter and will take what bandwidth they can get
QUALITY OF SERVICE    Traffic Shaping Before the network can make QoS guarantees  it must know what traffic is being guaranteed
In the telephone network  this characterization is simple
For example  a voice call (in uncompressed format) needs   kbps and consists of one  -bit sample every Î¼
However  traffic in data networks is bursty
It typically arrives at nonuniform rates as the traffic rate varies (
videoconferencing with compression)  users interact with applications (
browsing a new Web page)  and computers switch between tasks
Bursts of traffic are more difficult to handle than constant-rate traffic because they can fill buffers and cause packets to be lost
Traffic shaping is a technique for regulating the average rate and burstiness of a flow of data that enters the network
The goal is to allow applications to transmit a wide variety of traffic that suits their needs  including some bursts  yet have a simple and useful way to describe the possible traffic patterns to the network
When a flow is set up  the user and the network (
the customer and the provider) agree on a certain traffic pattern (
shape) for that flow
In effect  the customer says to the provider ââMy transmission pattern will look like this; can you handle it?ââ Sometimes this agreement is called an SLA (Service Level Agreement)  especially when it is made over aggregate flows and long periods of time  such as all of the traffic for a given customer
As long as the customer fulfills her part of the bargain and only sends packets according to the agreed-on contract  the provider promises to deliver them all in a timely fashion
Traffic shaping reduces congestion and thus helps the network live up to its promise
However  to make it work  there is also the issue of how the provider can tell if the customer is following the agreement and what to do if the customer is not
Packets in excess of the agreed pattern might be dropped by the network  or they might be marked as having lower priority
Monitoring a traffic flow is called traffic policing
Shaping and policing are not so important for peer-to-peer and other transfers that will consume any and all available bandwidth  but they are of great importance for real-time data  such as audio and video connections  which have stringent quality-of-service requirements
Leaky and Token Buckets We have already seen one way to limit the amount of data an application sends: the sliding window  which uses one parameter to limit how much data is in transit at any given time  which indirectly limits the rate
Now we will look at a more general way to characterize traffic  with the leaky bucket and token bucket algorithms
The formulations are slightly different but give an equivalent result
THE NETWORK LAYER
Try to imagine a bucket with a small hole in the bottom  as illustrated in Fig
No matter the rate at which water enters the bucket  the outflow is at a constant rate  R  when there is any water in the bucket and zero when the bucket is empty
Also  once the bucket is full to capacity B  any additional water entering it spills over the sides and is lost
Check bucket here Host Packets Rate R B B Rate R Take out water/tokens Put in water Network (a) (b) (c) Figure  -
(a) Shaping packets
(b) A leaky bucket
(c) A token bucket
This bucket can be used to shape or police packets entering the network  as shown in Fig
Conceptually  each host is connected to the network by an interface containing a leaky bucket
To send a packet into the network  it must be possible to put more water into the bucket
If a packet arrives when the bucket is full  the packet must either be queued until enough water leaks out to hold it or be discarded
The former might happen at a host shaping its traffic for the network as part of the operating system
The latter might happen in hardware at a provider network interface that is policing traffic entering the network
This technique was proposed by Turner (   ) and is called the leaky bucket algorithm
A different but equivalent formulation is to imagine the network interface as a bucket that is being filled  as shown in Fig
The tap is running at rate R and the bucket has a capacity of B  as before
Now  to send a packet we must be able to take water  or tokens  as the contents are commonly called  out of the bucket (rather than putting water into the bucket)
No more than a fixed number of tokens  B  can accumulate in the bucket  and if the bucket is empty  we must wait until more tokens arrive before we can send another packet
This algorithm is called the token bucket algorithm
Leaky and token buckets limit the long-term rate of a flow but allow shortterm bursts up to a maximum regulated length to pass through unaltered and without suffering any artificial delays
Large bursts will be smoothed by a leaky bucket traffic shaper to reduce congestion in the network
As an example  imagine that a computer can produce data at up to  Mbps (   million bytes/) and that the first link of the network also runs at this speed
The pattern of traffic the host generates is shown in Fig
This pattern is bursty
The average   QUALITY OF SERVICE rate over one ond is Mbps  even though the host sends a burst of   KB at the top speed of  Mbps (for  /  of the ond)
MB/s for m MB/s for m Time (m)   Rate (Mbps) (a) (d) (b) (e) (c) (f)  Bucket (KB) With R =   MB/s  B =  With R =   MB/s  B =  KB Bucket always empty Bucket empties  traffic delayed Time (m)    Figure  -
(a) Traffic from a host
Output shaped by a token bucket of rate Mbps and capacity (b)  KB and (c)  KB
Token bucket level for shaping with rate Mbps and capacity (d)   KB  (e)  KB  and (f)  KB
Now suppose that the routers can accept data at the top speed only for short intervals  until their buffers fill up
The buffer size is  KB  smaller than the traffic burst
For long intervals  the routers work best at rates not exceeding Mbps (say  because this is all the bandwidth given to the customer)
The implication is that if traffic is sent in this pattern  some of it will be dropped in the network because it does not fit into the buffers at routers
To avoid this packet loss  we can shape the traffic at the host with a token bucket
If we use a rate  R  of Mbps and a capacity  B  of  KB  the traffic will fall within what the network can handle
The output of this token bucket is shown in Fig
The host can send full throttle at  Mbps for a short while until it has drained the bucket
Then it has to cut back to Mbps until the burst has been sent
The effect is to spread out the burst over time because it was too large to handle all at once
The level of the token bucket is shown in Fig
It starts off full and is depleted by the initial burst
When it reaches zero  new packets can be sent only at the rate at which the buffer is filling; there can be no more bursts until the bucket has recovered
The bucket fills when no traffic is being sent and stays flat when traffic is being sent at the fill rate
We can also shape the traffic to be less bursty
-  (c) shows the output of a token bucket with R = Mbps and a capacity of
This is the extreme case THE NETWORK LAYER
in which the traffic has been completely smoothed
No bursts are allowed  and the traffic enters the network at a steady rate
The corresponding bucket level  shown in Fig
Traffic is being queued on the host for release into the network and there is always a packet waiting to be sent when it is allowed
Finally  Fig
-  (d) shows the bucket level for a token bucket with R = Mbps and a capacity of B =   KB
This is the smallest token bucket through which the traffic passes unaltered
It might be used at a router in the network to police the traffic that the host sends
If the host is sending traffic that conforms to the token bucket on which it has agreed with the network  the traffic will fit through that same token bucket run at the router at the edge of the network
If the host sends at a faster or burstier rate  the token bucket will run out of water
If this happens  a traffic policer will know that the traffic is not as described
It will then either drop the excess packets or lower their priority  depending on the design of the network
In our example  the bucket empties only momentarily  at the end of the initial burst  then recovers enough for the next burst
Leaky and token buckets are easy to implement
We will now describe the operation of a token bucket
Even though we have described water flowing continuously into and out of the bucket  real implementations must work with discrete quantities
A token bucket is implemented with a counter for the level of the bucket
The counter is advanced by R /ÎT units at every clock tick of ÎT onds
This would be Kbit every  m in our example above
Every time a unit of traffic is sent into the network  the counter is decremented  and traffic may be sent until the counter reaches zero
When the packets are all the same size  the bucket level can just be counted in packets (
Mbit is   packets of  bytes)
However  often variablesized packets are being used
In this case  the bucket level is counted in bytes
If the residual byte count is too low to send a large packet  the packet must wait until the next tick (or even longer  if the fill rate is small)
Calculating the length of the maximum burst (until the bucket empties) is slightly tricky
It is longer than just  KB divided by MB/ because while the burst is being output  more tokens arrive
If we call the burst length S
the maximum output rate M bytes/  the token bucket capacity B bytes  and the token arrival rate R bytes/  we can see that an output burst contains a maximum of B + RS bytes
We also know that the number of bytes in a maximumspeed burst of length S onds is MS
Hence  we have B + RS = MS We can solve this equation to get S = B /(M â R)
For our parameters of B =  KB  M = MB/  and R =   MB/  we get a burst time of about   m
A potential problem with the token bucket algorithm is that it reduces large bursts down to the long-term rate R
It is frequently desirable to reduce the peak rate  but without going down to the long-term rate (and also without raising the   QUALITY OF SERVICE long-term rate to allow more traffic into the network)
One way to get smoother traffic is to insert a ond token bucket after the first one
The rate of the ond bucket should be much higher than the first one
Basically  the first bucket characterizes the traffic  fixing its average rate but allowing some bursts
The ond bucket reduces the peak rate at which the bursts are sent into the network
For example  if the rate of the ond token bucket is set to be Mbps and the capacity is set to   the initial burst will enter the network at a peak rate of Mbps  which is lower than the  Mbps rate we had previously
Using all of these buckets can be a bit tricky
When token buckets are used for traffic shaping at hosts  packets are queued and delayed until the buckets permit them to be sent
When token buckets are used for traffic policing at routers in the network  the algorithm is simulated to make sure that no more packets are sent than permitted
Nevertheless  these tools provide ways to shape the network traffic into more manageable forms to assist in meeting quality-of-service requirements
Packet Scheduling Being able to regulate the shape of the offered traffic is a good start
However  to provide a performance guarantee  we must reserve sufficient resources along the route that the packets take through the network
To do this  we are assuming that the packets of a flow follow the same route
Spraying them over routers at random makes it hard to guarantee anything
As a consequence  something similar to a virtual circuit has to be set up from the source to the destination  and all the packets that belong to the flow must follow this route
Algorithms that allocate router resources among the packets of a flow and between competing flows are called packet scheduling algorithms
Three different kinds of resources can potentially be reserved for different flows:
Bandwidth Buffer space CPU cycles
The first one  bandwidth  is the most obvious
If a flow requires  Mbps and the outgoing line has a capacity of  Mbps  trying to direct three flows through that line is not going to work
Thus  reserving bandwidth means not oversubscribing any output line
A ond resource that is often in short supply is buffer space
When a packet arrives  it is buffered inside the router until it can be transmitted on the chosen outgoing line
The purpose of the buffer is to absorb small bursts of traffic as the flows contend with each other
If no buffer is available  the packet has to be discarded since there is no place to put it
For good quality of service  some buffers might be reserved for a specific flow so that flow does not have to compete for THE NETWORK LAYER
buffers with other flows
Up to some maximum value  there will always be a buffer available when the flow needs one
Finally  CPU cycles may also be a scarce resource
It takes router CPU time to process a packet  so a router can process only a certain number of packets per ond
While modern routers are able to process most packets quickly  some kinds of packets require greater CPU processing  such as the ICMP packets we will describe in
Making sure that the CPU is not overloaded is needed to ensure timely processing of these packets
Packet scheduling algorithms allocate bandwidth and other router resources by determining which of the buffered packets to send on the output line next
We already described the most straightforward scheduler when explaining how routers work
Each router buffers packets in a queue for each output line until they can be sent  and they are sent in the same order that they arrived
This algorithm is known as FIFO (First-In First-Out)  or equivalently FCFS (First-Come First-Serve)
FIFO routers usually drop newly arriving packets when the queue is full
Since the newly arrived packet would have been placed at the end of the queue  this behavior is called tail drop
It is intuitive  and you may be wondering what alternatives exist
In fact  the RED algorithm we described in
chose a newly arriving packet to drop at random when the average queue length grew large
The other scheduling algorithms that we will describe also create other opportunities for deciding which packet to drop when the buffers are full
FIFO scheduling is simple to implement  but it is not suited to providing good quality of service because when there are multiple flows  one flow can easily affect the performance of the other flows
If the first flow is aggressive and sends large bursts of packets  they will lodge in the queue
Processing packets in the order of their arrival means that the aggressive sender can hog most of the capacity of the routers its packets traverse  starving the other flows and reducing their quality of service
To add insult to injury  the packets of the other flows that do get through are likely to be delayed because they had to sit in the queue behind many packets from the aggressive sender
Many packet scheduling algorithms have been devised that provide stronger isolation between flows and thwart attempts at interference (Bhatti and Crowcroft  )
One of the first ones was the fair queueing algorithm devised by Nagle (   )
The essence of this algorithm is that routers have separate queues  one for each flow for a given output line
When the line becomes idle  the router scans the queues round-robin  as shown in Fig
It then takes the first packet on the next queue
In this way  with n hosts competing for the output line  each host gets to send one out of every n packets
It is fair in the sense that all flows get to send packets at the same rate
Sending more packets will not improve this rate
Although a start  the algorithm has a flaw: it gives more bandwidth to hosts that use large packets than to hosts that use small packets
Demers et al
(   ) suggested an improvement in which the round-robin is done in such a way as to   QUALITY OF SERVICE Input queues Round-robin service     Output line Figure  -
Round-robin fair queueing
simulate a byte-by-byte round-robin  instead of a packet-by-packet round-robin
The trick is to compute a virtual time that is the number of the round at which each packet would finish being sent
Each round drains a byte from all of the queues that have data to send
The packets are then sorted in order of their finishing times and sent in that order
This algorithm and an example of finish times for packets arriving in three flows are illustrated in Fig
If a packet has length L  the round at which it will finish is simply L rounds after the start time
The start time is either the finish time of the previous packet  or the arrival time of the packet  if the queue is empty when it arrives
Input queues Fair queueing Packet Arrival time Length Finish time Output order A    B  C  D  E  F  G   H  A B G E C D F H Arrives late (a) (b) Arrives after D but goes first Weight is   X Figure  -
(a) Weighted Fair Queueing
(b) Finishing times for the packets
From the table in Fig
Packet A arrives at round  and is  bytes long  so its finish time is round
Similarly the finish time for packet B is
Packet D arrives while B is being sent
Its finish time is  byte-rounds after it starts when B finishes  or
Similarly  the finish time for F is
In the absence of new arrivals  the relative sending order is A  B  F  D  even though F arrived after D
It is possible that another small packet will arrive on the top flow and obtain a finish time before D
It will only jump ahead of D if the THE NETWORK LAYER
transmission of that packet has not started
Fair queueing does not preempt packets that are currently being transmitted
Because packets are sent in their entirety  fair queueing is only an approximation of the ideal byte-by-byte scheme
But it is a very good approximation  staying within one packet transmission of the ideal scheme at all times
One shortcoming of this algorithm in practice is that it gives all hosts the same priority
In many situations  it is desirable to give  for example  video servers more bandwidth than  say  file servers
This is easily possible by giving the video server two or more bytes per round
This modified algorithm is called WFQ (Weighted Fair Queueing)
Letting the number of bytes per round be the weight of a flow  W  we can now give the formula for computing the finish time: Fi = max(Ai  Fi â )+Li /W where Ai is the arrival time  Fi is the finish time  and Li is the length of packet i
The bottom queue of Fig
Another practical consideration is implementation complexity
WFQ requires that packets be inserted by their finish time into a sorted queue
With N flows  this is at best an O(logN) operation per packet  which is difficult to achieve for many flows in high-speed routers
Shreedhar and Varghese (   ) describe an approximation called deficit round robin that can be implemented very efficiently  with only O( ) operations per packet
WFQ is widely used given this approximation
Other kinds of scheduling algorithms exist  too
A simple example is priority scheduling  in which each packet is marked with a priority
High-priority packets are always sent before any low-priority packets that are buffered
Within a priority  packets are sent in FIFO order
However  priority scheduling has the disadvantage that a burst of high-priority packets can starve low-priority packets  which may have to wait indefinitely
WFQ often provides a better alternative
By giving the high-priority queue a large weight  say   high-priority packets will often go through a short line (as relatively few packets should be high priority) yet some fraction of low priority packets will continue to be sent even when there is high priority traffic
A high and low priority system is essentially a two-queue WFQ system in which the high priority has infinite weight
As a final example of a scheduler  packets might carry timestamps and be sent in timestamp order
Clark et al
(   ) describe a design in which the timestamp records how far the packet is behind or ahead of schedule as it is sent through a sequence of routers on the path
Packets that have been queued behind other packets at a router will tend to be behind schedule  and the packets that have been serviced first will tend to be ahead of schedule
Sending packets in order of their timestamps has the beneficial effect of speeding up slow packets while at the same time slowing down fast packets
The result is that all packets are delivered by the network with a more consistent delay
QUALITY OF SERVICE    Admission Control We have now seen all the necessary elements for QoS and it is time to put them together to actually provide it
QoS guarantees are established through the process of admission control
We first saw admission control used to control congestion  which is a performance guarantee  albeit a weak one
The guarantees we are considering now are stronger  but the model is the same
The user offers a flow with an accompanying QoS requirement to the network
The network then decides whether to accept or reject the flow based on its capacity and the commitments it has made to other flows
If it accepts  the network reserves capacity in advance at routers to guarantee QoS when traffic is sent on the new flow
The reservations must be made at all of the routers along the route that the packets take through the network
Any routers on the path without reservations might become congested  and a single congested router can break the QoS guarantee
Many routing algorithms find the single best path between each source and each destination and send all traffic over the best path
This may cause some flows to be rejected if there is not enough spare capacity along the best path
QoS guarantees for new flows may still be accommodated by choosing a different route for the flow that has excess capacity
This is called QoS routing
Chen and Nahrstedt (   ) give an overview of these techniques
It is also possible to split the traffic for each destination over multiple paths to more easily find excess capacity
A simple method is for routers to choose equal-cost paths and to divide the traffic equally or in proportion to the capacity of the outgoing links
However  more sophisticated algorithms are also available (Nelakuditi and Zhang  )
Given a path  the decision to accept or reject a flow is not a simple matter of comparing the resources (bandwidth  buffers  cycles) requested by the flow with the routerâs excess capacity in those three dimensions
It is a little more complicated than that
To start with  although some applications may know about their bandwidth requirements  few know about buffers or CPU cycles  so at the minimum  a different way is needed to describe flows and translate this description to router resources
We will get to this shortly
Next  some applications are far more tolerant of an occasional missed deadline than others
The applications must choose from the type of guarantees that the network can make  whether hard guarantees or behavior that will hold most of the time
All else being equal  everyone would like hard guarantees  but the difficulty is that they are expensive because they constrain worst case behavior
Guarantees for most of the packets are often sufficient for applications  and more flows with this guarantee can be supported for a fixed capacity
Finally  some applications may be willing to haggle about the flow parameters and others may not
For example  a movie viewer that normally runs at   frames/ may be willing to drop back to   frames/ if there is not enough free bandwidth to support   frames/
Similarly  the number of pixels per frame  audio bandwidth  and other properties may be adjustable
THE NETWORK LAYER
Because many parties may be involved in the flow negotiation (the sender  the receiver  and all the routers along the path between them)  flows must be described accurately in terms of specific parameters that can be negotiated
A set of such parameters is called a flow specification
Typically  the sender (
the video server) produces a flow specification proposing the parameters it would like to use
As the specification propagates along the route  each router examines it and modifies the parameters as need be
The modifications can only reduce the flow  not increase it (
a lower data rate  not a higher one)
When it gets to the other end  the parameters can be established
As an example of what can be in a flow specification  consider the example of Fig
This is based on RFCs  and  for Integrated Services  a QoS design we will cover in the next tion
It has five parameters
The first two parameters  the token bucket rate and token bucket size  use a token bucket to give the maximum sustained rate the sender may transmit  averaged over a long time interval  and the largest burst it can send over a short time interval
Parameter Unit Token bucket rate Bytes/ Token bucket size Bytes Peak data rate Bytes/ Minimum packet size Bytes Maximum packet size Bytes Figure  -
An example flow specification
The third parameter  the peak data rate  is the maximum transmission rate tolerated  even for brief time intervals
The sender must never exceed this rate even for short bursts
The last two parameters specify the minimum and maximum packet sizes  including the transport and network layer headers (
TCP and IP)
The minimum size is useful because processing each packet takes some fixed time  no matter how short
A router may be prepared to handle   packets/ of  KB each  but not be prepared to handle    packets/ of   bytes each  even though this represents a lower data rate
The maximum packet size is important due to internal network limitations that may not be exceeded
For example  if part of the path goes over an Ethernet  the maximum packet size will be restricted to no more than  bytes no matter what the rest of the network can handle
An interesting question is how a router turns a flow specification into a set of specific resource reservations
At first glance  it might appear that if a router has a link that runs at  say   Gbps and the average packet is  bits  it can process  million packets/
This observation is not the case  though  because there will always be idle periods on the link due to statistical fluctuations in the load
If the   QUALITY OF SERVICE link needs every bit of capacity to get its work done  idling for even a few bits creates a backlog it can never get rid of
Even with a load slightly below the theoretical capacity  queues can build up and delays can occur
Consider a situation in which packets arrive at random with a mean arrival rate of Î» packets/
The packets have random lengths and can be sent on the link with a mean service rate of Î¼ packets/
Under the assumption that both the arrival and service distributions are Poisson distributions (what is called an M/M/  queueing system  where ââMââ stands for Markov
Poisson)  it can be proven using queueing theory that the mean delay experienced by a packet  T  is T = Î¼  Ã  â Î»/Î¼  = Î¼  Ã  â Ï  where Ï = Î»/Î¼ is the CPU utilization
The first factor  /Î¼  is what the service time would be in the absence of competition
The ond factor is the slowdown due to competition with other flows
For example  if Î» =    packets/ and Î¼ =  packets/  then Ï =
and the mean delay experienced by each packet will be   Î¼ instead of  Î¼
This time accounts for both the queueing time and the service time  as can be seen when the load is very low (Î»/Î¼â¼â¼  )
If there are  say routers along the flowâs route  queueing delay alone will account for Î¼ of delay
One method of relating flow specifications to router resources that correspond to bandwidth and delay performance guarantees is given by Parekh and Gallagher (  )
It is based on traffic sources shaped by (R  B) token buckets and WFQ at routers
Each flow is given a WFQ weight W large enough to drain its token bucket rate R as shown in Fig
For example  if the flow has a rate of  Mbps and the router and output link have a capacity of  Gbps  the weight for the flow must be greater than  /   th of the total of the weights for all of the flows at that router for the output link
This guarantees the flow a minimum bandwidth
If it cannot be given a large enough rate  the flow cannot be admitted
Weighted fair queue (R  B) Traffic source Router Capacity C W wi wi R < W x C weights Figure  -
Bandwidth and delay guarantees with token buckets and WFQ
The largest queueing delay the flow will see is a function of the burst size of the token bucket
Consider the two extreme cases
If the traffic is smooth  without THE NETWORK LAYER
any bursts  packets will be drained from the router just as quickly as they arrive
There will be no queueing delay (ignoring packetization effects)
On the other hand  if the traffic is saved up in bursts  then a maximum-size burst  B  may arrive at the router all at once
In this case the maximum queueing delay  D  will be the time taken to drain this burst at the guaranteed bandwidth  or B/R (again  ignoring packetization effects)
If this delay is too large  the flow must request more bandwidth from the network
These guarantees are hard
The token buckets bound the burstiness of the source  and fair queueing isolates the bandwidth given to different flows
This means that the flow will meet its bandwidth and delay guarantees regardless of how the other competing flows behave at the router
Those other flows cannot break the guarantee even by saving up traffic and all sending at once
Moreover  the result holds for a path through multiple routers in any network topology
Each flow gets a minimum bandwidth because that bandwidth is guaranteed at each router
The reason each flow gets a maximum delay is more subtle
In the worst case that a burst of traffic hits the first router and competes with the traffic of other flows  it will be delayed up to the maximum delay of D
However  this delay will also smooth the burst
In turn  this means that the burst will incur no further queueing delays at later routers
The overall queueing delay will be at most D
Integrated Services Between  and  IETF put a lot of effort into devising an architecture for streaming multimedia
This work resulted in over two dozen RFCs  starting with RFCs â
The generic name for this work is integrated services
It was aimed at both unicast and multicast applications
An example of the former is a single user streaming a video clip from a news site
An example of the latter is a collection of digital television stations broadcasting their programs as streams of IP packets to many receivers at various locations
Below we will concentrate on multicast  since unicast is a special case of multicast
In many multicast applications  groups can change membership dynamically  for example  as people enter a video conference and then get bored and switch to a soap opera or the croquet channel
Under these conditions  the approach of having the senders reserve bandwidth in advance does not work well  since it would require each sender to track all entries and exits of its audience
For a system designed to transmit television with millions of subscribers  it would not work at all
RSVPâThe Resource reSerVation Protocol The main part of the integrated services architecture that is visible to the users of the network is RSVP
It is described in RFCs â
This protocol is used for making the reservations; other protocols are used for sending the data
QUALITY OF SERVICE RSVP allows multiple senders to transmit to multiple groups of receivers  permits individual receivers to switch channels freely  and optimizes bandwidth use while at the same time eliminating congestion
In its simplest form  the protocol uses multicast routing using spanning trees  as discussed earlier
Each group is assigned a group address
To send to a group  a sender puts the groupâs address in its packets
The standard multicast routing algorithm then builds a spanning tree covering all group members
The routing algorithm is not part of RSVP
The only difference from normal multicasting is a little extra information that is multicast to the group periodically to tell the routers along the tree to maintain certain data structures in their memories
As an example  consider the network of Fig
Hosts  and  are multicast senders  and hosts  and  are multicast receivers
In this example  the senders and receivers are disjoint  but in general  the two sets may overlap
The multicast trees for hosts  and  are shown in Fig
-  (c)  respectively
A D G J C F I L B K H E  Receivers Senders A D G J C F I L B K H E    A D G J C F I L B K H E (a) (b) (c) Figure  -
(a) A network
(b) The multicast spanning tree for host
(c) The multicast spanning tree for host
To get better reception and eliminate congestion  any of the receivers in a group can send a reservation message up the tree to the sender
The message is propagated using the reverse path forwarding algorithm discussed earlier
At each THE NETWORK LAYER
hop  the router notes the reservation and reserves the necessary bandwidth
We saw in the previous tion how a weighted fair queueing scheduler can be used to make this reservation
If insufficient bandwidth is available  it reports back failure
By the time the message gets back to the source  bandwidth has been reserved all the way from the sender to the receiver making the reservation request along the spanning tree
An example of such a reservation is shown in Fig
Here host  has requested a channel to host
Once it has been established  packets can flow from  to  without congestion
Now consider what happens if host  next reserves a channel to the other sender  host   so the user can watch two television programs at once
A ond path is reserved  as illustrated in Fig
Note that two separate channels are needed from host  to router E because two independent streams are being transmitted
A D G J C F Bandwidth reserved for source  Bandwidth reserved for source  I L B K H E  A D G J C F I L B K H E    A D G J C F I L B K H E (a) (b) (c)    Figure  -
(a) Host  requests a channel to host
(b) Host  then requests a ond channel  to host
(c) Host  requests a channel to host
Finally  in Fig
-  (c)  host  decides to watch the program being transmitted by host  and also makes a reservation
First  dedicated bandwidth is reserved as far as router H
However  this router sees that it already has a feed from host   so if the necessary bandwidth has already been reserved  it does not have to reserve any more
Note that hosts  and  might have asked for different amounts of bandwidth (
if host  is playing on a small screen and only wants the lowresolution information)  so the capacity reserved must be large enough to satisfy the greediest receiver
When making a reservation  a receiver can (optionally) specify one or more sources that it wants to receive from
It can also specify whether these choices   QUALITY OF SERVICE are fixed for the duration of the reservation or whether the receiver wants to keep open the option of changing sources later
The routers use this information to optimize bandwidth planning
In particular  two receivers are only set up to share a path if they both agree not to change sources later on
The reason for this strategy in the fully dynamic case is that reserved bandwidth is decoupled from the choice of source
Once a receiver has reserved bandwidth  it can switch to another source and keep that portion of the existing path that is valid for the new source
If host  is transmitting several video streams in real time  for example a TV broadcaster with multiple channels  host  may switch between them at will without changing its reservation: the routers do not care what program the receiver is watching
Differentiated Services Flow-based algorithms have the potential to offer good quality of service to one or more flows because they reserve whatever resources are needed along the route
However  they also have a downside
They require an advance setup to establish each flow  something that does not scale well when there are thousands or millions of flows
Also  they maintain internal per-flow state in the routers  making them vulnerable to router crashes
Finally  the changes required to the router code are substantial and involve complex router-to-router exchanges for setting up the flows
As a consequence  while work continues to advance integrated services  few deployments of it or anything like it exist yet
For these reasons  IETF has also devised a simpler approach to quality of service  one that can be largely implemented locally in each router without advance setup and without having the whole path involved
This approach is known as class-based (as opposed to flow-based) quality of service
IETF has standardized an architecture for it  called differentiated services  which is described in RFCs    and numerous others
We will now describe it
Differentiated services can be offered by a set of routers forming an administrative domain (
an ISP or a telco)
The administration defines a set of service classes with corresponding forwarding rules
If a customer subscribes to differentiated services  customer packets entering the domain are marked with the class to which they belong
This information is carried in the Differentiated services field of IPv  and IPv  packets (described in  )
The classes are defined as per hop behaviors because they correspond to the treatment the packet will receive at each router  not a guarantee across the network
Better service is provided to packets with some per-hop behaviors (
premium service) than to others (
regular service)
Traffic within a class may be required to conform to some specific shape  such as a leaky bucket with some specified drain rate
An operator with a nose for business might charge extra for each premium packet transported or might allow up to N premium packets per month for a fixed additional monthly fee
Note that this scheme requires no advance setup  no resource THE NETWORK LAYER
reservation  and no time-consuming end-to-end negotiation for each flow  as with integrated services
This makes differentiated services relatively easy to implement
Class-based service also occurs in other industries
For example  package delivery companies often offer overnight  two-day  and three-day service
Airlines offer first class  business class  and cattle-class service
Long-distance trains often have multiple service classes
Even the Paris subway has two different service classes
For packets  the classes may differ in terms of delay  jitter  and probability of being discarded in the event of congestion  among other possibilities (but probably not roomier Ethernet frames)
To make the difference between flow-based quality of service and class-based quality of service clearer  consider an example: Internet telephony
With a flowbased scheme  each telephone call gets its own resources and guarantees
With a class-based scheme  all the telephone calls together get the resources reserved for the class telephony
These resources cannot be taken away by packets from the Web browsing class or other classes  but no telephone call gets any private resources reserved for it alone
Expedited Forwarding The choice of service classes is up to each operator  but since packets are often forwarded between networks run by different operators  IETF has defined some network-independent service classes
The simplest class is expedited forwarding  so let us start with that one
It is described in RFC
The idea behind expedited forwarding is very simple
Two classes of service are available: regular and expedited
The vast majority of the traffic is expected to be regular  but a limited fraction of the packets are expedited
The expedited packets should be able to transit the network as though no other packets were present
In this way they will get low loss  low delay and low jitter serviceâjust what is needed for VoIP
A symbolic representation of this ââtwo-tubeââ system is given in Fig
Note that there is still just one physical line
The two logical pipes shown in the figure represent a way to reserve bandwidth for different classes of service  not a ond physical line
One way to implement this strategy is as follows
Packets are classified as expedited or regular and marked accordingly
This step might be done on the sending host or in the ingress (first) router
The advantage of doing classification on the sending host is that more information is available about which packets belong to which flows
This task may be performed by networking software or even the operating system  to avoid having to change existing applications
For example  it is becoming common for VoIP packets to be marked for expedited service by hosts
If the packets pass through a corporate network or ISP that supports expedited service  they will receive preferential treatment
If the network does not support expedited service  no harm is done
QUALITY OF SERVICE Regular packets Expedited packets Figure  -
Expedited packets experience a traffic-free network
Of course  if the marking is done by the host  the ingress router is likely to police the traffic to make sure that customers are not sending more expedited traffic than they have paid for
Within the network  the routers may have two output queues for each outgoing line  one for expedited packets and one for regular packets
When a packet arrives  it is queued accordingly
The expedited queue is given priority over the regular one  for example  by using a priority scheduler
In this way  expedited packets see an unloaded network  even when there is  in fact  a heavy load of regular traffic
Assured Forwarding A somewhat more elaborate scheme for managing the service classes is called assured forwarding
It is described in RFC
Assured forwarding specifies that there shall be four priority classes  each class having its own resources
The top three classes might be called gold  silver  and bronze
In addition  it defines three discard classes for packets that are experiencing congestion: low  medium  and high
Taken together  these two factors define   service classes
Figure  -  shows one way packets might be processed under assured forwarding
The first step is to classify the packets into one of the four priority classes
As before  this step might be done on the sending host (as shown in the figure) or in the ingress router  and the rate of higher-priority packets may be limited by the operator as part of the service offering
The next step is to determine the discard class for each packet
This is done by passing the packets of each priority class through a traffic policer such as a token bucket
The policer lets all of the traffic through  but it identifies packets that fit within small bursts as low discard  packets that exceed small bursts as medium discard  and packets that exceed large bursts as high discard
The combination of priority and discard class is then encoded in each packet
Finally  the packets are processed by routers in the network with a packet scheduler that distinguishes the different classes
A common choice is to use THE NETWORK LAYER
Weighted fair queues Router Silver Gold Bronze Packet source Four priority classes Classifier Policer Twelve priority/drop classes Packets with DiffServ mark Figure  -
A possible implementation of assured forwarding
weighted fair queueing for the four priority classes  with higher classes given higher weights
In this way  the higher classes will get most of the bandwidth  but the lower classes will not be starved of bandwidth entirely
For example  if the weights double from one class to the next higher class  the higher class will get twice the bandwidth
Within a priority class  packets with a higher discard class can be preferentially dropped by running an algorithm such as RED (Random Early Detection)  which we saw in
RED will start to drop packets as congestion builds but before the router has run out of buffer space
At this stage  there is still buffer space with which to accept low discard packets while dropping high discard packets  INTERNETWORKING Until now  we have implicitly assumed that there is a single homogeneous network  with each machine using the same protocol in each layer
Unfortunately  this assumption is wildly optimistic
Many different networks exist  including PANs  LANs  MANs  and WANs
We have described Ethernet  Internet over cable  the fixed and mobile telephone networks    and more
Numerous protocols are in widespread use across these networks in every layer
In the following tions  we will take a careful look at the issues that arise when two or more networks are connected to form an internetwork  or more simply an internet
It would be much simpler to join networks together if everyone used a single networking technology  and it is often the case that there is a dominant kind of network  such as Ethernet
Some pundits speculate that the multiplicity of technologies will go away as soon as everyone realizes how wonderful [fill in your favorite network] is
Do not count on it
History shows this to be wishful thinking
Different kinds of networks grapple with different problems  so  for example  Ethernet and satellite networks are always likely to differ
Reusing existing systems  such as running data networks on top of cable  the telephone network  and power   INTERNETWORKING lines  adds constraints that cause the features of the networks to diverge
Heterogeneity is here to stay
If there will always be different networks  it would be simpler if we did not need to interconnect them
This also is unlikely
Bob Metcalfe postulated that the value of a network with N nodes is the number of connections that may be made between the nodes  or N  (Gilder  )
This means that large networks are much more valuable than small networks because they allow many more connections  so there always will be an incentive to combine smaller networks
The Internet is the prime example of this interconnection
(We will write Internet with a capital ââIââ to distinguish it from other internets  or connected networks
) The purpose of joining all these networks is to allow users on any of them to communicate with users on all the other ones
When you pay an ISP for Internet service  you may be charged depending on the bandwidth of your line  but what you are really paying for is the ability to exchange packets with any other host that is also connected to the Internet
After all  the Internet would not be very popular if you could only send packets to other hosts in the same city
Since networks often differ in important ways  getting packets from one network to another is not always so easy
We must address problems of heterogeneity  and also problems of scale as the resulting internet grows very large
We will begin by looking at how networks can differ to see what we are up against
Then we shall see the approach used so successfully by IP (Internet Protocol)  the network layer protocol of the Internet  including techniques for tunneling through networks  routing in internetworks  and packet fragmentation
How Networks Differ Networks can differ in many ways
Some of the differences  such as different modulation techniques or frame formats  are internal to the physical and data link layers
These differences will not concern us here
Instead  in Fig
It is papering over these differences that makes internetworking more difficult than operating within a single network
When packets sent by a source on one network must transit one or more foreign networks before reaching the destination network  many problems can occur at the interfaces between networks
To start with  the source needs to be able to address the destination
What do we do if the source is on an Ethernet network and the destination is on a WiMAX network? Assuming we can even specify a WiMAX destination from an Ethernet network  packets would cross from a connectionless network to a connection-oriented one
This may require that a new connection be set up on short notice  which injects a delay  and much overhead if the connection is not used for many more packets
Many specific differences may have to be accommodated as well
How do we multicast a packet to a group with some members on a network that does not THE NETWORK LAYER
Item Some Possibilities Service offered Connectionless versus connection oriented Addressing Different sizes  flat or hierarchical Broadcasting Present or absent (also multicast) Packet size Every network has its own maximum Ordering Ordered and unordered delivery Quality of service Present or absent; many different kinds Reliability Different levels of loss urity Privacy rules  encryption  etc
Parameters Different timeouts  flow specifications  etc
Accounting By connect time  packet  byte  or not at all Figure  -
Some of the many ways networks can differ
support multicast? The differing max packet sizes used by different networks can be a major nuisance  too
How do you pass an -byte packet through a network whose maximum size is  bytes? If packets on a connection-oriented network transit a connectionless network  they may arrive in a different order than they were sent
That is something the sender likely did not expect  and it might come as an (unpleasant) surprise to the receiver as well
These kinds of differences can be papered over  with some effort
For example  a gateway joining two networks might generate separate packets for each destination in lieu of better network support for multicasting
A large packet might be broken up  sent in pieces  and then joined back together
Receivers might buffer packets and deliver them in order
Networks also can differ in large respects that are more difficult to reconcile
The clearest example is quality of service
If one network has strong QoS and the other offers best effort service  it will be impossible to make bandwidth and delay guarantees for real-time traffic end to end
In fact  they can likely only be made while the best-effort network is operated at a low utilization  or hardly used  which is unlikely to be the goal of most ISPs
urity mechanisms are problematic  but at least encryption for confidentiality and data integrity can be layered on top of networks that do not already include it
Finally  differences in accounting can lead to unwelcome bills when normal usage suddenly becomes expensive  as roaming mobile phone users with data plans have discovered
How Networks Can Be Connected There are two basic choices for connecting different networks: we can build devices that translate or convert packets from each kind of network into packets for each other network  or  like good computer scientists  we can try to solve the   INTERNETWORKING problem by adding a layer of indirection and building a common layer on top of the different networks
In either case  the devices are placed at the boundaries between networks
Early on  Cerf and Kahn (   ) argued for a common layer to hide the differences of existing networks
This approach has been tremendously successful  and the layer they proposed was eventually separated into the TCP and IP protocols
Almost four decades later  IP is the foundation of the modern Internet
For this accomplishment  Cerf and Kahn were awarded the  Turing Award  informally known as the Nobel Prize of computer science
IP provides a universal packet format that all routers recognize and that can be passed through almost every network
IP has extended its reach from computer networks to take over the telephone network
It also runs on sensor networks and other tiny devices that were once presumed too resource-constrained to support it
We have discussed several different devices that connect networks  including repeaters  hubs  switches  bridges  routers  and gateways
Repeaters and hubs just move bits from one wire to another
They are mostly analog devices and do not understand anything about higher layer protocols
Bridges and switches operate at the link layer
They can be used to build networks  but only with minor protocol translation in the process  for example  between    and  Mbps Ethernet switches
Our focus in this tion is interconnection devices that operate at the network layer  namely the routers
We will leave gateways  which are higherlayer interconnection devices  until later
Let us first explore at a high level how interconnection with a common network layer can be used to interconnect dissimilar networks
An internet comprised of
MPLS  and Ethernet networks is shown in Fig
Suppose that the source machine on the
network wants to send a packet to the destination machine on the Ethernet network
Since these technologies are different  and they are further separated by another kind of network (MPLS)  some added processing is needed at the boundaries between the networks
Because different networks may  in general  have different forms of addressing  the packet carries a network layer address that can identify any host across the three networks
The first boundary the packet reaches is when it transitions from an
network to an MPLS network
provides a connectionless service  but MPLS provides a connection-oriented service
This means that a virtual circuit must be set up to cross that network
Once the packet has traveled along the virtual circuit  it will reach the Ethernet network
At this boundary  the packet may be too large to be carried  since
can work with larger frames than Ethernet
To handle this problem  the packet is divided into fragments  and each fragment is sent separately
When the fragments reach the destination  they are reassembled
Then the packet has completed its journey
The protocol processing for this journey is shown in Fig
The source accepts data from the transport layer and generates a packet with the common network layer header  which is IP in this example
The network header contains the THE NETWORK LAYER    MPLS Ethernet Source Destination Packet Virtual circuit
IP IP Router Router
IP IP MPLSIP Eth IP MPLS IP IP IP Eth IP Physical (a) (b) Data from transport layer Figure  -
(a) A packet crossing different networks
(b) Network and link layer protocol processing
ultimate destination address  which is used to determine that the packet should be sent via the first router
So the packet is encapsulated in an
frame whose destination is the first router and transmitted
At the router  the packet is removed from the frameâs data field and the
frame header is discarded
The router now examines the IP address in the packet and looks up this address in its routing table
Based on this address  it decides to send the packet to the ond router next
For this part of the path  an MPLS virtual circuit must be established to the ond router and the packet must be encapsulated with MPLS headers that travel this circuit
At the far end  the MPLS header is discarded and the network address is again consulted to find the next network layer hop
It is the destination itself
Since the packet is too long to be sent over Ethernet  it is split into two portions
Each of these portions is put into the data field of an Ethernet frame and sent to the Ethernet address of the destination
At the destination  the Ethernet header is stripped from each of the frames  and the contents are reassembled
The packet has finally reached its destination
Observe that there is an essential difference between the routed case and the switched (or bridged) case
With a router  the packet is extracted from the frame and the network address in the packet is used for deciding where to send it
With a switch (or bridge)  the entire frame is transported on the basis of its MAC address
Switches do not have to understand the network layer protocol being used to switch packets
Routers do
Unfortunately  internetworking is not as easy as we have made it sound
In fact  when bridges were introduced  it was intended that they would join different types of networks  or at least different types of LANs
They were to do this by translating frames from one LAN into frames from another LAN
However  this   INTERNETWORKING did not work well  for the same reason that internetworking is difficult: the differences in the features of LANs  such as different maximum packet sizes and LANs with and without priority classes  are hard to mask
Today  bridges are predominantly used to connect the same kind of network at the link layer  and routers connect different networks at the network layer
Internetworking has been very successful at building large networks  but it only works when there is a common network layer
There have  in fact  been many network protocols over time
Getting everybody to agree on a single format is difficult when companies perceive it to their commercial advantage to have a proprietary format that they control
Examples besides IP  which is now the near-universal network protocol  were IPX  SNA  and AppleTalk
None of these protocols are still in widespread use  but there will always be other protocols
The most relevant example now is probably IPv  and IPv
While these are both versions of IP  they are not compatible (or it would not have been necessary to create IPv )
A router that can handle multiple network protocols is called a multiprotocol router
It must either translate the protocols  or leave connection for a higher protocol layer
Neither approach is entirely satisfactory
Connection at a higher layer  say  by using TCP  requires that all the networks implement TCP (which may not be the case)
Then  it limits usage across the networks to applications that use TCP (which does not include many real-time applications)
The alternative is to translate packets between the networks
However  unless the packet formats are close relatives with the same information fields  such conversions will always be incomplete and often doomed to failure
For example  IPv  addresses are bits long
They will not fit in a  -bit IPv  address field  no matter how hard the router tries
Getting IPv  and IPv  to run in the same network has proven to be a major obstacle to the deployment of IPv
(To be fair  so has getting customers to understand why they should want IPv  in the first place
) Greater problems can be expected when translating between fundamentally different protocols  such as connectionless and connection-oriented network protocols
Given these difficulties  conversion is only rarely attempted
Arguably  even IP has only worked so well by serving as a kind of lowest common denominator
It requires little of the networks on which it runs  but offers only best-effort service as a result
Tunneling Handling the general case of making two different networks interwork is exceedingly difficult
However  there is a common special case that is manageable even for different network protocols
This case is where the source and destination hosts are on the same type of network  but there is a different network in between
As an example  think of an international bank with an IPv  network THE NETWORK LAYER
in Paris  an IPv  network in London and connectivity between the offices via the IPv  Internet
This situation is shown in Fig
IPv  IPv  IPv  Paris London Tunnel Router Router IPv  packet IPv  IPv  packet IPv  packet Figure  -
Tunneling a packet from Paris to London
The solution to this problem is a technique called tunneling
To send an IP packet to a host in the London office  a host in the Paris office constructs the packet containing an IPv  address in London  and sends it to the multiprotocol router that connects the Paris IPv  network to the IPv  Internet
When this router gets the IPv  packet  it encapsulates the packet with an IPv  header addressed to the IPv  side of the multiprotocol router that connects to the London IPv  network
That is  the router puts a (IPv ) packet inside a (IPv ) packet
When this wrapped packet arrives  the London router removes the original IPv  packet and sends it onward to the destination host
The path through the IPv  Internet can be seen as a big tunnel extending from one multiprotocol router to the other
The IPv  packet just travels from one end of the tunnel to the other  snug in its nice box
It does not have to worry about dealing with IPv  at all
Neither do the hosts in Paris or London
Only the multiprotocol routers have to understand both IPv  and IPv  packets
In effect  the entire trip from one multiprotocol router to the other is like a hop over a single link
An analogy may make tunneling clearer
Consider a person driving her car from Paris to London
Within France  the car moves under its own power  but when it hits the English Channel  it is loaded onto a high-speed train and transported to England through the Chunnel (cars are not permitted to drive through the Chunnel)
Effectively  the car is being carried as freight  as depicted in Fig
At the far end  the car is let loose on the English roads and once again continues to move under its own power
Tunneling of packets through a foreign network works the same way
Tunneling is widely used to connect isolated hosts and networks using other networks
The network that results is called an overlay since it has effectively been overlaid on the base network
Deployment of a network protocol with a new feature is a common reason  as our ââIPv  over IPv ââ example shows
The disadvantage of tunneling is that none of the hosts on the network that is tunneled over can be reached because the packets cannot escape in the middle of the tunnel
INTERNETWORKING Car English Channel Paris London Railroad track Railroad carriage Figure  -
Tunneling a car from France to England
However  this limitation of tunnels is turned into an advantage with VPNs (Virtual Private Networks)
A VPN is simply an overlay that is used to provide a measure of urity
We will explore VPNs when we get to      Internetwork Routing Routing through an internet poses the same basic problem as routing within a single network  but with some added complications
To start  the networks may internally use different routing algorithms
For example  one network may use link state routing and another distance vector routing
Since link state algorithms need to know the topology but distance vector algorithms do not  this difference alone would make it unclear how to find the shortest paths across the internet
Networks run by different operators lead to bigger problems
First  the operators may have different ideas about what is a good path through the network
One operator may want the route with the least delay  while another may want the most inexpensive route
This will lead the operators to use different quantities to set the shortest-path costs (
millionds of delay vs
monetary cost)
The weights will not be comparable across networks  so shortest paths on the internet will not be well defined
Worse yet  one operator may not want another operator to even know the details of the paths in its network  perhaps because the weights and paths may reflect sensitive information (such as the monetary cost) that represents a competitive business advantage
Finally  the internet may be much larger than any of the networks that comprise it
It may therefore require routing algorithms that scale well by using a hierarchy  even if none of the individual networks need to use a hierarchy
All of these considerations lead to a two-level routing algorithm
Within each network  an intradomain or interior gateway protocol is used for routing
(ââGatewayââ is an older term for âârouter
ââ) It might be a link state protocol of the kind we have already described
Across the networks that make up the internet  an interdomain or exterior gateway protocol is used
The networks may all use different intradomain protocols  but they must use the same interdomain protocol
THE NETWORK LAYER
In the Internet  the interdomain routing protocol is called BGP (Border Gateway Protocol)
We will describe it in the next tion
There is one more important term to introduce
Since each network is operated independently of all the others  it is often referred to as an AS (Autonomous System)
A good mental model for an AS is an ISP network
In fact  an ISP network may be comprised of more than one AS  if it is managed  or  has been acquired  as multiple networks
But the difference is usually not significant
The two levels are usually not strictly hierarchical  as highly suboptimal paths might result if a large international network and a small regional network were both abstracted to be a single network
However  relatively little information about routes within the networks is exposed to find routes across the internetwork
This helps to address all of the complications
It improves scaling and lets operators freely select routes within their own networks using a protocol of their choosing
It also does not require weights to be compared across networks or expose sensitive information outside of networks
However  we have said little so far about how the routes across the networks of the internet are determined
In the Internet  a large determining factor is the business arrangements between ISPs
Each ISP may charge or receive money from the other ISPs for carrying traffic
Another factor is that if internetwork routing requires crossing international boundaries  various laws may suddenly come into play  such as Swedenâs strict privacy laws about exporting personal data about Swedish citizens from Sweden
All of these nontechnical factors are wrapped up in the concept of a routing policy that governs the way autonomous networks select the routes that they use
We will return to routing policies when we describe BGP
Packet Fragmentation Each network or link imposes some maximum size on its packets
These limits have various causes  among them
Hardware (
the size of an Ethernet frame) Operating system (
all buffers are bytes) Protocols (
the number of bits in the packet length field) Compliance with some (inter)national standard Desire to reduce error-induced retransmissions to some level Desire to prevent one packet from occupying the channel too long
The result of all these factors is that the network designers are not free to choose any old maximum packet size they wish
Maximum payloads for some common   INTERNETWORKING technologies are  bytes for Ethernet and  bytes for    IP is more generous  allows for packets as big as   bytes
Hosts usually prefer to transmit large packets because this reduces packet overheads such as bandwidth wasted on header bytes
An obvious internetworking problem appears when a large packet wants to travel through a network whose maximum packet size is too small
This nuisance has been a persistent issue  and solutions to it have evolved along with much experience gained on the Internet
One solution is to make sure the problem does not occur in the first place
However  this is easier said than done
A source does not usually know the path a packet will take through the network to a destination  so it certainly does not know how small packets must be to get there
This packet size is called the Path MTU (Path Maximum Transmission Unit)
Even if the source did know the path MTU  packets are routed independently in a connectionless network such as the Internet
This routing means that paths may suddenly change  which can unexpectedly change the path MTU
The alternative solution to the problem is to allow routers to break up packets into fragments  sending each fragment as a separate network layer packet
However  as every parent of a small child knows  converting a large object into small fragments is considerably easier than the reverse process
(Physicists have even given this effect a name: the ond law of thermodynamics
) Packet-switching networks  too  have trouble putting the fragments back together again
Two opposing strategies exist for recombining the fragments back into the original packet
The first strategy is to make fragmentation caused by a ââsmallpacketââ network transparent to any subsequent networks through which the packet must pass on its way to the ultimate destination
This option is shown in Fig
In this approach  when an oversized packet arrives at G  the router breaks it up into fragments
Each fragment is addressed to the same exit router  G  where the pieces are recombined
In this way  passage through the small-packet network is made transparent
Subsequent networks are not even aware that fragmentation has occurred
Transparent fragmentation is straightforward but has some problems
For one thing  the exit router must know when it has received all the pieces  so either a count field or an ââend of packetââ bit must be provided
Also  because all packets must exit via the same router so that they can be reassembled  the routes are constrained
By not allowing some fragments to follow one route to the ultimate destination and other fragments a disjoint route  some performance may be lost
More significant is the amount of work that the router may have to do
It may need to buffer the fragments as they arrive  and decide when to throw them away if not all of the fragments arrive
Some of this work may be wasteful  too  as the packet may pass through a series of small packet networks and need to be repeatedly fragmented and reassembled
The other fragmentation strategy is to refrain from recombining fragments at any intermediate routers
Once a packet has been fragmented  each fragment is THE NETWORK LAYER
G  G  G  G  G  G  G  G  Packet Network  G  fragments a large packet G  reassembles the fragments G  fragments again G  reassembles again Network  (a) Packet G  fragments a large packet The fragments are not reassembled until the final destination (a host) is reached (b) Figure  -
(a) Transparent fragmentation
(b) Nontransparent fragmentation
treated as though it were an original packet
The routers pass the fragments  as shown in Fig
The main advantage of nontransparent fragmentation is that it requires routers to do less work
IP works this way
A complete design requires that the fragments be numbered in such a way that the original data stream can be reconstructed
The design used by IP is to give every fragment a packet number (carried on all packets)  an absolute byte offset within the packet  and a flag indicating whether it is the end of the packet
An example is shown in Fig
While simple  this design has some attractive properties
Fragments can be placed in a buffer at the destination in the right place for reassembly  even if they arrive out of order
Fragments can also be fragmented if they pass over a network with a yet smaller MTU
This is shown in Fig
Retransmissions of the packet (if all fragments were not received) can be fragmented into different pieces
Finally  fragments can be of arbitrary size  down to a single byte plus the packet header
In all cases  the destination simply uses the packet number and fragment offset to place the data in the right position  and the end-of-packet flag to determine when it has the complete packet
Unfortunately  this design still has problems
The overhead can be higher than with transparent fragmentation because fragment headers are now carried over some links where they may not be needed
But the real problem is the existence of fragments in the first place
Kent and Mogul (   ) argued that fragmentation is detrimental to performance because  as well as the header overheads  a whole packet is lost if any of its fragments are lost  and because fragmentation is more of a burden for hosts than was originally realized
INTERNETWORKING Number of the first elementary fragment in this packet Packet number End of packet bit   A B C D E F G H I J   A B C D E F G H   I J   A B C D E   F G H   I J Header  byte Header Header Header Header Header (a) (b) (c) Figure  -
Fragmentation when the elementary data size is  byte
(a) Original packet  containing   data bytes
(b) Fragments after passing through a network with maximum packet size of  payload bytes plus header
(c) Fragments after passing through a size  gateway
This leads us back to the original solution of getting rid of fragmentation in the network  the strategy used in the modern Internet
The process is called path MTU discovery (Mogul and Deering  )
It works as follows
Each IP packet is sent with its header bits set to indicate that no fragmentation is allowed to be performed
If a router receives a packet that is too large  it generates an error packet  returns it to the source  and drops the packet
This is shown in Fig
When the source receives the error packet  it uses the information inside to refragment the packet into pieces that are small enough for the router to handle
If a router further down the path has an even smaller MTU  the process is repeated
Source Destination Packet (with length) âTry â âTry   â  Figure  -
Path MTU discovery
THE NETWORK LAYER
The advantage of path MTU discovery is that the source now knows what length packet to send
If the routes and path MTU change  new error packets will be triggered and the source will adapt to the new path
However  fragmentation is still needed between the source and the destination unless the higher layers learn the path MTU and pass the right amount of data to IP
TCP and IP are typically implemented together (as ââTCP/IPââ) to be able to pass this sort of information
Even if this is not done for other protocols  fragmentation has still been moved out of the network and into the hosts
The disadvantage of path MTU discovery is that there may be added startup delays simply to send a packet
More than one round-trip delay may be needed to probe the path and find the MTU before any data is delivered to the destination
This begs the question of whether there are better designs
The answer is probably ââYes
ââ Consider the design in which each router simply truncates packets that exceed its MTU
This would ensure that the destination learns the MTU as rapidly as possible (from the amount of data that was delivered) and receives some of the data  THE NETWORK LAYER IN THE INTERNET It is now time to discuss the network layer of the Internet in detail
But before getting into specifics  it is worth taking a look at the principles that drove its design in the past and made it the success that it is today
All too often  nowadays  people seem to have forgotten them
These principles are enumerated and discussed in RFC  which is well worth reading (and should be mandatory for all protocol designersâwith a final exam at the end)
This RFC draws heavily on ideas put forth by Clark (   ) and Saltzer et al
We will now summarize what we consider to be the top   principles (from most important to least important) Make sure it works
Do not finalize the design or standard until multiple prototypes have successfully communicated with each other
All too often  designers first write a -page standard  get it approved  then discover it is deeply flawed and does not work
Then they write version
of the standard
This is not the way to go Keep it simple
When in doubt  use the simplest solution
William of Occam stated this principle (Occamâs razor) in the  th century
Put in modern terms: fight features
If a feature is not absolutely essential  leave it out  especially if the same effect can be achieved by combining other features Make clear choices
If there are several ways of doing the same thing  choose one
Having two or more ways to do the same thing is looking for trouble
Standards often have multiple options or modes   THE NETWORK LAYER IN THE INTERNET or parameters because several powerful parties insist that their way is best
Designers should strongly resist this tendency
Just say no Exploit modularity
This principle leads directly to the idea of having protocol stacks  each of whose layers is independent of all the other ones
In this way  if circumstances require one module or layer to be changed  the other ones will not be affected Expect heterogeneity
Different types of hardware  transmission facilities  and applications will occur on any large network
To handle them  the network design must be simple  general  and flexible Avoid static options and parameters
If parameters are unavoidable (
maximum packet size)  it is best to have the sender and receiver negotiate a value rather than defining fixed choices Look for a good design; it need not be perfect
Often  the designers have a good design but it cannot handle some weird special case
Rather than messing up the design  the designers should go with the good design and put the burden of working around it on the people with the strange requirements Be strict when sending and tolerant when receiving
In other words  send only packets that rigorously comply with the standards  but expect incoming packets that may not be fully conformant and try to deal with them Think about scalability
If the system is to handle millions of hosts and billions of users effectively  no centralized databases of any kind are tolerable and load must be spread as evenly as possible over the available resources Consider performance and cost
If a network has poor performance or outrageous costs  nobody will use it
Let us now leave the general principles and start looking at the details of the Internetâs network layer
In the network layer  the Internet can be viewed as a collection of networks or ASes (Autonomous Systems) that are interconnected
There is no real structure  but several major backbones exist
These are constructed from high-bandwidth lines and fast routers
The biggest of these backbones  to which everyone else connects to reach the rest of the Internet  are called Tier  networks
Attached to the backbones are ISPs (Internet Service Providers) that provide Internet access to homes and businesses  data centers and colocation facilities full of server machines  and regional (mid-level) networks
The data centers serve much of the content that is sent over the Internet
Attached THE NETWORK LAYER
to the regional networks are more ISPs  LANs at many universities and companies  and other edge networks
A sketch of this quasihierarchical organization is given in Fig
Leased lines to Asia A
backbone Leased transatlantic lines A European backbone National network Company network Ethernet IP router Mobile network WiMAX Cable Home network Regional network Figure  -
The Internet is an interconnected collection of many networks
The glue that holds the whole Internet together is the network layer protocol  IP (Internet Protocol)
Unlike most older network layer protocols  IP was designed from the beginning with internetworking in mind
A good way to think of the network layer is this: its job is to provide a best-effort (
not guaranteed) way to transport packets from source to destination  without regard to whether these machines are on the same network or whether there are other networks in between them
Communication in the Internet works as follows
The transport layer takes data streams and breaks them up so that they may be sent as IP packets
In theory  packets can be up to   KB each  but in practice they are usually not more than  bytes (so they fit in one Ethernet frame)
IP routers forward each packet through the Internet  along a path from one router to the next  until the destination is reached
At the destination  the network layer hands the data to the transport layer  which gives it to the receiving process
When all the pieces finally get to the destination machine  they are reassembled by the network layer into the original datagram
This datagram is then handed to the transport layer
In the example of Fig
This is   THE NETWORK LAYER IN THE INTERNET not unusual in practice  and there are many longer paths
There is also much redundant connectivity in the Internet  with backbones and ISPs connecting to each other in multiple locations
This means that there are many possible paths between two hosts
It is the job of the IP routing protocols to decide which paths to use
The IP Version  Protocol An appropriate place to start our study of the network layer in the Internet is with the format of the IP datagrams themselves
An IPv  datagram consists of a header part and a body or payload part
The header has a  -byte fixed part and a variable-length optional part
The header format is shown in Fig
The bits are transmitted from left to right and top to bottom  with the high-order bit of the Version field going first
(This is a ââbig-endianââ network byte order
On littleendian machines  such as Intel x  computers  a software conversion is required on both transmission and reception
) In retrospect  little endian would have been a better choice  but at the time IP was designed  no one knew it would come to dominate computing
Version IHL Total length Time to live Protocol Differentiated services Identification Header checksum Fragment offset Source address Destination address Options (  or more words) DF MF   Bits Figure  -
The IPv  (Internet Protocol) header
The Version field keeps track of which version of the protocol the datagram belongs to
Version  dominates the Internet today  and that is where we have started our discussion
By including the version at the start of each datagram  it becomes possible to have a transition between versions over a long period of time
In fact  IPv  the next version of IP  was defined more than a decade ago  yet is only just beginning to be deployed
We will describe it later in this tion
Its use will eventually be forced when each of Chinaâs almost people has a desktop PC  a laptop  and an IP phone
As an aside on numbering  IPv  was an experimental real-time stream protocol that was never widely used
THE NETWORK LAYER
Since the header length is not constant  a field in the header  IHL  is provided to tell how long the header is  in  -bit words
The minimum value is   which applies when no options are present
The maximum value of this  -bit field is which limits the header to   bytes  and thus the Options field to   bytes
For some options  such as one that records the route a packet has taken bytes is far too small  making those options useless
The Differentiated services field is one of the few fields that has changed its meaning (slightly) over the years
Originally  it was called the Type of service field
It was and still is intended to distinguish between different classes of service
Various combinations of reliability and speed are possible
For digitized voice  fast delivery beats accurate delivery
For file transfer  error-free transmission is more important than fast transmission
The Type of service field provided  bits to signal priority and  bits to signal whether a host cared more about delay  throughput  or reliability
However  no one really knew what to do with these bits at routers  so they were left unused for many years
When differentiated services were designed  IETF threw in the towel and reused this field
Now  the top  bits are used to mark the packet with its service class; we described the expedited and assured services earlier in this  ter
The bottom  bits are used to carry explicit congestion notification information  such as whether the packet has experienced congestion; we described explicit congestion notification as part of congestion control earlier in this  ter
The Total length includes everything in the datagramâboth header and data
The maximum length is   bytes
At present  this upper limit is tolerable  but with future networks  larger datagrams may be needed
The Identification field is needed to allow the destination host to determine which packet a newly arrived fragment belongs to
All the fragments of a packet contain the same Identification value
Next comes an unused bit  which is surprising  as available real estate in the IP header is extremely scarce
As an April Foolâs joke  Bellovin (   ) proposed using this bit to detect malicious traffic
This would greatly simplify urity  as packets with the ââevilââ bit set would be known to have been sent by attackers and could just be discarded
Unfortunately  network urity is not this simple
Then come two  -bit fields related to fragmentation
DF stands for Donât Fragment
It is an order to the routers not to fragment the packet
Originally  it was intended to support hosts incapable of putting the pieces back together again
Now it is used as part of the process to discover the path MTU  which is the largest packet that can travel along a path without being fragmented
By marking the datagram with the DF bit  the sender knows it will either arrive in one piece  or an error message will be returned to the sender
MF stands for More Fragments
All fragments except the last one have this bit set
It is needed to know when all fragments of a datagram have arrived
The Fragment offset tells where in the current packet this fragment belongs
All fragments except the last one in a datagram must be a multiple of  bytes  the   THE NETWORK LAYER IN THE INTERNET elementary fragment unit
Since   bits are provided  there is a maximum of  fragments per datagram  supporting a maximum packet length up to the limit of the Total length field
Working together  the Identification  MF  and Fragment offset fields are used to implement fragmentation as described in
The TtL (Time to live) field is a counter used to limit packet lifetimes
It was originally supposed to count time in onds  allowing a maximum lifetime of
It must be decremented on each hop and is supposed to be decremented multiple times when a packet is queued for a long time in a router
In practice  it just counts hops
When it hits zero  the packet is discarded and a warning packet is sent back to the source host
This feature prevents packets from wandering around forever  something that otherwise might happen if the routing tables ever become corrupted
When the network layer has assembled a complete packet  it needs to know what to do with it
The Protocol field tells it which transport process to give the packet to
TCP is one possibility  but so are UDP and some others
The numbering of protocols is global across the entire Internet
Protocols and other assigned numbers were formerly listed in RFC  but nowadays they are contained in an online database located at
Since the header carries vital information such as addresses  it rates its own checksum for protection  the Header checksum
The algorithm is to add up all the  -bit halfwords of the header as they arrive  using oneâs complement arithmetic  and then take the oneâs complement of the result
For purposes of this algorithm  the Header checksum is assumed to be zero upon arrival
Such a checksum is useful for detecting errors while the packet travels through the network
Note that it must be recomputed at each hop because at least one field always changes (the Time to live field)  but tricks can be used to speed up the computation
The Source address and Destination address indicate the IP address of the source and destination network interfaces
We will discuss Internet addresses in the next tion
The Options field was designed to provide an escape to allow subsequent versions of the protocol to include information not present in the original design  to permit experimenters to try out new ideas  and to avoid allocating header bits to information that is rarely needed
The options are of variable length
Each begins with a  -byte code identifying the option
Some options are followed by a  -byte option length field  and then one or more data bytes
The Options field is padded out to a multiple of  bytes
Originally  the five options listed in Fig
The urity option tells how ret the information is
In theory  a military router might use this field to specify not to route packets through certain countries the military considers to be ââbad guys
ââ In practice  all routers ignore it  so its only practical function is to help spies find the good stuff more easily
The Strict source routing option gives the complete path from source to destination as a sequence of IP addresses
The datagram is required to follow that THE NETWORK LAYER
Option Description urity Specifies how ret the datagram is Strict source routing Gives the complete path to be followed Loose source routing Gives a list of routers not to be missed Record route Makes each router append its IP address Timestamp Makes each router append its address and timestamp Figure  -
Some of the IP options
exact route
It is most useful for system managers who need to send emergency packets when the routing tables have been corrupted  or for making timing measurements
The Loose source routing option requires the packet to traverse the list of routers specified  in the order specified  but it is allowed to pass through other routers on the way
Normally  this option will provide only a few routers  to force a particular path
For example  to force a packet from London to Sydney to go west instead of east  this option might specify routers in New York  Los Angeles  and Honolulu
This option is most useful when political or economic considerations dictate passing through or avoiding certain countries
The Record route option tells each router along the path to append its IP address to the Options field
This allows system managers to track down bugs in the routing algorithms (ââWhy are packets from Houston to Dallas visiting Tokyo first?ââ)
When the ARPANET was first set up  no packet ever passed through more than nine routers  so   bytes of options was plenty
As mentioned above  now it is too small
Finally  the Timestamp option is like the Record route option  except that in addition to recording its  -bit IP address  each router also records a  -bit timestamp
This option  too  is mostly useful for network measurement
Today  IP options have fallen out of favor
Many routers ignore them or do not process them efficiently  shunting them to the side as an uncommon case
That is  they are only partly supported and they are rarely used
IP Addresses A defining feature of IPv  is its  -bit addresses
Every host and router on the Internet has an IP address that can be used in the Source address and Destination address fields of IP packets
It is important to note that an IP address does not actually refer to a host
It really refers to a network interface  so if a host is on two networks  it must have two IP addresses
However  in practice  most hosts are on one network and thus have one IP address
In contrast  routers have multiple interfaces and thus multiple IP addresses
THE NETWORK LAYER IN THE INTERNET Prefixes IP addresses are hierarchical  unlike Ethernet addresses
Each  -bit address is comprised of a variable-length network portion in the top bits and a host portion in the bottom bits
The network portion has the same value for all hosts on a single network  such as an Ethernet LAN
This means that a network corresponds to a contiguous block of IP address space
This block is called a prefix
IP addresses are written in dotted decimal notation
In this format  each of the  bytes is written in decimal  from  to
For example  the  -bit hexadecimal address  D  is written as
Prefixes are written by giving the lowest IP address in the block and the size of the block
The size is determined by the number of bits in the network portion; the remaining bits in the host portion can vary
This means that the size must be a power of two
By convention  it is written after the prefix IP address as a slash followed by the length in bits of the network portion
In our example  if the prefix contains   addresses and so leaves   bits for the network portion  it is written as
Since the prefix length cannot be inferred from the IP address alone  routing protocols must carry the prefixes to routers
Sometimes prefixes are simply described by their length  as in a ââ/  ââ which is pronounced ââslash
ââ The length of the prefix corresponds to a binary mask of  s in the network portion
When written out this way  it is called a subnet mask
It can be ANDed with the IP address to extract only the network portion
For our example  the subnet mask is      Fig
bits Network Prefix length = L bits Host Subnet mask           â L bits Figure  -
An IP prefix and a subnet mask
Hierarchical addresses have significant advantages and disadvantages
The key advantage of prefixes is that routers can forward packets based on only the network portion of the address  as long as each of the networks has a unique address block
The host portion does not matter to the routers because all hosts on the same network will be sent in the same direction
It is only when the packets reach the network for which they are destined that they are forwarded to the correct host
This makes the routing tables much smaller than they would otherwise be
Consider that the number of hosts on the Internet is approaching one billion
That would be a very large table for every router to keep
However  by using a hierarchy  routers need to keep routes for only around    prefixes
THE NETWORK LAYER
While using a hierarchy lets Internet routing scale  it has two disadvantages
First  the IP address of a host depends on where it is located in the network
An Ethernet address can be used anywhere in the world  but every IP address belongs to a specific network  and routers will only be able to deliver packets destined to that address to the network
Designs such as mobile IP are needed to support hosts that move between networks but want to keep the same IP addresses
The ond disadvantage is that the hierarchy is wasteful of addresses unless it is carefully managed
If addresses are assigned to networks in (too) large blocks  there will be (many) addresses that are allocated but not in use
This allocation would not matter much if there were plenty of addresses to go around
However  it was realized more than two decades ago that the tremendous growth of the Internet was rapidly depleting the free address space
IPv  is the solution to this shortage  but until it is widely deployed there will be great pressure to allocate IP addresses so that they are used very efficiently
Subnets Network numbers are managed by a nonprofit corporation called ICANN (Internet Corporation for Assigned Names and Numbers)  to avoid conflicts
In turn  ICANN has delegated parts of the address space to various regional authorities  which dole out IP addresses to ISPs and other companies
This is the process by which a company is allocated a block of IP addresses
However  this process is only the start of the story  as IP address assignment is ongoing as companies grow
We have said that routing by prefix requires all the hosts in a network to have the same network number
This property can cause problems as networks grow
For example  consider a university that started out with our example /  prefix for use by the Computer Science Dept
for the computers on its Ethernet
A year later  the Electrical Engineering Dept
wants to get on the Internet
The Art Dept
soon follows suit
What IP addresses should these departments use? Getting further blocks requires going outside the university and may be expensive or inconvenient
Moreover  the /  already allocated has enough addresses for over   hosts
It might be intended to allow for significant growth  but until that happens  it is wasteful to allocate further blocks of IP addresses to the same university
A different organization is required
The solution is to allow the block of addresses to be split into several parts for internal use as multiple networks  while still acting like a single network to the outside world
This is called subnetting and the networks (such as Ethernet LANs) that result from dividing up a larger network are called subnets
As we mentioned in
you should be aware that this new usage of the term conflicts with older usage of ââsubnetââ to mean the set of all routers and communication lines in a network
The single /  has been split into pieces
This split does not need to be even  but each piece must be   THE NETWORK LAYER IN THE INTERNET aligned so that any bits can be used in the lower host portion
In this case  half of the block (a /  ) is allocated to the Computer Science Dept  a quarter is allocated to the Electrical Engineering Dept
(a /  )  and one eighth (a /  ) to the Art Dept
The remaining eighth is unallocated
A different way to see how the block was divided is to look at the resulting prefixes when written in binary notation: Computer Science:   |xxxxxxx xxxxxxxx Electrical Eng
:    |xxxxxx xxxxxxxx Art:    |xxxxx xxxxxxxx Here  the vertical bar (|) shows the boundary between the subnet number and the host portion
/  (to Internet)
/  Figure  -
Splitting an IP prefix into separate networks with subnetting
When a packet comes into the main router  how does the router know which subnet to give it to? This is where the details of our prefixes come in
One way would be for each router to have a table with   entries telling it which outgoing line to use for each host on campus
But this would undermine the main scaling benefit we get from using a hierarchy
Instead  the routers simply need to know the subnet masks for the networks on campus
When a packet arrives  the router looks at the destination address of the packet and checks which subnet it belongs to
The router can do this by ANDing the destination address with the mask for each subnet and checking to see if the result is the corresponding prefix
For example  consider a packet destined for IP address
To see if it is for the Computer Science Dept
we AND with
to take the first   bits (which is
) and see if they match the prefix address (which is
They do not match
Checking the first   bits for the Electrical Engineering Dept
when ANDing with the subnet mask
This does match the prefix address  so the packet is forwarded onto the interface which leads to the Electrical Engineering network
THE NETWORK LAYER
The subnet divisions can be changed later if necessary  by updating all subnet masks at routers inside the university
Outside the network  the subnetting is not visible  so allocating a new subnet does not require contacting ICANN or changing any external databases
CIDRâClassless InterDomain Routing Even if blocks of IP addresses are allocated so that the addresses are used efficiently  there is still a problem that remains: routing table explosion
Routers in organizations at the edge of a network  such as a university  need to have an entry for each of their subnets  telling the router which line to use to get to that network
For routes to destinations outside of the organization  they can use the simple default rule of sending the packets on the line toward the ISP that connects the organization to the rest of the Internet
The other destination addresses must all be out there somewhere
Routers in ISPs and backbones in the middle of the Internet have no such luxury
They must know which way to go to get to every network and no simple default will work
These core routers are said to be in the default-free zone of the Internet
No one really knows how many networks are connected to the Internet any more  but it is a large number  probably at least a million
This can make for a very large table
It may not sound large by computer standards  but realize that routers must perform a lookup in this table to forward every packet  and routers at large ISPs may forward up to millions of packets per ond
Specialized hardware and fast memory are needed to process packets at these rates  not a generalpurpose computer
In addition  routing algorithms require each router to exchange information about the addresses it can reach with other routers
The larger the tables  the more information needs to be communicated and processed
The processing grows at least linearly with the table size
Greater communication increases the likelihood that some parts will get lost  at least temporarily  possibly leading to routing instabilities
The routing table problem could have been solved by going to a deeper hierarchy  like the telephone network
For example  having each IP address contain a country  state/province  city  network  and host field might work
Then  each router would only need to know how to get to each country  the states or provinces in its own country  the cities in its state or province  and the networks in its city
Unfortunately  this solution would require considerably more than   bits for IP addresses and would use addresses inefficiently (and Liechtenstein would have as many bits in its addresses as the United States)
Fortunately  there is something we can do to reduce routing table sizes
We can apply the same insight as subnetting: routers at different locations can know about a given IP address as belonging to prefixes of different sizes
However  instead of splitting an address block into subnets  here we combine multiple small   THE NETWORK LAYER IN THE INTERNET prefixes into a single larger prefix
This process is called route aggregation
The resulting larger prefix is sometimes called a supernet  to contrast with subnets as the division of blocks of addresses
With aggregation  IP addresses are contained in prefixes of varying sizes
The same IP address that one router treats as part of a /  (a block containing addresses) may be treated by another router as part of a larger /  (which contains addresses)
It is up to each router to have the corresponding prefix information
This design works with subnetting and is called CIDR (Classless Inter- Domain Routing)  which is pronounced ââcider ââ as in the drink
The most recent version of it is specified in RFC  (Fuller and Li  )
The name highlights the contrast with addresses that encode hierarchy with classes  which we will describe shortly
To make CIDR easier to understand  let us consider an example in which a block of  IP addresses is available starting at
Suppose that Cambridge University needs  addresses and is assigned the addresses
along with mask      This is a /  prefix
Next  Oxford University asks for  addresses
Since a block of  addresses must lie on a -byte boundary  Oxford cannot be given addresses starting at
Instead  it gets
along with subnet mask      Finally  the University of Edinburgh asks for  addresses and is assigned addresses
and mask      These assignments are summarized in Fig
University First address Last address How many Prefix Cambridge
/  Edinburgh
/  (Available)
/  Figure  -
A set of IP address assignments
All of the routers in the default-free zone are now told about the IP addresses in the three networks
Routers close to the universities may need to send on a different outgoing line for each of the prefixes  so they need an entry for each of the prefixes in their routing tables
An example is the router in London in Fig
Now let us look at these three universities from the point of view of a distant router in New York
All of the IP addresses in the three prefixes should be sent from New York (or the
in general) to London
The routing process in London notices this and combines the three prefixes into a single aggregate entry for the prefix
/  that it passes to the New York router
This prefix contains  K addresses and covers the three universities and the otherwise unallocated  addresses
By using aggregation  three prefixes have been reduced to one  reducing THE NETWORK LAYER
/  (  aggregate prefix)
/  Cambridge Oxford
/  New York London (  prefixes) Figure  -
Aggregation of IP prefixes
the prefixes that the New York router must be told about and the routing table entries in the New York router
When aggregation is turned on  it is an automatic process
It depends on which prefixes are located where in the Internet not on the actions of an administrator assigning addresses to networks
Aggregation is heavily used throughout the Internet and can reduce the size of router tables to around    prefixes
As a further twist  prefixes are allowed to overlap
The rule is that packets are sent in the direction of the most specific route  or the longest matching prefix that has the fewest IP addresses
Longest matching prefix routing provides a useful degree of flexibility  as seen in the behavior of the router at New York in Fig
This router still uses a single aggregate prefix to send traffic for the three universities to London
However  the previously available block of addresses within this prefix has now been allocated to a network in San Francisco
One possibility is for the New York router to keep four prefixes  sending packets for three of them to London and packets for the fourth to San Francisco
Instead  longest matching prefix routing can handle this forwarding with the two prefixes that are shown
One overall prefix is used to direct traffic for the entire block to London
One more specific prefix is also used to direct a portion of the larger prefix to San Francisco
With the longest matching prefix rule  IP addresses within the San Francisco network will be sent on the outgoing line to San Francisco  and all other IP addresses in the larger prefix will be sent to London
Conceptually  CIDR works as follows
When a packet comes in  the routing table is scanned to determine if the destination lies within the prefix
It is possible that multiple entries with different prefix lengths will match  in which case the entry with the longest prefix is used
Thus  if there is a match for a /  mask and a /  mask  the /  entry is used to look up the outgoing line for the packet
However  this process would be tedious if the table were really scanned entry by entry
THE NETWORK LAYER IN THE INTERNET
/  New York London
/  San Francisco
/  Figure  -
Longest matching prefix routing at the New York router
Instead  complex algorithms have been devised to speed up the address matching process (Ruiz-Sanchez et al
Commercial routers use custom VLSI chips with these algorithms embedded in hardware
Classful and Special Addressing To help you better appreciate why CIDR is so useful  we will briefly relate the design that predated it
Before  IP addresses were divided into the five categories listed in Fig
This allocation has come to be called classful addressing
Bits Range of host addresses
Class  Network Host   Network Host Network Host  Multicast address  Reserved for future use A B C D E Figure  -
IP address formats
The class A  B  and C formats allow for up to networks with   million hosts each    networks with up to   hosts each  and  million networks (
LANs) with up to hosts each (although a few of these are special)
Also supported is multicast (the class D format)  in which a datagram is directed to multiple hosts
Addresses beginning with  are reserved for use in the future
They would be valuable to use now given the depletion of the IPv  address space
THE NETWORK LAYER
Unfortunately  many hosts will not accept these addresses as valid because they have been off-limits for so long and it is hard to teach old hosts new tricks
This is a hierarchical design  but unlike CIDR the sizes of the address blocks are fixed
Over  billion addresses exist  but organizing the address space by classes wastes millions of them
In particular  the real villain is the class B network
For most organizations  a class A network  with   million addresses  is too big  and a class C network  with addresses is too small
A class B network  with    is just right
In Internet folklore  this situation is known as the three bears problem [as in Goldilocks and the Three Bears (Southey  )]
In reality  though  a class B address is far too large for most organizations
Studies have shown that more than half of all class B networks have fewer than   hosts
A class C network would have done the job  but no doubt every organization that asked for a class B address thought that one day it would outgrow the  - bit host field
In retrospect  it might have been better to have had class C networks use   bits instead of  for the host number  allowing  hosts per network
Had this been the case  most organizations would probably have settled for a class C network  and there would have been half a million of them (versus only   class B networks)
It is hard to fault the Internetâs designers for not having provided more (and smaller) class B addresses
At the time the decision was made to create the three classes  the Internet was a research network connecting the major research universities in the
(plus a very small number of companies and military sites doing networking research)
No one then perceived the Internet becoming a massmarket communication system rivaling the telephone network
At the time  someone no doubt said: ââThe
has about  colleges and universities
Even if all of them connect to the Internet and many universities in other countries join  too  we are never going to hit    since there are not that many universities in the whole world
Furthermore  having the host number be an integral number of bytes speeds up packet processingââ (which was then done entirely in software)
Perhaps some day people will look back and fault the folks who designed the telephone number scheme and say: ââWhat idiots
Why didnât they include the planet number in the phone number?ââ But at the time  it did not seem necessary
To handle these problems  subnets were introduced to flexibly assign blocks of addresses within an organization
Later  CIDR was added to reduce the size of the global routing table
Today  the bits that indicate whether an IP address belongs to class A  B  or C network are no longer used  though references to these classes in the literature are still common
To see how dropping the classes made forwarding more complicated  consider how simple it was in the old classful system
When a packet arrived at a router  a copy of the IP address was shifted right   bits to yield a  -bit class number
A  -way branch then sorted packets into A  B  C (and D and E) classes  with eight of the cases for class A  four of the cases for class B  and two of the cases for class C
The code for each class then masked off the  -   -  or  -bit network   THE NETWORK LAYER IN THE INTERNET number and right aligned it in a  -bit word
The network number was then looked up in the A  B  or C table  usually by indexing for A and B networks and hashing for C networks
Once the entry was found  the outgoing line could be looked up and the packet forwarded
This is much simpler than the longest matching prefix operation  which can no longer use a simple table lookup because an IP address may have any length prefix
Class D addresses continue to be used in the Internet for multicast
Actually  it might be more accurate to say that they are starting to be used for multicast  since Internet multicast has not been widely deployed in the past
There are also several other addresses that have special meanings  as shown in Fig
The IP address
the lowest address  is used by hosts when they are being booted
It means ââthis networkââ or ââthis host
ââ IP addresses with  as the network number refer to the current network
These addresses allow machines to refer to their own network without knowing its number (but they have to know the network mask to know how many  s to include)
The address consisting of all  s  or
âthe highest addressâis used to mean all hosts on the indicated network
It allows broadcasting on the local network  typically a LAN
The addresses with a proper network number and all  s in the host field allow machines to send broadcast packets to distant LANs anywhere in the Internet
However  many network administrators disable this feature as it is mostly a urity hazard
Finally  all addresses of the form   zz are reserved for loopback testing
Packets sent to that address are not put out onto the wire; they are processed locally and treated as incoming packets
This allows packets to be sent to the host without the sender knowing its number  which is useful for testing
This host A host on this network Broadcast on the local network  Host Network (Anything) Broadcast on a distant network Loopback
Special IP addresses
NATâNetwork Address Translation IP addresses are scarce
An ISP might have a /  address  giving it   usable host numbers
If it has more customers than that  it has a problem
THE NETWORK LAYER
This scarcity has led to techniques to use IP addresses sparingly
One approach is to dynamically assign an IP address to a computer when it is on and using the network  and to take the IP address back when the host becomes inactive
The IP address can then be assigned to another computer that becomes active
In this way  a single /  address can handle up to   active users
This strategy works well in some cases  for example  for dialup networking and mobile and other computers that may be temporarily absent or powered off
However  it does not work very well for business customers
Many PCs in businesses are expected to be on continuously
Some are employee machines  backed up at night  and some are servers that may have to serve a remote request at a momentâs notice
These businesses have an access line that always provides connectivity to the rest of the Internet
Increasingly  this situation also applies to home users subscribing to ADSL or Internet over cable  since there is no connection charge (just a monthly flat rate charge)
Many of these users have two or more computers at home  often one for each family member  and they all want to be online all the time
The solution is to connect all the computers into a home network via a LAN and put a (wireless) router on it
The router then connects to the ISP
From the ISPâs point of view  the family is now the same as a small business with a handful of computers
Welcome to Jones  Inc
With the techniques we have seen so far  each computer must have its own IP address all day long
For an ISP with many thousands of customers  particularly business customers and families that are just like small businesses  the demand for IP addresses can quickly exceed the block that is available
The problem of running out of IP addresses is not a theoretical one that might occur at some point in the distant future
It is happening right here and right now
The long-term solution is for the whole Internet to migrate to IPv  which has   -bit addresses
This transition is slowly occurring  but it will be years before the process is complete
To get by in the meantime  a quick fix was needed
The quick fix that is widely used today came in the form of NAT (Network Address Translation)  which is described in RFC  and which we will summarize below
For additional information  see Dutcher (   )
The basic idea behind NAT is for the ISP to assign each home or business a single IP address (or at most  a small number of them) for Internet traffic
Within the customer network  every computer gets a unique IP address  which is used for routing intramural traffic
However  just before a packet exits the customer network and goes to the ISP  an address translation from the unique internal IP address to the shared public IP address takes place
This translation makes use of three ranges of IP addresses that have been declared as private
Networks may use them internally as they wish
The only rule is that no packets containing these addresses may appear on the Internet itself
The three reserved ranges are:
/  (  hosts)
/  (  hosts)
/  (  hosts)   THE NETWORK LAYER IN THE INTERNET The first range provides for   addresses (except for all  s and all  s  as usual) and is the usual choice  even if the network is not large
The operation of NAT is shown in Fig
Within the customer premises  every machine has a unique address of the form  z
However  before a packet leaves the customer premises  it passes through a NAT box that converts the internal IP source address
in the figure  to the customerâs true IP address
in this example
The NAT box is often combined in a single device with a firewall  which provides urity by carefully controlling what goes into the customer network and what comes out of it
We will study firewalls in   It is also possible to integrate the NAT box into a router or ADSL modem
Packet after translation Boundary of customer premises NAT box/firewall ISP router IP =
port =  IP =
port =  (to Internet) Packet before translation Customer router and LAN Figure  -
Placement and operation of a NAT box
So far  we have glossed over one tiny but crucial detail: when the reply comes back (
from a Web server)  it is naturally addressed to
so how does the NAT box know which internal address to replace it with? Herein lies the problem with NAT
If there were a spare field in the IP header  that field could be used to keep track of who the real sender was  but only  bit is still unused
In principle  a new option could be created to hold the true source address  but doing so would require changing the IP code on all the machines on the entire Internet to handle the new option
This is not a promising alternative for a quick fix
What actually happens is as follows
The NAT designers observed that most IP packets carry either TCP or UDP payloads
When we study TCP and UDP in
we will see that both of these have headers containing a source port and a destination port
Below we will just discuss TCP ports  but exactly the same story holds for UDP ports
The ports are  -bit integers that indicate where the TCP connection begins and ends
These ports provide the field needed to make NAT work
When a process wants to establish a TCP connection with a remote process  it attaches itself to an unused TCP port on its own machine
This is called the source port and tells the TCP code where to send incoming packets belonging to this connection
The process also supplies a destination port to tell who to give THE NETWORK LAYER
the packets to on the remote side
Ports  â are reserved for well-known services
For example  port   is the port used by Web servers  so remote clients can locate them
Each outgoing TCP message contains both a source port and a destination port
Together  these ports serve to identify the processes using the connection on both ends
An analogy may make the use of ports clearer
Imagine a company with a single main telephone number
When people call the main number  they reach an operator who asks which extension they want and then puts them through to that extension
The main number is analogous to the customerâs IP address and the extensions on both ends are analogous to the ports
Ports are effectively an extra   bits of addressing that identify which process gets which incoming packet
Using the Source port field  we can solve our mapping problem
Whenever an outgoing packet enters the NAT box  the  z source address is replaced by the customerâs true IP address
In addition  the TCP Source port field is replaced by an index into the NAT boxâs  -entry translation table
This table entry contains the original IP address and the original source port
Finally  both the IP and TCP header checksums are recomputed and inserted into the packet
It is necessary to replace the Source port because connections from machines
may both happen to use port  for example  so the Source port alone is not enough to identify the sending process
When a packet arrives at the NAT box from the ISP  the Source port in the TCP header is extracted and used as an index into the NAT boxâs mapping table
From the entry located  the internal IP address and original TCP Source port are extracted and inserted into the packet
Then  both the IP and TCP checksums are recomputed and inserted into the packet
The packet is then passed to the customer router for normal delivery using the  z address
Although this scheme sort of solves the problem  networking purists in the IP community have a tendency to regard it as an abomination-on-the-face-of-theearth
Briefly summarized  here are some of the objections
First  NAT violates the architectural model of IP  which states that every IP address uniquely identifies a single machine worldwide
The whole software structure of the Internet is built on this fact
With NAT  thousands of machines may (and do) use address    ond  NAT breaks the end-to-end connectivity model of the Internet  which says that any host can send a packet to any other host at any time
Since the mapping in the NAT box is set up by outgoing packets  incoming packets cannot be accepted until after outgoing ones
In practice  this means that a home user with NAT can make TCP/IP connections to a remote Web server  but a remote user cannot make connections to a game server on the home network
Special configuration or NAT traversal techniques are needed to support this kind of situation
Third  NAT changes the Internet from a connectionless network to a peculiar kind of connection-oriented network
The problem is that the NAT box must maintain information (
the mapping) for each connection passing through it
THE NETWORK LAYER IN THE INTERNET Having the network maintain connection state is a property of connection-oriented networks  not connectionless ones
If the NAT box crashes and its mapping table is lost  all its TCP connections are destroyed
In the absence of NAT  a router can crash and restart with no long-term effect on TCP connections
The sending process just times out within a few onds and retransmits all unacknowledged packets
With NAT  the Internet becomes as vulnerable as a circuit-switched network
Fourth  NAT violates the most fundamental rule of protocol layering: layer k may not make any assumptions about what layer k +  has put into the payload field
This basic principle is there to keep the layers independent
If TCP is later upgraded to TCP-  with a different header layout (
-bit ports)  NAT will fail
The whole idea of layered protocols is to ensure that changes in one layer do not require changes in other layers
NAT destroys this independence
Fifth  processes on the Internet are not required to use TCP or UDP
If a user on machine A decides to use some new transport protocol to talk to a user on machine B (for example  for a multimedia application)  introduction of a NAT box will cause the application to fail because the NAT box will not be able to locate the TCP Source port correctly
A sixth and related problem is that some applications use multiple TCP/IP connections or UDP ports in prescribed ways
For example  FTP  the standard File Transfer Protocol  inserts IP addresses in the body of packet for the receiver to extract and use
Since NAT knows nothing about these arrangements  it cannot rewrite the IP addresses or otherwise account for them
This lack of understanding means that FTP and other applications such as the H
Internet telephony protocol (which we will study in
) will fail in the presence of NAT unless special precautions are taken
It is often possible to patch NAT for these cases  but having to patch the code in the NAT box every time a new application comes along is not a good idea
Finally  since the TCP Source port field is   bits  at most   machines can be mapped onto an IP address
Actually  the number is slightly less because the first  ports are reserved for special uses
However  if multiple IP addresses are available  each one can handle up to   machines
A view of these and other problems with NAT is given in RFC
Despite the issues  NAT is widely used in practice  especially for home and small business networks  as the only expedient technique to deal with the IP address shortage
It has become wrapped up with firewalls and privacy because it blocks unsolicited incoming packets by default
For this reason  it is unlikely to go away even when IPv  is widely deployed
IP Version  IP has been in heavy use for decades
It has worked extremely well  as demonstrated by the exponential growth of the Internet
Unfortunately  IP has become a victim of its own popularity: it is close to running out of addresses
Even THE NETWORK LAYER
with CIDR and NAT using addresses more sparingly  the last IPv  addresses are expected to be assigned by ICANN before the end of
This looming disaster was recognized almost two decades ago  and it sparked a great deal of discussion and controversy within the Internet community about what to do about it
In this tion  we will describe both the problem and several proposed solutions
The only long-term solution is to move to larger addresses
IPv  (IP version  ) is a replacement design that does just that
It uses   -bit addresses; a shortage of these addresses is not likely any time in the foreseeable future
However  IPv  has proved very difficult to deploy
It is a different network layer protocol that does not really interwork with IPv  despite many similarities
Also  companies and users are not really sure why they should want IPv  in any case
The result is that IPv  is deployed and used on only a tiny fraction of the Internet (estimates are  %) despite having been an Internet Standard since
The next several years will be an interesting time  as the few remaining IPv  addresses are allocated
Will people start to auction off their IPv  addresses on eBay? Will a black market in them spring up? Who knows
In addition to the address problems  other issues loom in the background
In its early years  the Internet was largely used by universities  high-tech industries  and the
Government (especially the Dept
of Defense)
With the explosion of interest in the Internet starting in the mid-   s  it began to be used by a different group of people  often with different requirements
For one thing  numerous people with smart phones use it to keep in contact with their home bases
For another  with the impending convergence of the computer  communication  and entertainment industries  it may not be that long before every telephone and television set in the world is an Internet node  resulting in a billion machines being used for audio and video on demand
Under these circumstances  it became apparent that IP had to evolve and become more flexible
Seeing these problems on the horizon  in  IETF started work on a new version of IP  one that would never run out of addresses  would solve a variety of other problems  and be more flexible and efficient as well
Its major goals were:
Support billions of hosts  even with inefficient address allocation Reduce the size of the routing tables Simplify the protocol  to allow routers to process packets faster Provide better urity (authentication and privacy) Pay more attention to the type of service  particularly for real-time data Aid multicasting by allowing scopes to be specified Make it possible for a host to roam without changing its address Allow the protocol to evolve in the future Permit the old and new protocols to coexist for years
THE NETWORK LAYER IN THE INTERNET The design of IPv  presented a major opportunity to improve all of the features in IPv  that fall short of what is now wanted
To develop a protocol that met all these requirements  IETF issued a call for proposals and discussion in RFC
Twenty-one responses were initially received
By December  seven serious proposals were on the table
They ranged from making minor patches to IP  to throwing it out altogether and replacing it with a completely different protocol
One proposal was to run TCP over CLNP  the network layer protocol designed for OSI
With its   -bit addresses  CLNP would have provided enough address space forever as it could give every molecule of water in the oceans enough addresses (roughly  ) to set up a small network
This choice would also have unified two major network layer protocols
However  many people felt that this would have been an admission that something in the OSI world was actually done right  a statement considered Politically Incorrect in Internet circles
CLNP was patterned closely on IP  so the two are not really that different
In fact  the protocol ultimately chosen differs from IP far more than CLNP does
Another strike against CLNP was its poor support for service types  something required to transmit multimedia efficiently
Three of the better proposals were published in IEEE Network (Deering  ; Francis  ; and Katz and Ford  )
After much discussion  revision  and jockeying for position  a modified combined version of the Deering and Francis proposals  by now called SIPP (Simple Internet Protocol Plus) was selected and given the designation IPv
IPv  meets IETFâs goals fairly well
It maintains the good features of IP  discards or deemphasizes the bad ones  and adds new ones where needed
In general  IPv  is not compatible with IPv  but it is compatible with the other auxiliary Internet protocols  including TCP  UDP  ICMP  IGMP  OSPF  BGP  and DNS  with small modifications being required to deal with longer addresses
The main features of IPv  are discussed below
More information about it can be found in RFCs  through
First and foremost  IPv  has longer addresses than IPv
They are bits long  which solves the problem that IPv  set out to solve: providing an effectively unlimited supply of Internet addresses
We will have more to say about addresses shortly
The ond major improvement of IPv  is the simplification of the header
It contains only seven fields (versus   in IPv )
This change allows routers to process packets faster and thus improves throughput and delay
We will discuss the header shortly  too
The third major improvement is better support for options
This change was essential with the new header because fields that previously were required are now optional (because they are not used so often)
In addition  the way options are represented is different  making it simple for routers to skip over options not intended for them
This feature speeds up packet processing time
THE NETWORK LAYER
A fourth area in which IPv  represents a big advance is in urity
IETF had its fill of newspaper stories about precocious  -year-olds using their personal computers to break into banks and military bases all over the Internet
There was a strong feeling that something had to be done to improve urity
Authentication and privacy are key features of the new IP
These were later retrofitted to IPv  however  so in the area of urity the differences are not so great any more
Finally  more attention has been paid to quality of service
Various halfhearted efforts to improve QoS have been made in the past  but now  with the growth of multimedia on the Internet  the sense of urgency is greater
The Main IPv  Header The IPv  header is shown in Fig
The Version field is always  for IPv  (and  for IPv )
During the transition period from IPv  which has already taken more than a decade  routers will be able to examine this field to tell what kind of packet they have
As an aside  making this test wastes a few instructions in the critical path  given that the data link header usually indicates the network protocol for demultiplexing  so some routers may skip the check
For example  the Ethernet Type field has different values to indicate an IPv  or an IPv  payload
The discussions between the ââDo it rightââ and ââMake it fastââ camps will no doubt be lengthy and vigorous
Bits Version Diff
services Flow label Payload length Next header Hop limit Source address (  bytes) Destination address (  bytes) Figure  -
The IPv  fixed header (required)
The Differentiated services field (originally called Traffic class) is used to distinguish the class of service for packets with different real-time delivery   THE NETWORK LAYER IN THE INTERNET requirements
It is used with the differentiated service architecture for quality of service in the same manner as the field of the same name in the IPv  packet
Also  the low-order  bits are used to signal explicit congestion indications  again in the same way as with IPv
The Flow label field provides a way for a source and destination to mark groups of packets that have the same requirements and should be treated in the same way by the network  forming a pseudoconnection
For example  a stream of packets from one process on a certain source host to a process on a specific destination host might have stringent delay requirements and thus need reserved bandwidth
The flow can be set up in advance and given an identifier
When a packet with a nonzero Flow label shows up  all the routers can look it up in internal tables to see what kind of special treatment it requires
In effect  flows are an attempt to have it both ways: the flexibility of a datagram network and the guarantees of a virtual-circuit network
Each flow for quality of service purposes is designated by the source address  destination address  and flow number
This design means that up to flows may be active at the same time between a given pair of IP addresses
It also means that even if two flows coming from different hosts but with the same flow label pass through the same router  the router will be able to tell them apart using the source and destination addresses
It is expected that flow labels will be chosen randomly  rather than assigned sequentially starting at   so routers are expected to hash them
The Payload length field tells how many bytes follow the  -byte header of Fig
The name was changed from the IPv  Total length field because the meaning was changed slightly: the   header bytes are no longer counted as part of the length (as they used to be)
This change means the payload can now be   bytes instead of a mere   bytes
The Next header field lets the cat out of the bag
The reason the header could be simplified is that there can be additional (optional) extension headers
This field tells which of the (currently) six extension headers  if any  follow this one
If this header is the last IP header  the Next header field tells which transport protocol handler (
TCP  UDP) to pass the packet to
The Hop limit field is used to keep packets from living forever
It is  in practice  the same as the Time to live field in IPv  namely  a field that is decremented on each hop
In theory  in IPv  it was a time in onds  but no router used it that way  so the name was changed to reflect the way it is actually used
Next come the Source address and Destination address fields
Deeringâs original proposal  SIP  used  -byte addresses  but during the review process many people felt that with  -byte addresses IPv  would run out of addresses within a few decades  whereas with  -byte addresses it would never run out
Other people argued that   bytes was overkill  whereas still others favored using  -byte addresses to be compatible with the OSI datagram protocol
Still another faction wanted variable-sized addresses
After much debate and more than a few words THE NETWORK LAYER
unprintable in an academic textbook  it was decided that fixed-length  -byte addresses were the best compromise
A new notation has been devised for writing  -byte addresses
They are written as eight groups of four hexadecimal digits with colons between the groups  like this: :   :   :   :   :   :  AB:CDEF Since many addresses will have many zeros inside them  three optimizations have been authorized
First  leading zeros within a group can be omitted  so  can be written as
ond  one or more groups of   zero bits can be replaced by a pair of colons
Thus  the above address now becomes ::  :   :  AB:CDEF Finally  IPv  addresses can be written as a pair of colons and an old dotted decimal number  for example: ::
Perhaps it is unnecessary to be so explicit about it  but there are a lot of  - byte addresses
Specifically  there are  of them  which is approximately  Ã
If the entire earth  land and water  were covered with computers  IPv  would allow  Ã  IP addresses per square meter
Students of chemistry will notice that this number is larger than Avogadroâs number
While it was not the intention to give every molecule on the surface of the earth its own IP address  we are not that far off
In practice  the address space will not be used efficiently  just as the telephone number address space is not (the area code for Manhattan  is nearly full  but that for Wyoming  is nearly empty)
In RFC  Durand and Huitema calculated that  using the allocation of telephone numbers as a guide  even in the most pessimistic scenario there will still be well over  IP addresses per square meter of the entire earthâs surface (land and water)
In any likely scenario  there will be trillions of them per square meter
In short  it seems unlikely that we will run out in the foreseeable future
It is instructive to compare the IPv  header (Fig
-  ) with the IPv  header (Fig
-  ) to see what has been left out in IPv
The IHL field is gone because the IPv  header has a fixed length
The Protocol field was taken out because the Next header field tells what follows the last IP header (
a UDP or TCP segment)
All the fields relating to fragmentation were removed because IPv  takes a different approach to fragmentation
To start with  all IPv -conformant hosts are expected to dynamically determine the packet size to use
They do this using the path MTU discovery procedure we described in
In brief  when a host sends an IPv  packet that is too large  instead of fragmenting it  the router that is unable to forward it drops the packet and sends an error message back to the   THE NETWORK LAYER IN THE INTERNET sending host
This message tells the host to break up all future packets to that destination
Having the host send packets that are the right size in the first place is ultimately much more efficient than having the routers fragment them on the fly
Also  the minimum-size packet that routers must be able to forward has been raised from to  bytes to allow  bytes of data and many headers
Finally  the Checksum field is gone because calculating it greatly reduces performance
With the reliable networks now used  combined with the fact that the data link layer and transport layers normally have their own checksums  the value of yet another checksum was deemed not worth the performance price it extracted
Removing all these features has resulted in a lean and mean network layer protocol
Thus  the goal of IPv âa fast  yet flexible  protocol with plenty of address spaceâis met by this design
Extension Headers Some of the missing IPv  fields are occasionally still needed  so IPv  introduces the concept of (optional) extension headers
These headers can be supplied to provide extra information  but encoded in an efficient way
Six kinds of extension headers are defined at present  as listed in Fig
Each one is optional  but if more than one is present they must appear directly after the fixed header  and preferably in the order listed
Extension header Description Hop-by-hop options Miscellaneous information for routers Destination options Additional information for the destination Routing Loose list of routers to visit Fragmentation Management of datagram fragments Authentication Verification of the senderâs identity Encrypted urity payload Information about the encrypted contents Figure  -
IPv  extension headers
Some of the headers have a fixed format; others contain a variable number of variable-length options
For these  each item is encoded as a (Type  Length  Value) tuple
The Type is a  -byte field telling which option this is
The Type values have been chosen so that the first  bits tell routers that do not know how to process the option what to do
The choices are: skip the option; discard the packet; discard the packet and send back an ICMP packet; and discard the packet but do not send ICMP packets for multicast addresses (to prevent one bad multicast packet from generating millions of ICMP reports)
The Length is also a  -byte field
It tells how long the value is (  to bytes)
The Value is any information required  up to bytes
THE NETWORK LAYER
The hop-by-hop header is used for information that all routers along the path must examine
So far  one option has been defined: support of datagrams exceeding   KB
The format of this header is shown in Fig
When it is used  the Payload length field in the fixed header is set to
Next header Jumbo payload length    Figure  -
The hop-by-hop extension header for large datagrams (jumbograms)
As with all extension headers  this one starts with a byte telling what kind of header comes next
This byte is followed by one telling how long the hop-by-hop header is in bytes  excluding the first  bytes  which are mandatory
All extensions begin this way
The next  bytes indicate that this option defines the datagram size (code   ) and that the size is a  -byte number
The last  bytes give the size of the datagram
Sizes less than   bytes are not permitted and will result in the first router discarding the packet and sending back an ICMP error message
Datagrams using this header extension are called jumbograms
The use of jumbograms is important for supercomputer applications that must transfer gigabytes of data efficiently across the Internet
The destination options header is intended for fields that need only be interpreted at the destination host
In the initial version of IPv  the only options defined are null options for padding this header out to a multiple of  bytes  so initially it will not be used
It was included to make sure that new routing and host software can handle it  in case someone thinks of a destination option some day
The routing header lists one or more routers that must be visited on the way to the destination
It is very similar to the IPv  loose source routing in that all addresses listed must be visited in order  but other routers not listed may be visited in between
The format of the routing header is shown in Fig
Next header Header extension length Routing type Segments left Type-specific data Figure  -
The extension header for routing
THE NETWORK LAYER IN THE INTERNET The first  bytes of the routing extension header contain four  -byte integers
The Next header and Header extension length fields were described above
The Routing type field gives the format of the rest of the header
Type  says that a reserved  -bit word follows the first word  followed by some number of IPv  addresses
Other types may be invented in the future  as needed
Finally  the Segments left field keeps track of how many of the addresses in the list have not yet been visited
It is decremented every time one is visited
When it hits   the packet is on its own with no more guidance about what route to follow
Usually  at this point it is so close to the destination that the best route is obvious
The fragment header deals with fragmentation similarly to the way IPv  does
The header holds the datagram identifier  fragment number  and a bit telling whether more fragments will follow
In IPv  unlike in IPv  only the source host can fragment a packet
Routers along the way may not do this
This change is a major philosophical break with the original IP  but in keeping with current practice for IPv
Plus  it simplifies the routersâ work and makes routing go faster
As mentioned above  if a router is confronted with a packet that is too big  it discards the packet and sends an ICMP error packet back to the source
This information allows the source host to fragment the packet into smaller pieces using this header and try again
The authentication header provides a mechanism by which the receiver of a packet can be sure of who sent it
The encrypted urity payload makes it possible to encrypt the contents of a packet so that only the intended recipient can read it
These headers use the cryptographic techniques that we will describe in
to accomplish their missions
Controversies Given the open design process and the strongly held opinions of many of the people involved  it should come as no surprise that many choices made for IPv  were highly controversial  to say the least
We will summarize a few of these briefly below
For all the gory details  see the RFCs
We have already mentioned the argument about the address length
The result was a compromise:  -byte fixed-length addresses
Another fight developed over the length of the Hop limit field
One camp felt strongly that limiting the maximum number of hops to (implicit in using an  -bit field) was a gross mistake
After all  paths of   hops are common now  and   years from now much longer paths may be common
These people argued that using a huge address size was farsighted but using a tiny hop count was shortsighted
In their view  the greatest sin a computer scientist can commit is to provide too few bits somewhere
The response was that arguments could be made to increase every field  leading to a bloated header
Also  the function of the Hop limit field is to keep packets from wandering around for too long a time and   hops is far  far too long
THE NETWORK LAYER
Finally  as the Internet grows  more and more long-distance links will be built  making it possible to get from any country to any other country in half a dozen hops at most
If it takes more than hops to get from the source and the destination to their respective international gateways  something is wrong with the national backbones
The  -bitters won this one
Another hot potato was the maximum packet size
The supercomputer community wanted packets in excess of   KB
When a supercomputer gets started transferring  it really means business and does not want to be interrupted every   KB
The argument against large packets is that if a  -MB packet hits a
A compromise was reached here: normal packets are limited to   KB  but the hop-by-hop extension header can be used to permit jumbograms
A third hot topic was removing the IPv  checksum
Some people likened this move to removing the brakes from a car
Doing so makes the car lighter so it can go faster  but if an unexpected event happens  you have a problem
The argument against checksums was that any application that really cares about data integrity has to have a transport layer checksum anyway  so having another one in IP (in addition to the data link layer checksum) is overkill
Furthermore  experience showed that computing the IP checksum was a major expense in IPv
The antichecksum camp won this one  and IPv  does not have a checksum
Mobile hosts were also a point of contention
If a portable computer flies halfway around the world  can it continue operating there with the same IPv  address  or does it have to use a scheme with home agents? Some people wanted to build explicit support for mobile hosts into IPv
That effort failed when no consensus could be found for any specific proposal
Probably the biggest battle was about urity
Everyone agreed it was essential
The war was about where to put it and how
First where
The argument for putting it in the network layer is that it then becomes a standard service that all applications can use without any advance planning
The argument against it is that really ure applications generally want nothing less than end-to-end encryption  where the source application does the encryption and the destination application undoes it
With anything less  the user is at the mercy of potentially buggy network layer implementations over which he has no control
The response to this argument is that these applications can just refrain from using the IP urity features and do the job themselves
The rejoinder to that is that the people who do not trust the network to do it right do not want to pay the price of slow  bulky IP implementations that have this capability  even if it is disabled
Another aspect of where to put urity relates to the fact that many (but not all) countries have very stringent export laws concerning cryptography
Some  notably France and Iraq  also restrict its use domestically  so that people cannot have rets from the government
As a result  any IP implementation that used a   THE NETWORK LAYER IN THE INTERNET cryptographic system strong enough to be of much value could not be exported from the United States (and many other countries) to customers worldwide
Having to maintain two sets of software  one for domestic use and one for export  is something most computer vendors vigorously oppose
One point on which there was no controversy is that no one expects the IPv  Internet to be turned off on a Sunday evening and come back up as an IPv  Internet Monday morning
Instead  isolated ââislandsââ of IPv  will be converted  initially communicating via tunnels  as we showed in
As the IPv  islands grow  they will merge into bigger islands
Eventually  all the islands will merge  and the Internet will be fully converted
At least  that was the plan
Deployment has proved the Achilles heel of IPv
It remains little used  even though all major operating systems fully support it
Most deployments are new situations in which a network operatorâfor example  a mobile phone operatorâ needs a large number of IP addresses
Many strategies have been defined to help ease the transition
Among them are ways to automatically configure the tunnels that carry IPv  over the IPv  Internet  and ways for hosts to automatically find the tunnel endpoints
Dual-stack hosts have an IPv  and an IPv  implementation so that they can select which protocol to use depending on the destination of the packet
These strategies will streamline the substantial deployment that seems inevitable when IPv  addresses are exhausted
For more information about IPv  see Davies (   )
Internet Control Protocols In addition to IP  which is used for data transfer  the Internet has several companion control protocols that are used in the network layer
They include ICMP  ARP  and DHCP
In this tion  we will look at each of these in turn  describing the versions that correspond to IPv  because they are the protocols that are in common use
ICMP and DHCP have similar versions for IPv ; the equivalent of ARP is called NDP (Neighbor Discovery Protocol) for IPv
IMCPâThe Internet Control Message Protocol The operation of the Internet is monitored closely by the routers
When something unexpected occurs during packet processing at a router  the event is reported to the sender by the ICMP (Internet Control Message Protocol)
ICMP is also used to test the Internet
About a dozen types of ICMP messages are defined
Each ICMP message type is carried encapsulated in an IP packet
The most important ones are listed in Fig
The DESTINATION UNREACHABLE message is used when the router cannot locate the destination or when a packet with the DF bit cannot be delivered because a ââsmall-packetââ network stands in the way
THE NETWORK LAYER
Message type Description Destination unreachable Packet could not be delivered Time exceeded Time to live field hit  Parameter problem Invalid header field Source quench Choke packet Redirect Teach a router about geography Echo and echo reply Check if a machine is alive Timestamp request/reply Same as Echo  but with timestamp Router advertisement/solicitation Find a nearby router Figure  -
The principal ICMP message types
The TIME EXCEEDED message is sent when a packet is dropped because its TtL (Time to live) counter has reached zero
This event is a symptom that packets are looping  or that the counter values are being set too low
One clever use of this error message is the traceroute utility that was developed by Van Jacobson in
Traceroute finds the routers along the path from the host to a destination IP address
It finds this information without any kind of privileged network support
The method is simply to send a sequence of packets to the destination  first with a TtL of   then a TtL of  and so on
The counters on these packets will reach zero at successive routers along the path
These routers will each obediently send a TIME EXCEEDED message back to the host
From those messages  the host can determine the IP addresses of the routers along the path  as well as keep statistics and timings on parts of the path
It is not what the TIME EXCEEDED message was intended for  but it is perhaps the most useful network debugging tool of all time
The PARAMETER PROBLEM message indicates that an illegal value has been detected in a header field
This problem indicates a bug in the sending hostâs IP software or possibly in the software of a router transited
The SOURCE QUENCH message was long ago used to throttle hosts that were sending too many packets
When a host received this message  it was expected to slow down
It is rarely used anymore because when congestion occurs  these packets tend to add more fuel to the fire and it is unclear how to respond to them
Congestion control in the Internet is now done largely by taking action in the transport layer  using packet losses as a congestion signal; we will study it in detail in   The REDIRECT message is used when a router notices that a packet seems to be routed incorrectly
It is used by the router to tell the sending host to update to a better route
The ECHO and ECHO REPLY messages are sent by hosts to see if a given destination is reachable and currently alive
Upon receiving the ECHO message    THE NETWORK LAYER IN THE INTERNET the destination is expected to send back an ECHO REPLY message
These messages are used in the ping utility that checks if a host is up and on the Internet
The TIMESTAMP REQUEST and TIMESTAMP REPLY messages are similar  except that the arrival time of the message and the departure time of the reply are recorded in the reply
This facility can be used to measure network performance
The ROUTER ADVERTISEMENT and ROUTER SOLICITATION messages are used to let hosts find nearby routers
A host needs to learn the IP address of at least one router to be able to send packets off the local network
In addition to these messages  others have been defined
The online list is now kept at  /assignments/icmp-parameters
ARPâThe Address Resolution Protocol Although every machine on the Internet has one or more IP addresses  these addresses are not sufficient for sending packets
Data link layer NICs (Network Interface Cards) such as Ethernet cards do not understand Internet addresses
In the case of Ethernet  every NIC ever manufactured comes equipped with a unique  -bit Ethernet address
Manufacturers of Ethernet NICs request a block of Ethernet addresses from IEEE to ensure that no two NICs have the same address (to avoid conflicts should the two NICs ever appear on the same LAN)
The NICs send and receive frames based on  -bit Ethernet addresses
They know nothing at all about  -bit IP addresses
The question now arises  how do IP addresses get mapped onto data link layer addresses  such as Ethernet? To explain how this works  let us use the example of Fig
One network (CS) is a switched Ethernet in the Computer Science Dept
It has the prefix
The other LAN (EE)  also switched Ethernet  is in Electrical Engineering and has the prefix
The two LANs are connected by an IP router
Each machine on an Ethernet and each interface on the router has a unique Ethernet address  labeled E  through E  and a unique IP address on the CS or EE network
Let us start out by seeing how a user on host  sends a packet to a user on host  on the CS network
Let us assume the sender knows the name of the intended receiver  possibly something like   The first step is to find the IP address for host
This lookup is performed by DNS  which we will study in   For the moment  we will just assume that DNS returns the IP address for host  (
The upper layer software on host  now builds a packet with
in the Destination address field and gives it to the IP software to transmit
The IP software can look at the address and see that the destination is on the CS network  (
its own network)
However  it still needs some way to find the destinationâs Ethernet address to send the frame
One solution is to have a configuration file somewhere in the system that maps IP addresses onto Ethernet addresses
While THE NETWORK LAYER
Ethernet switch E  CS Network
E  E  E  E  E
IP  =         IP  =
EE Network
/  Router Host  Host  Host  Host  Frame Source IP Source Eth
Destination IP Destination Eth
Host  to   on CS net IP  E  IP  E  Host  to   on CS net IP  E  IP  E  Host  to   on EE net IP  E  IP  E  Figure  -
Two switched Ethernet LANs joined by a router
this solution is certainly possible  for organizations with thousands of machines keeping all these files up to date is an error-prone  time-consuming job
A better solution is for host  to output a broadcast packet onto the Ethernet asking who owns IP address      The broadcast will arrive at every machine on the CS Ethernet  and each one will check its IP address
Host  alone will respond with its Ethernet address (E )
In this way host  learns that IP address
is on the host with Ethernet address E
The protocol used for asking this question and getting the reply is called ARP (Address Resolution Protocol)
Almost every machine on the Internet runs it
ARP is defined in RFC
The advantage of using ARP over configuration files is the simplicity
The system manager does not have to do much except assign each machine an IP address and decide about subnet masks
ARP does the rest
At this point  the IP software on host  builds an Ethernet frame addressed to E  puts the IP packet (addressed to
) in the payload field  and dumps it onto the Ethernet
The IP and Ethernet addresses of this packet are given in Fig
The Ethernet NIC of host  detects this frame  recognizes it as a frame for itself  scoops it up  and causes an interrupt
The Ethernet driver extracts the IP packet from the payload and passes it to the IP software  which sees that it is correctly addressed and processes it
Various optimizations are possible to make ARP work more efficiently
To start with  once a machine has run ARP  it caches the result in case it needs to contact the same machine shortly
Next time it will find the mapping in its own cache  thus eliminating the need for a ond broadcast
In many cases  host    THE NETWORK LAYER IN THE INTERNET will need to send back a reply  forcing it  too  to run ARP to determine the senderâs Ethernet address
This ARP broadcast can be avoided by having host  include its IP-to-Ethernet mapping in the ARP packet
When the ARP broadcast arrives at host   the pair (
E ) is entered into host  âs ARP cache
In fact  all machines on the Ethernet can enter this mapping into their ARP caches
To allow mappings to change  for example  when a host is configured to use a new IP address (but keeps its old Ethernet address)  entries in the ARP cache should time out after a few minutes
A clever way to help keep the cached information current and to optimize performance is to have every machine broadcast its mapping when it is configured
This broadcast is generally done in the form of an ARP looking for its own IP address
There should not be a response  but a side effect of the broadcast is to make or update an entry in everyoneâs ARP cache
This is known as a gratuitous ARP
If a response does (unexpectedly) arrive  two machines have been assigned the same IP address
The error must be resolved by the network manager before both machines can use the network
Now let us look at Fig
-  again  only this time assume that host  wants to send a packet to host  (
) on the EE network
Host  will see that the destination IP address is not on the CS network
It knows to send all such off-network traffic to the router  which is also known as the default gateway
By convention  the default gateway is the lowest address on the network (
To send a frame to the router  host  must still know the Ethernet address of the router interface on the CS network
It discovers this by sending an ARP broadcast for
from which it learns E
It then sends the frame
The same lookup mechanisms are used to send a packet from one router to the next over a sequence of routers in an Internet path
When the Ethernet NIC of the router gets this frame  it gives the packet to the IP software
It knows from the network masks that the packet should be sent onto the EE network where it will reach host
If the router does not know the Ethernet address for host   then it will use ARP again
The table in Fig
Observe that the Ethernet addresses change with the frame on each network while the IP addresses remain constant (because they indicate the endpoints across all of the interconnected networks)
It is also possible to send a packet from host  to host  without host  knowing that host  is on a different network
The solution is to have the router answer ARPs on the CS network for host  and give its Ethernet address  E  as the response
It is not possible to have host  reply directly because it will not see the ARP request (as routers do not forward Ethernet-level broadcasts)
The router will then receive frames sent to
and forward them onto the EE network
This solution is called proxy ARP
It is used in special cases in which a host wants to appear on a network even though it actually resides on another network
A common situation  for example  is a mobile computer that wants some other node to pick up packets for it when it is not on its home network
THE NETWORK LAYER
DHCPâThe Dynamic Host Configuration Protocol ARP (as well as other Internet protocols) makes the assumption that hosts are configured with some basic information  such as their own IP addresses
How do hosts get this information? It is possible to manually configure each computer  but that is tedious and error-prone
There is a better way  and it is called DHCP (Dynamic Host Configuration Protocol)
With DHCP  every network must have a DHCP server that is responsible for configuration
When a computer is started  it has a built-in Ethernet or other link layer address embedded in the NIC  but no IP address
Much like ARP  the computer broadcasts a request for an IP address on its network
It does this by using a DHCP DISCOVER packet
This packet must reach the DHCP server
If that server is not directly attached to the network  the router will be configured to receive DHCP broadcasts and relay them to the DHCP server  wherever it is located
When the server receives the request  it allocates a free IP address and sends it to the host in a DHCP OFFER packet (which again may be relayed via the router)
To be able to do this work even when hosts do not have IP addresses  the server identifies a host using its Ethernet address (which is carried in the DHCP DISCOVER packet) An issue that arises with automatic assignment of IP addresses from a pool is for how long an IP address should be allocated
If a host leaves the network and does not return its IP address to the DHCP server  that address will be permanently lost
After a period of time  many addresses may be lost
To prevent that from happening  IP address assignment may be for a fixed period of time  a technique called leasing
Just before the lease expires  the host must ask for a DHCP renewal
If it fails to make a request or the request is denied  the host may no longer use the IP address it was given earlier
DHCP is described in RFCs  and
It is widely used in the Internet to configure all sorts of parameters in addition to providing hosts with IP addresses
As well as in business and home networks  DHCP is used by ISPs to set the parameters of devices over the Internet access link  so that customers do not need to phone their ISPs to get this information
Common examples of the information that is configured include the network mask  the IP address of the default gateway  and the IP addresses of DNS and time servers
DHCP has largely replaced earlier protocols (called RARP and BOOTP) with more limited functionality
Label Switching and MPLS So far  on our tour of the network layer of the Internet  we have focused exclusively on packets as datagrams that are forwarded by IP routers
There is also another kind of technology that is starting to be widely used  especially by ISPs  in order to move Internet traffic across their networks
This technology is   THE NETWORK LAYER IN THE INTERNET called MPLS (MultiProtocol Label Switching) and it is perilously close to circuit switching
Despite the fact that many people in the Internet community have an intense dislike for connection-oriented networking  the idea seems to keep coming back
As Yogi Berra once put it  it is like deja vu all over again
However  there are essential differences between the way the Internet handles route construction and the way connection-oriented networks do it  so the technique is certainly not traditional circuit switching
MPLS adds a label in front of each packet  and forwarding is based on the label rather than on the destination address
Making the label an index into an internal table makes finding the correct output line just a matter of table lookup
Using this technique  forwarding can be done very quickly
This advantage was the original motivation behind MPLS  which began as proprietary technology known by various names including tag switching
Eventually  IETF began to standardize the idea
It is described in RFC  and many other RFCs
The main benefits over time have come to be routing that is flexible and forwarding that is suited to quality of service as well as fast
The first question to ask is where does the label go? Since IP packets were not designed for virtual circuits  there is no field available for virtual-circuit numbers within the IP header
For this reason  a new MPLS header had to be added in front of the IP header
On a router-to-router line using PPP as the framing protocol  the frame format  including the PPP  MPLS  IP  and TCP headers  is as shown in Fig
PPP MPLS IP Label QoS S TtL Bits   Headers  TCP User data CRC Figure  -
Transmitting a TCP segment using IP  MPLS  and PPP
The generic MPLS header is  bytes long and has four fields
Most important is the Label field  which holds the index
The QoS field indicates the class of service
The S field relates to stacking multiple labels (which is discussed below)
The TtL field indicates how many more times the packet may be forwarded
It is decremented at each router  and if it hits   the packet is discarded
This feature prevents infinite looping in the case of routing instability
MPLS falls between the IP network layer protocol and the PPP link layer protocol
It is not really a layer  protocol because it depends on IP or other network THE NETWORK LAYER
layer addresses to set up label paths
It is not really a layer  protocol either because it forwards packets across multiple hops  not a single link
For this reason  MPLS is sometimes described as a layer
It is an illustration that real protocols do not always fit neatly into our ideal layered protocol model
On the brighter side  because the MPLS headers are not part of the network layer packet or the data link layer frame  MPLS is to a large extent independent of both layers
Among other things  this property means it is possible to build MPLS switches that can forward both IP packets and non-IP packets  depending on what shows up
This feature is where the ââmultiprotocolââ in the name MPLS came from
MPLS can also carry IP packets over non-IP networks
When an MPLS-enhanced packet arrives at a LSR (Label Switched Router)  the label is used as an index into a table to determine the outgoing line to use and also the new label to use
This label swapping is used in all virtual-circuit networks
Labels have only local significance and two different routers can feed unrelated packets with the same label into another router for transmission on the same outgoing line
To be distinguishable at the other end  labels have to be remapped at every hop
We saw this mechanism in action in Fig
MPLS uses the same technique
As an aside  some people distinguish between forwarding and switching
Forwarding is the process of finding the best match for a destination address in a table to decide where to send packets
An example is the longest matching prefix algorithm used for IP forwarding
In contrast  switching uses a label taken from the packet as an index into a forwarding table
It is simpler and faster
These definitions are far from universal  however
Since most hosts and routers do not understand MPLS  we should also ask when and how the labels are attached to packets
This happens when an IP packet reaches the edge of an MPLS network
The LER (Label Edge Router) inspects the destination IP address and other fields to see which MPLS path the packet should follow  and puts the right label on the front of the packet
Within the MPLS network  this label is used to forward the packet
At the other edge of the MPLS network  the label has served its purpose and is removed  revealing the IP packet again for the next network
This process is shown in Fig
One difference from traditional virtual circuits is the level of aggregation
It is certainly possible for each flow to have its own set of labels through the MPLS network
However  it is more common for routers to group multiple flows that end at a particular router or LAN and use a single label for them
The flows that are grouped together under a single label are said to belong to the same FEC (Forwarding Equivalence Class)
This class covers not only where the packets are going  but also their service class (in the differentiated services sense) because all the packets are treated the same way for forwarding purposes
With traditional virtual-circuit routing  it is not possible to group several distinct paths with different endpoints onto the same virtual-circuit identifier because there would be no way to distinguish them at the final destination
With MPLS    THE NETWORK LAYER IN THE INTERNET Switching on label only Label switch router IP IP Label IP Label edge router Add label Remove label (to next network) Label Label Figure  -
Forwarding an IP packet through an MPLS network
the packets still contain their final destination address  in addition to the label
At the end of the labeled route  the label header can be removed and forwarding can continue the usual way  using the network layer destination address
Actually  MPLS goes even further
It can operate at multiple levels at once by adding more than one label to the front of a packet
For example  suppose that there are many packets that already have different labels (because we want to treat the packets differently somewhere in the network) that should follow a common path to some destination
Instead of setting up many label switching paths  one for each of the different labels  we can set up a single path
When the already- labeled packets reach the start of this path  another label is added to the front
This is called a stack of labels
The outermost label guides the packets along the path
It is removed at the end of the path  and the labels revealed  if any  are used to forward the packet further
The S bit in Fig
It is set to  for the bottom label and  for all the other labels
The final question we will ask is how the label forwarding tables are set up so that packets follow them
This is one area of major difference between MPLS and conventional virtual-circuit designs
In traditional virtual-circuit networks  when a user wants to establish a connection  a setup packet is launched into the network to create the path and make the forwarding table entries
MPLS does not involve users in the setup phase
Requiring users to do anything other than send a datagram would break too much existing Internet software
Instead  the forwarding information is set up by protocols that are a combination of routing protocols and connection setup protocols
These control protocols are cleanly separated from label forwarding  which allows multiple  different control protocols to be used
One of the variants works like this
When a router is booted  it checks to see which routes it is the final destination for (
which prefixes belong to its interfaces)
It then creates one or more FECs for them  allocates a label for each one  and passes the labels to its neighbors
They  in turn  enter the labels in their forwarding tables and send new labels to their neighbors  until all the routers have acquired the path
Resources can also be reserved as the THE NETWORK LAYER
path is constructed to guarantee an appropriate quality of service
Other variants can set up different paths  such as traffic engineering paths that take unused capacity into account  and create paths on-demand to support service offerings such as quality of service
Although the basic ideas behind MPLS are straightforward  the details are complicated  with many variations and use cases that are being actively developed
For more information  see Davie and Farrel (   ) and Davie and Rekhter (   )
OSPFâAn Interior Gateway Routing Protocol We have now finished our study of how packets are forwarded in the Internet
It is time to move on to the next topic: routing in the Internet
As we mentioned earlier  the Internet is made up of a large number of independent networks or ASes (Autonomous Systems) that are operated by different organizations  usually a company  university  or ISP
Inside of its own network  an organization can use its own algorithm for internal routing  or intradomain routing  as it is more commonly known
Nevertheless  there are only a handful of standard protocols that are popular
In this tion  we will study the problem of intradomain routing and look at the OSPF protocol that is widely used in practice
An intradomain routing protocol is also called an interior gateway protocol
In the next tion  we will study the problem of routing between independently operated networks  or interdomain routing
For that case  all networks must use the same interdomain routing protocol or exterior gateway protocol
The protocol that is used in the Internet is BGP (Border Gateway Protocol)
Early intradomain routing protocols used a distance vector design  based on the distributed Bellman-Ford algorithm inherited from the ARPANET
RIP (Routing Information Protocol) is the main example that is used to this day
It works well in small systems  but less well as networks get larger
It also suffers from the count-to-infinity problem and generally slow convergence
The ARPANET switched over to a link state protocol in May  because of these problems  and in  IETF began work on a link state protocol for intradomain routing
That protocol  called OSPF (Open Shortest Path First)  became a standard in
It drew on a protocol called IS-IS (Intermediate-System to Intermediate-System)  which became an ISO standard
Because of their shared heritage  the two protocols are much more alike than different
For the complete story  see RFC
They are the dominant intradomain routing protocols  and most router vendors now support both of them
OSPF is more widely used in company networks  and IS-IS is more widely used in ISP networks
Of the two  we will give a sketch of how OSPF works
Given the long experience with other routing protocols  the group designing OSPF had a long list of requirements that had to be met
First  the algorithm had to be published in the open literature  hence the ââOââ in OSPF
A proprietary   THE NETWORK LAYER IN THE INTERNET solution owned by one company would not do
ond  the new protocol had to support a variety of distance metrics  including physical distance  delay  and so on
Third  it had to be a dynamic algorithm  one that adapted to changes in the topology automatically and quickly
Fourth  and new for OSPF  it had to support routing based on type of service
The new protocol had to be able to route real-time traffic one way and other traffic a different way
At the time  IP had a Type of service field  but no existing routing protocol used it
This field was included in OSPF but still nobody used it  and it was eventually removed
Perhaps this requirement was ahead of its time  as it preceded IETFâs work on differentiated services  which has rejuvenated classes of service
Fifth  and related to the above  OSPF had to do load balancing  splitting the load over multiple lines
Most previous protocols sent all packets over a single best route  even if there were two routes that were equally good
The other route was not used at all
In many cases  splitting the load over multiple routes gives better performance
Sixth  support for hierarchical systems was needed
By  some networks had grown so large that no router could be expected to know the entire topology
OSPF had to be designed so that no router would have to
Seventh  some modicum of urity was required to prevent fun-loving students from spoofing routers by sending them false routing information
Finally  provision was needed for dealing with routers that were connected to the Internet via a tunnel
Previous protocols did not handle this well
OSPF supports both point-to-point links (
SONET) and broadcast networks (
most LANs)
Actually  it is able to support networks with multiple routers  each of which can communicate directly with the others (called multiaccess networks) even if they do not have broadcast capability
Earlier protocols did not handle this case well
An example of an autonomous system network is given in Fig
Hosts are omitted because they do not generally play a role in OSPF  while routers and networks (which may contain hosts) do
Most of the routers in Fig
However  routers R  R  and R  are connected by a broadcast LAN such as switched Ethernet
OSPF operates by abstracting the collection of actual networks  routers  and links into a directed graph in which each arc is assigned a weight (distance  delay  etc
A point-to-point connection between two routers is represented by a pair of arcs  one in each direction
Their weights may be different
A broadcast network is represented by a node for the network itself  plus a node for each router
The arcs from that network node to the routers have weight
They are important nonetheless  as without them there is no path through the network
Other networks  which have only hosts  have only an arc reaching them and not one returning
This structure gives routes to hosts  but not through them
THE NETWORK LAYER
LAN  LAN  LAN  LAN  R  R  R  R  R  R  R  R  R  R  LAN  LAN  LAN  LAN  (a) (b)     Figure  -
(a) An autonomous system
(b) A graph representation of (a)
Figure  -  (b) shows the graph representation of the network of Fig
What OSPF fundamentally does is represent the actual network as a graph like this and then use the link state method to have every router compute the shortest path from itself to all other nodes
Multiple paths may be found that are equally short
In this case  OSPF remembers the set of shortest paths and during packet forwarding  traffic is split across them
This helps to balance load
It is called ECMP (Equal Cost MultiPath)
Many of the ASes in the Internet are themselves large and nontrivial to manage
To work at this scale  OSPF allows an AS to be divided into numbered areas  where an area is a network or a set of contiguous networks
Areas do not overlap but need not be exhaustive  that is  some routers may belong to no area
Routers that lie wholly within an area are called internal routers
An area is a generalization of an individual network
Outside an area  its destinations are visible but not its topology
This characteristic helps routing to scale
Every AS has a backbone area  called area
The routers in this area are called backbone routers
All areas are connected to the backbone  possibly by tunnels  so it is possible to go from any area in the AS to any other area in the AS via the backbone
A tunnel is represented in the graph as just another arc with a cost
As with other areas  the topology of the backbone is not visible outside the backbone
Each router that is connected to two or more areas is called an area border router
It must also be part of the backbone
The job of an area border router is to summarize the destinations in one area and to inject this summary into the other   THE NETWORK LAYER IN THE INTERNET areas to which it is connected
This summary includes cost information but not all the details of the topology within an area
Passing cost information allows hosts in other areas to find the best area border router to use to enter an area
Not passing topology information reduces traffic and simplifies the shortest-path computations of routers in other areas
However  if there is only one border router out of an area  even the summary does not need to be passed
Routes to destinations out of the area always start with the instruction ââGo to the border router
ââ This kind of area is called a stub area
The last kind of router is the AS boundary router
It injects routes to external destinations on other ASes into the area
The external routes then appear as destinations that can be reached via the AS boundary router with some cost
An external route can be injected at one or more AS boundary routers
The relationship between ASes  areas  and the various kinds of routers is shown in Fig
One router may play multiple roles  for example  a border router is also a backbone router
Area  (stub) Area  (backbone) Area  Backbone router AS boundary router Internal router Area border router One autonomous system Figure  -
The relation between ASes  backbones  and areas in OSPF
During normal operation  each router within an area has the same link state database and runs the same shortest path algorithm
Its main job is to calculate the shortest path from itself to every other router and network in the entire AS
An area border router needs the databases for all the areas to which it is connected and must run the shortest path algorithm for each area separately
For a source and destination in the same area  the best intra-area route (that lies wholly within the area) is chosen
For a source and destination in different areas  the inter-area route must go from the source to the backbone  across the backbone to the destination area  and then to the destination
This algorithm forces a star configuration on OSPF  with the backbone being the hub and the other areas being spokes
Because the route with the lowest cost is chosen  routers in different parts of the network may use different area border routers to enter the backbone and destination area
Packets are routed from source to destination ââas is
ââ They are not encapsulated or tunneled (unless going to an area whose THE NETWORK LAYER
only connection to the backbone is a tunnel)
Also  routes to external destinations may include the external cost from the AS boundary router over the external path  if desired  or just the cost internal to the AS
When a router boots  it sends HELLO messages on all of its point-to-point lines and multicasts them on LANs to the group consisting of all the other routers
From the responses  each router learns who its neighbors are
Routers on the same LAN are all neighbors
OSPF works by exchanging information between adjacent routers  which is not the same as between neighboring routers
In particular  it is inefficient to have every router on a LAN talk to every other router on the LAN
To avoid this situation  one router is elected as the designated router
It is said to be adjacent to all the other routers on its LAN  and exchanges information with them
In effect  it is acting as the single node that represents the LAN
Neighboring routers that are not adjacent do not exchange information with each other
A backup designated router is always kept up to date to ease the transition should the primary designated router crash and need to be replaced immediately
During normal operation  each router periodically floods LINK STATE UPDATE messages to each of its adjacent routers
These messages gives its state and provide the costs used in the topological database
The flooding messages are acknowledged  to make them reliable
Each message has a sequence number  so a router can see whether an incoming LINK STATE UPDATE is older or newer than what it currently has
Routers also send these messages when a link goes up or down or its cost changes
DATABASE DESCRIPTION messages give the sequence numbers of all the link state entries currently held by the sender
By comparing its own values with those of the sender  the receiver can determine who has the most recent values
These messages are used when a link is brought up
Either partner can request link state information from the other one by using LINK STATE REQUEST messages
The result of this algorithm is that each pair of adjacent routers checks to see who has the most recent data  and new information is spread throughout the area this way
All these messages are sent directly in IP packets
The five kinds of messages are summarized in Fig
Message type Description Hello Used to discover who the neighbors are Link state update Provides the senderâs costs to its neighbors Link state ack Acknowledges link state update Database description Announces which updates the sender has Link state request Requests information from the partner Figure  -
The five types of OSPF messages
THE NETWORK LAYER IN THE INTERNET Finally  we can put all the pieces together
Using flooding  each router informs all the other routers in its area of its links to other routers and networks and the cost of these links
This information allows each router to construct the graph for its area(s) and compute the shortest paths
The backbone area does this work  too
In addition  the backbone routers accept information from the area border routers in order to compute the best route from each backbone router to every other router
This information is propagated back to the area border routers  which advertise it within their areas
Using this information  internal routers can select the best route to a destination outside their area  including the best exit router to the backbone
BGPâThe Exterior Gateway Routing Protocol Within a single AS  OSPF and IS-IS are the protocols that are commonly used
Between ASes  a different protocol  called BGP (Border Gateway Protocol)  is used
A different protocol is needed because the goals of an intradomain protocol and an interdomain protocol are not the same
All an intradomain protocol has to do is move packets as efficiently as possible from the source to the destination
It does not have to worry about politics
In contrast  interdomain routing protocols have to worry about politics a great deal (Metz  )
For example  a corporate AS might want the ability to send packets to any Internet site and receive packets from any Internet site
However  it might be unwilling to carry transit packets originating in a foreign AS and ending in a different foreign AS  even if its own AS is on the shortest path between the two foreign ASes (ââThatâs their problem  not oursââ)
On the other hand  it might be willing to carry transit traffic for its neighbors  or even for specific other ASes that paid it for this service
Telephone companies  for example  might be happy to act as carriers for their customers  but not for others
Exterior gateway protocols in general  and BGP in particular  have been designed to allow many kinds of routing policies to be enforced in the interAS traffic
Typical policies involve political  urity  or economic considerations
A few examples of possible routing constraints are:
Do not carry commercial traffic on the educational network Never send traffic from the Pentagon on a route through Iraq Use TeliaSonera instead of Verizon because it is cheaper Donât use AT&T in Australia because performance is poor Traffic starting or ending at Apple should not transit Google
As you might imagine from this list  routing policies can be highly individual
They are often proprietary because they contain sensitive business information
THE NETWORK LAYER
However  we can describe some patterns that capture the reasoning of the company above and that are often used as a starting point
A routing policy is implemented by deciding what traffic can flow over which of the links between ASes
One common policy is that a customer ISP pays another provider ISP to deliver packets to any other destination on the Internet and receive packets sent from any other destination
The customer ISP is said to buy transit service from the provider ISP
This is just like a customer at home buying Internet access service from an ISP
To make it work  the provider should advertise routes to all destinations on the Internet to the customer over the link that connects them
In this way  the customer will have a route to use to send packets anywhere
Conversely  the customer should advertise routes only to the destinations on its network to the provider
This will let the provider send traffic to the customer only for those addresses; the customer does not want to handle traffic intended for other destinations
We can see an example of transit service in Fig
There are four ASes that are connected
The connection is often made with a link at IXPs (Internet eXchange Points)  facilities to which many ISPs have a link for the purpose of connecting with other ISPs
AS  AS  and AS  are customers of AS
They buy transit service from it
Thus  when source A sends to destination C  the packets travel from AS  to AS  and finally to AS
The routing advertisements travel in the opposite direction to the packets
AS  advertises C as a destination to its transit provider  AS  to let sources reach C via AS
Later  AS  advertises a route to C to its other customers  including AS  to let the customers know that they can send traffic to C via AS
TR AS  AS  AS  AS  A PE CU PE CU CU TR TR Path of BGP routing advertisements (dash) Path of IP packets (solid) Routing policy: TR = Transit CU = Customer PE = Peer B C Figure  -
Routing policies between four autonomous systems
This provides them with connectivity so they can interact with any host on the Internet
However  they have to pay for this privilege
Suppose that AS  and AS  exchange a lot of traffic
Given that their networks are connected already  if they want to  they   THE NETWORK LAYER IN THE INTERNET can use a different policyâthey can send traffic directly to each other for free
This will reduce the amount of traffic they must have AS  deliver on their behalf  and hopefully it will reduce their bills
This policy is called peering
To implement peering  two ASes send routing advertisements to each other for the addresses that reside in their networks
Doing so makes it possible for AS  to send AS  packets from A destined to B and vice versa
However  note that peering is not transitive
This peering allows traffic from C destined for B to be sent directly to AS
What happens if C sends a packet to A? AS  is only advertising a route to B to AS
It is not advertising a route to A
The consequence is that traffic will not pass from AS  to AS  to AS  even though a physical path exists
This restriction is exactly what AS  wants
It peers with AS  to exchange traffic  but does not want to carry traffic from AS  to other parts of the Internet since it is not being paid to so do
Instead  AS  gets transit service from AS
Thus  it is AS  who will carry the packet from C to A
Now that we know about transit and peering  we can also see that A  B  and C have transit arrangements
For example  A must buy Internet access from AS
A might be a single home computer or a company network with many LANs
However  it does not need to run BGP because it is a stub network that is connected to the rest of the Internet by only one link
So the only place for it to send packets destined outside of the network is over the link to AS
There is nowhere else to go
This path can be arranged simply by setting up a default route
For this reason  we have not shown A  B  and C as ASes that participate in interdomain routing
On the other hand  some company networks are connected to multiple ISPs
This technique is used to improve reliability  since if the path through one ISP fails  the company can use the path via the other ISP
This technique is called multihoming
In this case  the company network is likely to run an interdomain routing protocol (
BGP) to tell other ASes which addresses should be reached via which ISP links
Many variations on these transit and peering policies are possible  but they already illustrate how business relationships and control over where route advertisements go can implement different kinds of policies
Now we will consider in more detail how routers running BGP advertise routes to each other and select paths over which to forward packets
BGP is a form of distance vector protocol  but it is quite unlike intradomain distance vector protocols such as RIP
We have already seen that policy  instead of minimum distance  is used to pick which routes to use
Another large difference is that instead of maintaining just the cost of the route to each destination  each BGP router keeps track of the path used
This approach is called a path vector protocol
The path consists of the next hop router (which may be on the other side of the ISP  not adjacent) and the sequence of ASes  or AS path  that the route has followed (given in reverse order)
Finally  pairs of BGP routers communicate THE NETWORK LAYER
with each other by establishing TCP connections
Operating this way provides reliable communication and also hides all the details of the network being passed through
An example of how BGP routes are advertised is shown in Fig
There are three ASes and the middle one is providing transit to the left and right ISPs
A route advertisement to prefix C starts in AS
When it is propagated across the link to R c at the top of the figure  it has the AS path of simply AS  and the next hop router of R a
At the bottom  it has the same AS path but a different next hop because it came across a different link
This advertisement continues to propagate and crosses the boundary into AS
At router R a  at the top of the figure  the AS path is AS  AS  and the next hop is R a
R a Prefix A B C AS  AS  AS  Path of packets R b R c R d R a R b R a R b C  AS  AS  R a C  AS  R a C  AS  AS  R b C  AS  AS  R a C  AS  AS  R b AS path Next hop C  AS  R b Figure  -
Propagation of BGP route advertisements
Carrying the complete path with the route makes it easy for the receiving router to detect and break routing loops
The rule is that each router that sends a route outside of the AS prepends its own AS number to the route
(This is why the list is in reverse order
) When a router receives a route  it checks to see if its own AS number is already in the AS path
If it is  a loop has been detected and the advertisement is discarded
However  and somewhat ironically  it was realized in the late s that despite this precaution BGP suffers from a version of the count-to-infinity problem (Labovitz et al
There are no long-lived loops  but routes can sometimes be slow to converge and have transient loops
Giving a list of ASes is a very coarse way to specify a path
An AS might be a small company  or an international backbone network
There is no way of telling from the route
BGP does not even try because different ASes may use different intradomain protocols whose costs cannot be compared
Even if they could be compared  an AS may not want to reveal its internal metrics
This is one of the ways that interdomain routing protocols differ from intradomain protocols
THE NETWORK LAYER IN THE INTERNET So far we have seen how a route advertisement is sent across the link between two ISPs
We still need some way to propagate BGP routes from one side of the ISP to the other  so they can be sent on to the next ISP
This task could be handled by the intradomain protocol  but because BGP is very good at scaling to large networks  a variant of BGP is often used
It is called iBGP (internal BGP) to distinguish it from the regular use of BGP as eBGP (external BGP)
The rule for propagating routes inside an ISP is that every router at the boundary of the ISP learns of all the routes seen by all the other boundary routers  for consistency
If one boundary router on the ISP learns of a prefix to IP
all the other routers will learn of this prefix
The prefix will then be reachable from all parts of the ISP  no matter how packets enter the ISP from other ASes
We have not shown this propagation in Fig
-  to avoid clutter  but  for example  router R b will know that it can reach C via either router R c at top or router R d at bottom
The next hop is updated as the route crosses within the ISP so that routers on the far side of the ISP know which router to use to exit the ISP on the other side
This can be seen in the leftmost routes in which the next hop points to a router in the same ISP and not a router in the next ISP
We can now describe the key missing piece  which is how BGP routers choose which route to use for each destination
Each BGP router may learn a route for a given destination from the router it is connected to in the next ISP and from all of the other boundary routers (which have heard different routes from the routers they are connected to in other ISPs)
Each router must decide which route in this set of routes is the best one to use
Ultimately the answer is that it is up to the ISP to write some policy to pick the preferred route
However  this explanation is very general and not at all satisfying  so we can at least describe some common strategies
The first strategy is that routes via peered networks are chosen in preference to routes via transit providers
The former are free; the latter cost money
A similar strategy is that customer routes are given the highest preference
It is only good business to send traffic directly to the paying customers
A different kind of strategy is the default rule that shorter AS paths are better
This is debatable given that an AS could be a network of any size  so a path through three small ASes could actually be shorter than a path through one big AS
However  shorter tends to be better on average  and this rule is a common tiebreaker
The final strategy is to prefer the route that has the lowest cost within the ISP
This is the strategy implemented in Fig
Packets sent from A to C exit AS  at the top router  R a
Packets sent from B exit via the bottom router  R b
The reason is that both A and B are taking the lowest-cost path or quickest route out of AS
Because they are located in different parts of the ISP  the quickest exit for each one is different
The same thing happens as the packets pass through AS
On the last leg  AS  has to carry the packet from B through its own network
THE NETWORK LAYER
This strategy is known as early exit or hot-potato routing
It has the curious side effect of tending to make routes asymmetric
For example  consider the path taken when C sends a packet back to B
The packet will exit AS  quickly  at the top router  to avoid wasting its resources
Similarly  it will stay at the top when AS  passes it to AS  as quickly as possible
Then the packet will have a longer journey in AS
This is a mirror image of the path taken from B to C
The above discussion should make clear that each BGP router chooses its own best route from the known possibilities
It is not the case  as might naively be expected  that BGP chooses a path to follow at the AS level and OSPF chooses paths within each of the ASes
BGP and the interior gateway protocol are integrated much more deeply
This means that  for example  BGP can find the best exit point from one ISP to the next and this point will vary across the ISP  as in the case of the hot-potato policy
It also means that BGP routers in different parts of one AS may choose different AS paths to reach the same destination
Care must be exercised by the ISP to configure all of the BGP routers to make compatible choices given all of this freedom  but this can be done in practice
Amazingly  we have only scratched the surface of BGP
For more information  see the BGP version  specification in RFC  and related RFCs
However  realize that much of its complexity lies with policies  which are not described in the specification of the BGP protocol
Internet Multicasting Normal IP communication is between one sender and one receiver
However  for some applications  it is useful for a process to be able to send to a large number of receivers simultaneously
Examples are streaming a live sports event to many viewers  delivering program updates to a pool of replicated servers  and handling digital conference (
multiparty) telephone calls
IP supports one-to-many communication  or multicasting  using class D IP addresses
Each class D address identifies a group of hosts
Twenty-eight bits are available for identifying groups  so over million groups can exist at the same time
When a process sends a packet to a class D address  a best-effort attempt is made to deliver it to all the members of the group addressed  but no guarantees are given
Some members may not get the packet
The range of IP addresses
/  is reserved for multicast on the local network
In this case  no routing protocol is needed
The packets are multicast by simply broadcasting them on the LAN with a multicast address
All hosts on the LAN receive the broadcasts  and hosts that are members of the group process the packet
Routers do not forward the packet off the LAN
Some examples of local multicast addresses are:
All systems on a LAN
All routers on a LAN
All OSPF routers on a LAN
All DNS servers on a LAN   THE NETWORK LAYER IN THE INTERNET Other multicast addresses may have members on different networks
In this case  a routing protocol is needed
But first the multicast routers need to know which hosts are members of a group
A process asks its host to join in a specific group
It can also ask its host to leave the group
Each host keeps track of which groups its processes currently belong to
When the last process on a host leaves a group  the host is no longer a member of that group
About once a minute  each multicast router sends a query packet to all the hosts on its LAN (using the local multicast address of
of course) asking them to report back on the groups to which they currently belong
The multicast routers may or may not be colocated with the standard routers
Each host sends back responses for all the class D addresses it is interested in
These query and response packets use a protocol called IGMP (Internet Group Management Protocol)
It is described in RFC
Any of several multicast routing protocols may be used to build multicast spanning trees that give paths from senders to all of the members of the group
The algorithms that are used are the ones we described in
Within an AS  the main protocol used is PIM (Protocol Independent Multicast)
PIM comes in several flavors
In Dense Mode PIM  a pruned reverse path forwarding tree is created
This is suited to situations in which members are everywhere in the network  such as distributing files to many servers within a data center network
In Sparse Mode PIM  spanning trees that are built are similar to core-based trees
This is suited to situations such as a content provider multicasting TV to subscribers on its IP network
A variant of this design  called Source-Specific Multicast PIM  is optimized for the case that there is only one sender to the group
Finally  multicast extensions to BGP or tunnels need to be used to create multicast routes when the group members are in more than one AS
Mobile IP Many users of the Internet have mobile computers and want to stay connected when they are away from home and even on the road in between
Unfortunately  the IP addressing system makes working far from home easier said than done  as we will describe shortly
When people began demanding the ability anyway  IETF set up a Working Group to find a solution
The Working Group quickly formulated a number of goals considered desirable in any solution
The major ones were:
Each mobile host must be able to use its home IP address anywhere Software changes to the fixed hosts were not permitted Changes to the router software and tables were not permitted Most packets for mobile hosts should not make detours on the way No overhead should be incurred when a mobile host is at home
THE NETWORK LAYER
The solution chosen was the one described in
In brief  every site that wants to allow its users to roam has to create a helper at the site called a home agent
When a mobile host shows up at a foreign site  it obtains a new IP address (called a care-of address) at the foreign site
The mobile then tells the home agent where it is now by giving it the care-of address
When a packet for the mobile arrives at the home site and the mobile is elsewhere  the home agent grabs the packet and tunnels it to the mobile at the current care-of address
The mobile can send reply packets directly to whoever it is communicating with  but still using its home address as the source address
This solution meets all the requirements stated above except that packets for mobile hosts do make detours
Now that we have covered the network layer of the Internet  we can go into the solution in more detail
The need for mobility support in the first place comes from the IP addressing scheme itself
Every IP address contains a network number and a host number
For example  consider the machine with IP address
gives the network number; the
is the host number
Routers all over the world have routing tables telling which link to use to get to network    Whenever a packet comes in with a destination IP address of the form      it goes out on that line
If all of a sudden  the machine with that address is carted off to some distant site  the packets for it will continue to be routed to its home LAN (or router)
At this stage  there are two optionsâboth unattractive
The first is that we could create a route to a more specific prefix
That is  if the distant site advertises a route to
/   packets sent to the destination will start arriving in the right place again
This option depends on the longest matching prefix algorithm that is used at routers
However  we have added a route to an IP prefix with a single IP address in it
All ISPs in the world will learn about this prefix
If everyone changes global IP routes in this way when they move their computer  each router would have millions of table entries  at astronomical cost to the Internet
This option is not workable
The ond option is to change the IP address of the mobile
True  packets sent to the home IP address will no longer be delivered until all the relevant people  programs  and databases are informed of the change
But the mobile can still use the Internet at the new location to browse the Web and run other applications
This option handles mobility at a higher layer
It is what typically happens when a user takes a laptop to a coffee store and uses the Internet via the local wireless network
The disadvantage is that it breaks some applications  and it does not keep connectivity as the mobile moves around
As an aside  mobility can also be handled at a lower layer  the link layer
This is what happens when using a laptop on a single
wireless network
The IP address of the mobile does not change and the network path remains the same
It is the wireless link that is providing mobility
However  the degree of mobility is limited
If the laptop moves too far  it will have to connect to the Internet via another network with a different IP address
THE NETWORK LAYER IN THE INTERNET The mobile IP solution for IPv  is given in RFC
It works with the existing Internet routing and allows hosts to stay connected with their own IP addresses as they move about
For it to work  the mobile must be able to discover when it has moved
This is accomplished with ICMP router advertisement and solicitation messages
Mobiles listen for periodic router advertisements or send a solicitation to discover the nearest router
If this router is not the usual address of the router when the mobile is at home  it must be on a foreign network
If this router has changed since last time  the mobile has moved to another foreign network
This same mechanism lets mobile hosts find their home agents
To get a care-of IP address on the foreign network  a mobile can simply use DHCP
Alternatively  if IPv  addresses are in short supply  the mobile can send and receive packets via a foreign agent that already has an IP address on the network
The mobile host finds a foreign agent using the same ICMP mechanism used to find the home agent
After the mobile obtains an IP address or finds a foreign agent  it is able to use the network to send a message to its home agent  informing the home agent of its current location
The home agent needs a way to intercept packets sent to the mobile only when the mobile is not at home
ARP provides a convenient mechanism
To send a packet over an Ethernet to an IP host  the router needs to know the Ethernet address of the host
The usual mechanism is for the router to send an ARP query to ask  for example  what is the Ethernet address of      When the mobile is at home  it answers ARP queries for its IP address with its own Ethernet address
When the mobile is away  the home agent responds to this query by giving its Ethernet address
The router then sends packets for
to the home agent
Recall that this is called a proxy ARP
To quickly update ARP mappings back and forth when the mobile leaves home or arrives back home  another ARP technique called a gratuitous ARP can be used
Basically  the mobile or home agent send themselves an ARP query for the mobile IP address that supplies the right answer so that the router notices and updates its mapping
Tunneling to send a packet between the home agent and the mobile host at the care-of address is done by encapsulating the packet with another IP header destined for the care-of address
When the encapsulated packet arrives at the care-of address  the outer IP header is removed to reveal the packet
As with many Internet protocols  the devil is in the details  and most often the details of compatibility with other protocols that are deployed
There are two complications
First  NAT boxes depend on peeking past the IP header to look at the TCP or UDP header
The original form of tunneling for mobile IP did not use these headers  so it did not work with NAT boxes
The solution was to change the encapsulation to include a UDP header
The ond complication is that some ISPs check the source IP addresses of packets to see that they match where the routing protocol believes the source should be located
This technique is called ingress filtering  and it is a urity THE NETWORK LAYER
measure intended to discard traffic with seemingly incorrect addresses that may be malicious
However  packets sent from the mobile to other Internet hosts when it is on a foreign network will have a source IP address that is out of place  so they will be discarded
To get around this problem  the mobile can use the care-of address as a source to tunnel the packets back to the home agent
From here  they are sent into the Internet from what appears to be the right location
The cost is that the route is more roundabout
Another issue we have not discussed is urity
When a home agent gets a message asking it to please forward all of Robertaâs packets to some IP address  it had better not comply unless it is convinced that Roberta is the source of this request  and not somebody trying to impersonate her
Cryptographic authentication protocols are used for this purpose
We will study such protocols in   Mobility protocols for IPv  build on the IPv  foundation
The scheme above suffers from the triangle routing problem in which packets sent to the mobile take a dogleg through a distant home agent
In IPv  route optimization is used to follow a direct path between the mobile and other IP addresses after the initial packets have followed the long route
Mobile IPv  is defined in RFC
There is another kind of mobility that is also being defined for the Internet
Some airplanes have built-in wireless networking that passengers can use to connect their laptops to the Internet
The plane has a router that connects to the rest of the Internet via a wireless link
(Did you expect a wired link?) So now we have a flying router  which means that the whole network is mobile
Network mobility designs support this situation without the laptops realizing that the plane is mobile
As far as they are concerned  it is just another network
Of course  some of the laptops may be using mobile IP to keep their home addresses while they are on the plane  so we have two levels of mobility
Network mobility is defined for IPv  in RFC   SUMMARY The network layer provides services to the transport layer
It can be based on either datagrams or virtual circuits
In both cases  its main job is routing packets from the source to the destination
In datagram networks  a routing decision is made on every packet
In virtual-circuit networks  it is made when the virtual circuit is set up
Many routing algorithms are used in computer networks
Flooding is a simple algorithm to send a packet along all paths
Most algorithms find the shortest path and adapt to changes in the network topology
The main algorithms are distance vector routing and link state routing
Most actual networks use one of these
Other important routing topics are the use of hierarchy in large networks  routing for mobile hosts  and broadcast  multicast  and anycast routing
SUMMARY Networks can easily become congested  leading to increased delay and lost packets
Network designers attempt to avoid congestion by designing the network to have enough capacity  choosing uncongested routes  refusing to accept more traffic  signaling sources to slow down  and shedding load
The next step beyond just dealing with congestion is to actually try to achieve a promised quality of service
Some applications care more about throughput whereas others care more about delay and jitter
The methods that can be used to provide different qualities of service include a combination of traffic shaping  reserving resources at routers  and admission control
Approaches that have been designed for good quality of service include IETF integrated services (including RSVP) and differentiated services
Networks differ in various ways  so when multiple networks are interconnected  problems can occur
When different networks have different maximum packet sizes  fragmentation may be needed
Different networks may run different routing protocols internally but need to run a common protocol externally
Sometimes the problems can be finessed by tunneling a packet through a hostile network  but if the source and destination networks are different  this approach fails
The Internet has a rich variety of protocols related to the network layer
These include the datagram protocol  IP  and associated control protocols such as ICMP  ARP  and DHCP
A connection-oriented protocol called MPLS carries IP packets across some networks
One of the main routing protocols used within networks is OSPF  and the routing protocol used across networks is BGP
The Internet is rapidly running out of IP addresses  so a new version of IP  IPv  has been developed and is ever-so-slowly being deployed
Give two example computer applications for which connection-oriented service is appropriate
Now give two examples for which connectionless service is best Datagram networks route each packet as a separate unit  independent of all others
Virtual-circuit networks do not have to do this  since each data packet follows a predetermined route
Does this observation mean that virtual-circuit networks do not need the capability to route isolated packets from an arbitrary source to an arbitrary destination? Explain your answer Give three examples of protocol parameters that might be negotiated when a connection is set up Assuming that all routers and hosts are working properly and that all software in both is free of all errors  is there any chance  however small  that a packet will be delivered to the wrong destination? THE NETWORK LAYER
Give a simple heuristic for finding two paths through a network from a given source to a given destination that can survive the loss of any communication line (assuming two such paths exist)
The routers are considered reliable enough  so it is not necessary to worry about the possibility of router crashes Consider the network of Fig
Distance vector routing is used  and the following vectors have just come in to router C: from B: (    ); from D: (     ); and from E: (   )
The cost of the links from C to B  D  and E  are  and   respectively
What is Câs new routing table? Give both the outgoing line to use and the cost If costs are recorded as  -bit numbers in a  -router network  and distance vectors are exchanged twice a ond  how much bandwidth per (full-duplex) line is chewed up by the distributed routing algorithm? Assume that each router has three lines to other routers In Fig
Is this just an accident here  or does it hold for all networks under all circumstances?
For hierarchical routing with  routers  what region and cluster sizes should be chosen to minimize the size of the routing table for a three-layer hierarchy? A good starting place is the hypothesis that a solution with k clusters of k regions of k routers is close to optimal  which means that k is about the cube root of  (around  )
Use trial and error to check out combinations where all three parameters are in the general vicinity of   In the text it was stated that when a mobile host is not at home  packets sent to its home LAN are intercepted by its home agent on that LAN
For an IP network on an
LAN  how does the home agent accomplish this interception?
Looking at the network of Fig
Consider the network of Fig
Imagine that one new line is added  between F and G  but the sink tree of Fig
What changes occur to Fig
Compute a multicast spanning tree for router C in the following network for a group with members at routers A  B  C  D  E  F  I  and K
A G H I L D K B C F E J
Suppose that node B in Fig
It suddenly needs a route to H
It sends out broadcasts with TtL set to  and so on
How many rounds does it take to find a route?
As a possible congestion control mechanism in a network using virtual circuits internally  a router could refrain from acknowledging a received packet until ( ) it knows its last transmission along the virtual circuit was received successfully and ( ) it has a free buffer
For simplicity  assume that the routers use a stop-and-wait protocol and that each virtual circuit has one buffer dedicated to it for each direction of traffic
If it takes T  to transmit a packet (data or acknowledgement) and there are n routers on the path  what is the rate at which packets are delivered to the destination host? Assume that transmission errors are rare and that the host-router connection is infinitely fast A datagram network allows routers to drop packets whenever they need to
The probability of a router discarding a packet is p
Consider the case of a source host connected to the source router  which is connected to the destination router  and then to the destination host
If either of the routers discards a packet  the source host eventually times out and tries again
If both host-router and router-router lines are counted as hops  what is the mean number of (a) hops a packet makes per transmission? (b) transmissions a packet makes? (c) hops required per received packet?
Describe two major differences between the ECN method and the RED method of congestion avoidance A token bucket scheme is used for traffic shaping
A new token is put into the bucket every  Î¼
Each token is good for one short packet  which contains   bytes of data
What is the maximum sustainable data rate?
A computer on a  -Mbps network is regulated by a token bucket
The token bucket is filled at a rate of  Mbps
It is initially filled to capacity with  megabits
How long can the computer transmit at the full  Mbps?
The network of Fig
Suppose that host  requests a channel of bandwidth  MB/ for a flow from host  and another channel of bandwidth  MB/ for a flow from host
At the same time  host  requests a channel of bandwidth  MB/ for a flow from host  and host  requests a channel of bandwidth  MB/ for a flow from host
How much total bandwidth will be reserved for these requests at routers A  B  C  E  H  J  K  and L?
A router can process  million packets/
The load offered to it is
million packets/  on average
If a route from source to destination contains   routers  how much time is spent being queued and serviced by the router?
Consider the user of differentiated services with expedited forwarding
Is there a guarantee that expedited packets experience a shorter delay than regular packets? Why or why not? THE NETWORK LAYER   Suppose that host A is connected to a router R   R  is connected to another router  R   and R  is connected to host B
Suppose that a TCP message that contains bytes of data and   bytes of TCP header is passed to the IP code at host A for delivery to B
Show the Total length  Identification  DF  MF  and Fragment offset fields of the IP header in each packet transmitted over the three links
Assume that link A-R  can support a maximum frame size of  bytes including a  -byte frame header  link R -R  can support a maximum frame size of bytes  including an  -byte frame header  and link R -B can support a maximum frame size of bytes including a  -byte frame header A router is blasting out IP packets whose total length (data plus header) is  bytes
Assuming that packets live for     what is the maximum line speed the router can operate at without danger of cycling through the IP datagram ID number space?
An IP datagram using the Strict source routing option has to be fragmented
Do you think the option is copied into each fragment  or is it sufficient to just put it in the first fragment? Explain your answer Suppose that instead of using   bits for the network part of a class B address originally bits had been used
How many class B networks would there have been?
Convert the IP address whose hexadecimal representation is C  F to dotted decimal notation A network on the Internet has a subnet mask of      What is the maximum number of hosts it can handle?
While IP addresses are tried to specific networks  Ethernet addresses are not
Can you think of a good reason why they are not?
A large number of conutive IP addresses are available starting at
Suppose that four organizations  A  B  C  and D  request     and  addresses  respectively  and in that order
For each of these  give the first IP address assigned  the last IP address assigned  and the mask in the
/s notation A router has just received the following new IP addresses:
If all of them use the same outgoing line  can they be aggregated? If so  to what? If not  why not?
The set of IP addresses from
has been aggregated to
However  there is a gap of  unassigned addresses from
that are now suddenly assigned to a host using a different outgoing line
Is it now necessary to split up the aggregate address into its constituent blocks  add the new block to the table  and then see if any reaggregation is possible? If not  what can be done instead?
A router has the following (CIDR) entries in its routing table: Address/mask Next hop
/  Interface
/  Interface
/  Router  default Router
PROBLEMS For each of the following IP addresses  what does the router do if a packet with that address arrives? (a)
Many companies have a policy of having two (or more) routers connecting the company to the Internet to provide some redundancy in case one of them goes down
Is this policy still possible with NAT? Explain your answer You have just explained the ARP protocol to a friend
When you are all done  he says: ââIâve got it
ARP provides a service to the network layer  so it is part of the data link layer
ââ What do you say to him?
Describe a way to reassemble IP fragments at the destination Most IP datagram reassembly algorithms have a timer to avoid having a lost fragment tie up reassembly buffers forever
Suppose that a datagram is fragmented into four fragments
The first three fragments arrive  but the last one is delayed
Eventually  the timer goes off and the three fragments in the receiverâs memory are discarded
A little later  the last fragment stumbles in
What should be done with it?
In IP  the checksum covers only the header and not the data
Why do you suppose this design was chosen?
A person who lives in Boston travels to Minneapolis  taking her portable computer with her
To her surprise  the LAN at her destination in Minneapolis is a wireless IP LAN  so she does not have to plug in
Is it still necessary to go through the entire business with home agents and foreign agents to make email and other traffic arrive correctly?
IPv  uses  -byte addresses
If a block of  million addresses is allocated every picoond  how long will the addresses last?
The Protocol field used in the IPv  header is not present in the fixed IPv  header
When the IPv  protocol is introduced  does the ARP protocol have to be changed? If so  are the changes conceptual or technical?
Write a program to simulate routing using flooding
Each packet should contain a counter that is decremented on each hop
When the counter gets to zero  the packet is discarded
Time is discrete  with each line handling one packet per time interval
Make three versions of the program: all lines are flooded  all lines except the input line are flooded  and only the (statically chosen) best k lines are flooded
Compare flooding with deterministic routing (k =  ) in terms of both delay and the bandwidth used Write a program that simulates a computer network using discrete time
The first packet on each router queue makes one hop per time interval
Each router has only a finite number of buffers
If a packet arrives and there is no room for it  it is discarded THE NETWORK LAYER
and not retransmitted
Instead  there is an end-to-end protocol  complete with timeouts and acknowledgement packets  that eventually regenerates the packet from the source router
Plot the throughput of the network as a function of the end-to-end timeout interval  parameterized by error rate Write a function to do forwarding in an IP router
The procedure has one parameter  an IP address
It also has access to a global table consisting of an array of triples
Each triple contains three integers: an IP address  a subnet mask  and the outline line to use
The function looks up the IP address in the table using CIDR and returns the line to use as its value Use the traceroute (UNIX) or tracert (Windows) programs to trace the route from your computer to various universities on other continents
Make a list of transoceanic links you have discovered
Some sites to try are   (California)   (Massachusetts)   (Amsterdam)
(Sydney)  -  (Tokyo)
(Cape Town)  THE TRANSPORT LAYER Together with the network layer  the transport layer is the heart of the protocol hierarchy
The network layer provides end-to-end packet delivery using datagrams or virtual circuits
The transport layer builds on the network layer to provide data transport from a process on a source machine to a process on a destination machine with a desired level of reliability that is independent of the physical networks currently in use
It provides the abstractions that applications need to use the network
Without the transport layer  the whole concept of layered protocols would make little sense
In this  ter  we will study the transport layer in detail  including its services and choice of API design to tackle issues of reliability  connections and congestion control  protocols such as TCP and UDP  and performance  THE TRANSPORT SERVICE In the following tions  we will provide an introduction to the transport service
We look at what kind of service is provided to the application layer
To make the issue of transport service more concrete  we will examine two sets of transport layer primitives
First comes a simple (but hypothetical) one to show the basic ideas
Then comes the interface commonly used in the Internet
THE TRANSPORT LAYER
Services Provided to the Upper Layers The ultimate goal of the transport layer is to provide efficient  reliable  and cost-effective data transmission service to its users  normally processes in the application layer
To achieve this  the transport layer makes use of the services provided by the network layer
The software and/or hardware within the transport layer that does the work is called the transport entity
The transport entity can be located in the operating system kernel  in a library package bound into network applications  in a separate user process  or even on the network interface card
The first two options are most common on the Internet
The (logical) relationship of the network  transport  and application layers is illustrated in Fig
Application/transport interface Transport/network interface Application (or session) layer Transport entity Transport address Network address Network layer Application (or session) layer Transport entity Network layer Segment Transport protocol Host  Host  Figure  -
The network  transport  and application layers
Just as there are two types of network service  connection-oriented and connectionless  there are also two types of transport service
The connection-oriented transport service is similar to the connection-oriented network service in many ways
In both cases  connections have three phases: establishment  data transfer  and release
Addressing and flow control are also similar in both layers
Furthermore  the connectionless transport service is also very similar to the connectionless network service
However  note that it can be difficult to provide a connectionless transport service on top of a connection-oriented network service  since it is inefficient to set up a connection to send a single packet and then tear it down immediately afterwards
The obvious question is this: if the transport layer service is so similar to the network layer service  why are there two distinct layers? Why is one layer not   THE TRANSPORT SERVICE adequate? The answer is subtle  but crucial
The transport code runs entirely on the usersâ machines  but the network layer mostly runs on the routers  which are operated by the carrier (at least for a wide area network)
What happens if the network layer offers inadequate service? What if it frequently loses packets? What happens if routers crash from time to time? Problems occur  thatâs what
The users have no real control over the network layer  so they cannot solve the problem of poor service by using better routers or putting more error handling in the data link layer because they donât own the routers
The only possibility is to put on top of the network layer another layer that improves the quality of the service
If  in a connectionless network  packets are lost or mangled  the transport entity can detect the problem and compensate for it by using retransmissions
If  in a connection-oriented network  a transport entity is informed halfway through a long transmission that its network connection has been abruptly terminated  with no indication of what has happened to the data currently in transit  it can set up a new network connection to the remote transport entity
Using this new network connection  it can send a query to its peer asking which data arrived and which did not  and knowing where it was  pick up from where it left off
In essence  the existence of the transport layer makes it possible for the transport service to be more reliable than the underlying network
Furthermore  the transport primitives can be implemented as calls to library procedures to make them independent of the network primitives
The network service calls may vary considerably from one network to another (
calls based on a connectionless Ethernet may be quite different from calls on a connection-oriented WiMAX network)
Hiding the network service behind a set of transport service primitives ensures that changing the network merely requires replacing one set of library procedures with another one that does the same thing with a different underlying service
Thanks to the transport layer  application programmers can write code according to a standard set of primitives and have these programs work on a wide variety of networks  without having to worry about dealing with different network interfaces and levels of reliability
If all real networks were flawless and all had the same service primitives and were guaranteed never  ever to change  the transport layer might not be needed
However  in the real world it fulfills the key function of isolating the upper layers from the technology  design  and imperfections of the network
For this reason  many people have made a qualitative distinction between layers  through  on the one hand and layer(s) above  on the other
The bottom four layers can be seen as the transport service provider  whereas the upper layer(s) are the transport service user
This distinction of provider versus user has a considerable impact on the design of the layers and puts the transport layer in a key position  since it forms the major boundary between the provider and user of the reliable data transmission service
It is the level that applications see
THE TRANSPORT LAYER
Transport Service Primitives To allow users to access the transport service  the transport layer must provide some operations to application programs  that is  a transport service interface
Each transport service has its own interface
In this tion  we will first examine a simple (hypothetical) transport service and its interface to see the bare essentials
In the following tion  we will look at a real example
The transport service is similar to the network service  but there are also some important differences
The main difference is that the network service is intended to model the service offered by real networks  warts and all
Real networks can lose packets  so the network service is generally unreliable
The connection-oriented transport service  in contrast  is reliable
Of course  real networks are not error-free  but that is precisely the purpose of the transport layerâto provide a reliable service on top of an unreliable network
As an example  consider two processes on a single machine connected by a pipe in UNIX (or any other interprocess communication facility)
They assume the connection between them is   % perfect
They do not want to know about acknowledgements  lost packets  congestion  or anything at all like that
What they want is a   % reliable connection
Process A puts data into one end of the pipe  and process B takes it out of the other
This is what the connection-oriented transport service is all aboutâhiding the imperfections of the network service so that user processes can just assume the existence of an error-free bit stream even when they are on different machines
As an aside  the transport layer can also provide unreliable (datagram) service
However  there is relatively little to say about that besides ââitâs datagrams ââ so we will mainly concentrate on the connection-oriented transport service in this  ter
Nevertheless  there are some applications  such as client-server computing and streaming multimedia  that build on a connectionless transport service  and we will say a little bit about that later on
A ond difference between the network service and transport service is whom the services are intended for
The network service is used only by the transport entities
Few users write their own transport entities  and thus few users or programs ever see the bare network service
In contrast  many programs (and thus programmers) see the transport primitives
Consequently  the transport service must be convenient and easy to use
To get an idea of what a transport service might be like  consider the five primitives listed in Fig
This transport interface is truly bare bones  but it gives the essential flavor of what a connection-oriented transport interface has to do
It allows application programs to establish  use  and then release connections  which is sufficient for many applications
To see how these primitives might be used  consider an application with a server and a number of remote clients
To start with  the server executes a LISTEN primitive  typically by calling a library procedure that makes a system call that   THE TRANSPORT SERVICE Primitive Packet sent Meaning LISTEN (none) Block until some process tries to connect CONNECT CONNECTION REQ
Actively attempt to establish a connection SEND DATA Send information RECEIVE (none) Block until a DATA packet arrives DISCONNECT DISCONNECTION REQ
Request a release of the connection Figure  -
The primitives for a simple transport service
blocks the server until a client turns up
When a client wants to talk to the server  it executes a CONNECT primitive
The transport entity carries out this primitive by blocking the caller and sending a packet to the server
Encapsulated in the payload of this packet is a transport layer message for the serverâs transport entity
A quick note on terminology is now in order
For lack of a better term  we will use the term segment for messages sent from transport entity to transport entity
TCP  UDP and other Internet protocols use this term
Some older protocols used the ungainly name TPDU (Transport Protocol Data Unit)
That term is not used much any more now but you may see it in older papers and books
Thus  segments (exchanged by the transport layer) are contained in packets (exchanged by the network layer)
In turn  these packets are contained in frames (exchanged by the data link layer)
When a frame arrives  the data link layer processes the frame header and  if the destination address matches for local delivery  passes the contents of the frame payload field up to the network entity
The network entity similarly processes the packet header and then passes the contents of the packet payload up to the transport entity
This nesting is illustrated in Fig
Frame header Packet header Segment header Segment payload Frame payload Packet payload Figure  -
Nesting of segments  packets  and frames
Getting back to our client-server example  the clientâs CONNECT call causes a CONNECTION REQUEST segment to be sent to the server
When it arrives  the THE TRANSPORT LAYER
transport entity checks to see that the server is blocked on a LISTEN (
is interested in handling requests)
If so  it then unblocks the server and sends a CONNECTION ACCEPTED segment back to the client
When this segment arrives  the client is unblocked and the connection is established
Data can now be exchanged using the SEND and RECEIVE primitives
In the simplest form  either party can do a (blocking) RECEIVE to wait for the other party to do a SEND
When the segment arrives  the receiver is unblocked
It can then process the segment and send a reply
As long as both sides can keep track of whose turn it is to send  this scheme works fine
Note that in the transport layer  even a simple unidirectional data exchange is more complicated than at the network layer
Every data packet sent will also be acknowledged (eventually)
The packets bearing control segments are also acknowledged  implicitly or explicitly
These acknowledgements are managed by the transport entities  using the network layer protocol  and are not visible to the transport users
Similarly  the transport entities need to worry about timers and retransmissions
None of this machinery is visible to the transport users
To the transport users  a connection is a reliable bit pipe: one user stuffs bits in and they magically appear in the same order at the other end
This ability to hide complexity is the reason that layered protocols are such a powerful tool
When a connection is no longer needed  it must be released to free up table space within the two transport entities
Disconnection has two variants: asymmetric and symmetric
In the asymmetric variant  either transport user can issue a DISCONNECT primitive  which results in a DISCONNECT segment being sent to the remote transport entity
Upon its arrival  the connection is released
In the symmetric variant  each direction is closed separately  independently of the other one
When one side does a DISCONNECT  that means it has no more data to send but it is still willing to accept data from its partner
In this model  a connection is released when both sides have done a DISCONNECT
A state diagram for connection establishment and release for these simple primitives is given in Fig
Each transition is triggered by some event  either a primitive executed by the local transport user or an incoming packet
For simplicity  we assume here that each segment is separately acknowledged
We also assume that a symmetric disconnection model is used  with the client going first
Please note that this model is quite unsophisticated
We will look at more realistic models later on when we describe how TCP works
Berkeley Sockets Let us now briefly inspect another set of transport primitives  the socket primitives as they are used for TCP
Sockets were first released as part of the Berkeley UNIX
BSD software distribution in
They quickly became popular
The primitives are now widely used for Internet programming on many operating   THE TRANSPORT SERVICE ACTIVE ESTABLISHMENT PENDING PASSIVE ESTABLISHMENT PENDING PASSIVE DISCONNECT PENDING ACTIVE DISCONNECT PENDING IDLE IDLE ESTABLISHED Disconnection request segment received Disconnect primitive executed Disconnect primitive executed Disconnection request segment received Connection request segment received Connection accepted segment received Connect primitive executed Connect primitive executed Figure  -
A state diagram for a simple connection management scheme
Transitions labeled in italics are caused by packet arrivals
The solid lines show the clientâs state sequence
The dashed lines show the serverâs state sequence
systems  especially UNIX-based systems  and there is a socket-style API for Windows called ââwinsock
ââ The primitives are listed in Fig
Roughly speaking  they follow the model of our first example but offer more features and flexibility
We will not look at the corresponding segments here
That discussion will come later
Primitive Meaning SOCKET Create a new communication endpoint BIND Associate a local address with a socket LISTEN Announce willingness to accept connections; give queue size ACCEPT Passively establish an incoming connection CONNECT Actively attempt to establish a connection SEND Send some data over the connection RECEIVE Receive some data from the connection CLOSE Release the connection Figure  -
The socket primitives for TCP
THE TRANSPORT LAYER
The first four primitives in the list are executed in that order by servers
The SOCKET primitive creates a new endpoint and allocates table space for it within the transport entity
The parameters of the call specify the addressing format to be used  the type of service desired (
reliable byte stream)  and the protocol
A successful SOCKET call returns an ordinary file descriptor for use in succeeding calls  the same way an OPEN call on a file does
Newly created sockets do not have network addresses
These are assigned using the BIND primitive
Once a server has bound an address to a socket  remote clients can connect to it
The reason for not having the SOCKET call create an address directly is that some processes care about their addresses (
they have been using the same address for years and everyone knows this address)  whereas others do not
Next comes the LISTEN call  which allocates space to queue incoming calls for the case that several clients try to connect at the same time
In contrast to LISTEN in our first example  in the socket model LISTEN is not a blocking call
To block waiting for an incoming connection  the server executes an ACCEPT primitive
When a segment asking for a connection arrives  the transport entity creates a new socket with the same properties as the original one and returns a file descriptor for it
The server can then fork off a process or thread to handle the connection on the new socket and go back to waiting for the next connection on the original socket
ACCEPT returns a file descriptor  which can be used for reading and writing in the standard way  the same as for files
Now let us look at the client side
Here  too  a socket must first be created using the SOCKET primitive  but BIND is not required since the address used does not matter to the server
The CONNECT primitive blocks the caller and actively starts the connection process
When it completes (
when the appropriate segment is received from the server)  the client process is unblocked and the connection is established
Both sides can now use SEND and RECEIVE to transmit and receive data over the full-duplex connection
The standard UNIX READ and WRITE system calls can also be used if none of the special options of SEND and RECEIVE are required
Connection release with sockets is symmetric
When both sides have executed a CLOSE primitive  the connection is released
Sockets have proved tremendously popular and are the de facto standard for abstracting transport services to applications
The socket API is often used with the TCP protocol to provide a connection-oriented service called a reliable byte stream  which is simply the reliable bit pipe that we described
However  other protocols could be used to implement this service using the same API
It should all be the same to the transport service users
A strength of the socket API is that is can be used by an application for other transport services
For instance  sockets can be used with a connectionless transport service
In this case  CONNECT sets the address of the remote transport peer and SEND and RECEIVE send and receive datagrams to and from the remote peer
THE TRANSPORT SERVICE (It is also common to use an expanded set of calls  for example  SENDTO and RECEIVEFROM  that emphasize messages and do not limit an application to a single transport peer
) Sockets can also be used with transport protocols that provide a message stream rather than a byte stream and that do or do not have congestion control
For example  DCCP (Datagram Congestion Controlled Protocol) is a version of UDP with congestion control (Kohler et al
It is up to the transport users to understand what service they are getting
However  sockets are not likely to be the final word on transport interfaces
For example  applications often work with a group of related streams  such as a Web browser that requests several objects from the same server
With sockets  the most natural fit is for application programs to use one stream per object
This structure means that congestion control is applied separately for each stream  not across the group  which is suboptimal
It punts to the application the burden of managing the set
Newer protocols and interfaces have been devised that support groups of related streams more effectively and simply for the application
Two examples are SCTP (Stream Control Transmission Protocol) defined in RFC  and SST (Structured Stream Transport) (Ford  )
These protocols must change the socket API slightly to get the benefits of groups of related streams  and they also support features such as a mix of connection-oriented and connectionless traffic and even multiple network paths
Time will tell if they are successful
An Example of Socket Programming: An Internet File Server As an example of the nitty-gritty of how real socket calls are made  consider the client and server code of Fig
Here we have a very primitive Internet file server along with an example client that uses it
The code has many limitations (discussed below)  but in principle the server code can be compiled and run on any UNIX system connected to the Internet
The client code can be compiled and run on any other UNIX machine on the Internet  anywhere in the world
The client code can be executed with appropriate parameters to fetch any file to which the server has access on its machine
The file is written to standard output  which  of course  can be redirected to a file or pipe
Let us look at the server code first
It starts out by including some standard headers  the last three of which contain the main Internet-related definitions and data structures
Next comes a definition of SERVER PORT as
This number was chosen arbitrarily
Any number between  and  will work just as well  as long as it is not in use by some other process; ports below  are reserved for privileged users
The next two lines in the server define two constants needed
The first one determines the chunk size in bytes used for the file transfer
The ond one determines how many pending connections can be held before additional ones are discarded upon arrival
THE TRANSPORT LAYER
/* This page contains a client program that can request a file from the server program * on the next page
The server responds by sending the whole file
"*/ #include <sys/ > #include <sys/ > #include <netinet/ > #include < > #define SERVER PORT  /* arbitrary  but client & server must agree */ #define BUF SIZE  /* block transfer size */ int main(int argc  char **argv) { int c  s  bytes; char buf[BUF SIZE]; /* buffer for incoming file */ struct hostent *h; /* info about server */ struct sockaddr in channel; /* holds IP address */ if (argc !=  ) fatal(""Usage: client server-name file-name""); h = gethostbyname(argv[ ]); /* look up hostâs IP address */ if (!h) fatal(""gethostbyname failed""); s = socket(PF INET  SOCK STREAM  IPPROTO TCP); if (s < ) fatal(""socket""); memset(&channel sizeof(channel));   family= AF INET; memcpy(&    addr  h->h addr  h->h length);   port= htons(SERVER PORT); c = connect(s  (struct sockaddr *) &channel  sizeof(channel)); if (c <  ) fatal(""connect failed""); /* Connection is now established"
Send file name including  byte at end
*/ write(s  argv[ ]  strlen(argv[ ])+ ); /* Go get the file and write it to standard output
"*/ while ( ) { bytes = read(s  buf  BUF SIZE); /* read from socket */ if (bytes <=  ) exit( ); /* check for end of file */ write(  buf  bytes); /* write to standard output */ } } fatal(char *string) { printf(""%s"
"""  string); exit( ); } Figure  -"
Client code using sockets
The server code is on the next page
THE TRANSPORT SERVICE #include <sys/ > /* This is the server code */ #include <sys/ > #include <sys/ > #include <netinet/ > #include < > #define SERVER PORT  /* arbitrary  but client & server must agree */ #define BUF SIZE  /* block transfer size */ #define QUEUE SIZE   int main(int argc  char *argv[]) { int s  b  l  fd  sa  bytes  on =  ; char buf[BUF SIZE]; /* buffer for outgoing file */ struct sockaddr in channel; /* holds IP address */ /* Build address structure to bind to socket
*/ memset(&channel sizeof(channel)); /* zero channel */   family = AF INET;     addr = htonl(INADDR ANY);   port = htons(SERVER PORT); /* Passive open
Wait for connection
"*/ s = socket(AF INET  SOCK STREAM  IPPROTO TCP); /* create socket */ if (s <  ) fatal(""socket failed""); setsockopt(s  SOL SOCKET  SO REUSEADDR  (char *) &on  sizeof(on)); b = bind(s  (struct sockaddr *) &channel  sizeof(channel)); if (b <  ) fatal(""bind failed""); l = listen(s  QUEUE SIZE); /* specify queue size */ if (l <  ) fatal(""listen failed""); /* Socket is now set up and bound"
Wait for connection and process it
"*/ while ( ) { sa = accept(s  ); /* block for connection request */ if (sa <  ) fatal(""accept failed""); read(sa  buf  BUF SIZE); /* read file name from socket */ /* Get and return the file"
"*/ fd = open(buf  O RDONLY); /* open the file to be sent back */ if (fd <  ) fatal(""open failed""); while ( ) { bytes = read(fd  buf  BUF SIZE); /* read from file */ if (bytes <=  ) break; /* check for end of file */ write(sa  buf  bytes); /* write bytes to socket */ } close(fd); /* close file */ close(sa); /* close connection */ } } THE TRANSPORT LAYER"
After the declarations of local variables  the server code begins
It starts out by initializing a data structure that will hold the serverâs IP address
This data structure will soon be bound to the serverâs socket
The call to memset sets the data structure to all  s
The three assignments following it fill in three of its fields
The last of these contains the serverâs port
The functions htonl and htons have to do with converting values to a standard format so the code runs correctly on both little-endian machines (
Intel x  ) and big-endian machines (
the SPARC)
Their exact semantics are not relevant here
Next  the server creates a socket and checks for errors (indicated by s <  )
In a production version of the code  the error message could be a trifle more explanatory
The call to setsockopt is needed to allow the port to be reused so the server can run indefinitely  fielding request after request
Now the IP address is bound to the socket and a check is made to see if the call to bind succeeded
The final step in the initialization is the call to listen to announce the serverâs willingness to accept incoming calls and tell the system to hold up to QUEUE SIZE of them in case new requests arrive while the server is still processing the current one
If the queue is full and additional requests arrive  they are quietly discarded
At this point  the server enters its main loop  which it never leaves
The only way to stop it is to kill it from outside
The call to accept blocks the server until some client tries to establish a connection with it
If the accept call succeeds  it returns a socket descriptor that can be used for reading and writing  analogous to how file descriptors can be used to read from and write to pipes
However  unlike pipes  which are unidirectional  sockets are bidirectional  so sa (the accepted socket) can be used for reading from the connection and also for writing to it
A pipe file descriptor is for reading or writing but not both
After the connection is established  the server reads the file name from it
If the name is not yet available  the server blocks waiting for it
After getting the file name  the server opens the file and enters a loop that alternately reads blocks from the file and writes them to the socket until the entire file has been copied
Then the server closes the file and the connection and waits for the next connection to show up
It repeats this loop forever
Now let us look at the client code
To understand how it works  it is necessary to understand how it is invoked
Assuming it is called client  a typical call is client
/usr/tom/filename >f This call only works if the server is already running on
and the file /usr/tom/filename exists and the server has read access to it
If the call is successful  the file is transferred over the Internet and written to f  after which the client program exits
Since the server continues after a transfer  the client can be started again and again to get other files
The client code starts with some includes and declarations
Execution begins by checking to see if it has been called with the right number of arguments (argc =  means the program name plus two arguments)
Note that argv [ ] contains the   THE TRANSPORT SERVICE name of the server (
) and is converted to an IP address by gethostbyname
This function uses DNS to look up the name
We will study DNS in   Next  a socket is created and initialized
After that  the client attempts to establish a TCP connection to the server  using connect
If the server is up and running on the named machine and attached to SERVER PORT and is either idle or has room in its listen queue  the connection will (eventually) be established
Using the connection  the client sends the name of the file by writing on the socket
The number of bytes sent is one larger than the name proper  since the  byte terminating the name must also be sent to tell the server where the name ends
Now the client enters a loop  reading the file block by block from the socket and copying it to standard output
When it is done  it just exits
The procedure fatal prints an error message and exits
The server needs the same procedure  but it was omitted due to lack of space on the page
Since the client and server are compiled separately and normally run on different computers  they cannot share the code of fatal
These two programs (as well as other material related to this book) can be fetched from the bookâs Web site http:// /tanenbaum Just for the record  this server is not the last word in serverdom
Its error checking is meager and its error reporting is mediocre
Since it handles all requests strictly sequentially (because it has only a single thread)  its performance is poor
It has clearly never heard about urity  and using bare UNIX system calls is not the way to gain platform independence
It also makes some assumptions that are technically illegal  such as assuming that the file name fits in the buffer and is transmitted atomically
These shortcomings notwithstanding  it is a working Internet file server
In the exercises  the reader is invited to improve it
For more information about programming with sockets  see Donahoo and Calvert (  )  ELEMENTS OF TRANSPORT PROTOCOLS The transport service is implemented by a transport protocol used between the two transport entities
In some ways  transport protocols resemble the data link protocols we studied in detail in   Both have to deal with error control  sequencing  and flow control  among other issues
However  significant differences between the two also exist
These differences are due to major dissimilarities between the environments in which the two protocols operate  as shown in Fig
At the data link layer  two routers THE TRANSPORT LAYER
communicate directly via a physical channel  whether wired or wireless  whereas at the transport layer  this physical channel is replaced by the entire network
This difference has many important implications for the protocols
Router Router Physical communication channel Host (a) (b) Network Figure  -
(a) Environment of the data link layer
(b) Environment of the transport layer
For one thing  over point-to-point links such as wires or optical fiber  it is usually not necessary for a router to specify which router it wants to talk toâeach outgoing line leads directly to a particular router
In the transport layer  explicit addressing of destinations is required
For another thing  the process of establishing a connection over the wire of Fig
Either way  there is not much to do
Even on wireless links  the process is not much different
Just sending a message is sufficient to have it reach all other destinations
If the message is not acknowledged due to an error  it can be resent
In the transport layer  initial connection establishment is complicated  as we will see
Another (exceedingly annoying) difference between the data link layer and the transport layer is the potential existence of storage capacity in the network
When a router sends a packet over a link  it may arrive or be lost  but it cannot bounce around for a while  go into hiding in a far corner of the world  and suddenly emerge after other packets that were sent much later
If the network uses datagrams  which are independently routed inside  there is a nonnegligible probability that a packet may take the scenic route and arrive late and out of the expected order  or even that duplicates of the packet will arrive
The consequences of the networkâs ability to delay and duplicate packets can sometimes be disastrous and can require the use of special protocols to correctly transport information
A final difference between the data link and transport layers is one of degree rather than of kind
Buffering and flow control are needed in both layers  but the presence in the transport layer of a large and varying number of connections with bandwidth that fluctuates as the connections compete with each other may require a different approach than we used in the data link layer
Some of the protocols discussed in
allocate a fixed number of buffers to each line  so that when a frame arrives a buffer is always available
In the transport layer  the larger number of connections that must be managed and variations in the bandwidth each   ELEMENTS OF TRANSPORT PROTOCOLS connection may receive make the idea of dedicating many buffers to each one less attractive
In the following tions  we will examine all of these important issues  and others
Addressing When an application (
a user) process wishes to set up a connection to a remote application process  it must specify which one to connect to
(Connectionless transport has the same problem: to whom should each message be sent?) The method normally used is to define transport addresses to which processes can listen for connection requests
In the Internet  these endpoints are called ports
We will use the generic term TSAP (Transport Service Access Point) to mean a specific endpoint in the transport layer
The analogous endpoints in the network layer (
network layer addresses) are not-surprisingly called NSAPs (Network Service Access Points)
IP addresses are examples of NSAPs
Figure  -  illustrates the relationship between the NSAPs  the TSAPs  and a transport connection
Application processes  both clients and servers  can attach themselves to a local TSAP to establish a connection to a remote TSAP
These connections run through NSAPs on each host  as shown
The purpose of having TSAPs is that in some networks  each computer has a single NSAP  so some way is needed to distinguish multiple transport endpoints that share that NSAP
Application process Application layer Transport connection TSAP  TSAP  NSAP NSAP Transport layer Network layer Data link layer Physical layer Server  Host  Host  Server  TSAP Figure  -
TSAPs  NSAPs  and transport connections
THE TRANSPORT LAYER
A possible scenario for a transport connection is as follows:
A mail server process attaches itself to TSAP  on host  to wait for an incoming call
How a process attaches itself to a TSAP is outside the networking model and depends entirely on the local operating system
A call such as our LISTEN might be used  for example An application process on host  wants to send an email message  so it attaches itself to TSAP  and issues a CONNECT request
The request specifies TSAP  on host  as the source and TSAP  on host  as the destination
This action ultimately results in a transport connection being established between the application process and the server The application process sends over the mail message The mail server responds to say that it will deliver the message The transport connection is released
Note that there may well be other servers on host  that are attached to other TSAPs and are waiting for incoming connections that arrive over the same NSAP
The picture painted above is fine  except we have swept one little problem under the rug: how does the user process on host  know that the mail server is attached to TSAP ? One possibility is that the mail server has been attaching itself to TSAP  for years and gradually all the network users have learned this
In this model  services have stable TSAP addresses that are listed in files in well-known places
For example  the /etc/services file on UNIX systems lists which servers are permanently attached to which ports  including the fact that the mail server is found on TCP port
While stable TSAP addresses work for a small number of key services that never change (
the Web server)  user processes  in general  often want to talk to other user processes that do not have TSAP addresses that are known in advance  or that may exist for only a short time
To handle this situation  an alternative scheme can be used
In this scheme  there exists a special process called a portmapper
To find the TSAP address corresponding to a given service name  such as ââBitTorrent ââ a user sets up a connection to the portmapper (which listens to a well-known TSAP)
The user then sends a message specifying the service name  and the portmapper sends back the TSAP address
Then the user releases the connection with the portmapper and establishes a new one with the desired service
In this model  when a new service is created  it must register itself with the portmapper  giving both its service name (typically  an ASCII string) and its TSAP
The portmapper records this information in its internal database so that when queries come in later  it will know the answers
ELEMENTS OF TRANSPORT PROTOCOLS The function of the portmapper is analogous to that of a directory assistance operator in the telephone systemâit provides a mapping of names onto numbers
Just as in the telephone system  it is essential that the address of the well-known TSAP used by the portmapper is indeed well known
If you do not know the number of the information operator  you cannot call the information operator to find it out
If you think the number you dial for information is obvious  try it in a foreign country sometime
Many of the server processes that can exist on a machine will be used only rarely
It is wasteful to have each of them active and listening to a stable TSAP address all day long
An alternative scheme is shown in Fig
It is known as the initial connection protocol
Instead of every conceivable server listening at a well-known TSAP  each machine that wishes to offer services to remote users has a special process server that acts as a proxy for less heavily used servers
This server is called inetd on UNIX systems
It listens to a set of ports at the same time  waiting for a connection request
Potential users of a service begin by doing a CONNECT request  specifying the TSAP address of the service they want
If no server is waiting for them  they get a connection to the process server  as shown in Fig
Layer  TSAP Mail server (a) (b) Host  Host  Host  Host  Process server User Process User server Figure  -
How a user process in host  establishes a connection with a mail server in host  via a process server
After it gets the incoming request  the process server spawns the requested server  allowing it to inherit the existing connection with the user
The new server THE TRANSPORT LAYER
does the requested work  while the process server goes back to listening for new requests  as shown in Fig
This method is only applicable when servers can be created on demand
Connection Establishment Establishing a connection sounds easy  but it is actually surprisingly tricky
At first glance  it would seem sufficient for one transport entity to just send a CONNECTION REQUEST segment to the destination and wait for a CONNECTION ACCEPTED reply
The problem occurs when the network can lose  delay  corrupt  and duplicate packets
This behavior causes serious complications
Imagine a network that is so congested that acknowledgements hardly ever get back in time and each packet times out and is retransmitted two or three times
Suppose that the network uses datagrams inside and that every packet follows a different route
Some of the packets might get stuck in a traffic jam inside the network and take a long time to arrive
That is  they may be delayed in the network and pop out much later  when the sender thought that they had been lost
The worst possible nightmare is as follows
A user establishes a connection with a bank  sends messages telling the bank to transfer a large amount of money to the account of a not-entirely-trustworthy person
Unfortunately  the packets decide to take the scenic route to the destination and go off exploring a remote corner of the network
The sender then times out and sends them all again
This time the packets take the shortest route and are delivered quickly so the sender releases the connection
Unfortunately  eventually the initial batch of packets finally come out of hiding and arrive at the destination in order  asking the bank to establish a new connection and transfer money (again)
The bank has no way of telling that these are duplicates
It must assume that this is a ond  independent transaction  and transfers the money again
This scenario may sound unlikely  or even implausible but the point is this: protocols must be designed to be correct in all cases
Only the common cases need be implemented efficiently to obtain good network performance  but the protocol must be able to cope with the uncommon cases without breaking
If it cannot  we have built a fair-weather network that can fail without warning when the conditions get tough
For the remainder of this tion  we will study the problem of delayed duplicates  with emphasis on algorithms for establishing connections in a reliable way  so that nightmares like the one above cannot happen
The crux of the problem is that the delayed duplicates are thought to be new packets
We cannot prevent packets from being duplicated and delayed
But if and when this happens  the packets must be rejected as duplicates and not processed as fresh packets
The problem can be attacked in various ways  none of them very satisfactory
One way is to use throwaway transport addresses
In this approach  each time a   ELEMENTS OF TRANSPORT PROTOCOLS transport address is needed  a new one is generated
When a connection is released  the address is discarded and never used again
Delayed duplicate packets then never find their way to a transport process and can do no damage
However  this approach makes it more difficult to connect with a process in the first place
Another possibility is to give each connection a unique identifier (
a sequence number incremented for each connection established) chosen by the initiating party and put in each segment  including the one requesting the connection
After each connection is released  each transport entity can update a table listing obsolete connections as (peer transport entity  connection identifier) pairs
Whenever a connection request comes in  it can be checked against the table to see if it belongs to a previously released connection
Unfortunately  this scheme has a basic flaw: it requires each transport entity to maintain a certain amount of history information indefinitely
This history must persist at both the source and destination machines
Otherwise  if a machine crashes and loses its memory  it will no longer know which connection identifiers have already been used by its peers
Instead  we need to take a different tack to simplify the problem
Rather than allowing packets to live forever within the network  we devise a mechanism to kill off aged packets that are still hobbling about
With this restriction  the problem becomes somewhat more manageable
Packet lifetime can be restricted to a known maximum using one (or more) of the following techniques:
Restricted network design Putting a hop counter in each packet Timestamping each packet
The first technique includes any method that prevents packets from looping  combined with some way of bounding delay including congestion over the (now known) longest possible path
It is difficult  given that internets may range from a single city to international in scope
The ond method consists of having the hop count initialized to some appropriate value and decremented each time the packet is forwarded
The network protocol simply discards any packet whose hop counter becomes zero
The third method requires each packet to bear the time it was created  with the routers agreeing to discard any packet older than some agreed-upon time
This latter method requires the router clocks to be synchronized  which itself is a nontrivial task  and in practice a hop counter is a close enough approximation to age
In practice  we will need to guarantee not only that a packet is dead  but also that all acknowledgements to it are dead  too  so we will now introduce a period T  which is some small multiple of the true maximum packet lifetime
The maximum packet lifetime is a conservative constant for a network; for the Internet  it is somewhat arbitrarily taken to be onds
The multiple is protocol dependent THE TRANSPORT LAYER
and simply has the effect of making T longer
If we wait a time T s after a packet has been sent  we can be sure that all traces of it are now gone and that neither it nor its acknowledgements will suddenly appear out of the blue to complicate matters
With packet lifetimes bounded  it is possible to devise a practical and foolproof way to reject delayed duplicate segments
The method described below is due to Tomlinson (   )  as refined by Sunshine and Dalal (   )
Variants of it are widely used in practice  including in TCP
The heart of the method is for the source to label segments with sequence numbers that will not be reused within T s
The period  T  and the rate of packets per ond determine the size of the sequence numbers
In this way  only one packet with a given sequence number may be outstanding at any given time
Duplicates of this packet may still occur  and they must be discarded by the destination
However  it is no longer the case that a delayed duplicate of an old packet may beat a new packet with the same sequence number and be accepted by the destination in its stead
To get around the problem of a machine losing all memory of where it was after a crash  one possibility is to require transport entities to be idle for T s after a recovery
The idle period will let all old segments die off  so the sender can start again with any sequence number
However  in a complex internetwork  T may be large  so this strategy is unattractive
Instead  Tomlinson proposed equipping each host with a time-of-day clock
The clocks at different hosts need not be synchronized
Each clock is assumed to take the form of a binary counter that increments itself at uniform intervals
Furthermore  the number of bits in the counter must equal or exceed the number of bits in the sequence numbers
Last  and most important  the clock is assumed to continue running even if the host goes down
When a connection is set up  the low-order k bits of the clock are used as the k-bit initial sequence number
Thus  unlike our protocols of
each connection starts numbering its segments with a different initial sequence number
The sequence space should be so large that by the time sequence numbers wrap around  old segments with the same sequence number are long gone
This linear relation between time and initial sequence numbers is shown in Fig
The forbidden region shows the times for which segment sequence numbers are illegal leading up to their use
If any segment is sent with a sequence number in this region  it could be delayed and impersonate a different packet with the same sequence number that will be issued slightly later
For example  if the host crashes and restarts at time   onds  it will use initial sequence numbers based on the clock to pick up after it left off; the host does not start with a lower sequence number in the forbidden region
Once both transport entities have agreed on the initial sequence number  any sliding window protocol can be used for data flow control
This window protocol will correctly find and discard duplicates of packets after they have already been   ELEMENTS OF TRANSPORT PROTOCOLS      Time (a) Time (b)  Sequence numbers Sequence numbers Restart after crash with   T T Actual sequence numbers used  kâ  Forbidden region Figure  -
(a) Segments may not enter the forbidden region
(b) The resynchronization problem
In reality  the initial sequence number curve (shown by the heavy line) is not linear  but a staircase  since the clock advances in discrete steps
For simplicity  we will ignore this detail
To keep packet sequence numbers out of the forbidden region  we need to take care in two respects
We can get into trouble in two distinct ways
If a host sends too much data too fast on a newly opened connection  the actual sequence number versus time curve may rise more steeply than the initial sequence number versus time curve  causing the sequence number to enter the forbidden region
To prevent this from happening  the maximum data rate on any connection is one segment per clock tick
This also means that the transport entity must wait until the clock ticks before opening a new connection after a crash restart  lest the same number be used twice
Both of these points argue in favor of a short clock tick (  Î¼ or less)
But the clock cannot tick too fast relative to the sequence number
For a clock rate of C and a sequence number space of size S  we must have S/C>T so that the sequence numbers cannot wrap around too quickly
Entering the forbidden region from underneath by sending too fast is not the only way to get into trouble
The greater the slope of the actual sequence numbers  the longer this event will be delayed
Avoiding this situation limits how slowly sequence numbers can advance on a connection (or how long the connections may last)
The clock-based method solves the problem of not being able to distinguish delayed duplicate segments from new segments
However  there is a practical snag for using it for establishing connections
Since we do not normally remember sequence numbers across connections at the destination  we still have no way of THE TRANSPORT LAYER
knowing if a CONNECTION REQUEST segment containing an initial sequence number is a duplicate of a recent connection
This snag does not exist during a connection because the sliding window protocol does remember the current sequence number
To solve this specific problem  Tomlinson (   ) introduced the three-way handshake
This establishment protocol involves one peer checking with the other that the connection request is indeed current
The normal setup procedure when host  initiates is shown in Fig
Host  chooses a sequence number  x  and sends a CONNECTION REQUEST segment containing it to host
Host  replies with an ACK segment acknowledging x and announcing its own initial sequence number  y
Finally  host  acknowledges host  âs choice of an initial sequence number in the first data segment that it sends
Now let us see how the three-way handshake works in the presence of delayed duplicate control segments
This segment arrives at host  without host  âs knowledge
Host  reacts to this segment by sending host  an ACK segment  in effect asking for verification that host  was indeed trying to set up a new connection
When host  rejects host  âs attempt to establish a connection  host  realizes that it was tricked by a delayed duplicate and abandons the connection
In this way  a delayed duplicate does no damage
The worst case is when both a delayed CONNECTION REQUEST and an ACK are floating around in the subnet
This case is shown in Fig
As in the previous example  host  gets a delayed CONNECTION REQUEST and replies to it
At this point  it is crucial to realize that host  has proposed using y as the initial sequence number for host  to host  traffic  knowing full well that no segments containing sequence number y or acknowledgements to y are still in existence
When the ond delayed segment arrives at host   the fact that z has been acknowledged rather than y tells host  that this  too  is an old duplicate
The important thing to realize here is that there is no combination of old segments that can cause the protocol to fail and have a connection set up by accident when no one wants it
TCP uses this three-way handshake to establish connections
Within a connection  a timestamp is used to extend the  -bit sequence number so that it will not wrap within the maximum packet lifetime  even for gigabit-per-ond connections
This mechanism is a fix to TCP that was needed as it was used on faster and faster links
It is described in RFC  and called PAWS (Protection Against Wrapped Sequence numbers)
Across connections  for the initial sequence numbers and before PAWS can come into play  TCP originally used the clock-based scheme just described
However  this turned out to have a urity vulnerability
The clock made it easy for an attacker to predict the next initial sequence number and send packets that tricked the three-way handshake and established a forged connection
To close this hole  pseudorandom initial sequence numbers are used for connections in practice
However  it remains important that   ELEMENTS OF TRANSPORT PROTOCOLS Time Time Time DATA (seq = x  ACK = y) ACK (seq = y  ACK = x) CR (seq = x) Host  Host  REJECT (ACK = y) DATA (seq = x  ACK = z) ACK (seq = y  ACK = x) CR (seq = x) Host  Host  REJECT (ACK =y) ACK (seq = y  ACK =x) CR (seq = x) Host  Host  Old duplicate Old duplicate Old duplicate (a) (b) (c) Figure  -
Three protocol scenarios for establishing a connection using a three-way handshake
CR denotes CONNECTION REQUEST
(a) Normal operation
(b) Old duplicate CONNECTION REQUEST appearing out of nowhere
(c) Duplicate CONNECTION REQUEST and duplicate ACK
the initial sequence numbers not repeat for an interval even though they appear random to an observer
Otherwise  delayed duplicates can wreak havoc
Connection Release Releasing a connection is easier than establishing one
Nevertheless  there are more pitfalls than one might expect here
As we mentioned earlier  there are two styles of terminating a connection: asymmetric release and symmetric release
THE TRANSPORT LAYER
Asymmetric release is the way the telephone system works: when one party hangs up  the connection is broken
Symmetric release treats the connection as two separate unidirectional connections and requires each one to be released separately
Asymmetric release is abrupt and may result in data loss
Consider the scenario of Fig
After the connection is established  host  sends a segment that arrives properly at host
Then host  sends another segment
Unfortunately  host  issues a DISCONNECT before the ond segment arrives
The result is that the connection is released and data are lost
Time CR DATA DATA Host  Host  ACK DR No data are delivered after a disconnect request Figure  -
Abrupt disconnection with loss of data
Clearly  a more sophisticated release protocol is needed to avoid data loss
One way is to use symmetric release  in which each direction is released independently of the other one
Here  a host can continue to receive data even after it has sent a DISCONNECT segment
Symmetric release does the job when each process has a fixed amount of data to send and clearly knows when it has sent it
In other situations  determining that all the work has been done and the connection should be terminated is not so obvious
One can envision a protocol in which host  says ââI am done
Are you done too?ââ If host  responds: ââI am done too
Goodbye  the connection can be safely released
ââ Unfortunately  this protocol does not always work
There is a famous problem that illustrates this issue
It is called the two-army problem
Imagine that a white army is encamped in a valley  as shown in Fig
On both of the surrounding hillsides are blue armies
The white army is larger than either of the blue armies alone  but together the blue armies are larger than the white army
If either blue army attacks by itself  it will be defeated  but if the two blue armies attack simultaneously  they will be victorious
The blue armies want to synchronize their attacks
However  their only communication medium is to send messengers on foot down into the valley  where   ELEMENTS OF TRANSPORT PROTOCOLS W B B White army Blue army #  Blue army #  Figure  -
The two-army problem
they might be captured and the message lost (
they have to use an unreliable communication channel)
The question is: does a protocol exist that allows the blue armies to win? Suppose that the commander of blue army #  sends a message reading: ââI propose we attack at dawn on March
How about it?ââ Now suppose that the message arrives  the commander of blue army #  agrees  and his reply gets safely back to blue army #
Will the attack happen? Probably not  because commander #  does not know if his reply got through
If it did not  blue army #  will not attack  so it would be foolish for him to charge into battle
Now let us improve the protocol by making it a three-way handshake
The initiator of the original proposal must acknowledge the response
Assuming no messages are lost  blue army #  will get the acknowledgement  but the commander of blue army #  will now hesitate
After all  he does not know if his acknowledgement got through  and if it did not  he knows that blue army #  will not attack
We could now make a four-way handshake protocol  but that does not help either
In fact  it can be proven that no protocol exists that works
Suppose that some protocol did exist
Either the last message of the protocol is essential  or it is not
If it is not  we can remove it (and any other unessential messages) until we are left with a protocol in which every message is essential
What happens if the final message does not get through? We just said that it was essential  so if it is lost  the attack does not take place
Since the sender of the final message can never be sure of its arrival  he will not risk attacking
Worse yet  the other blue army knows this  so it will not attack either
To see the relevance of the two-army problem to releasing connections  rather than to military affairs  just substitute ââdisconnectââ for ââattack
ââ If neither side is THE TRANSPORT LAYER
prepared to disconnect until it is convinced that the other side is prepared to disconnect too  the disconnection will never happen
In practice  we can avoid this quandary by foregoing the need for agreement and pushing the problem up to the transport user  letting each side independently decide when it is done
This is an easier problem to solve
Figure  -  illustrates four scenarios of releasing using a three-way handshake
While this protocol is not infallible  it is usually adequate
When it arrives  the recipient sends back a DR segment and starts a timer  just in case its DR is lost
When this DR arrives  the original sender sends back an ACK segment and releases the connection
Finally  when the ACK segment arrives  the receiver also releases the connection
Releasing a connection means that the transport entity removes the information about the connection from its table of currently open connections and signals the connectionâs owner (the transport user) somehow
This action is different from a transport user issuing a DISCONNECT primitive
If the final ACK segment is lost  as shown in Fig
When the timer expires  the connection is released anyway
Now consider the case of the ond DR being lost
The user initiating the disconnection will not receive the expected response  will time out  and will start all over again
-  (c)  we see how this works  assuming that the ond time no segments are lost and all segments are delivered correctly and on time
Our last scenario  Fig
-  (c) except that now we assume all the repeated attempts to retransmit the DR also fail due to lost segments
After N retries  the sender just gives up and releases the connection
Meanwhile  the receiver times out and also exits
While this protocol usually suffices  in theory it can fail if the initial DR and N retransmissions are all lost
The sender will give up and release the connection  while the other side knows nothing at all about the attempts to disconnect and is still fully active
This situation results in a half-open connection
We could have avoided this problem by not allowing the sender to give up after N retries and forcing it to go on forever until it gets a response
However  if the other side is allowed to time out  the sender will indeed go on forever  because no response will ever be forthcoming
If we do not allow the receiving side to time out  the protocol hangs in Fig
One way to kill off half-open connections is to have a rule saying that if no segments have arrived for a certain number of onds  the connection is automatically disconnected
That way  if one side ever disconnects  the other side will detect the lack of activity and also disconnect
This rule also takes care of the case where the connection is broken (because the network can no longer deliver packets between the hosts) without either end disconnecting first
Of course  if this rule is introduced  it is necessary for each transport entity to have a timer that is stopped and then restarted whenever a segment is sent
If this timer expires  a   ELEMENTS OF TRANSPORT PROTOCOLS DR ACK ACK Host  Host  DR DR Send DR + start timer Send DR + start timer Send ACK Release connection (Timeout) release connection (Timeout) release connection (N Timeouts) release connection ( Timeout) send DR + start timer Release connection DR DR Host  Host  DR Send DR + start timer Send DR & start timer Send DR & start timer Send DR & start timer Send ACK Release connection Release connection DR ACK Host  Host  DR Send DR + start timer Send DR + start timer Send ACK Release connection Lost Lost ( Timeout) send DR + start timer DR Host  Host  Send DR + start timer Lost Lost (a) (b) (c) (d) Figure  -
Four protocol scenarios for releasing a connection
(a) Normal case of three-way handshake
(b) Final ACK lost
(c) Response lost
(d) Response lost and subsequent DRs lost
dummy segment is transmitted  just to keep the other side from disconnecting
On the other hand  if the automatic disconnect rule is used and too many dummy segments in a row are lost on an otherwise idle connection  first one side  then the other will automatically disconnect
We will not belabor this point any more  but by now it should be clear that releasing a connection without data loss is not nearly as simple as it first appears
The lesson here is that the transport user must be involved in deciding when to THE TRANSPORT LAYER
disconnectâthe problem cannot be cleanly solved by the transport entities themselves
To see the importance of the application  consider that while TCP normally does a symmetric close (with each side independently closing its half of the connection with a FIN packet when it has sent its data)  many Web servers send the client a RST packet that causes an abrupt close of the connection that is more like an asymmetric close
This works only because the Web server knows the pattern of data exchange
First it receives a request from the client  which is all the data the client will send  and then it sends a response to the client
When the Web server is finished with its response  all of the data has been sent in either direction
The server can send the client a warning and abruptly shut the connection
If the client gets this warning  it will release its connection state then and there
If the client does not get the warning  it will eventually realize that the server is no longer talking to it and release the connection state
The data has been successfully transferred in either case
Error Control and Flow Control Having examined connection establishment and release in some detail  let us now look at how connections are managed while they are in use
The key issues are error control and flow control
Error control is ensuring that the data is delivered with the desired level of reliability  usually that all of the data is delivered without any errors
Flow control is keeping a fast transmitter from overrunning a slow receiver
Both of these issues have come up before  when we studied the data link layer
The solutions that are used at the transport layer are the same mechanisms that we studied in   As a very brief recap:
A frame carries an error-detecting code (
a CRC or checksum) that is used to check if the information was correctly received A frame carries a sequence number to identify itself and is retransmitted by the sender until it receives an acknowledgement of successful receipt from the receiver
This is called ARQ (Automatic Repeat reQuest) There is a maximum number of frames that the sender will allow to be outstanding at any time  pausing if the receiver is not acknowledging frames quickly enough
If this maximum is one packet the protocol is called stop-and-wait
Larger windows enable pipelining and improve performance on long  fast links The sliding window protocol combines these features and is also used to support bidirectional data transfer
Given that these mechanisms are used on frames at the link layer  it is natural to wonder why they would be used on segments at the transport layer as well
ELEMENTS OF TRANSPORT PROTOCOLS However  there is little duplication between the link and transport layers in practice
Even though the same mechanisms are used  there are differences in function and degree
For a difference in function  consider error detection
The link layer checksum protects a frame while it crosses a single link
The transport layer checksum protects a segment while it crosses an entire network path
It is an end-to-end check  which is not the same as having a check on every link
Saltzer et al
(   ) describe a situation in which packets were corrupted inside a router
The link layer checksums protected the packets only while they traveled across a link  not while they were inside the router
Thus  packets were delivered incorrectly even though they were correct according to the checks on every link
This and other examples led Saltzer et al
to articulate the end-to-end argument
According to this argument  the transport layer check that runs end-to-end is essential for correctness  and the link layer checks are not essential but nonetheless valuable for improving performance (since without them a corrupted packet can be sent along the entire path unnecessarily)
As a difference in degree  consider retransmissions and the sliding window protocol
Most wireless links  other than satellite links  can have only a single frame outstanding from the sender at a time
That is  the bandwidth-delay product for the link is small enough that not even a whole frame can be stored inside the link
In this case  a small window size is sufficient for good performance
For example
uses a stop-and-wait protocol  transmitting or retransmitting each frame and waiting for it to be acknowledged before moving on to the next frame
Having a window size larger than one frame would add complexity without improving performance
For wired and optical fiber links  such as (switched) Ethernet or ISP backbones  the error-rate is low enough that link-layer retransmissions can be omitted because the end-to-end retransmissions will repair the residual frame loss
On the other hand  many TCP connections have a bandwidth-delay product that is much larger than a single segment
Consider a connection sending data across the
at  Mbps with a round-trip time of m
Even for this slow connection  Kbit of data will be stored at the receiver in the time it takes to send a segment and receive an acknowledgement
For these situations  a large sliding window must be used
Stop-and-wait will cripple performance
In our example it would limit performance to one segment every m  or  segments/  no matter how fast the network really is
Given that transport protocols generally use larger sliding windows  we will look at the issue of buffering data more carefully
Since a host may have many connections  each of which is treated separately  it may need a substantial amount of buffering for the sliding windows
The buffers are needed at both the sender and the receiver
Certainly they are needed at the sender to hold all transmitted but as yet unacknowledged segments
They are needed there because these segments may be lost and need to be retransmitted
THE TRANSPORT LAYER
However  since the sender is buffering  the receiver may or may not dedicate specific buffers to specific connections  as it sees fit
The receiver may  for example  maintain a single buffer pool shared by all connections
When a segment comes in  an attempt is made to dynamically acquire a new buffer
If one is available  the segment is accepted; otherwise  it is discarded
Since the sender is prepared to retransmit segments lost by the network  no permanent harm is done by having the receiver drop segments  although some resources are wasted
The sender just keeps trying until it gets an acknowledgement
The best trade-off between source buffering and destination buffering depends on the type of traffic carried by the connection
For low-bandwidth bursty traffic  such as that produced by an interactive terminal  it is reasonable not to dedicate any buffers  but rather to acquire them dynamically at both ends  relying on buffering at the sender if segments must occasionally be discarded
On the other hand  for file transfer and other high-bandwidth traffic  it is better if the receiver does dedicate a full window of buffers  to allow the data to flow at maximum speed
This is the strategy that TCP uses
There still remains the question of how to organize the buffer pool
If most segments are nearly the same size  it is natural to organize the buffers as a pool of identically sized buffers  with one segment per buffer  as in Fig
However  if there is wide variation in segment size  from short requests for Web pages to large packets in peer-to-peer file transfers  a pool of fixed-sized buffers presents problems
If the buffer size is chosen to be equal to the largest possible segment  space will be wasted whenever a short segment arrives
If the buffer size is chosen to be less than the maximum segment size  multiple buffers will be needed for long segments  with the attendant complexity
Another approach to the buffer size problem is to use variable-sized buffers  as in Fig
The advantage here is better memory utilization  at the price of more complicated buffer management
A third possibility is to dedicate a single large circular buffer per connection  as in Fig
This system is simple and elegant and does not depend on segment sizes  but makes good use of memory only when the connections are heavily loaded
As connections are opened and closed and as the traffic pattern changes  the sender and receiver need to dynamically adjust their buffer allocations
Consequently  the transport protocol should allow a sending host to request buffer space at the other end
Buffers could be allocated per connection  or collectively  for all the connections running between the two hosts
Alternatively  the receiver  knowing its buffer situation (but not knowing the offered traffic) could tell the sender ââI have reserved X buffers for you
ââ If the number of open connections should increase  it may be necessary for an allocation to be reduced  so the protocol should provide for this possibility
A reasonably general way to manage dynamic buffer allocation is to decouple the buffering from the acknowledgements  in contrast to the sliding window protocols of   Dynamic buffer management means  in effect  a variable-sized   ELEMENTS OF TRANSPORT PROTOCOLS Segment  Segment  Segment  Segment  (a) (b) (c) Unused space Figure  -
(a) Chained fixed-size buffers
(b) Chained variable-sized buffers
(c) One large circular buffer per connection
Initially  the sender requests a certain number of buffers  based on its expected needs
The receiver then grants as many of these as it can afford
Every time the sender transmits a segment  it must decrement its allocation  stopping altogether when the allocation reaches zero
The receiver separately piggybacks both acknowledgements and buffer allocations onto the reverse traffic
TCP uses this scheme  carrying buffer allocations in a header field called Window size
Figure  -  shows an example of how dynamic window management might work in a datagram network with  -bit sequence numbers
In this example  data flows in segments from host A to host B and acknowledgements and buffer allocations flow in segments in the reverse direction
Initially  A wants eight buffers  but it is granted only four of these
It then sends three segments  of which the third is lost
Segment  acknowledges receipt of all segments up to and including sequence number   thus allowing A to release those buffers  and furthermore informs A that it has permission to send three more segments starting beyond  (
segments  and  )
A knows that it has already sent number   so it thinks that it may send segments  and   which it proceeds to do
At this point it is blocked and must wait for more buffer allocation
Timeout-induced retransmissions (line  )  however  may occur while blocked  since they use buffers that have already been allocated
In line B acknowledges receipt of all segments up to and including  but refuses to let A continue
Such a situation is impossible with the fixed-window protocols of   The next segment from B to A allocates THE TRANSPORT LAYER
another buffer and allows A to continue
This will happen when B has buffer space  likely because the transport user has accepted more segment data
< request  buffers> <ack = buf =  > <seq =   data = m > <seq =   data = m > <seq =   data = m > <ack =   buf =  > <seq =   data = m > <seq =   data = m > <seq =   data = m > <ack =   buf =  > <ack =   buf =  > <ack =   buf =  > <seq =   data = m > <seq =   data = m > <ack =   buf =  > <ack =   buf =  > A wants  buffers B grants messages  -  only A has  buffers left now A has  buffers left now Message lost but A thinks it has  left B acknowledges  and   permits  -  A has  buffer left A has  buffers left  and must stop A times out and retransmits Everything acknowledged  but A still blocked A may now send  B found a new buffer somewhere A has  buffer left A is now blocked again A is still blocked Potential deadlock A Message B Comments Figure  -
Dynamic buffer allocation
The arrows show the direction of transmission
An ellipsis (
) indicates a lost segment
Problems with buffer allocation schemes of this kind can arise in datagram networks if control segments can get lostâwhich they most certainly can
Look at line
B has now allocated more buffers to A  but the allocation segment was lost
Since control segments are not sequenced or timed out  A is now deadlocked
To prevent this situation  each host should periodically send control segments giving the acknowledgement and buffer status on each connection
That way  the deadlock will be broken  sooner or later
Until now we have tacitly assumed that the only limit imposed on the senderâs data rate is the amount of buffer space available in the receiver
This is often not the case
Memory was once expensive but prices have fallen dramatically
Hosts may be equipped with sufficient memory that the lack of buffers is rarely  if ever  a problem  even for wide area connections
Of course  this depends on the buffer size being set to be large enough  which has not always been the case for TCP (Zhang et al
When buffer space no longer limits the maximum flow  another bottleneck will appear: the carrying capacity of the network
If adjacent routers can exchange at most x packets/ and there are k disjoint paths between a pair of hosts  there is no way that those hosts can exchange more than kx segments/  no matter how much buffer space is available at each end
If the sender pushes too hard   ELEMENTS OF TRANSPORT PROTOCOLS (
sends more than kx segments/)  the network will become congested because it will be unable to deliver segments as fast as they are coming in
What is needed is a mechanism that limits transmissions from the sender based on the networkâs carrying capacity rather than on the receiverâs buffering capacity
Belsnes (   ) proposed using a sliding window flow-control scheme in which the sender dynamically adjusts the window size to match the networkâs carrying capacity
This means that a dynamic sliding window can implement both flow control and congestion control
If the network can handle c segments/ and the round-trip time (including transmission  propagation  queueing  processing at the receiver  and return of the acknowledgement) is r  the senderâs window should be cr
With a window of this size  the sender normally operates with the pipeline full
Any small decrease in network performance will cause it to block
Since the network capacity available to any given flow varies over time  the window size should be adjusted frequently  to track changes in the carrying capacity
As we will see later  TCP uses a similar scheme
Multiplexing Multiplexing  or sharing several conversations over connections  virtual circuits  and physical links plays a role in several layers of the network architecture
In the transport layer  the need for multiplexing can arise in a number of ways
For example  if only one network address is available on a host  all transport connections on that machine have to use it
When a segment comes in  some way is needed to tell which process to give it to
This situation  called multiplexing  is shown in Fig
In this figure  four distinct transport connections all use the same network connection (
IP address) to the remote host
Multiplexing can also be useful in the transport layer for another reason
Suppose  for example  that a host has multiple network paths that it can use
If a user needs more bandwidth or more reliability than one of the network paths can provide  a way out is to have a connection that distributes the traffic among multiple network paths on a round-robin basis  as indicated in Fig
This modus operandi is called inverse multiplexing
With k network connections open  the effective bandwidth might be increased by a factor of k
An example of inverse multiplexing is SCTP (Stream Control Transmission Protocol)  which can run a connection using multiple network interfaces
In contrast  TCP uses a single network endpoint
Inverse multiplexing is also found at the link layer  when several low-rate links are used in parallel as one high-rate link
Crash Recovery If hosts and routers are subject to crashes or connections are long-lived (
large software or media downloads)  recovery from these crashes becomes an issue
If the transport entity is entirely within the hosts  recovery from network THE TRANSPORT LAYER
Layer    To router Router lines Transport address Network address (a) (b) Figure  -
(a) Multiplexing
(b) Inverse multiplexing
and router crashes is straightforward
The transport entities expect lost segments all the time and know how to cope with them by using retransmissions
A more troublesome problem is how to recover from host crashes
In particular  it may be desirable for clients to be able to continue working when servers crash and quickly reboot
To illustrate the difficulty  let us assume that one host  the client  is sending a long file to another host  the file server  using a simple stop-and-wait protocol
The transport layer on the server just passes the incoming segments to the transport user  one by one
Partway through the transmission  the server crashes
When it comes back up  its tables are reinitialized  so it no longer knows precisely where it was
In an attempt to recover its previous status  the server might send a broadcast segment to all other hosts  announcing that it has just crashed and requesting that its clients inform it of the status of all open connections
Each client can be in one of two states: one segment outstanding  S  or no segments outstanding  S
Based on only this state information  the client must decide whether to retransmit the most recent segment
At first glance  it would seem obvious: the client should retransmit if and only if it has an unacknowledged segment outstanding (
is in state S ) when it learns of the crash
However  a closer inspection reveals difficulties with this naive approach
Consider  for example  the situation in which the serverâs transport entity first sends an acknowledgement and then  when the acknowledgement has been sent  writes to the application process
Writing a segment onto the output stream and sending an acknowledgement are two distinct events that cannot be done simultaneously
If a crash occurs after the acknowledgement has been sent but before the write has been fully completed  the client will receive the   ELEMENTS OF TRANSPORT PROTOCOLS acknowledgement and thus be in state S  when the crash recovery announcement arrives
The client will therefore not retransmit  (incorrectly) thinking that the segment has arrived
This decision by the client leads to a missing segment
At this point you may be thinking: ââThat problem can be solved easily
All you have to do is reprogram the transport entity to first do the write and then send the acknowledgement
ââ Try again
Imagine that the write has been done but the crash occurs before the acknowledgement can be sent
The client will be in state S  and thus retransmit  leading to an undetected duplicate segment in the output stream to the server application process
No matter how the client and server are programmed  there are always situations where the protocol fails to recover properly
The server can be programmed in one of two ways: acknowledge first or write first
The client can be programmed in one of four ways: always retransmit the last segment  never retransmit the last segment  retransmit only in state S  or retransmit only in state S
This gives eight combinations  but as we shall see  for each combination there is some set of events that makes the protocol fail
Three events are possible at the server: sending an acknowledgement (A)  writing to the output process (W)  and crashing (C)
The three events can occur in six different orderings: AC(W)  AWC  C(AW)  C(WA)  WAC  and WC(A)  where the parentheses are used to indicate that neither A nor W can follow C (
once it has crashed  it has crashed)
Figure  -  shows all eight combinations of client and server strategies and the valid event sequences for each one
Notice that for each strategy there is some sequence of events that causes the protocol to fail
For example  if the client always retransmits  the AWC event will generate an undetected duplicate  even though the other two events work properly
Always retransmit OK DUP OK LOST OK LOST OK DUP LOST LOST OK OK Never retransmit Retransmit in S  Retransmit in S  AC(W) Strategy used by sending host AWC First ACK  then write First write  then ACK C(AW) OK DUP DUP LOST OK OK LOST DUP OK OK OK DUP C(WA) W AC WC(A) OK = Protocol functions correctly DUP = Protocol generates a duplicate message LOST = Protocol loses a message Strategy used by receiving host Figure  -
Different combinations of client and server strategies
THE TRANSPORT LAYER
Making the protocol more elaborate does not help
Even if the client and server exchange several segments before the server attempts to write  so that the client knows exactly what is about to happen  the client has no way of knowing whether a crash occurred just before or just after the write
The conclusion is inescapable: under our ground rules of no simultaneous eventsâthat is  separate events happen one after another not at the same timeâhost crash and recovery cannot be made transparent to higher layers
Put in more general terms  this result can be restated as âârecovery from a layer N crash can only be done by layer N +  ââ and then only if the higher layer retains enough status information to reconstruct where it was before the problem occurred
This is consistent with the case mentioned above that the transport layer can recover from failures in the network layer  provided that each end of a connection keeps track of where it is
This problem gets us into the issue of what a so-called end-to-end acknowledgement really means
In principle  the transport protocol is end-to-end and not chained like the lower layers
Now consider the case of a user entering requests for transactions against a remote database
Suppose that the remote transport entity is programmed to first pass segments to the next layer up and then acknowledge
Even in this case  the receipt of an acknowledgement back at the userâs machine does not necessarily mean that the remote host stayed up long enough to actually update the database
A truly end-to-end acknowledgement  whose receipt means that the work has actually been done and lack thereof means that it has not  is probably impossible to achieve
This point is discussed in more detail by Saltzer et al
(   )  CONGESTION CONTROL If the transport entities on many machines send too many packets into the network too quickly  the network will become congested  with performance degraded as packets are delayed and lost
Controlling congestion to avoid this problem is the combined responsibility of the network and transport layers
Congestion occurs at routers  so it is detected at the network layer
However  congestion is ultimately caused by traffic sent into the network by the transport layer
The only effective way to control congestion is for the transport protocols to send packets into the network more slowly
we studied congestion control mechanisms in the network layer
In this tion  we will study the other half of the problem  congestion control mechanisms in the transport layer
After describing the goals of congestion control  we will describe how hosts can regulate the rate at which they send packets into the network
The Internet relies heavily on the transport layer for congestion control  and specific algorithms are built into TCP and other protocols
CONGESTION CONTROL    Desirable Bandwidth Allocation Before we describe how to regulate traffic  we must understand what we are trying to achieve by running a congestion control algorithm
That is  we must specify the state in which a good congestion control algorithm will operate the network
The goal is more than to simply avoid congestion
It is to find a good allocation of bandwidth to the transport entities that are using the network
A good allocation will deliver good performance because it uses all the available bandwidth but avoids congestion  it will be fair across competing transport entities  and it will quickly track changes in traffic demands
We will make each of these criteria more precise in turn
Efficiency and Power An efficient allocation of bandwidth across transport entities will use all of the network capacity that is available
However  it is not quite right to think that if there is a   -Mbps link  five transport entities should get   Mbps each
They should usually get less than   Mbps for good performance
The reason is that the traffic is often bursty
Recall that in   we described the goodput (or rate of useful packets arriving at the receiver) as a function of the offered load
This curve and a matching curve for the delay as a function of the offered load are given in Fig
Capacity (a) Offered load (packets/) Congestion collapse Offered load (packets/) Goodput (packets/) Desired response Delay (onds) (b) Onset of congestion Figure  -
(a) Goodput and (b) delay as a function of offered load
As the load increases in Fig
This falloff is because bursts of traffic can occasionally mount up and cause some losses at buffers inside the network
If the transport protocol is poorly designed and retransmits packets that have been delayed but not lost  the network can enter congestion collapse
In this state  senders are furiously sending packets  but increasingly little useful work is being accomplished
THE TRANSPORT LAYER
The corresponding delay is given in Fig
As the load approaches the capacity  the delay rises  slowly at first and then much more rapidly
This is again because of bursts of traffic that tend to mound up at high load
The delay cannot really go to infinity  except in a model in which the routers have infinite buffers
Instead  packets will be lost after experiencing the maximum buffering delay
For both goodput and delay  performance begins to degrade at the onset of congestion
Intuitively  we will obtain the best performance from the network if we allocate bandwidth up until the delay starts to climb rapidly
This point is below the capacity
To identify it  Kleinrock (   ) proposed the metric of power  where power = delay load Power will initially rise with offered load  as delay remains small and roughly constant  but will reach a maximum and fall as delay grows rapidly
The load with the highest power represents an efficient load for the transport entity to place on the network
Max-Min Fairness In the preceding discussion  we did not talk about how to divide bandwidth between different transport senders
This sounds like a simple question to answerâgive all the senders an equal fraction of the bandwidthâbut it involves several considerations
Perhaps the first consideration is to ask what this problem has to do with congestion control
After all  if the network gives a sender some amount of bandwidth to use  the sender should just use that much bandwidth
However  it is often the case that networks do not have a strict bandwidth reservation for each flow or connection
They may for some flows if quality of service is supported  but many connections will seek to use whatever bandwidth is available or be lumped together by the network under a common allocation
For example  IETFâs differentiated services separates traffic into two classes and connections compete for bandwidth within each class
IP routers often have all connections competing for the same bandwidth
In this situation  it is the congestion control mechanism that is allocating bandwidth to the competing connections
A ond consideration is what a fair portion means for flows in a network
It is simple enough if N flows use a single link  in which case they can all have  /N of the bandwidth (although efficiency will dictate that they use slightly less if the traffic is bursty)
But what happens if the flows have different  but overlapping  network paths? For example  one flow may cross three links  and the other flows may cross one link
The three-link flow consumes more network resources
It might be fairer in some sense to give it less bandwidth than the one-link flows
It   CONGESTION CONTROL should certainly be possible to support more one-link flows by reducing the bandwidth of the three-link flow
This point demonstrates an inherent tension between fairness and efficiency
However  we will adopt a notion of fairness that does not depend on the length of the network path
Even with this simple model  giving connections an equal fraction of bandwidth is a bit complicated because different connections will take different paths through the network and these paths will themselves have different capacities
In this case  it is possible for a flow to be bottlenecked on a downstream link and take a smaller portion of an upstream link than other flows; reducing the bandwidth of the other flows would slow them down but would not help the bottlenecked flow at all
The form of fairness that is often desired for network usage is max-min fairness
An allocation is max-min fair if the bandwidth given to one flow cannot be increased without decreasing the bandwidth given to another flow with an allocation that is no larger
That is  increasing the bandwidth of a flow will only make the situation worse for flows that are less well off
Let us see an example
A max-min fair allocation is shown for a network with four flows  A  B  C  and D  in Fig
Each of the links between routers has the same capacity  taken to be  unit  though in the general case the links will have different capacities
Three flows compete for the bottom-left link between routers R  and R
Each of these flows therefore gets  /  of the link
The remaining flow  A  competes with B on the link from R  to R
Since B has an allocation of  /  A gets the remaining  /  of the link
Notice that all of the other links have spare capacity
However  this capacity cannot be given to any of the flows without decreasing the capacity of another  lower flow
For example  if more of the bandwidth on the link between R  and R  is given to flow B  there will be less for flow A
This is reasonable as flow A already has more bandwidth
However  the capacity of flow C or D (or both) must be decreased to give more bandwidth to B  and these flows will have less bandwidth than B
Thus  the allocation is max-min fair
/  R  R  D C B A  /  /  /  /  /  /  D C B A R  R  R  R  /  /  Figure  -
Max-min bandwidth allocation for four flows
Max-min allocations can be computed given a global knowledge of the network
An intuitive way to think about them is to imagine that the rate for all of the THE TRANSPORT LAYER
flows starts at zero and is slowly increased
When the rate reaches a bottleneck for any flow  then that flow stops increasing
The other flows all continue to increase  sharing equally in the available capacity  until they too reach their respective bottlenecks
A third consideration is the level over which to consider fairness
A network could be fair at the level of connections  connections between a pair of hosts  or all connections per host
We examined this issue when we were discussing WFQ (Weighted Fair Queueing) in   and concluded that each of these definitions has its problems
For example  defining fairness per host means that a busy server will fare no better than a mobile phone  while defining fairness per connection encourages hosts to open more connections
Given that there is no clear answer  fairness is often considered per connection  but precise fairness is usually not a concern
It is more important in practice that no connection be starved of bandwidth than that all connections get precisely the same amount of bandwidth
In fact  with TCP it is possible to open multiple connections and compete for bandwidth more aggressively
This tactic is used by bandwidth-hungry applications such as BitTorrent for peer-to-peer file sharing
Convergence A final criterion is that the congestion control algorithm converge quickly to a fair and efficient allocation of bandwidth
The discussion of the desirable operating point above assumes a static network environment
However  connections are always coming and going in a network  and the bandwidth needed by a given connection will vary over time too  for example  as a user browses Web pages and occasionally downloads large videos
Because of the variation in demand  the ideal operating point for the network varies over time
A good congestion control algorithm should rapidly converge to the ideal operating point  and it should track that point as it changes over time
If the convergence is too slow  the algorithm will never be close to the changing operating point
If the algorithm is not stable  it may fail to converge to the right point in some cases  or even oscillate around the right point
An example of a bandwidth allocation that changes over time and converges quickly is shown in Fig
Initially  flow  has all of the bandwidth
One ond later  flow  starts
It needs bandwidth as well
The allocation quickly changes to give each of these flows half the bandwidth
At  onds  a third flow joins
However  this flow uses only  % of the bandwidth  which is less than its fair share (which is a third)
Flows  and  quickly adjust  dividing the available bandwidth to each have  % of the bandwidth
At  onds  the ond flow leaves  and the third flow remains unchanged
The first flow quickly captures  % of the bandwidth
At all times  the total allocated bandwidth is approximately   %  so that the network is fully used  and competing flows get equal treatment (but do not have to use more bandwidth than they need)
CONGESTION CONTROL Flow
Time (s) Bandwidth allocation  Flow  Flow  stops Flow  starts Figure  -
Changing bandwidth allocation over time
Regulating the Sending Rate Now it is time for the main course
How do we regulate the sending rates to obtain a desirable bandwidth allocation? The sending rate may be limited by two factors
The first is flow control  in the case that there is insufficient buffering at the receiver
The ond is congestion  in the case that there is insufficient capacity in the network
This is a flow-control limited situation
As long as the sender does not send more water than the bucket can contain  no water will be lost
If too much water comes in too fast  it will back up and some will be lost (in this case  by overflowing the funnel)
These cases may appear similar to the sender  as transmitting too fast causes packets to be lost
However  they have different causes and call for different solutions
We have already talked about a flow-control solution with a variable-sized window
Now we will consider a congestion control solution
Since either of these problems can occur  the transport protocol will in general need to run both solutions and slow down if either problem occurs
The way that a transport protocol should regulate the sending rate depends on the form of the feedback returned by the network
Different network layers may return different kinds of feedback
The feedback may be explicit or implicit  and it may be precise or imprecise
An example of an explicit  precise design is when routers tell the sources the rate at which they may send
Designs in the literature such as XCP (eXplicit Congestion Protocol) operate in this manner (Katabi et al
An explicit  imprecise design is the use of ECN (Explicit Congestion Notification) with TCP
In this design  routers set bits on packets that experience congestion to warn the senders to slow down  but they do not tell them how much to slow down
THE TRANSPORT LAYER
Transmission rate adjustment Transmission network Internal congestion Small-capacity receiver Large-capacity receiver (a) (b) Figure  -
(a) A fast network feeding a low-capacity receiver
(b) A slow network feeding a high-capacity receiver
In other designs  there is no explicit signal
FAST TCP measures the roundtrip delay and uses that metric as a signal to avoid congestion (Wei et al
Finally  in the form of congestion control most prevalent in the Internet today  TCP with drop-tail or RED routers  packet loss is inferred and used to signal that the network has become congested
There are many variants of this form of TCP  including CUBIC TCP  which is used in Linux (Ha et al
Combinations are also possible
For example  Windows includes Compound TCP that uses both packet loss and delay as feedback signals (Tan et al
These designs are summarized in Fig
If an explicit and precise signal is given  the transport entity can use that signal to adjust its rate to the new operating point
For example  if XCP tells senders the rate to use  the senders may simply use that rate
In the other cases  however  some guesswork is involved
In the absence of a congestion signal  the senders should decrease their rates
When a congestion signal is given  the senders should decrease their rates
The way in which the rates are increased or decreased is given by a control law
These laws have a major effect on performance
CONGESTION CONTROL Protocol Signal Explicit? Precise? XCP Rate to use Yes Yes TCP with ECN Congestion warning Yes No FAST TCP End-to-end delay No Yes Compound TCP Packet loss & end-to-end delay No Yes CUBIC TCP Packet loss No No TCP Packet loss No No Figure  -
Signals of some congestion control protocols
Chiu and Jain (   ) studied the case of binary congestion feedback and concluded that AIMD (Additive Increase Multiplicative Decrease) is the appropriate control law to arrive at the efficient and fair operating point
To argue this case  they constructed a graphical argument for the simple case of two connections competing for the bandwidth of a single link
The graph in Fig
When the allocation is fair  both users will receive the same amount of bandwidth
This is shown by the dotted fairness line
When the allocations sum to   %  the capacity of the link  the allocation is efficient
This is shown by the dotted efficiency line
A congestion signal is given by the network to both users when the sum of their allocations crosses this line
The intertion of these lines is the desired operating point  when both users have the same bandwidth and all of the network bandwidth is used
Additive increase and decrease User  âs bandwidth Fairness line Efficiency line Optimal point User  âs bandwidth  Multiplicative increase and decrease   %   % Figure  -
Additive and multiplicative bandwidth adjustments
Consider what happens from some starting allocation if both user  and user  additively increase their respective bandwidths over time
For example  the users may each increase their sending rate by  Mbps every ond
Eventually  the THE TRANSPORT LAYER
operating point crosses the efficiency line and both users receive a congestion signal from the network
At this stage  they must reduce their allocations
However  an additive decrease would simply cause them to oscillate along an additive line
This situation is shown in Fig
The behavior will keep the operating point close to efficient  but it will not necessarily be fair
Similarly  consider the case when both users multiplicatively increase their bandwidth over time until they receive a congestion signal
For example  the users may increase their sending rate by  % every ond
If they then multiplicatively decrease their sending rates  the operating point of the users will simply oscillate along a multiplicative line
This behavior is also shown in Fig
The multiplicative line has a different slope than the additive line
(It points to the origin  while the additive line has an angle of   degrees
) But it is otherwise no better
In neither case will the users converge to the optimal sending rates that are both fair and efficient
Now consider the case that the users additively increase their bandwidth allocations and then multiplicatively decrease them when congestion is signaled
This behavior is the AIMD control law  and it is shown in Fig
It can be seen that the path traced by this behavior does converge to the optimal point that is both fair and efficient
This convergence happens no matter what the starting point  making AIMD broadly useful
By the same argument  the only other combination  multiplicative increase and additive decrease  would diverge from the optimal point
Start User  âs bandwidth   % Fairness line Efficiency line Optimal point User  âs bandwidth = Additive increase (up at   ) = Multiplicative decrease (line points to origin) Legend:   % Figure  -
Additive Increase Multiplicative Decrease (AIMD) control law
AIMD is the control law that is used by TCP  based on this argument and another stability argument (that it is easy to drive the network into congestion and difficult to recover  so the increase policy should be gentle and the decrease policy aggressive)
It is not quite fair  since TCP connections adjust their window size by a given amount every round-trip time
Different connections will have different round-trip times
This leads to a bias in which connections to closer hosts receive more bandwidth than connections to distant hosts  all else being equal
CONGESTION CONTROL In   we will describe in detail how TCP implements an AIMD control law to adjust the sending rate and provide congestion control
This task is more difficult than it sounds because rates are measured over some interval and traffic is bursty
Instead of adjusting the rate directly  a strategy that is often used in practice is to adjust the size of a sliding window
TCP uses this strategy
If the window size is W and the round-trip time is RTT  the equivalent rate is W/RTT
This strategy is easy to combine with flow control  which already uses a window  and has the advantage that the sender paces packets using acknowledgements and hence slows down in one RTT if it stops receiving reports that packets are leaving the network
As a final issue  there may be many different transport protocols that send traffic into the network
What will happen if the different protocols compete with different control laws to avoid congestion? Unequal bandwidth allocations  that is what
Since TCP is the dominant form of congestion control in the Internet  there is significant community pressure for new transport protocols to be designed so that they compete fairly with it
The early streaming media protocols caused problems by excessively reducing TCP throughput because they did not compete fairly
This led to the notion of TCP-friendly congestion control in which TCP and non-TCP transport protocols can be freely mixed with no ill effects (Floyd et al
Wireless Issues Transport protocols such as TCP that implement congestion control should be independent of the underlying network and link layer technologies
That is a good theory  but in practice there are issues with wireless networks
The main issue is that packet loss is often used as a congestion signal  including by TCP as we have just discussed
Wireless networks lose packets all the time due to transmission errors
With the AIMD control law  high throughput requires very small levels of packet loss
Analyses by Padhye et al
(   ) show that the throughput goes up as the inverse square-root of the packet loss rate
What this means in practice is that the loss rate for fast TCP connections is very small;  % is a moderate loss rate  and by the time the loss rate reaches  % the connection has effectively stopped working
However  for wireless networks such as
LANs  frame loss rates of at least  % are common
This difference means that  absent protective measures  congestion control schemes that use packet loss as a signal will unnecessarily throttle connections that run over wireless links to very low rates
To function well  the only packet losses that the congestion control algorithm should observe are losses due to insufficient bandwidth  not losses due to transmission errors
One solution to this problem is to mask the wireless losses by using retransmissions over the wireless link
For example
uses a stopand- wait protocol to deliver each frame  retrying transmissions multiple times if THE TRANSPORT LAYER
need be before reporting a packet loss to the higher layer
In the normal case  each packet is delivered despite transient transmission errors that are not visible to the higher layers
There are two aspects to note
First  the sender does not necessarily know that the path includes a wireless link  since all it sees is the wired link to which it is attached
Internet paths are heterogeneous and there is no general method for the sender to tell what kind of links comprise the path
This complicates the congestion control problem  as there is no easy way to use one protocol for wireless links and another protocol for wired links
Wired link Sender Receiver Transport with end-to-end congestion control (loss = congestion) Link layer retransmission (loss = transmission error) Wireless link Figure  -
Congestion control over a path with a wireless link
The ond aspect is a puzzle
The figure shows two mechanisms that are driven by loss: link layer frame retransmissions  and transport layer congestion control
The puzzle is how these two mechanisms can co-exist without getting confused
After all  a loss should cause only one mechanism to take action because it is either a transmission error or a congestion signal
It cannot be both
If both mechanisms take action (by retransmitting the frame and slowing down the sending rate) then we are back to the original problem of transports that run far too slowly over wireless links
Consider this puzzle for a moment and see if you can solve it
The solution is that the two mechanisms act at different timescales
Link layer retransmissions happen on the order of microonds to millionds for wireless links such as    Loss timers in transport protocols fire on the order of millionds to onds
The difference is three orders of magnitude
This allows wireless links to detect frame losses and retransmit frames to repair transmission errors long before packet loss is inferred by the transport entity
The masking strategy is sufficient to let most transport protocols run well across most wireless links
However  it is not always a fitting solution
Some wireless links have long round-trip times  such as satellites
For these links other techniques must be used to mask loss  such as FEC (Forward Error Correction)  or the transport protocol must use a non-loss signal for congestion control
CONGESTION CONTROL A ond issue with congestion control over wireless links is variable capacity
That is  the capacity of a wireless link changes over time  sometimes abruptly  as nodes move and the signal-to-noise ratio varies with the changing channel conditions
This is unlike wired links whose capacity is fixed
The transport protocol must adapt to the changing capacity of wireless links  otherwise it will either congest the network or fail to use the available capacity
One possible solution to this problem is simply not to worry about it
This strategy is feasible because congestion control algorithms must already handle the case of new users entering the network or existing users changing their sending rates
Even though the capacity of wired links is fixed  the changing behavior of other users presents itself as variability in the bandwidth that is available to a given user
Thus it is possible to simply run TCP over a path with an
wireless link and obtain reasonable performance
However  when there is much wireless variability  transport protocols designed for wired links may have trouble keeping up and deliver poor performance
The solution in this case is a transport protocol that is designed for wireless links
A particularly challenging setting is a wireless mesh network in which multiple  interfering wireless links must be crossed  routes change due to mobility  and there is lots of loss
Research in this area is ongoing
See Li et al
(   ) for an example of wireless transport protocol design  THE INTERNET TRANSPORT PROTOCOLS: UDP The Internet has two main protocols in the transport layer  a connectionless protocol and a connection-oriented one
The protocols complement each other
The connectionless protocol is UDP
It does almost nothing beyond sending packets between applications  letting applications build their own protocols on top as needed
The connection-oriented protocol is TCP
It does almost everything
It makes connections and adds reliability with retransmissions  along with flow control and congestion control  all on behalf of the applications that use it
In the following tions  we will study UDP and TCP
We will start with UDP because it is simplest
We will also look at two uses of UDP
Since UDP is a transport layer protocol that typically runs in the operating system and protocols that use UDP typically run in user space  these uses might be considered applications
However  the techniques they use are useful for many applications and are better considered to belong to a transport service  so we will cover them here
Introduction to UDP The Internet protocol suite supports a connectionless transport protocol called UDP (User Datagram Protocol)
UDP provides a way for applications to send encapsulated IP datagrams without having to establish a connection
UDP is described in RFC
THE TRANSPORT LAYER
UDP transmits segments consisting of an  -byte header followed by the payload
The header is shown in Fig
The two ports serve to identify the endpoints within the source and destination machines
When a UDP packet arrives  its payload is handed to the process attached to the destination port
This attachment occurs when the BIND primitive or something similar is used  as we saw in Fig
Think of ports as mailboxes that applications can rent to receive packets
We will have more to say about them when we describe TCP  which also uses ports
In fact  the main value of UDP over just using raw IP is the addition of the source and destination ports
Without the port fields  the transport layer would not know what to do with each incoming packet
With them  it delivers the embedded segment to the correct application
Bits Source port UDP length Destination port UDP checksum Figure  -
The UDP header
The source port is primarily needed when a reply must be sent back to the source
By copying the Source port field from the incoming segment into the Destination port field of the outgoing segment  the process sending the reply can specify which process on the sending machine is to get it
The UDP length field includes the  -byte header and the data
The minimum length is  bytes  to cover the header
The maximum length is   bytes  which is lower than the largest number that will fit in   bits because of the size limit on IP packets
An optional Checksum is also provided for extra reliability
It checksums the header  the data  and a conceptual IP pseudoheader
When performing this computation  the Checksum field is set to zero and the data field is padded out with an additional zero byte if its length is an odd number
The checksum algorithm is simply to add up all the  -bit words in oneâs complement and to take the oneâs complement of the sum
As a consequence  when the receiver performs the calculation on the entire segment  including the Checksum field  the result should be
If the checksum is not computed  it is stored as a   since by a happy coincidence of oneâs complement arithmetic a true computed  is stored as all  s
However  turning it off is foolish unless the quality of the data does not matter (
for digitized speech)
The pseudoheader for the case of IPv  is shown in Fig
It contains the  -bit IPv  addresses of the source and destination machines  the protocol number for UDP (  )  and the byte count for the UDP segment (including the header)
It   THE INTERNET TRANSPORT PROTOCOLS: UDP is different but analogous for IPv
Including the pseudoheader in the UDP checksum computation helps detect misdelivered packets  but including it also violates the protocol hierarchy since the IP addresses in it belong to the IP layer  not to the UDP layer
TCP uses the same pseudoheader for its checksum
Bits Source address Destination address    Protocol =   UDP length Figure  -
The IPv  pseudoheader included in the UDP checksum
It is probably worth mentioning explicitly some of the things that UDP does not do
It does not do flow control  congestion control  or retransmission upon receipt of a bad segment
All of that is up to the user processes
What it does do is provide an interface to the IP protocol with the added feature of demultiplexing multiple processes using the ports and optional end-to-end error detection
That is all it does
For applications that need to have precise control over the packet flow  error control  or timing  UDP provides just what the doctor ordered
One area where it is especially useful is in client-server situations
Often  the client sends a short request to the server and expects a short reply back
If either the request or the reply is lost  the client can just time out and try again
Not only is the code simple  but fewer messages are required (one in each direction) than with a protocol requiring an initial setup like TCP
An application that uses UDP this way is DNS (Domain Name System)  which we will study in   In brief  a program that needs to look up the IP address of some host name  for example
can send a UDP packet containing the host name to a DNS server
The server replies with a UDP packet containing the hostâs IP address
No setup is needed in advance and no release is needed afterward
Just two messages go over the network
Remote Procedure Call In a certain sense  sending a message to a remote host and getting a reply back is a lot like making a function call in a programming language
In both cases  you start with one or more parameters and you get back a result
This observation has led people to try to arrange request-reply interactions on networks to be cast in the THE TRANSPORT LAYER
form of procedure calls
Such an arrangement makes network applications much easier to program and more familiar to deal with
For example  just imagine a procedure named get IP address (host name) that works by sending a UDP packet to a DNS server and waiting for the reply  timing out and trying again if one is not forthcoming quickly enough
In this way  all the details of networking can be hidden from the programmer
The key work in this area was done by Birrell and Nelson (   )
In a nutshell  what Birrell and Nelson suggested was allowing programs to call procedures located on remote hosts
When a process on machine  calls a procedure on machine   the calling process on  is suspended and execution of the called procedure takes place on
Information can be transported from the caller to the callee in the parameters and can come back in the procedure result
No message passing is visible to the application programmer
This technique is known as RPC (Remote Procedure Call) and has become the basis for many networking applications
Traditionally  the calling procedure is known as the client and the called procedure is known as the server  and we will use those names here too
The idea behind RPC is to make a remote procedure call look as much as possible like a local one
In the simplest form  to call a remote procedure  the client program must be bound with a small library procedure  called the client stub  that represents the server procedure in the clientâs address space
Similarly  the server is bound with a procedure called the server stub
These procedures hide the fact that the procedure call from the client to the server is not local
The actual steps in making an RPC are shown in Fig
Step  is the client calling the client stub
This call is a local procedure call  with the parameters pushed onto the stack in the normal way
Step  is the client stub packing the parameters into a message and making a system call to send the message
Packing the parameters is called marshaling
Step  is the operating system sending the message from the client machine to the server machine
Step  is the operating system passing the incoming packet to the server stub
Finally  step  is the server stub calling the server procedure with the unmarshaled parameters
The reply traces the same path in the other direction
The key item to note here is that the client procedure  written by the user  just makes a normal (
local) procedure call to the client stub  which has the same name as the server procedure
Since the client procedure and client stub are in the same address space  the parameters are passed in the usual way
Similarly  the server procedure is called by a procedure in its address space with the parameters it expects
To the server procedure  nothing is unusual
In this way  instead of I/O being done on sockets  network communication is done by faking a normal procedure call
Despite the conceptual elegance of RPC  there are a few snakes hiding under the grass
A big one is the use of pointer parameters
Normally  passing a pointer to a procedure is not a problem
The called procedure can use the pointer in the same way the caller can because both procedures live in the same virtual address   THE INTERNET TRANSPORT PROTOCOLS: UDP Client CPU Client stub Client Operating system Server CPU Server stub  Operating system Server Network Figure  -
Steps in making a remote procedure call
The stubs are shaded
With RPC  passing pointers is impossible because the client and server are in different address spaces
In some cases  tricks can be used to make it possible to pass pointers
Suppose that the first parameter is a pointer to an integer  k
The client stub can marshal k and send it along to the server
The server stub then creates a pointer to k and passes it to the server procedure  just as it expects
When the server procedure returns control to the server stub  the latter sends k back to the client  where the new k is copied over the old one  just in case the server changed it
In effect  the standard calling sequence of call-by-reference has been replaced by call-bycopy- restore
Unfortunately  this trick does not always work  for example  if the pointer points to a graph or other complex data structure
For this reason  some restrictions must be placed on parameters to procedures called remotely  as we shall see
A ond problem is that in weakly typed languages  like C  it is perfectly legal to write a procedure that computes the inner product of two vectors (arrays)  without specifying how large either one is
Each could be terminated by a special value known only to the calling and called procedures
Under these circumstances  it is essentially impossible for the client stub to marshal the parameters: it has no way of determining how large they are
A third problem is that it is not always possible to deduce the types of the parameters  not even from a formal specification or the code itself
An example is printf  which may have any number of parameters (at least one)  and the parameters can be an arbitrary mixture of integers  shorts  longs  characters  strings  floating- point numbers of various lengths  and other types
Trying to call printf as a remote procedure would be practically impossible because C is so permissive
However  a rule saying that RPC can be used provided that you do not program in C (or C++) would not be popular with a lot of programmers
THE TRANSPORT LAYER
A fourth problem relates to the use of global variables
Normally  the calling and called procedure can communicate by using global variables  in addition to communicating via parameters
But if the called procedure is moved to a remote machine  the code will fail because the global variables are no longer shared
These problems are not meant to suggest that RPC is hopeless
In fact  it is widely used  but some restrictions are needed to make it work well in practice
In terms of transport layer protocols  UDP is a good base on which to implement RPC
Both requests and replies may be sent as a single UDP packet in the simplest case and the operation can be fast
However  an implementation must include other machinery as well
Because the request or the reply may be lost  the client must keep a timer to retransmit the request
Note that a reply serves as an implicit acknowledgement for a request  so the request need not be separately acknowledged
Sometimes the parameters or results may be larger than the maximum UDP packet size  in which case some protocol is needed to deliver large messages
If multiple requests and replies can overlap (as in the case of concurrent programming)  an identifier is needed to match the request with the reply
A higher-level concern is that the operation may not be idempotent (
safe to repeat)
The simple case is idempotent operations such as DNS requests and replies
The client can safely retransmit these requests again and again if no replies are forthcoming
It does not matter whether the server never received the request  or it was the reply that was lost
The answer  when it finally arrives  will be the same (assuming the DNS database is not updated in the meantime)
However  not all operations are idempotent  for example  because they have important side-effects such as incrementing a counter
RPC for these operations requires stronger semantics so that when the programmer calls a procedure it is not executed multiple times
In this case  it may be necessary to set up a TCP connection and send the request over it rather than using UDP
Real-Time Transport Protocols Client-server RPC is one area in which UDP is widely used
Another one is for real-time multimedia applications
In particular  as Internet radio  Internet telephony  music-on-demand  videoconferencing  video-on-demand  and other multimedia applications became more commonplace  people have discovered that each application was reinventing more or less the same real-time transport protocol
It gradually became clear that having a generic real-time transport protocol for multiple applications would be a good idea
Thus was RTP (Real-time Transport Protocol) born
It is described in RFC  and is now in widespread use for multimedia applications
We will describe two aspects of real-time transport
The first is the RTP protocol for transporting audio and video data in packets
The ond is the processing that takes place  mostly at the receiver  to play out the audio and video at the right time
These functions fit into the protocol stack as shown in Fig
THE INTERNET TRANSPORT PROTOCOLS: UDP Multimedia application RTP Socket interface UDP IP Ethernet (a) (b) Ethernet header IP header UDP header RTP header RTP payload UDP payload IP payload Ethernet payload User space OS Kernel Figure  -
(a) The position of RTP in the protocol stack
(b) Packet nesting
RTP normally runs in user space over UDP (in the operating system)
It operates as follows
The multimedia application consists of multiple audio  video  text  and possibly other streams
These are fed into the RTP library  which is in user space along with the application
This library multiplexes the streams and encodes them in RTP packets  which it stuffs into a socket
On the operating system side of the socket  UDP packets are generated to wrap the RTP packets and handed to IP for transmission over a link such as Ethernet
The reverse process happens at the receiver
The multimedia application eventually receives multimedia data from the RTP library
It is responsible for playing out the media
The protocol stack for this situation is shown in Fig
The packet nesting is shown in Fig
As a consequence of this design  it is a little hard to say which layer RTP is in
Since it runs in user space and is linked to the application program  it certainly looks like an application protocol
On the other hand  it is a generic  applicationindependent protocol that just provides transport facilities  so it also looks like a transport protocol
Probably the best description is that it is a transport protocol that just happens to be implemented in the application layer  which is why we are covering it in this  ter
RTPâThe Real-time Transport Protocol The basic function of RTP is to multiplex several real-time data streams onto a single stream of UDP packets
The UDP stream can be sent to a single destination (unicasting) or to multiple destinations (multicasting)
Because RTP just uses normal UDP  its packets are not treated specially by the routers unless some normal IP quality-of-service features are enabled
In particular  there are no special guarantees about delivery  and packets may be lost  delayed  corrupted  etc
The RTP format contains several features to help receivers work with multimedia information
Each packet sent in an RTP stream is given a number one THE TRANSPORT LAYER
higher than its predecessor
This numbering allows the destination to determine if any packets are missing
If a packet is missing  the best action for the destination to take is up to the application
It may be to skip a video frame if the packets are carrying video data  or to approximate the missing value by interpolation if the packets are carrying audio data
Retransmission is not a practical option since the retransmitted packet would probably arrive too late to be useful
As a consequence  RTP has no acknowledgements  and no mechanism to request retransmissions
Each RTP payload may contain multiple samples  and they may be coded any way that the application wants
To allow for interworking  RTP defines several profiles (
a single audio stream)  and for each profile  multiple encoding formats may be allowed
For example  a single audio stream may be encoded as  - bit PCM samples at  kHz using delta encoding  predictive encoding  GSM encoding  MP  encoding  and so on
RTP provides a header field in which the source can specify the encoding but is otherwise not involved in how encoding is done
Another facility many real-time applications need is timestamping
The idea here is to allow the source to associate a timestamp with the first sample in each packet
The timestamps are relative to the start of the stream  so only the differences between timestamps are significant
The absolute values have no meaning
As we will describe shortly  this mechanism allows the destination to do a small amount of buffering and play each sample the right number of millionds after the start of the stream  independently of when the packet containing the sample arrived
Not only does timestamping reduce the effects of variation in network delay  but it also allows multiple streams to be synchronized with each other
For example  a digital television program might have a video stream and two audio streams
The two audio streams could be for stereo broadcasts or for handling films with an original language soundtrack and a soundtrack dubbed into the local language  giving the viewer a choice
Each stream comes from a different physical device  but if they are timestamped from a single counter  they can be played back synchronously  even if the streams are transmitted and/or received somewhat erratically
The RTP header is illustrated in Fig
It consists of three  -bit words and potentially some extensions
The first word contains the Version field  which is already at
Let us hope this version is very close to the ultimate version since there is only one code point left (although  could be defined as meaning that the real version was in an extension word)
The P bit indicates that the packet has been padded to a multiple of  bytes
The last padding byte tells how many bytes were added
The X bit indicates that an extension header is present
The format and meaning of the extension header are not defined
The only thing that is defined is that the first word of the extension gives the length
This is an escape hatch for any unforeseen requirements
THE INTERNET TRANSPORT PROTOCOLS: UDP   bits Ver
P X M Payload type Sequence number Timestamp Synchronization source identifier Contributing source identifier CC Figure  -
The RTP header
The CC field tells how many contributing sources are present  from  to   (see below)
The M bit is an application-specific marker bit
It can be used to mark the start of a video frame  the start of a word in an audio channel  or something else that the application understands
The Payload type field tells which encoding algorithm has been used (
uncompressed  -bit audio  MP  etc
Since every packet carries this field  the encoding can change during transmission
The Sequence number is just a counter that is incremented on each RTP packet sent
It is used to detect lost packets
The Timestamp is produced by the streamâs source to note when the first sample in the packet was made
This value can help reduce timing variability called jitter at the receiver by decoupling the playback from the packet arrival time
The Synchronization source identifier tells which stream the packet belongs to
It is the method used to multiplex and demultiplex multiple data streams onto a single stream of UDP packets
Finally  the Contributing source identifiers  if any  are used when mixers are present in the studio
In that case  the mixer is the synchronizing source  and the streams being mixed are listed here
RTCPâThe Real-time Transport Control Protocol RTP has a little sister protocol (little sibling protocol?) called RTCP (Realtime Transport Control Protocol)
It is defined along with RTP in RFC  and handles feedback  synchronization  and the user interface
It does not transport any media samples
The first function can be used to provide feedback on delay  variation in delay or jitter  bandwidth  congestion  and other network properties to the sources
This information can be used by the encoding process to increase the data rate (and give better quality) when the network is functioning well and to cut back the data THE TRANSPORT LAYER
rate when there is trouble in the network
By providing continuous feedback  the encoding algorithms can be continuously adapted to provide the best quality possible under the current circumstances
For example  if the bandwidth increases or decreases during the transmission  the encoding may switch from MP  to  -bit PCM to delta encoding as required
The Payload type field is used to tell the destination what encoding algorithm is used for the current packet  making it possible to vary it on demand
An issue with providing feedback is that the RTCP reports are sent to all participants
For a multicast application with a large group  the bandwidth used by RTCP would quickly grow large
To prevent this from happening  RTCP senders scale down the rate of their reports to collectively consume no more than  say  % of the media bandwidth
To do this  each participant needs to know the media bandwidth  which it learns from the sender  and the number of participants  which it estimates by listening to other RTCP reports
RTCP also handles interstream synchronization
The problem is that different streams may use different clocks  with different granularities and different drift rates
RTCP can be used to keep them in sync
Finally  RTCP provides a way for naming the various sources (
in ASCII text)
This information can be displayed on the receiverâs screen to indicate who is talking at the moment
More information about RTP can be found in Perkins (   )
Playout with Buffering and Jitter Control Once the media information reaches the receiver  it must be played out at the right time
In general  this will not be the time at which the RTP packet arrived at the receiver because packets will take slightly different amounts of time to transit the network
Even if the packets are injected with exactly the right intervals between them at the sender  they will reach the receiver with different relative times
This variation in delay is called jitter
Even a small amount of packet jitter can cause distracting media artifacts  such as jerky video frames and unintelligible audio  if the media is simply played out as it arrives
The solution to this problem is to buffer packets at the receiver before they are played out to reduce the jitter
As an example  in Fig
Packet  is sent from the server at t =   and arrives at the client at t =
Packet  undergoes more delay and takes   to arrive
As the packets arrive  they are buffered on the client machine
At t =     playback begins
At this time  packets  through  have been buffered so that they can be removed from the buffer at uniform intervals for smooth play
In the general case  it is not necessary to use uniform intervals because the RTP timestamps tell when the media should be played
THE INTERNET TRANSPORT PROTOCOLS: UDP      Time () Time in buffer  Gap in playback  Packet removed from buffer Packet arrives at buffer  Packet departs source    Figure  -
Smoothing the output stream by buffering packets
Unfortunately  we can see that packet  has been delayed so much that it is not available when its play slot comes up
There are two options
Packet  can be skipped and the player can move on to subsequent packets
Alternatively  playback can stop until packet  arrives  creating an annoying gap in the music or movie
In a live media application like a voice-over-IP call  the packet will typically be skipped
Live applications do not work well on hold
In a streaming media application  the player might pause
This problem can be alleviated by delaying the starting time even more  by using a larger buffer
For a streaming audio or video player  buffers of about   onds are often used to ensure that the player receives all of the packets (that are not dropped in the network) in time
For live applications like videoconferencing  short buffers are needed for responsiveness
A key consideration for smooth playout is the playback point  or how long to wait at the receiver for media before playing it out
Deciding how long to wait depends on the jitter
The difference between a low-jitter and high-jitter connection is shown in Fig
The average delay may not differ greatly between the two  but if there is high jitter the playback point may need to be much further out to capture  % of the packets than if there is low jitter
To pick a good playback point  the application can measure the jitter by looking at the difference between the RTP timestamps and the arrival time
Each difference gives a sample of the delay (plus an arbitrary  fixed offset)
However  the delay can change over time due to other  competing traffic and changing routes
To accommodate this change  applications can adapt their playback point while they are running
However  if not done well  changing the playback point can produce an observable glitch to the user
One way to avoid this problem for audio is to adapt the playback point between talkspurts  in the gaps in a conversation
No one will notice the difference between a short and slightly longer silence
RTP lets applications set the M marker bit to indicate the start of a new talkspurt for this purpose
If the absolute delay until media is played out is too long  live applications will suffer
Nothing can be done to reduce the propagation delay if a direct path is THE TRANSPORT LAYER
High jitter Low jitter Minimum delay (due to speed of light) Delay (a) Fraction of packets Fraction of packets Delay (b) Figure  -
(a) High jitter
(b) Low jitter
already being used
The playback point can be pulled in by simply accepting that a larger fraction of packets will arrive too late to be played
If this is not acceptable  the only way to pull in the playback point is to reduce the jitter by using a better quality of service  for example  the expedited forwarding differentiated service
That is  a better network is needed  THE INTERNET TRANSPORT PROTOCOLS: TCP UDP is a simple protocol and it has some very important uses  such as clientserver interactions and multimedia  but for most Internet applications  reliable  sequenced delivery is needed
UDP cannot provide this  so another protocol is required
It is called TCP and is the main workhorse of the Internet
Let us now study it in detail
Introduction to TCP TCP (Transmission Control Protocol) was specifically designed to provide a reliable end-to-end byte stream over an unreliable internetwork
An internetwork differs from a single network because different parts may have wildly different topologies  bandwidths  delays  packet sizes  and other parameters
TCP was designed to dynamically adapt to properties of the internetwork and to be robust in the face of many kinds of failures
TCP was formally defined in RFC in September
As time went on  many improvements have been made  and various errors and inconsistencies have been fixed
To give you a sense of the extent of TCP  the important RFCs are   THE INTERNET TRANSPORT PROTOCOLS: TCP now RFC plus: clarifications and bug fixes in RFC ; extensions for high-performance in RFC ; selective acknowledgements in RFC ; congestion control in RFC ; repurposing of header fields for quality of service in RFC ; improved retransmission timers in RFC ; and explicit congestion notification in RFC
The full collection is even larger  which led to a guide to the many RFCs  published of course as another RFC document  RFC
Each machine supporting TCP has a TCP transport entity  either a library procedure  a user process  or most commonly part of the kernel
In all cases  it manages TCP streams and interfaces to the IP layer
A TCP entity accepts user data streams from local processes  breaks them up into pieces not exceeding   KB (in practice  often  data bytes in order to fit in a single Ethernet frame with the IP and TCP headers)  and sends each piece as a separate IP datagram
When datagrams containing TCP data arrive at a machine  they are given to the TCP entity  which reconstructs the original byte streams
For simplicity  we will sometimes use just ââTCPââ to mean the TCP transport entity (a piece of software) or the TCP protocol (a set of rules)
From the context it will be clear which is meant
For example  in ââThe user gives TCP the data ââ the TCP transport entity is clearly intended
The IP layer gives no guarantee that datagrams will be delivered properly  nor any indication of how fast datagrams may be sent
It is up to TCP to send datagrams fast enough to make use of the capacity but not cause congestion  and to time out and retransmit any datagrams that are not delivered
Datagrams that do arrive may well do so in the wrong order; it is also up to TCP to reassemble them into messages in the proper sequence
In short  TCP must furnish good performance with the reliability that most applications want and that IP does not provide
The TCP Service Model TCP service is obtained by both the sender and the receiver creating end points  called sockets  as discussed in
Each socket has a socket number (address) consisting of the IP address of the host and a  -bit number local to that host  called a port
A port is the TCP name for a TSAP
For TCP service to be obtained  a connection must be explicitly established between a socket on one machine and a socket on another machine
The socket calls are listed in Fig
A socket may be used for multiple connections at the same time
In other words  two or more connections may terminate at the same socket
Connections are identified by the socket identifiers at both ends  that is  (socket  socket )
No virtual circuit numbers or other identifiers are used
Port numbers below  are reserved for standard services that can usually only be started by privileged users (
root in UNIX systems)
They are called well-known ports
For example  any process wishing to remotely retrieve mail from a host can connect to the destination hostâs port to contact its IMAP THE TRANSPORT LAYER
The list of well-known ports is given at
Over have been assigned
A few of the better-known ones are listed in Fig
Port Protocol Use   FTP File transfer   SSH Remote login  replacement for Telnet   SMTP Email   HTTP World Wide Web POP-  Remote email access IMAP Remote email access HTTPS ure Web (HTTP over SSL/TLS) RTSP Media player control IPP Printer sharing Figure  -
Some assigned ports
Other ports from  through  can be registered with IANA for use by unprivileged users  but applications can and do choose their own ports
For example  the BitTorrent peer-to-peer file-sharing application (unofficially) uses ports â  but may run on other ports as well
It would certainly be possible to have the FTP daemon attach itself to port   at boot time  the SSH daemon attach itself to port   at boot time  and so on
However  doing so would clutter up memory with daemons that were idle most of the time
Instead  what is commonly done is to have a single daemon  called inetd (Internet daemon) in UNIX  attach itself to multiple ports and wait for the first incoming connection
When that occurs  inetd forks off a new process and executes the appropriate daemon in it  letting that daemon handle the request
In this way  the daemons other than inetd are only active when there is work for them to do
Inetd learns which ports it is to use from a configuration file
Consequently  the system administrator can set up the system to have permanent daemons on the busiest ports (
port  ) and inetd on the rest
All TCP connections are full duplex and point-to-point
Full duplex means that traffic can go in both directions at the same time
Point-to-point means that each connection has exactly two end points
TCP does not support multicasting or broadcasting
A TCP connection is a byte stream  not a message stream
Message boundaries are not preserved end to end
For example  if the sending process does four   -byte writes to a TCP stream  these data may be delivered to the receiving process as four   -byte chunks  two -byte chunks  one -byte chunk (see Fig
-  )  or some other way
There is no way for the receiver to detect the unit(s) in which the data were written  no matter how hard it tries
THE INTERNET TRANSPORT PROTOCOLS: TCP A B C D AB C D IP header TCP header (a) (b) Figure  -
(a) Four   -byte segments sent as separate IP datagrams
(b) The  bytes of data delivered to the application in a single READ call
Files in UNIX have this property too
The reader of a file cannot tell whether the file was written a block at a time  a byte at a time  or all in one blow
As with a UNIX file  the TCP software has no idea of what the bytes mean and no interest in finding out
A byte is just a byte
When an application passes data to TCP  TCP may send it immediately or buffer it (in order to collect a larger amount to send at once)  at its discretion
However  sometimes the application really wants the data to be sent immediately
For example  suppose a user of an interactive game wants to send a stream of updates
It is essential that the updates be sent immediately  not buffered until there is a collection of them
To force data out  TCP has the notion of a PUSH flag that is carried on packets
The original intent was to let applications tell TCP implementations via the PUSH flag not to delay the transmission
However  applications cannot literally set the PUSH flag when they send data
Instead  different operating systems have evolved different options to expedite transmission (
TCP NODELAY in Windows and Linux)
For Internet archaeologists  we will also mention one interesting feature of TCP service that remains in the protocol but is rarely used: urgent data
When an application has high priority data that should be processed immediately  for example  if an interactive user hits the CTRL-C key to break off a remote computation that has already begun  the sending application can put some control information in the data stream and give it to TCP along with the URGENT flag
This event causes TCP to stop accumulating data and transmit everything it has for that connection immediately
When the urgent data are received at the destination  the receiving application is interrupted (
given a signal in UNIX terms) so it can stop whatever it was doing and read the data stream to find the urgent data
The end of the urgent data is marked so the application knows when it is over
The start of the urgent data is not marked
It is up to the application to figure that out
This scheme provides a crude signaling mechanism and leaves everything else up to the application
However  while urgent data is potentially useful  it found no compelling application early on and fell into disuse
Its use is now discouraged because of implementation differences  leaving applications to handle their own signaling
Perhaps future transport protocols will provide better signaling
THE TRANSPORT LAYER
The TCP Protocol In this tion  we will give a general overview of the TCP protocol
In the next one  we will go over the protocol header  field by field
A key feature of TCP  and one that dominates the protocol design  is that every byte on a TCP connection has its own  -bit sequence number
When the Internet began  the lines between routers were mostly  -kbps leased lines  so a host blasting away at full speed took over  week to cycle through the sequence numbers
At modern network speeds  the sequence numbers can be consumed at an alarming rate  as we will see later
Separate  -bit sequence numbers are carried on packets for the sliding window position in one direction and for acknowledgements in the reverse direction  as discussed below
The sending and receiving TCP entities exchange data in the form of segments
A TCP segment consists of a fixed  -byte header (plus an optional part) followed by zero or more data bytes
The TCP software decides how big segments should be
It can accumulate data from several writes into one segment or can split data from one write over multiple segments
Two limits restrict the segment size
First  each segment  including the TCP header  must fit in the  - byte IP payload
ond  each link has an MTU (Maximum Transfer Unit)
Each segment must fit in the MTU at the sender and receiver so that it can be sent and received in a single  unfragmented packet
In practice  the MTU is generally  bytes (the Ethernet payload size) and thus defines the upper bound on segment size
However  it is still possible for IP packets carrying TCP segments to be fragmented when passing over a network path for which some link has a small MTU
If this happens  it degrades performance and causes other problems (Kent and Mogul  )
Instead  modern TCP implementations perform path MTU discovery by using the technique outlined in RFC  that we described in
This technique uses ICMP error messages to find the smallest MTU for any link on the path
TCP then adjusts the segment size downwards to avoid fragmentation
The basic protocol used by TCP entities is the sliding window protocol with a dynamic window size
When a sender transmits a segment  it also starts a timer
When the segment arrives at the destination  the receiving TCP entity sends back a segment (with data if any exist  and otherwise without) bearing an acknowledgement number equal to the next sequence number it expects to receive and the remaining window size
If the senderâs timer goes off before the acknowledgement is received  the sender transmits the segment again
Although this protocol sounds simple  there are many sometimes subtle ins and outs  which we will cover below
Segments can arrive out of order  so bytes â can arrive but cannot be acknowledged because bytes â have not turned up yet
Segments can also be delayed so long in transit that the sender times out and retransmits them
The retransmissions may include different byte   THE INTERNET TRANSPORT PROTOCOLS: TCP ranges than the original transmission  requiring careful administration to keep track of which bytes have been correctly received so far
However  since each byte in the stream has its own unique offset  it can be done
TCP must be prepared to deal with these problems and solve them in an efficient way
A considerable amount of effort has gone into optimizing the performance of TCP streams  even in the face of network problems
A number of the algorithms used by many TCP implementations will be discussed below
The TCP Segment Header Figure  -  shows the layout of a TCP segment
Every segment begins with a fixed-format   -byte header
The fixed header may be followed by header options
After the options  if any  up to   â   â   =   data bytes may follow  where the first   refer to the IP header and the ond to the TCP header
Segments without any data are legal and are commonly used for acknowledgements and control messages
Bits Source port Destination port Sequence number Acknowledgement number TCP header length URG ACK PSH RST SYN FIN Window size Checksum Urgent pointer Options (  or more  -bit words) Data (optional) ECE CWR Figure  -
The TCP header
Let us dist the TCP header field by field
The Source port and Destination port fields identify the local end points of the connection
A TCP port plus its hostâs IP address forms a  -bit unique end point
The source and destination end points together identify the connection
This connection identifier is called a  tuple because it consists of five pieces of information: the protocol (TCP)  source IP and source port  and destination IP and destination port
THE TRANSPORT LAYER
The Sequence number and Acknowledgement number fields perform their usual functions
Note that the latter specifies the next in-order byte expected  not the last byte correctly received
It is a cumulative acknowledgement because it summarizes the received data with a single number
It does not go beyond lost data
Both are   bits because every byte of data is numbered in a TCP stream
The TCP header length tells how many  -bit words are contained in the TCP header
This information is needed because the Options field is of variable length  so the header is  too
Technically  this field really indicates the start of the data within the segment  measured in  -bit words  but that number is just the header length in words  so the effect is the same
Next comes a  -bit field that is not used
The fact that these bits have remained unused for   years (as only  of the original reserved  bits have been reclaimed) is testimony to how well thought out TCP is
Lesser protocols would have needed these bits to fix bugs in the original design
Now come eight  -bit flags
CWR and ECE are used to signal congestion when ECN (Explicit Congestion Notification) is used  as specified in RFC
ECE is set to signal an ECN-Echo to a TCP sender to tell it to slow down when the TCP receiver gets a congestion indication from the network
CWR is set to signal Congestion Window Reduced from the TCP sender to the TCP receiver so that it knows the sender has slowed down and can stop sending the ECN-Echo
We discuss the role of ECN in TCP congestion control in
URG is set to  if the Urgent pointer is in use
The Urgent pointer is used to indicate a byte offset from the current sequence number at which urgent data are to be found
This facility is in lieu of interrupt messages
As we mentioned above  this facility is a bare-bones way of allowing the sender to signal the receiver without getting TCP itself involved in the reason for the interrupt  but it is seldom used
The ACK bit is set to  to indicate that the Acknowledgement number is valid
This is the case for nearly all packets
If ACK is   the segment does not contain an acknowledgement  so the Acknowledgement number field is ignored
The PSH bit indicates PUSHed data
The receiver is hereby kindly requested to deliver the data to the application upon arrival and not buffer it until a full buffer has been received (which it might otherwise do for efficiency)
The RST bit is used to abruptly reset a connection that has become confused due to a host crash or some other reason
It is also used to reject an invalid segment or refuse an attempt to open a connection
In general  if you get a segment with the RST bit on  you have a problem on your hands
The SYN bit is used to establish connections
The connection request has SYN =  and ACK =  to indicate that the piggyback acknowledgement field is not in use
The connection reply does bear an acknowledgement  however  so it has SYN =  and ACK =
In essence  the SYN bit is used to denote both CONNECTION REQUEST and CONNECTION ACCEPTED  with the ACK bit used to distinguish between those two possibilities
THE INTERNET TRANSPORT PROTOCOLS: TCP The FIN bit is used to release a connection
It specifies that the sender has no more data to transmit
However  after closing a connection  the closing process may continue to receive data indefinitely
Both SYN and FIN segments have sequence numbers and are thus guaranteed to be processed in the correct order
Flow control in TCP is handled using a variable-sized sliding window
The Window size field tells how many bytes may be sent starting at the byte acknowledged
A Window size field of  is legal and says that the bytes up to and including Acknowledgement number â  have been received  but that the receiver has not had a chance to consume the data and would like no more data for the moment  thank you
The receiver can later grant permission to send by transmitting a segment with the same Acknowledgement number and a nonzero Window size field
In the protocols of
acknowledgements of frames received and permission to send new frames were tied together
This was a consequence of a fixed window size for each protocol
In TCP  acknowledgements and permission to send additional data are completely decoupled
In effect  a receiver can say: ââI have received bytes up through k but I do not want any more just now  thank you
ââ This decoupling (in fact  a variable-sized window) gives additional flexibility
We will study it in detail below
A Checksum is also provided for extra reliability
It checksums the header  the data  and a conceptual pseudoheader in exactly the same way as UDP  except that the pseudoheader has the protocol number for TCP ( ) and the checksum is mandatory
Please see
for details
The Options field provides a way to add extra facilities not covered by the regular header
Many options have been defined and several are commonly used
The options are of variable length  fill a multiple of   bits by using padding with zeros  and may extend to   bytes to accommodate the longest TCP header that can be specified
Some options are carried when a connection is established to negotiate or inform the other side of capabilities
Other options are carried on packets during the lifetime of the connection
Each option has a Type-Length-Value encoding
A widely used option is the one that allows each host to specify the MSS (Maximum Segment Size) it is willing to accept
Using large segments is more efficient than using small ones because the  -byte header can be amortized over more data  but small hosts may not be able to handle big segments
During connection setup  each side can announce its maximum and see its partnerâs
If a host does not use this option  it defaults to a   -byte payload
All Internet hosts are required to accept TCP segments of +   = bytes
The maximum segment size in the two directions need not be the same
For lines with high bandwidth  high delay  or both  the  -KB window corresponding to a  -bit field is a problem
For example  on an OC-  line (of roughly Mbps)  it takes less than  m to output a full  -KB window
If the round-trip propagation delay is   m (which is typical for a transcontinental THE TRANSPORT LAYER
fiber)  the sender will be idle more than  % of the time waiting for acknowledgements
A larger window size would allow the sender to keep pumping data out
The window scale option allows the sender and receiver to negotiate a window scale factor at the start of a connection
Both sides use the scale factor to shift the Window size field up to   bits to the left  thus allowing windows of up to bytes
Most TCP implementations support this option
The timestamp option carries a timestamp sent by the sender and echoed by the receiver
It is included in every packet  once its use is established during connection setup  and used to compute round-trip time samples that are used to estimate when a packet has been lost
It is also used as a logical extension of the  - bit sequence number
On a fast connection  the sequence number may wrap around quickly  leading to possible confusion between old and new data
The PAWS (Protection Against Wrapped Sequence numbers) scheme discards arriving segments with old timestamps to prevent this problem
Finally  the SACK (Selective ACKnowledgement) option lets a receiver tell a sender the ranges of sequence numbers that it has received
It supplements the Acknowledgement number and is used after a packet has been lost but subsequent (or duplicate) data has arrived
The new data is not reflected by the Acknowledgement number field in the header because that field gives only the next in-order byte that is expected
With SACK  the sender is explicitly aware of what data the receiver has and hence can determine what data should be retransmitted
SACK is defined in RFC  and RFC  and is increasingly used
We describe the use of SACK along with congestion control in
TCP Connection Establishment Connections are established in TCP by means of the three-way handshake discussed in
To establish a connection  one side  say  the server  passively waits for an incoming connection by executing the LISTEN and ACCEPT primitives in that order  either specifying a specific source or nobody in particular
The other side  say  the client  executes a CONNECT primitive  specifying the IP address and port to which it wants to connect  the maximum TCP segment size it is willing to accept  and optionally some user data (
a password)
The CONNECT primitive sends a TCP segment with the SYN bit on and ACK bit off and waits for a response
When this segment arrives at the destination  the TCP entity there checks to see if there is a process that has done a LISTEN on the port given in the Destination port field
If not  it sends a reply with the RST bit on to reject the connection
If some process is listening to the port  that process is given the incoming TCP segment
It can either accept or reject the connection
If it accepts  an acknowledgement segment is sent back
The sequence of TCP segments sent in the normal case is shown in Fig
Note that a SYN segment consumes  byte of sequence space so that it can be acknowledged unambiguously
THE INTERNET TRANSPORT PROTOCOLS: TCP Time Host  Host  SYN (SEQ = y  ACK=x+ ) SYN (SEQ = x) (SEQ = x +   ACK = y +  ) Host  Host  SYN (SEQ = y  ACK = x +  ) SYN (SEQ = x) SYN (SEQ = y) SYN (SEQ = x  ACK = y +  ) (a) (b) Figure  -
(a) TCP connection establishment in the normal case
(b) Simultaneous connection establishment on both sides
In the event that two hosts simultaneously attempt to establish a connection between the same two sockets  the sequence of events is as illustrated in Fig
The result of these events is that just one connection is established  not two  because connections are identified by their end points
If the first setup results in a connection identified by (x  y) and the ond one does too  only one table entry is made  namely  for (x  y)
Recall that the initial sequence number chosen by each host should cycle slowly  rather than be a constant such as
This rule is to protect against delayed duplicate packets  as we discussed in
Originally this was accomplished with a clock-based scheme in which the clock ticked every  Î¼
However  a vulnerability with implementing the three-way handshake is that the listening process must remember its sequence number as soon it responds with its own SYN segment
This means that a malicious sender can tie up resources on a host by sending a stream of SYN segments and never following through to complete the connection
This attack is called a SYN flood  and it crippled many Web servers in the s
One way to defend against this attack is to use SYN cookies
Instead of remembering the sequence number  a host chooses a cryptographically generated sequence number  puts it on the outgoing segment  and forgets it
If the three-way handshake completes  this sequence number (plus  ) will be returned to the host
It can then regenerate the correct sequence number by running the same cryptographic function  as long as the inputs to that function are known  for example  the other hostâs IP address and port  and a local ret
This procedure allows the host to check that an acknowledged sequence number is correct without having to THE TRANSPORT LAYER
remember the sequence number separately
There are some caveats  such as the inability to handle TCP options  so SYN cookies may be used only when the host is subject to a SYN flood
However  they are an interesting twist on connection establishment
For more information  see RFC  and Lemon (   )
TCP Connection Release Although TCP connections are full duplex  to understand how connections are released it is best to think of them as a pair of simplex connections
Each simplex connection is released independently of its sibling
To release a connection  either party can send a TCP segment with the FIN bit set  which means that it has no more data to transmit
When the FIN is acknowledged  that direction is shut down for new data
Data may continue to flow indefinitely in the other direction  however
When both directions have been shut down  the connection is released
Normally  four TCP segments are needed to release a connection: one FIN and one ACK for each direction
However  it is possible for the first ACK and the ond FIN to be contained in the same segment  reducing the total count to three
Just as with telephone calls in which both people say goodbye and hang up the phone simultaneously  both ends of a TCP connection may send FIN segments at the same time
These are each acknowledged in the usual way  and the connection is shut down
There is  in fact  no essential difference between the two hosts releasing sequentially or simultaneously
To avoid the two-army problem (discussed in
)  timers are used
If a response to a FIN is not forthcoming within two maximum packet lifetimes  the sender of the FIN releases the connection
The other side will eventually notice that nobody seems to be listening to it anymore and will time out as well
While this solution is not perfect  given the fact that a perfect solution is theoretically impossible  it will have to do
In practice  problems rarely arise
TCP Connection Management Modeling The steps required to establish and release connections can be represented in a finite state machine with the   states listed in Fig
In each state  certain events are legal
When a legal event happens  some action may be taken
If some other event happens  an error is reported
Each connection starts in the CLOSED state
It leaves that state when it does either a passive open (LISTEN) or an active open (CONNECT)
If the other side does the opposite one  a connection is established and the state becomes ESTABLISHED
Connection release can be initiated by either side
When it is complete  the state returns to CLOSED
The finite state machine itself is shown in Fig
The common case of a client actively connecting to a passive server is shown with heavy linesâsolid for the client  dotted for the server
The lightface lines are unusual event sequences
THE INTERNET TRANSPORT PROTOCOLS: TCP State Description CLOSED No connection is active or pending LISTEN The server is waiting for an incoming call SYN RCVD A connection request has arrived; wait for ACK SYN SENT The application has started to open a connection ESTABLISHED The normal data transfer state FIN WAIT  The application has said it is finished FIN WAIT  The other side has agreed to release TIME WAIT Wait for all packets to die off CLOSING Both sides have tried to close simultaneously CLOSE WAIT The other side has initiated a release LAST ACK Wait for all packets to die off Figure  -
The states used in the TCP connection management finite state machine
Each line in Fig
The event can either be a user-initiated system call (CONNECT  LISTEN  SEND  or CLOSE)  a segment arrival (SYN  FIN  ACK  or RST)  or  in one case  a timeout of twice the maximum packet lifetime
The action is the sending of a control segment (SYN  FIN  or RST) or nothing  indicated by â
Comments are shown in parentheses
One can best understand the diagram by first following the path of a client (the heavy solid line)  then later following the path of a server (the heavy dashed line)
When an application program on the client machine issues a CONNECT request  the local TCP entity creates a connection record  marks it as being in the SYN SENT state  and shoots off a SYN segment
Note that many connections may be open (or being opened) at the same time on behalf of multiple applications  so the state is per connection and recorded in the connection record
When the SYN+ACK arrives  TCP sends the final ACK of the three-way handshake and switches into the ESTABLISHED state
Data can now be sent and received
When an application is finished  it executes a CLOSE primitive  which causes the local TCP entity to send a FIN segment and wait for the corresponding ACK (dashed box marked ââactive closeââ)
When the ACK arrives  a transition is made to the state FIN WAIT  and one direction of the connection is closed
When the other side closes  too  a FIN comes in  which is acknowledged
Now both sides are closed  but TCP waits a time equal to twice the maximum packet lifetime to guarantee that all packets from the connection have died off  just in case the acknowledgement was lost
When the timer goes off  TCP deletes the connection record
Now let us examine connection management from the serverâs viewpoint
The server does a LISTEN and settles down to see who turns up
When a SYN THE TRANSPORT LAYER
CLOSED LISTEN ESTABLISHED CLOSING CLOSE WAIT (Start) CONNECT/SYN (Step  of the  -way handshake) LISTEN/â SYN/SYN + ACK SYN RCVD FIN WAIT  TIME WAIT LAST ACK FIN WAIT  SYN SENT RST/â ACK/â (Active close) FIN/ACK FIN + ACK/ACK FIN/ACK ACK/â ACK/â ACK/â SEND/SYN SYN/SYN + ACK (simultaneous open) (Data transfer state) SYN + ACK/ACK (Step  of the  -way handshake) CLOSE/FIN CLOSE/FIN FIN/ACK CLOSE/â CLOSE/â CLOSE/FIN CLOSED (Passive close) (Timeout/) (Go back to start) (Step  of the  -way handshake) Figure  -
TCP connection management finite state machine
The heavy solid line is the normal path for a client
The heavy dashed line is the normal path for a server
The light lines are unusual events
Each transition is labeled with the event causing it and the action resulting from it  separated by a slash
comes in  it is acknowledged and the server goes to the SYN RCVD state
When the serverâs SYN is itself acknowledged  the three-way handshake is complete and the server goes to the ESTABLISHED state
Data transfer can now occur
When the client is done transmitting its data  it does a CLOSE  which causes a FIN to arrive at the server (dashed box marked ââpassive closeââ)
The server is then signaled
When it  too  does a CLOSE  a FIN is sent to the client
When the   THE INTERNET TRANSPORT PROTOCOLS: TCP clientâs acknowledgement shows up  the server releases the connection and deletes the connection record
TCP Sliding Window As mentioned earlier  window management in TCP decouples the issues of acknowledgement of the correct receipt of segments and receiver buffer allocation
For example  suppose the receiver has a -byte buffer  as shown in Fig
If the sender transmits a -byte segment that is correctly received  the receiver will acknowledge the segment
However  since it now has only  bytes of buffer space (until the application removes some data from the buffer)  it will advertise a window of  starting at the next byte expected
Application does a  -KB write Application does a  -KB write Application reads  KB Sender is blocked Sender may send up to  -KB Receiverâs buffer   KB  KB  KB Empty Full  KB SEQ = KB SEQ =   KB SEQ =  ACK =  WIN =  ACK =  WIN =  ACK =  WIN =   KB  KB Sender Receiver Figure  -
Window management in TCP
Now the sender transmits another  bytes  which are acknowledged  but the advertised window is of size
The sender must stop until the application THE TRANSPORT LAYER
process on the receiving host has removed some data from the buffer  at which time TCP can advertise a larger window and more data can be sent
When the window is   the sender may not normally send segments  with two exceptions
First  urgent data may be sent  for example  to allow the user to kill the process running on the remote machine
ond  the sender may send a  -byte segment to force the receiver to reannounce the next byte expected and the window size
This packet is called a window probe
The TCP standard explicitly provides this option to prevent deadlock if a window update ever gets lost
Senders are not required to transmit data as soon as they come in from the application
Neither are receivers required to send acknowledgements as soon as possible
For example  in Fig
This freedom can be used to improve performance
Consider a connection to a remote terminal  for example using SSH or telnet  that reacts on every keystroke
In the worst case  whenever a character arrives at the sending TCP entity  TCP creates a  -byte TCP segment  which it gives to IP to send as a  -byte IP datagram
At the receiving side  TCP immediately sends a  -byte acknowledgement (  bytes of TCP header and   bytes of IP header)
Later  when the remote terminal has read the byte  TCP sends a window update  moving the window  byte to the right
This packet is also   bytes
Finally  when the remote terminal has processed the character  it echoes the character for local display using a  -byte packet
In all  bytes of bandwidth are used and four segments are sent for each character typed
When bandwidth is scarce  this method of doing business is not desirable
One approach that many TCP implementations use to optimize this situation is called delayed acknowledgements
The idea is to delay acknowledgements and window updates for up to m in the hope of acquiring some data on which to hitch a free ride
Assuming the terminal echoes within m  only one  -byte packet now need be sent back by the remote side  cutting the packet count and bandwidth usage in half
Although delayed acknowledgements reduce the load placed on the network by the receiver  a sender that sends multiple short packets (
-byte packets containing  byte of data) is still operating inefficiently
A way to reduce this usage is known as Nagleâs algorithm (Nagle  )
What Nagle suggested is simple: when data come into the sender in small pieces  just send the first piece and buffer all the rest until the first piece is acknowledged
Then send all the buffered data in one TCP segment and start buffering again until the next segment is acknowledged
That is  only one short packet can be outstanding at any time
If many pieces of data are sent by the application in one round-trip time  Nagleâs algorithm will put the many pieces in one segment  greatly reducing the bandwidth used
The algorithm additionally says that a new segment should be sent if enough data have trickled in to fill a maximum segment
THE INTERNET TRANSPORT PROTOCOLS: TCP Nagleâs algorithm is widely used by TCP implementations  but there are times when it is better to disable it
In particular  in interactive games that are run over the Internet  the players typically want a rapid stream of short update packets
Gathering the updates to send them in bursts makes the game respond erratically  which makes for unhappy users
A more subtle problem is that Nagleâs algorithm can sometimes interact with delayed acknowledgements to cause a temporary deadlock: the receiver waits for data on which to piggyback an acknowledgement  and the sender waits on the acknowledgement to send more data
This interaction can delay the downloads of Web pages
Because of these problems  Nagleâs algorithm can be disabled (which is called the TCP NODELAY option)
Mogul and Minshall (   ) discuss this and other solutions
Another problem that can degrade TCP performance is the silly window syndrome (Clark  )
This problem occurs when data are passed to the sending TCP entity in large blocks  but an interactive application on the receiving side reads data only  byte at a time
To see the problem  look at Fig
Initially  the TCP buffer on the receiving side is full (
it has a window of size  ) and the sender knows this
Then the interactive application reads one character from the TCP stream
This action makes the receiving TCP happy  so it sends a window update to the sender saying that it is all right to send  byte
The sender obliges and sends  byte
The buffer is now full  so the receiver acknowledges the  -byte segment and sets the window to
This behavior can go on forever
Clarkâs solution is to prevent the receiver from sending a window update for  byte
Instead  it is forced to wait until it has a decent amount of space available and advertise that instead
Specifically  the receiver should not send a window update until it can handle the maximum segment size it advertised when the connection was established or until its buffer is half empty  whichever is smaller
Furthermore  the sender can also help by not sending tiny segments
Instead  it should wait until it can send a full segment  or at least one containing half of the receiverâs buffer size
Nagleâs algorithm and Clarkâs solution to the silly window syndrome are complementary
Nagle was trying to solve the problem caused by the sending application delivering data to TCP a byte at a time
Clark was trying to solve the problem of the receiving application sucking the data up from TCP a byte at a time
Both solutions are valid and can work together
The goal is for the sender not to send small segments and the receiver not to ask for them
The receiving TCP can go further in improving performance than just doing window updates in large units
Like the sending TCP  it can also buffer data  so it can block a READ request from the application until it has a large chunk of data for it
Doing so reduces the number of calls to TCP (and the overhead)
It also increases the response time  but for noninteractive applications like file transfer  efficiency may be more important than response time to individual requests
Another issue that the receiver must handle is that segments may arrive out of order
The receiver will buffer the data until it can be passed up to the application THE TRANSPORT LAYER
Application reads  byte Window update segment sent New byte arrives Header Header Receiver's buffer is full Receiver's buffer is full Room for one more byte  Byte Figure  -
Silly window syndrome
Actually  nothing bad would happen if out-of-order segments were discarded  since they would eventually be retransmitted by the sender  but it would be wasteful
Acknowledgements can be sent only when all the data up to the byte acknowledged have been received
This is called a cumulative acknowledgement
If the receiver gets segments     and   it can acknowledge everything up to and including the last byte in segment
When the sender times out  it then retransmits segment
As the receiver has buffered segments  through   upon receipt of segment  it can acknowledge all bytes up to the end of segment
TCP Timer Management TCP uses multiple timers (at least conceptually) to do its work
The most important of these is the RTO (Retransmission TimeOut)
When a segment is sent  a retransmission timer is started
If the segment is acknowledged before the timer expires  the timer is stopped
If  on the other hand  the timer goes off before the acknowledgement comes in  the segment is retransmitted (and the timer os started again)
The question that arises is: how long should the timeout be? This problem is much more difficult in the transport layer than in data link protocols such as    In the latter case  the expected delay is measured in   THE INTERNET TRANSPORT PROTOCOLS: TCP microonds and is highly predictable (
has a low variance)  so the timer can be set to go off just slightly after the acknowledgement is expected  as shown in Fig
Since acknowledgements are rarely delayed in the data link layer (due to lack of congestion)  the absence of an acknowledgement at the expected time generally means either the frame or the acknowledgement has been lost  T T  T     Round-trip time (microonds) (a) (b) Probability
Probability    Round-trip time (millionds)  Figure  -
(a) Probability density of acknowledgement arrival times in the data link layer
(b) Probability density of acknowledgement arrival times for TCP
TCP is faced with a radically different environment
The probability density function for the time it takes for a TCP acknowledgement to come back looks more like Fig
It is larger and more variable
Determining the round-trip time to the destination is tricky
Even when it is known  deciding on the timeout interval is also difficult
If the timeout is set too short  say  T  in Fig
If it is set too long (
T )  performance will suffer due to the long retransmission delay whenever a packet is lost
Furthermore  the mean and variance of the acknowledgement arrival distribution can change rapidly within a few onds as congestion builds up or is resolved
The solution is to use a dynamic algorithm that constantly adapts the timeout interval  based on continuous measurements of network performance
The algorithm generally used by TCP is due to Jacobson (   ) and works as follows
For each connection  TCP maintains a variable  SRTT (Smoothed Round-Trip Time)  that is the best current estimate of the round-trip time to the destination in question
When a segment is sent  a timer is started  both to see how long the acknowledgement takes and also to trigger a retransmission if it takes too long
If THE TRANSPORT LAYER
the acknowledgement gets back before the timer expires  TCP measures how long the acknowledgement took  say  R
It then updates SRTT according to the formula SRTT = Î± SRTT + (  â Î±) R where Î± is a smoothing factor that determines how quickly the old values are forgotten
Typically  Î± =  /
This kind of formula is an EWMA (Exponentially Weighted Moving Average) or low-pass filter that discards noise in the samples
Even given a good value of SRTT  choosing a suitable retransmission timeout is a nontrivial matter
Initial implementations of TCP used  xRTT  but experience showed that a constant value was too inflexible because it failed to respond when the variance went up
In particular  queueing models of random (
Poisson) traffic predict that when the load approaches capacity  the delay becomes large and highly variable
This can lead to the retransmission timer firing and a copy of the packet being retransmitted although the original packet is still transiting the network
It is all the more likely to happen under conditions of high load  which is the worst time at which to send additional packets into the network
To fix this problem  Jacobson proposed making the timeout value sensitive to the variance in round-trip times as well as the smoothed round-trip time
This change requires keeping track of another smoothed variable  RTTVAR (Round- Trip Time VARiation) that is updated using the formula RTTVAR = Î² RTTVAR + (  â Î²) |SRTT â R | This is an EWMA as before  and typically Î² =  /
The retransmission timeout  RTO  is set to be RTO = SRTT +  Ã RTTVAR The choice of the factor  is somewhat arbitrary  but multiplication by  can be done with a single shift  and less than  % of all packets come in more than four standard deviations late
Note that RTTVAR is not exactly the same as the standard deviation (it is really the mean deviation)  but it is close enough in practice
Jacobsonâs paper is full of clever tricks to compute timeouts using only integer adds  subtracts  and shifts
This economy is not needed for modern hosts  but it has become part of the culture that allows TCP to run on all manner of devices  from supercomputers down to tiny devices
So far nobody has put it on an RFID chip  but someday? Who knows
More details of how to compute this timeout  including initial settings of the variables  are given in RFC
The retransmission timer is also held to a minimum of  ond  regardless of the estimates
This is a conservative value chosen to prevent spurious retransmissions based on measurements (Allman and Paxson  )
One problem that occurs with gathering the samples  R  of the round-trip time is what to do when a segment times out and is sent again
When the acknowledgement comes in  it is unclear whether the acknowledgement refers to the first   THE INTERNET TRANSPORT PROTOCOLS: TCP transmission or a later one
Guessing wrong can seriously contaminate the retransmission timeout
Phil Karn discovered this problem the hard way
Karn is an amateur radio enthusiast interested in transmitting TCP/IP packets by ham radio  a notoriously unreliable medium
He made a simple proposal: do not update estimates on any segments that have been retransmitted
Additionally  the timeout is doubled on each successive retransmission until the segments get through the first time
This fix is called Karnâs algorithm (Karn and Partridge  )
Most TCP implementations use it
The retransmission timer is not the only timer TCP uses
A ond timer is the persistence timer
It is designed to prevent the following deadlock
The receiver sends an acknowledgement with a window size of   telling the sender to wait
Later  the receiver updates the window  but the packet with the update is lost
Now the sender and the receiver are each waiting for the other to do something
When the persistence timer goes off  the sender transmits a probe to the receiver
The response to the probe gives the window size
If it is still   the persistence timer is set again and the cycle repeats
If it is nonzero  data can now be sent
A third timer that some implementations use is the keepalive timer
When a connection has been idle for a long time  the keepalive timer may go off to cause one side to check whether the other side is still there
If it fails to respond  the connection is terminated
This feature is controversial because it adds overhead and may terminate an otherwise healthy connection due to a transient network partition
The last timer used on each TCP connection is the one used in the TIME WAIT state while closing
It runs for twice the maximum packet lifetime to make sure that when a connection is closed  all packets created by it have died off
TCP Congestion Control We have saved one of the key functions of TCP for last: congestion control
When the load offered to any network is more than it can handle  congestion builds up
The Internet is no exception
The network layer detects congestion when queues grow large at routers and tries to manage it  if only by dropping packets
It is up to the transport layer to receive congestion feedback from the network layer and slow down the rate of traffic that it is sending into the network
In the Internet  TCP plays the main role in controlling congestion  as well as the main role in reliable transport
That is why it is such a special protocol
We covered the general situation of congestion control in
One key takeaway was that a transport protocol using an AIMD (Additive Increase Multiplicative Decrease) control law in response to binary congestion signals from the network would converge to a fair and efficient bandwidth allocation
TCP congestion control is based on implementing this approach using a window and with packet loss as the binary signal
To do so  TCP maintains a congestion window THE TRANSPORT LAYER
whose size is the number of bytes the sender may have in the network at any time
The corresponding rate is the window size divided by the round-trip time of the connection
TCP adjusts the size of the window according to the AIMD rule
Recall that the congestion window is maintained in addition to the flow control window  which specifies the number of bytes that the receiver can buffer
Both windows are tracked in parallel  and the number of bytes that may be sent is the smaller of the two windows
Thus  the effective window is the smaller of what the sender thinks is all right and what the receiver thinks is all right
It takes two to tango
TCP will stop sending data if either the congestion or the flow control window is temporarily full
If the receiver says ââsend   KBââ but the sender knows that bursts of more than   KB clog the network  it will send   KB
On the other hand  if the receiver says ââsend   KBââ and the sender knows that bursts of up to KB get through effortlessly  it will send the full   KB requested
The flow control window was described earlier  and in what follows we will only describe the congestion window
Modern congestion control was added to TCP largely through the efforts of Van Jacobson (   )
It is a fascinating story
Starting in  the growing popularity of the early Internet led to the first occurrence of what became known as a congestion collapse  a prolonged period during which goodput dropped precipitously (
by more than a factor of   ) due to congestion in the network
Jacobson (and many others) set out to understand what was happening and remedy the situation
The high-level fix that Jacobson implemented was to approximate an AIMD congestion window
The interesting part  and much of the complexity of TCP congestion control  is how he added this to an existing implementation without changing any of the message formats  which made it instantly deployable
To start  he observed that packet loss is a suitable signal of congestion
This signal comes a little late (as the network is already congested) but it is quite dependable
After all  it is difficult to build a router that does not drop packets when it is overloaded
This fact is unlikely to change
Even when terabyte memories appear to buffer vast numbers of packets  we will probably have terabit/ networks to fill up those memories
However  using packet loss as a congestion signal depends on transmission errors being relatively rare
This is not normally the case for wireless links such as
which is why they include their own retransmission mechanism at the link layer
Because of wireless retransmissions  network layer packet loss due to transmission errors is normally masked on wireless networks
It is also rare on other links because wires and optical fibers typically have low bit-error rates
All the Internet TCP algorithms assume that lost packets are caused by congestion and monitor timeouts and look for signs of trouble the way miners watch their canaries
A good retransmission timer is needed to detect packet loss signals accurately and in a timely manner
We have already discussed how the TCP retransmission timer includes estimates of the mean and variation in round-trip   THE INTERNET TRANSPORT PROTOCOLS: TCP times
Fixing this timer  by including the variation factor  was an important step in Jacobsonâs work
Given a good retransmission timeout  the TCP sender can track the outstanding number of bytes  which are loading the network
It simply looks at the difference between the sequence numbers that are transmitted and acknowledged
Now it seems that our task is easy
All we need to do is to track the congestion window  using sequence and acknowledgement numbers  and adjust the congestion window using an AIMD rule
As you might have expected  it is more complicated than that
A first consideration is that the way packets are sent into the network  even over short periods of time  must be matched to the network path
Otherwise the traffic will cause congestion
For example  consider a host with a congestion window of   KB attached to a  -Gbps switched Ethernet
If the host sends the entire window at once  this burst of traffic may travel over a slow  -Mbps ADSL line further along the path
The burst that took only half a milliond on the  -Gbps line will clog the  -Mbps line for half a ond  completely disrupting protocols such as voice over IP
This behavior might be a good idea for a protocol designed to cause congestion  but not for a protocol to control it
However  it turns out that we can use small bursts of packets to our advantage
-  shows what happens when a sender on a fast network (the  -Gbps link) sends a small burst of four packets to a receiver on a slow network (the  - Mbps link) that is the bottleneck or slowest part of the path
Initially the four packets travel over the link as quickly as they can be sent by the sender
At the router  they are queued while being sent because it takes longer to send a packet over the slow link than to receive the next packet over the fast link
But the queue is not large because only a small number of packets were sent at once
Note the increased length of the packets on the slow link
The same packet  of  KB say  is now longer because it takes more time to send it on a slow link than on a fast one
Fast link Slow link (bottleneck)  : Burst of packets sent on fast link  : Burst queues at router and drains onto slow link  : Receive acks packets at slow link rate  : Acks preserve slow link timing at sender Ack clock Sender Receiver          Figure  -
A burst of packets from a sender and the returning ack clock
Eventually the packets get to the receiver  where they are acknowledged
The times for the acknowledgements reflect the times at which the packets arrived at the receiver after crossing the slow link
They are spread out compared to the original packets on the fast link
As these acknowledgements travel over the network and back to the sender they preserve this timing
THE TRANSPORT LAYER
The key observation is this: the acknowledgements return to the sender at about the rate that packets can be sent over the slowest link in the path
This is precisely the rate that the sender wants to use
If it injects new packets into the network at this rate  they will be sent as fast as the slow link permits  but they will not queue up and congest any router along the path
This timing is known as an ack clock
It is an essential part of TCP
By using an ack clock  TCP smoothes out traffic and avoids unnecessary queues at routers
A ond consideration is that the AIMD rule will take a very long time to reach a good operating point on fast networks if the congestion window is started from a small size
Consider a modest network path that can support   Mbps with an RTT of m
The appropriate congestion window is the bandwidth-delay product  which is  Mbit or packets of  bytes each
If the congestion window starts at  packet and increases by  packet every RTT  it will be RTTs or   onds before the connection is running at about the right rate
That is a long time to wait just to get to the right speed for a transfer
We could reduce this startup time by starting with a larger initial window  say of   packets
But this window would be far too large for slow or short links
It would cause congestion if used all at once  as we have just described
Instead  the solution Jacobson chose to handle both of these considerations is a mix of linear and multiplicative increase
When a connection is established  the sender initializes the congestion window to a small initial value of at most four segments; the details are described in RFC  and the use of four segments is an increase from an earlier initial value of one segment based on experience
The sender then sends the initial window
The packets will take a round-trip time to be acknowledged
For each segment that is acknowledged before the retransmission timer goes off  the sender adds one segmentâs worth of bytes to the congestion window
Plus  as that segment has been acknowledged  there is now one less segment in the network
The upshot is that every acknowledged segment allows two more segments to be sent
The congestion window is doubling every roundtrip time
This algorithm is called slow start  but it is not slow at allâit is exponential growthâexcept in comparison to the previous algorithm that let an entire flow control window be sent all at once
Slow start is shown in Fig
In the first round-trip time  the sender injects one packet into the network (and the receiver receives one packet)
Two packets are sent in the next round-trip time  then four packets in the third round-trip time
Slow-start works well over a range of link speeds and round-trip times  and uses an ack clock to match the rate of sender transmissions to the network path
Take a look at the way acknowledgements return from the sender to the receiver in Fig
When the sender gets an acknowledgement  it increases the congestion window by one and immediately sends two packets into the network
(One packet is the increase by one; the other packet is a replacement for the packet that has been acknowledged and left the network
At all times  the number of   THE INTERNET TRANSPORT PROTOCOLS: TCP cwnd = RTT   packet cwnd =  cwnd =  cwnd =  cwnd =  cwnd =  cwnd =  cwnd = RTT   packets  RTT   packets  RTT   packets (pipe is full) Data Acknowledgement TCP sender TCP receiver Figure  -
Slow start from an initial congestion window of one segment
unacknowledged packets is given by the congestion window
) However  these two packets will not necessarily arrive at the receiver as closely spaced as when they were sent
For example  suppose the sender is on a   -Mbps Ethernet
Each packet of  bytes takes Î¼ to send
So the delay between the packets can be as small as Î¼
The situation changes if these packets go across a  - Mbps ADSL link anywhere along the path
It now takes   m to send the same packet
This means that the minimum spacing between the two packets has grown by a factor of
Unless the packets have to wait together in a queue on a later link  the spacing will remain large
The same spacing is kept when the receiver sends acknowledgements  and thus when the sender receives the acknowledgements
If the network path is slow  acknowledgements will come in slowly (after a delay of an RTT)
If the network path is fast  acknowledgements will come in quickly (again  after the RTT)
All the sender has to do is follow the timing of the ack clock as it injects new packets  which is what slow start does
Because slow start causes exponential growth  eventually (and sooner rather than later) it will send too many packets into the network too quickly
When this happens  queues will build up in the network
When the queues are full  one or more packets will be lost
After this happens  the TCP sender will time out when an acknowledgement fails to arrive in time
There is evidence of slow start growing too fast in Fig
After three RTTs  four packets are in the network
These four packets take an entire RTT to arrive at the receiver
That is  a congestion window of four packets is the right size for this connection
However  as these packets are acknowledged  slow start continues to grow the congestion window  reaching eight packets in another RTT
Only four of these packets can reach the receiver in one RTT  no matter how many are sent
That is  the network pipe is full
Additional packets placed into the network by the sender will build up in THE TRANSPORT LAYER
router queues  since they cannot be delivered to the receiver quickly enough
Congestion and packet loss will occur soon
To keep slow start under control  the sender keeps a threshold for the connection called the slow start threshold
Initially this value is set arbitrarily high  to the size of the flow control window  so that it will not limit the connection
TCP keeps increasing the congestion window in slow start until a timeout occurs or the congestion window exceeds the threshold (or the receiverâs window is filled)
Whenever a packet loss is detected  for example  by a timeout  the slow start threshold is set to be half of the congestion window and the entire process is restarted
The idea is that the current window is too large because it caused congestion previously that is only now detected by a timeout
Half of the window  which was used successfully at an earlier time  is probably a better estimate for a congestion window that is close to the path capacity but will not cause loss
In our example in Fig
The congestion window is then reset to its small initial value and slow start resumes
Whenever the slow start threshold is crossed  TCP switches from slow start to additive increase
In this mode  the congestion window is increased by one segment every round-trip time
Like slow start  this is usually implemented with an increase for every segment that is acknowledged  rather than an increase once per RTT
Call the congestion window cwnd and the maximum segment size MSS
A common approximation is to increase cwnd by (MSS Ã MSS)/cwnd for each of the cwnd /MSS packets that may be acknowledged
This increase does not need to be fast
The whole idea is for a TCP connection to spend a lot of time with its congestion window close to the optimum valueânot so small that throughput will be low  and not so large that congestion will occur
Additive increase is shown in Fig
At the end of every RTT  the senderâs congestion window has grown enough that it can inject an additional packet into the network
Compared to slow start  the linear rate of growth is much slower
It makes little difference for small congestion windows  as is the case here  but a large difference in the time taken to grow the congestion window to segments  for example
There is something else that we can do to improve performance too
The defect in the scheme so far is waiting for a timeout
Timeouts are relatively long because they must be conservative
After a packet is lost  the receiver cannot acknowledge past it  so the acknowledgement number will stay fixed  and the sender will not be able to send any new packets into the network because its congestion window remains full
This condition can continue for a relatively long period until the timer fires and the lost packet is retransmitted
At that stage  TCP slow starts again
There is a quick way for the sender to recognize that one of its packets has been lost
As packets beyond the lost packet arrive at the receiver  they trigger   THE INTERNET TRANSPORT PROTOCOLS: TCP cwnd = RTT   packets cwnd =  cwnd =  cwnd = RTT   packets  RTT   packets  RTT   packets (pipe is full) Data Acknowledgement TCP sender TCP receiver cwnd = RTT   packet Figure  -
Additive increase from an initial congestion window of one segment
acknowledgements that return to the sender
These acknowledgements bear the same acknowledgement number
They are called duplicate acknowledgements
Each time the sender receives a duplicate acknowledgement  it is likely that another packet has arrived at the receiver and the lost packet still has not shown up
Because packets can take different paths through the network  they can arrive out of order
This will trigger duplicate acknowledgements even though no packets have been lost
However  this is uncommon in the Internet much of the time
When there is reordering across multiple paths  the received packets are usually not reordered too much
Thus  TCP somewhat arbitrarily assumes that three duplicate acknowledgements imply that a packet has been lost
The identity of the lost packet can be inferred from the acknowledgement number as well
It is the very next packet in sequence
This packet can then be retransmitted right away  before the retransmission timeout fires
This heuristic is called fast retransmission
After it fires  the slow start threshold is still set to half the current congestion window  just as with a timeout
Slow start can be restarted by setting the congestion window to one packet
With this window size  a new packet will be sent after the one round-trip time that it takes to acknowledge the retransmitted packet along with all data that had been sent before the loss was detected
An illustration of the congestion algorithm we have built up so far is shown in Fig
This version of TCP is called TCP Tahoe after the
BSD Tahoe release in  in which it was included
The maximum segment size here is  KB
Initially  the congestion window was   KB  but a timeout occurred  so the threshold is set to   KB and the congestion window to  KB for transmission
The congestion window grows exponentially until it hits the threshold (  KB)
The THE TRANSPORT LAYER
window is increased every time a new acknowledgement arrives rather than continuously  which leads to the discrete staircase pattern
After the threshold is passed  the window grows linearly
It is increased by one segment every RTT
Transmission round (RTTs) Additive increase Threshold  KB Packet loss Congestion window (KB or packets)          Slow start  Threshold  KB Figure  -
Slow start followed by additive increase in TCP Tahoe
The transmissions in round   are unlucky (they should have known)  and one of them is lost in the network
This is detected when three duplicate acknowledgements arrive
At that time  the lost packet is retransmitted  the threshold is set to half the current window (by now   KB  so half is   KB)  and slow start is initiated all over again
Restarting with a congestion window of one packet takes one round-trip time for all of the previously transmitted data to leave the network and be acknowledged  including the retransmitted packet
The congestion window grows with slow start as it did previously  until it reaches the new threshold of   KB
At that time  the growth becomes linear again
It will continue in this fashion until another packet loss is detected via duplicate acknowledgements or a timeout (or the receiverâs window becomes the limit)
TCP Tahoe (which included good retransmission timers) provided a working congestion control algorithm that solved the problem of congestion collapse
Jacobson realized that it is possible to do even better
At the time of the fast retransmission  the connection is running with a congestion window that is too large  but it is still running with a working ack clock
Every time another duplicate acknowledgement arrives  it is likely that another packet has left the network
Using duplicate acknowledgements to count the packets in the network  makes it possible to let some packets exit the network and continue to send a new packet for each additional duplicate acknowledgement
Fast recovery is the heuristic that implements this behavior
It is a temporary mode that aims to maintain the ack clock running with a congestion window that is the new threshold  or half the value of the congestion window at the time of the   THE INTERNET TRANSPORT PROTOCOLS: TCP fast retransmission
To do this  duplicate acknowledgements are counted (including the three that triggered fast retransmission) until the number of packets in the network has fallen to the new threshold
This takes about half a round-trip time
From then on  a new packet can be sent for each duplicate acknowledgement that is received
One round-trip time after the fast retransmission  the lost packet will have been acknowledged
At that time  the stream of duplicate acknowledgements will cease and fast recovery mode will be exited
The congestion window will be set to the new slow start threshold and grows by linear increase
The upshot of this heuristic is that TCP avoids slow start  except when the connection is first started and when a timeout occurs
The latter can still happen when more than one packet is lost and fast retransmission does not recover adequately
Instead of repeated slow starts  the congestion window of a running connection follows a sawtooth pattern of additive increase (by one segment every RTT) and multiplicative decrease (by half in one RTT)
This is exactly the AIMD rule that we sought to implement
This sawtooth behavior is shown in Fig
It is produced by TCP Reno  named after the
BSD Reno release in  in which it was included
TCP Reno is essentially TCP Tahoe plus fast recovery
After an initial slow start  the congestion window climbs linearly until a packet loss is detected by duplicate acknowledgements
The lost packet is retransmitted and fast recovery is used to keep the ack clock running until the retransmission is acknowledged
At that time  the congestion window is resumed from the new slow start threshold  rather than from
This behavior continues indefinitely  and the connection spends most of the time with its congestion window close to the optimum value of the bandwidth- delay product
Transmission round (RTTs) Additive Packet increase loss Congestion window (KB or packets)        Slow start  Thresh
Threshold Fast recovery Multiplicative decrease Threshold Figure  -
Fast recovery and the sawtooth pattern of TCP Reno
TCP Reno with its mechanisms for adjusting the congestion window has formed the basis for TCP congestion control for more than two decades
Most of THE TRANSPORT LAYER
the changes in the intervening years have adjusted these mechanisms in minor ways  for example  by changing the choices of the initial window and removing various ambiguities
Some improvements have been made for recovering from two or more losses in a window of packets
For example  the TCP NewReno version uses a partial advance of the acknowledgement number after a retransmission to find and repair another loss (Hoe  )  as described in RFC
Since the mid-   s  several variations have emerged that follow the principles we have described but use slightly different control laws
For example  Linux uses a variant called CUBIC TCP (Ha et al
) and Windows includes a variant called Compound TCP (Tan et al
Two larger changes have also affected TCP implementations
First  much of the complexity of TCP comes from inferring from a stream of duplicate acknowledgements which packets have arrived and which packets have been lost
The cumulative acknowledgement number does not provide this information
A simple fix is the use of SACK (Selective ACKnowledgements)  which lists up to three ranges of bytes that have been received
With this information  the sender can more directly decide what packets to retransmit and track the packets in flight to implement the congestion window
When the sender and receiver set up a connection  they each send the SACK permitted TCP option to signal that they understand selective acknowledgements
Once SACK is enabled for a connection  it works as shown in Fig
A receiver uses the TCP Acknowledgement number field in the normal manner  as a cumulative acknowledgement of the highest in-order byte that has been received
When it receives packet  out of order (because packet  was lost)  it sends a SACK option for the received data along with the (duplicate) cumulative acknowledgement for packet
The SACK option gives the byte ranges that have been received above the number given by the cumulative acknowledgement
The first range is the packet that triggered the duplicate acknowledgement
The next ranges  if present  are older blocks
Up to three ranges are commonly used
By the time packet  is received  two SACK byte ranges are used to indicate that packet  and packets  to  have been received  in addition to all packets up to packet
From the information in each SACK option that it receives  the sender can decide which packets to retransmit
In this case  retransmitting packets  and  would be a good idea
SACK is strictly advisory information
The actual detection of loss using duplicate acknowledgements and adjustments to the congestion window proceed just as before
However  with SACK  TCP can recover more easily from situations in which multiple packets are lost at roughly the same time  since the TCP sender knows which packets have not been received
SACK is now widely deployed
It is described in RFC  and TCP congestion control using SACK is described in RFC
The ond change is the use of ECN (Explicit Congestion Notification) in addition to packet loss as a congestion signal
ECN is an IP layer mechanism to   THE INTERNET TRANSPORT PROTOCOLS: TCP    Lost packets ACK:  ACK:  SACK:  ACK:  SACK:  -  ACK:  SACK: -  Sender Receiver Retransmit  and  ! Figure  -
Selective acknowledgements
notify hosts of congestion that we described in
With it  the TCP receiver can receive congestion signals from IP
The use of ECN is enabled for a TCP connection when both the sender and receiver indicate that they are capable of using ECN by setting the ECE and CWR bits during connection establishment
If ECN is used  each packet that carries a TCP segment is flagged in the IP header to show that it can carry an ECN signal
Routers that support ECN will set a congestion signal on packets that can carry ECN flags when congestion is approaching  instead of dropping those packets after congestion has occurred
The TCP receiver is informed if any packet that arrives carries an ECN congestion signal
The receiver then uses the ECE (ECN Echo) flag to signal the TCP sender that its packets have experienced congestion
The sender tells the receiver that it has heard the signal by using the CWR (Congestion Window Reduced) flag
The TCP sender reacts to these congestion notifications in exactly the same way as it does to packet loss that is detected via duplicate acknowledgements
However  the situation is strictly better
Congestion has been detected and no packet was harmed in any way
ECN is described in RFC
It requires both host and router support  and is not yet widely used on the Internet
For more information on the complete set of congestion control behaviors that are implemented in TCP  see RFC
The Future of TCP As the workhorse of the Internet  TCP has been used for many applications and extended over time to give good performance over a wide range of networks
Many versions are deployed with slightly different implementations than the classic algorithms we have described  especially for congestion control and robustness against attacks
It is likely that TCP will continue to evolve with the Internet
We will mention two particular issues
The first one is that TCP does not provide the transport semantics that all applications want
For example  some applications want to send messages or records whose boundaries need to be preserved
Other applications work with a group of THE TRANSPORT LAYER
related conversations  such as a Web browser that transfers several objects from the same server
Still other applications want better control over the network paths that they use
TCP with its standard sockets interface does not meet these needs well
Essentially  the application has the burden of dealing with any problem not solved by TCP
This has led to proposals for new protocols that would provide a slightly different interface
Two examples are SCTP (Stream Control Transmission Protocol)  defined in RFC  and SST (Structured Stream Transport) (Ford  )
However  whenever someone proposes changing something that has worked so well for so long  there is always a huge battle between the ââUsers are demanding more featuresââ and ââIf it ainât broke  donât fix itââ camps
The ond issue is congestion control
You may have expected that this is a solved problem after our deliberations and the mechanisms that have been developed over time
The form of TCP congestion control that we described  and which is widely used  is based on packet losses as a signal of congestion
When Padhye et al
(   ) modeled TCP throughput based on the sawtooth pattern  they found that the packet loss rate must drop off rapidly with increasing speed
To reach a throughput of  Gbps with a round-trip time of ms and  byte packets  one packet can be lost approximately every   minutes
That is a packet loss rate of  Ã  â  which is incredibly small
It is too infrequent to serve as a good congestion signal  and any other source of loss (
packet transmission error rates of  â ) can easily dominate it  limiting the throughput
This relationship has not been a problem in the past  but networks are getting faster and faster  leading many people to revisit congestion control
One possibility is to use an alternate congestion control in which the signal is not packet loss at all
We gave several examples in
The signal might be round-trip time  which grows when the network becomes congested  as is used by FAST TCP (Wei et al
Other approaches are possible too  and time will tell which is the best  PERFORMANCE ISSUES Performance issues are very important in computer networks
When hundreds or thousands of computers are interconnected  complex interactions  with unforeseen consequences  are common
Frequently  this complexity leads to poor performance and no one knows why
In the following tions  we will examine many issues related to network performance to see what kinds of problems exist and what can be done about them
Unfortunately  understanding network performance is more an art than a science
There is little underlying theory that is actually of any use in practice
The best we can do is give some rules of thumb gained from hard experience and present examples taken from the real world
We have delayed this discussion until we studied the transport layer because the performance that applications receive   PERFORMANCE ISSUES depends on the combined performance of the transport  network and link layers  and to be able to use TCP as an example in various places
In the next tions  we will look at six aspects of network performance:
Performance problems Measuring network performance Host design for fast networks Fast segment processing Header compression Protocols for ââlong fatââ networks
These aspects consider network performance both at the host and across the network  and as networks are increased in speed and size
Performance Problems in Computer Networks Some performance problems  such as congestion  are caused by temporary resource overloads
If more traffic suddenly arrives at a router than the router can handle  congestion will build up and performance will suffer
We studied congestion in detail in this and the previous  ter
Performance also degrades when there is a structural resource imbalance
For example  if a gigabit communication line is attached to a low-end PC  the poor host will not be able to process the incoming packets fast enough and some will be lost
These packets will eventually be retransmitted  adding delay  wasting bandwidth  and generally reducing performance
Overloads can also be synchronously triggered
As an example  if a segment contains a bad parameter (
the port for which it is destined)  in many cases the receiver will thoughtfully send back an error notification
Now consider what could happen if a bad segment is broadcast to  machines: each one might send back an error message
The resulting broadcast storm could cripple the network
UDP suffered from this problem until the ICMP protocol was changed to cause hosts to refrain from responding to errors in UDP segments sent to broadcast addresses
Wireless networks must be particularly careful to avoid unchecked broadcast responses because broadcast occurs naturally and the wireless bandwidth is limited
A ond example of synchronous overload is what happens after an electrical power failure
When the power comes back on  all the machines simultaneously start rebooting
A typical reboot sequence might require first going to some (DHCP) server to learn oneâs true identity  and then to some file server to get a copy of the operating system
If hundreds of machines in a data center all do this at once  the server will probably collapse under the load
THE TRANSPORT LAYER
Even in the absence of synchronous overloads and the presence of sufficient resources  poor performance can occur due to lack of system tuning
For example  if a machine has plenty of CPU power and memory but not enough of the memory has been allocated for buffer space  flow control will slow down segment reception and limit performance
This was a problem for many TCP connections as the Internet became faster but the default size of the flow control window stayed fixed at   KB
Another tuning issue is setting timeouts
When a segment is sent  a timer is set to guard against loss of the segment
If the timeout is set too short  unnecessary retransmissions will occur  clogging the wires
If the timeout is set too long  unnecessary delays will occur after a segment is lost
Other tunable parameters include how long to wait for data on which to piggyback before sending a separate acknowledgement  and how many retransmissions to make before giving up
Another performance problem that occurs with real-time applications like audio and video is jitter
Having enough bandwidth on average is not sufficient for good performance
Short transmission delays are also required
Consistently achieving short delays demands careful engineering of the load on the network  quality-of-service support at the link and network layers  or both
Network Performance Measurement When a network performs poorly  its users often complain to the folks running it  demanding improvements
To improve the performance  the operators must first determine exactly what is going on
To find out what is really happening  the operators must make measurements
In this tion  we will look at network performance measurements
Much of the discussion below is based on the seminal work of Mogul (   )
Measurements can be made in different ways and at many locations (both in the protocol stack and physically)
The most basic kind of measurement is to start a timer when beginning some activity and see how long that activity takes
For example  knowing how long it takes for a segment to be acknowledged is a key measurement
Other measurements are made with counters that record how often some event has happened (
number of lost segments)
Finally  one is often interested in knowing the amount of something  such as the number of bytes processed in a certain time interval
Measuring network performance and parameters has many potential pitfalls
We list a few of them here
Any systematic attempt to measure network performance should be careful to avoid these
Make Sure That the Sample Size Is Large Enough Do not measure the time to send one segment  but repeat the measurement  say  one million times and take the average
Startup effects  such as the
NIC or cable modem getting a bandwidth reservation after an idle period  can   PERFORMANCE ISSUES slow the first segment  and queueing introduces variability
Having a large sample will reduce the uncertainty in the measured mean and standard deviation
This uncertainty can be computed using standard statistical formulas
Make Sure That the Samples Are Representative Ideally  the whole sequence of one million measurements should be repeated at different times of the day and the week to see the effect of different network conditions on the measured quantity
Measurements of congestion  for example  are of little use if they are made at a moment when there is no congestion
Sometimes the results may be counterintuitive at first  such as heavy congestion at
but no congestion at noon (when all the users are at lunch)
With wireless networks  location is an important variable because of signal propagation
Even a measurement node placed close to a wireless client may not observe the same packets as the client due to differences in the antennas
It is best to take measurements from the wireless client under study to see what it sees
Failing that  it is possible to use techniques to combine the wireless measurements taken at different vantage points to gain a more complete picture of what is going on (Mahajan et al
Caching Can Wreak Havoc with Measurements Repeating a measurement many times will return an unexpectedly fast answer if the protocols use caching mechanisms
For instance  fetching a Web page or looking up a DNS name (to find the IP address) may involve a network exchange the first time  and then return the answer from a local cache without sending any packets over the network
The results from such a measurement are essentially worthless (unless you want to measure cache performance)
Buffering can have a similar effect
TCP/IP performance tests have been known to report that UDP can achieve a performance substantially higher than the network allows
How does this occur? A call to UDP normally returns control as soon as the message has been accepted by the kernel and added to the transmission queue
If there is sufficient buffer space  timing  UDP calls does not mean that all the data have been sent
Most of them may still be in the kernel  but the performance test program thinks they have all been transmitted
Caution is advised to be absolutely sure that you understand how data can be cached and buffered as part of a network operation
Be Sure That Nothing Unexpected Is Going On during Your Tests Making measurements at the same time that some user has decided to run a video conference over your network will often give different results than if there is no video conference
It is best to run tests on an idle network and create the THE TRANSPORT LAYER
entire workload yourself
Even this approach has pitfalls  though
While you might think nobody will be using the network at
that might be when the automatic backup program begins copying all the disks to tape
Or  there might be heavy traffic for your wonderful Web pages from distant time zones
Wireless networks are challenging in this respect because it is often not possible to separate them from all sources of interference
Even if there are no other wireless networks sending traffic nearby  someone may microwave popcorn and inadvertently cause interference that degrades
performance
For these reasons  it is a good practice to monitor the overall network activity so that you can at least realize when something unexpected does happen
Be Careful When Using a Coarse-Grained Clock Computer clocks function by incrementing some counter at regular intervals
For example  a milliond timer adds  to a counter every  m
Using such a timer to measure an event that takes less than  m is possible but requires some care
Some computers have more accurate clocks  of course  but there are always shorter events to measure too
Note that clocks are not always as accurate as the precision with which the time is returned when they are read
To measure the time to make a TCP connection  for example  the clock (say  in millionds) should be read out when the transport layer code is entered and again when it is exited
If the true connection setup time is Î¼  the difference between the two readings will be either  or   both wrong
However  if the measurement is repeated one million times and the total of all measurements is added up and divided by one million  the mean time will be accurate to better than  Î¼
Be Careful about Extrapolating the Results Suppose that you make measurements with simulated network loads running from  (idle) to
(  % of capacity)
For example  the response time to send a voice-over-IP packet over an
network might be as shown by the data points and solid line through them in Fig
It may be tempting to extrapolate linearly  as shown by the dotted line
However  many queueing results involve a factor of  /(  â Ï)  where Ï is the load  so the true values may look more like the dashed line  which rises much faster than linearly when the load gets high
That is  beware contention effects that become much more pronounced at high load
Host Design for Fast Networks Measuring and tinkering can improve performance considerably  but they cannot substitute for good design in the first place
A poorly designed network can be improved only so much
Beyond that  it has to be redesigned from scratch
PERFORMANCE ISSUES    Response time         Load      Figure  -
Response as a function of load
In this tion  we will present some rules of thumb for software implementation of network protocols on hosts
Surprisingly  experience shows that this is often a performance bottleneck on otherwise fast networks  for two reasons
First  NICs (Network Interface Cards) and routers have already been engineered (with hardware support) to run at ââwire speed
ââ This means that they can process packets as quickly as the packets can possibly arrive on the link
ond  the relevant performance is that which applications obtain
It is not the link capacity  but the throughput and delay after network and transport processing
Reducing software overheads improves performance by increasing throughput and decreasing delay
It can also reduce the energy that is spent on networking  which is an important consideration for mobile computers
Most of these ideas have been common knowledge to network designers for years
They were first stated explicitly by Mogul (   ); our treatment largely follows his
Another relevant source is Metcalfe (   )
Host Speed Is More Important Than Network Speed Long experience has shown that in nearly all fast networks  operating system and protocol overhead dominate actual time on the wire
For example  in theory  the minimum RPC time on a  -Gbps Ethernet is  Î¼  corresponding to a minimum (  -byte) request followed by a minimum (  -byte) reply
In practice  overcoming the software overhead and getting the RPC time anywhere near there is a substantial achievement
It rarely happens in practice
THE TRANSPORT LAYER
Similarly  the biggest problem in running at  Gbps is often getting the bits from the userâs buffer out onto the network fast enough and having the receiving host process them as fast as they come in
If you double the host (CPU and memory) speed  you often can come close to doubling the throughput
Doubling the network capacity has no effect if the bottleneck is in the hosts
Reduce Packet Count to Reduce Overhead Each segment has a certain amount of overhead (
the header) as well as data (
the payload)
Bandwidth is required for both components
Processing is also required for both components (
header processing and doing the checksum)
When  million bytes are being sent  the data cost is the same no matter what the segment size is
However  using   -byte segments means   times as much per-segment overhead as using  -KB segments
The bandwidth and processing overheads add up fast to reduce throughput
Per-packet overhead in the lower layers amplifies this effect
Each arriving packet causes a fresh interrupt if the host is keeping up
On a modern pipelined processor  each interrupt breaks the CPU pipeline  interferes with the cache  requires a change to the memory management context  voids the branch prediction table  and forces a substantial number of CPU registers to be saved
An n-fold reduction in segments sent thus reduces the interrupt and packet overhead by a factor of n
You might say that both people and computers are poor at multitasking
This observation underlies the desire to send MTU packets that are as large as will pass along the network path without fragmentation
Mechanisms such as Nagleâs algorithm and Clarkâs solution are also attempts to avoid sending small packets
Minimize Data Touching The most straightforward way to implement a layered protocol stack is with one module for each layer
Unfortunately  this leads to copying (or at least accessing the data on multiple passes) as each layer does its own work
For example  after a packet is received by the NIC  it is typically copied to a kernel buffer
From there  it is copied to a network layer buffer for network layer processing  then to a transport layer buffer for transport layer processing  and finally to the receiving application process
It is not unusual for an incoming packet to be copied three or four times before the segment enclosed in it is delivered
All this copying can greatly degrade performance because memory operations are an order of magnitude slower than registerâregister instructions
For example  if  % of the instructions actually go to memory (
are cache misses)  which is likely when touching incoming packets  the average instruction execution time is slowed down by a factor of
Hardware assistance will not help here
The problem is too much copying by the operating system
PERFORMANCE ISSUES A clever operating system will minimize copying by combining the processing of multiple layers
For example  TCP and IP are usually implemented together (as ââTCP/IPââ) so that it is not necessary to copy the payload of the packet as processing switches from network to transport layer
Another common trick is to perform multiple operations within a layer in a single pass over the data
For example  checksums are often computed while copying the data (when it has to be copied) and the newly computed checksum is appended to the end
Minimize Context Switches A related rule is that context switches (
from kernel mode to user mode) are deadly
They have the bad properties of interrupts and copying combined
This cost is why transport protocols are often implemented in the kernel
Like reducing packet count  context switches can be reduced by having the library procedure that sends data do internal buffering until it has a substantial amount of them
Similarly  on the receiving side  small incoming segments should be collected together and passed to the user in one fell swoop instead of individually  to minimize context switches
In the best case  an incoming packet causes a context switch from the current user to the kernel  and then a switch to the receiving process to give it the newly arrived data
Unfortunately  with some operating systems  additional context switches happen
For example  if the network manager runs as a special process in user space  a packet arrival is likely to cause a context switch from the current user to the kernel  then another one from the kernel to the network manager  followed by another one back to the kernel  and finally one from the kernel to the receiving process
This sequence is shown in Fig
All these context switches on each packet are wasteful of CPU time and can have a devastating effect on network performance
User space Kernel space    User process running at the time of the packet arrival Network manager Receiving process Figure  -
Four context switches to handle one packet with a user-space network manager
THE TRANSPORT LAYER
Avoiding Congestion Is Better Than Recovering from It The old maxim that an ounce of prevention is worth a pound of cure certainly holds for network congestion
When a network is congested  packets are lost  bandwidth is wasted  useless delays are introduced  and more
All of these costs are unnecessary  and recovering from congestion takes time and patience
Not having it occur in the first place is better
Congestion avoidance is like getting your DTP vaccination: it hurts a little at the time you get it  but it prevents something that would hurt a lot more in the future
Avoid Timeouts Timers are necessary in networks  but they should be used sparingly and timeouts should be minimized
When a timer goes off  some action is generally repeated
If it is truly necessary to repeat the action  so be it  but repeating it unnecessarily is wasteful
The way to avoid extra work is to be careful that timers are set a little bit on the conservative side
A timer that takes too long to expire adds a small amount of extra delay to one connection in the (unlikely) event of a segment being lost
A timer that goes off when it should not have uses up host resources  wastes bandwidth  and puts extra load on perhaps dozens of routers for no good reason
Fast Segment Processing Now that we have covered general rules  we will look at some specific methods for speeding up segment processing
For more information  see Clark et al
(   )  and Chase et al
Segment processing overhead has two components: overhead per segment and overhead per byte
Both must be attacked
The key to fast segment processing is to separate out the normal  successful case (one-way data transfer) and handle it specially
Many protocols tend to emphasize what to do when something goes wrong (
a packet getting lost)  but to make the protocols run fast  the designer should aim to minimize processing time when everything goes right
Minimizing processing time when an error occurs is ondary
Although a sequence of special segments is needed to get into the ESTABLISHED state  once there  segment processing is straightforward until one side starts to close the connection
Let us begin by examining the sending side in the ESTABLISHED state when there are data to be transmitted
For the sake of clarity  we assume here that the transport entity is in the kernel  although the same ideas apply if it is a user-space process or a library inside the sending process
The first thing the transport entity does is test to see if this is the normal case: the state is ESTABLISHED  neither side is trying to close the connection  a regular (
not an   PERFORMANCE ISSUES out-of-band) full segment is being sent  and enough window space is available at the receiver
If all conditions are met  no further tests are needed and the fast path through the sending transport entity can be taken
Typically  this path is taken most of the time
Trap into the kernel to send segment Test Segment passed to the receiving process Test S S Sending process Receiving process Network Figure  -
The fast path from sender to receiver is shown with a heavy line
The processing steps on this path are shaded
In the usual case  the headers of conutive data segments are almost the same
To take advantage of this fact  a prototype header is stored within the transport entity
At the start of the fast path  it is copied as fast as possible to a scratch buffer  word by word
Those fields that change from segment to segment are overwritten in the buffer
Frequently  these fields are easily derived from state variables  such as the next sequence number
A pointer to the full segment header plus a pointer to the user data are then passed to the network layer
Here  the same strategy can be followed (not shown in Fig
Finally  the network layer gives the resulting packet to the data link layer for transmission
As an example of how this principle works in practice  let us consider TCP/IP
The fields that are the same between conutive segments on a one-way flow are shaded
All the sending transport entity has to do is copy the five words from the prototype header into the output buffer  fill in the next sequence number (by copying it from a word in memory)  compute the checksum  and increment the sequence number in memory
It can then hand the header and data to a special IP procedure for sending a regular  maximum segment
IP then copies its five-word prototype header [see Fig
-  (b)] into the buffer  fills in the Identification field  and computes its checksum
The packet is now ready for transmission
Now let us look at fast path processing on the receiving side of Fig
Step  is locating the connection record for the incoming segment
For TCP  the THE TRANSPORT LAYER
Sequence number (a) (b) Header checksum Identification Source port Acknowledgement number Len Unused Window size Checksum Urgent pointer Destination port Fragment offset VER
Total length TTL Protocol Source address Destination address Diff
(a) TCP header
(b) IP header
In both cases  they are taken from the prototype without change
connection record can be stored in a hash table for which some simple function of the two IP addresses and two ports is the key
Once the connection record has been located  both addresses and both ports must be compared to verify that the correct record has been found
An optimization that often speeds up connection record lookup even more is to maintain a pointer to the last one used and try that one first
Clark et al
(   ) tried this and observed a hit rate exceeding  %
The segment is checked to see if it is a normal one: the state is ESTABLISHED  neither side is trying to close the connection  the segment is a full one  no special flags are set  and the sequence number is the one expected
These tests take just a handful of instructions
If all conditions are met  a special fast path TCP procedure is called
The fast path updates the connection record and copies the data to the user
While it is copying  it also computes the checksum  eliminating an extra pass over the data
If the checksum is correct  the connection record is updated and an acknowledgement is sent back
The general scheme of first making a quick check to see if the header is what is expected and then having a special procedure handle that case is called header prediction
Many TCP implementations use it
When this optimization and all the other ones discussed in this  ter are used together  it is possible to get TCP to run at  % of the speed of a local memory-to-memory copy  assuming the network itself is fast enough
Two other areas where major performance gains are possible are buffer management and timer management
The issue in buffer management is avoiding unnecessary copying  as mentioned above
Timer management is important because nearly all timers set do not expire
They are set to guard against segment loss  but most segments and their acknowledgements arrive correctly
Hence  it is important to optimize timer management for the case of timers rarely expiring
A common scheme is to use a linked list of timer events sorted by expiration time
The head entry contains a counter telling how many ticks away from expiry it is
Each successive entry contains a counter telling how many ticks after the   PERFORMANCE ISSUES previous entry it is
Thus  if timers expire in   and   ticks  respectively  the three counters are  and   respectively
At every clock tick  the counter in the head entry is decremented
When it hits zero  its event is processed and the next item on the list becomes the head
Its counter does not have to be changed
This way  inserting and deleting timers are expensive operations  with execution times proportional to the length of the list
A much more efficient approach can be used if the maximum timer interval is bounded and known in advance
Here  an array called a timing wheel can be used  as shown in Fig
Each slot corresponds to one clock tick
The current time shown is T =
Timers are scheduled to expire at   and   ticks from now
If a new timer suddenly is set to expire in seven ticks  an entry is just made in slot
Similarly  if the timer set for T +   has to be canceled  the list starting in slot   has to be searched and the required entry removed
Note that the array of Fig
-  cannot accommodate timers beyond T +
Slot     Pointer to list of timers for T +   Pointer to list of timers for T +  Pointer to list of timers for T +   Current time  T Figure  -
A timing wheel
When the clock ticks  the current time pointer is advanced by one slot (circularly)
If the entry now pointed to is nonzero  all of its timers are processed
Many variations on the basic idea are discussed by Varghese and Lauck (   )
Header Compression We have been looking at fast networks for too long
There is more out there
Let us now consider performance on wireless and other networks in which bandwidth is limited
Reducing software overhead can help mobile computers run THE TRANSPORT LAYER
more efficiently  but it does nothing to improve performance when the network links are the bottleneck
To use bandwidth well  protocol headers and payloads should be carried with the minimum of bits
For payloads  this means using compact encodings of information  such as images that are in JPEG format rather than a bitmap  or document formats such as PDF that include compression
It also means application-level caching mechanisms  such as Web caches that reduce transfers in the first place
What about for protocol headers? At the link layer  headers for wireless networks are typically compact because they were designed with scarce bandwidth in mind
For example
headers have short connection identifiers instead of longer addresses
However  higher layer protocols such as IP  TCP and UDP come in one version for all link layers  and they are not designed with compact headers
In fact  streamlined processing to reduce software overhead often leads to headers that are not as compact as they could otherwise be (
IPv  has a more loosely packed headers than IPv )
The higher-layer headers can be a significant performance hit
Consider  for example  voice-over-IP data that is being carried with the combination of IP  UDP  and RTP
These protocols require   bytes of header (  for IPv for UDP  and   for RTP)
With IPv  the situation is even worse:   bytes  including the  -byte IPv  header
The headers can wind up as the majority of the transmitted data and consume more than half the bandwidth
Header compression is used to reduce the bandwidth taken over links by higher-layer protocol headers
Specially designed schemes are used instead of general purpose methods
This is because headers are short  so they do not compress well individually  and decompression requires all prior data to be received
This will not be the case if a packet is lost
Header compression obtains large gains by using knowledge of the protocol format
One of the first schemes was designed by Van Jacobson (   ) for compressing TCP/IP headers over slow serial links
It is able to compress a typical TCP/IP header of   bytes down to an average of  bytes
The trick to this method is hinted at in Fig
Many of the header fields do not change from packet to packet
There is no need  for example  to send the same IP TTL or the same TCP port numbers in each and every packet
They can be omitted on the sending side of the link and filled in on the receiving side
Similarly  other fields change in a predictable manner
For example  barring loss  the TCP sequence number advances with the data
In these cases  the receiver can predict the likely value
The actual number only needs to be carried when it differs from what is expected
Even then  it may be carried as a small change from the previous value  as when the acknowledgement number increases when new data is received in the reverse direction
With header compression  it is possible to have simple headers in higher-layer protocols and compact encodings over low bandwidth links
ROHC (RObust Header Compression) is a modern version of header compression that is defined   PERFORMANCE ISSUES as a framework in RFC
It is designed to tolerate the loss that can occur on wireless links
There is a profile for each set of protocols to be compressed  such as IP/UDP/RTP
Compressed headers are carried by referring to a context  which is essentially a connection; header fields may easily be predicted for packets of the same connection  but not for packets of different connections
In typical operation  ROHC reduces IP/UDP/RTP headers from   bytes to  to  bytes
While header compression is mainly targeted at reducing bandwidth needs  it can also be useful for reducing delay
Delay is comprised of propagation delay  which is fixed given a network path  and transmission delay  which depends on the bandwidth and amount of data to be sent
For example  a  -Mbps link sends  bit in  Î¼
In the case of media over wireless networks  the network is relatively slow so transmission delay may be an important factor in overall delay and consistently low delay is important for quality of service
Header compression can help by reducing the amount of data that is sent  and hence reducing transmission delay
The same effect can be achieved by sending smaller packets
This will trade increased software overhead for decreased transmission delay
Note that another potential source of delay is queueing delay to access the wireless link
This can also be significant because wireless links are often heavily used as the limited resource in a network
In this case  the wireless link must have quality-of-service mechanisms that give low delay to real-time packets
Header compression alone is not sufficient
Protocols for Long Fat Networks Since the s  there have been gigabit networks that transmit data over large distances
Because of the combination of a fast network  or ââfat pipe ââ and long delay  these networks are called long fat networks
When these networks arose  peopleâs first reaction was to use the existing protocols on them  but various problems quickly arose
In this tion  we will discuss some of the problems with scaling up the speed and delay of network protocols
The first problem is that many protocols use  -bit sequence numbers
When the Internet began  the lines between routers were mostly  -kbps leased lines  so a host blasting away at full speed took over  week to cycle through the sequence numbers
To the TCP designers  was a pretty decent approximation of infinity because there was little danger of old packets still being around a week after they were transmitted
With  -Mbps Ethernet  the wrap time became   minutes  much shorter  but still manageable
With a  -Gbps Ethernet pouring data out onto the Internet  the wrap time is about   onds  well under the   - maximum packet lifetime on the Internet
All of a sudden  is not nearly as good an approximation to infinity since a fast sender can cycle through the sequence space while old packets still exist
The problem is that many protocol designers simply assumed  without stating it  that the time required to use up the entire sequence space would greatly exceed THE TRANSPORT LAYER
the maximum packet lifetime
Consequently  there was no need to even worry about the problem of old duplicates still existing when the sequence numbers wrapped around
At gigabit speeds  that unstated assumption fails
Fortunately  it proved possible to extend the effective sequence number by treating the timestamp that can be carried as an option in the TCP header of each packet as the high-order bits
This mechanism is called PAWS (Protection Against Wrapped Sequence numbers) and is described in RFC
A ond problem is that the size of the flow control window must be greatly increased
Consider  for example  sending a  -KB burst of data from San Diego to Boston in order to fill the receiverâs  -KB buffer
Suppose that the link is  Gbps and the one-way speed-of-light-in-fiber delay is   m
Initially  at t =   the pipe is empty  as illustrated in Fig
Only Î¼ later  in Fig
The lead segment will now be somewhere in the vicinity of Brawley  still deep in Southern California
However  the transmitter must stop until it gets a window update
(a) (b) (c) (d) Data Acknowledgements Figure  -
The state of transmitting  Mbit from San Diego to Boston
(a) At t =
(b) After Î¼
(c) After   m
(d) After   m
After   m  the lead segment hits Boston  as shown in Fig
-  (c)  and is acknowledged
Finally m after starting  the first acknowledgement gets   PERFORMANCE ISSUES back to the sender and the ond burst can be transmitted
Since the transmission line was used for
m out of  the efficiency is about
This situation is typical of an older protocols running over gigabit lines
A useful quantity to keep in mind when analyzing network performance is the bandwidth-delay product
It is obtained by multiplying the bandwidth (in bits/) by the round-trip delay time (in )
The product is the capacity of the pipe from the sender to the receiver and back (in bits)
For the example of Fig
In other words  the sender would have to transmit a burst of   million bits to be able to keep going full speed until the first acknowledgement came back
It takes this many bits to fill the pipe (in both directions)
This is why a burst of half a million bits only achieves a
% efficiency: it is only
% of the pipeâs capacity
The conclusion that can be drawn here is that for good performance  the receiverâs window must be at least as large as the bandwidth-delay product  and preferably somewhat larger since the receiver may not respond instantly
For a transcontinental gigabit line  at least  MB are required
A third and related problem is that simple retransmission schemes  such as the go-back-n protocol  perform poorly on lines with a large bandwidth-delay product
Consider  the  -Gbps transcontinental link with a round-trip transmission time of   m
A sender can transmit  MB in one round trip
If an error is detected  it will be   m before the sender is told about it
If go-back-n is used  the sender will have to retransmit not just the bad packet  but also the  MB worth of packets that came afterward
Clearly  this is a massive waste of resources
More complex protocols such as selective-repeat are needed
A fourth problem is that gigabit lines are fundamentally different from megabit lines in that long gigabit lines are delay limited rather than bandwidth limited
At speeds up to  Mbps  the transmission time is dominated by the rate at which the bits can be sent
By  Gbps  the  -m round-trip delay dominates the  m it takes to put the bits on the fiber
Further increases in bandwidth have hardly any effect at all
Figure  -  has unfortunate implications for network protocols
It says that stop-and-wait protocols  such as RPC  have an inherent upper bound on their performance
This limit is dictated by the speed of light
No amount of technological progress in optics will ever improve matters (new laws of physics would help  though)
Unless some other use can be found for a gigabit line while a host is waiting for a reply  the gigabit line is no better than a megabit line  just more expensive
A fifth problem is that communication speeds have improved faster than computing speeds
(Note to computer engineers: go out and beat those communication engineers! We are counting on you
) In the s  the ARPANET ran at   kbps and had computers that ran at about  MIPS
Compare these numbers to THE TRANSPORT LAYER
m   m  m File transfer time Data rate (bps)        Figure  -
Time to transfer and acknowledge a  -Mbit file over a -km line
The number of instructions per byte has decreased by more than a factor of
The exact numbers are debatable depending on dates and scenarios  but the conclusion is this: there is less time available for protocol processing than there used to be  so protocols must become simpler
Let us now turn from the problems to ways of dealing with them
The basic principle that all high-speed network designers should learn by heart is: Design for speed  not for bandwidth optimization
Old protocols were often designed to minimize the number of bits on the wire  frequently by using small fields and packing them together into bytes and words
This concern is still valid for wireless networks  but not for gigabit networks
Protocol processing is the problem  so protocols should be designed to minimize it
The IPv  designers clearly understood this principle
A tempting way to go fast is to build fast network interfaces in hardware
The difficulty with this strategy is that unless the protocol is exceedingly simple  hardware just means a plug-in board with a ond CPU and its own program
To make sure the network coprocessor is cheaper than the main CPU  it is often a slower chip
The consequence of this design is that much of the time the main (fast) CPU is idle waiting for the ond (slow) CPU to do the critical work
It is a myth to think that the main CPU has other work to do while waiting
Furthermore  when two general-purpose CPUs communicate  race conditions can occur  so elaborate protocols are needed between the two processors to synchronize   PERFORMANCE ISSUES them correctly and avoid races
Usually  the best approach is to make the protocols simple and have the main CPU do the work
Packet layout is an important consideration in gigabit networks
The header should contain as few fields as possible  to reduce processing time  and these fields should be big enough to do the job and be word-aligned for fast processing
In this context  ââbig enoughââ means that problems such as sequence numbers wrapping around while old packets still exist  receivers being unable to advertise enough window space because the window field is too small  etc
do not occur
The maximum data size should be large  to reduce software overhead and permit efficient operation
bytes is too small for high-speed networks  which is why gigabit Ethernet supports jumbo frames of up to  KB and IPv  supports jumbogram packets in excess of   KB
Let us now look at the issue of feedback in high-speed protocols
Due to the (relatively) long delay loop  feedback should be avoided: it takes too long for the receiver to signal the sender
One example of feedback is governing the transmission rate by using a sliding window protocol
Future protocols may switch to rate-based protocols to avoid the (long) delays inherent in the receiver sending window updates to the sender
In such a protocol  the sender can send all it wants to  provided it does not send faster than some rate the sender and receiver have agreed upon in advance
A ond example of feedback is Jacobsonâs slow start algorithm
This algorithm makes multiple probes to see how much the network can handle
With high-speed networks  making half a dozen or so small probes to see how the network responds wastes a huge amount of bandwidth
A more efficient scheme is to have the sender  receiver  and network all reserve the necessary resources at connection setup time
Reserving resources in advance also has the advantage of making it easier to reduce jitter
In short  going to high speeds inexorably pushes the design toward connection-oriented operation  or something fairly close to it
Another valuable feature is the ability to send a normal amount of data along with the connection request
In this way  one round-trip time can be saved  DELAY-TOLERANT NETWORKING We will finish this  ter by describing a new kind of transport that may one day be an important component of the Internet
TCP and most other transport protocols are based on the assumption that the sender and the receiver are continuously connected by some working path  or else the protocol fails and data cannot be delivered
In some networks there is often no end-to-end path
An example is a space network as LEO (Low-Earth Orbit) satellites pass in and out of range of ground stations
A given satellite may be able to communicate to a ground station only at particular times  and two satellites may never be able to communicate with each other at any time  even via a ground station  because one of the satellites THE TRANSPORT LAYER
may always be out of range
Other example networks involve submarines  buses  mobile phones  and other devices with computers for which there is intermittent connectivity due to mobility or extreme conditions
In these occasionally connected networks  data can still be communicated by storing them at nodes and forwarding them later when there is a working link
This technique is called message switching
Eventually the data will be relayed to the destination
A network whose architecture is based on this approach is called a DTN (Delay-Tolerant Network  or a Disruption-Tolerant Network)
Work on DTNs started in  when IETF set up a research group on the topic
The inspiration for DTNs came from an unlikely source: efforts to send packets in space
Space networks must deal with intermittent communication and very long delays
Kevin Fall observed that the ideas for these Interplanetary Internets could be applied to networks on Earth in which intermittent connectivity was the norm (Fall  )
This model gives a useful generalization of the Internet in which storage and delays can occur during communication
Data delivery is akin to delivery in the postal system  or electronic mail  rather than packet switching at routers
Since  the DTN architecture has been refined  and the applications of the DTN model have grown
As a mainstream application  consider large datasets of many terabytes that are produced by scientific experiments  media events  or Web-based services and need to be copied to datacenters at different locations around the world
Operators would like to send this bulk traffic at off-peak times to make use of bandwidth that has already been paid for but is not being used  and are willing to tolerate some delay
It is like doing the backups at night when other applications are not making heavy use of the network
The problem is that  for global services  the off-peak times are different at locations around the world
There may be little overlap in the times when datacenters in Boston and Perth have off-peak network bandwidth because night for one city is day for the other
However  DTN models allow for storage and delays during transfer
With this model  it becomes possible to send the dataset from Boston to Amsterdam using off-peak bandwidth  as the cities have time zones that are only  hours apart
The dataset is then stored in Amsterdam until there is off-peak bandwidth between Amsterdam and Perth
It is then sent to Perth to complete the transfer
Laoutaris et al
(   ) have studied this model and find that it can provide substantial capacity at little cost  and that the use of a DTN model often doubles that capacity compared with a traditional end-to-end model
In what follows  we will describe the IETF DTN architecture and protocols
DTN Architecture The main assumption in the Internet that DTNs seek to relax is that an endto- end path between a source and a destination exists for the entire duration of a communication session
When this is not the case  the normal Internet protocols   DELAY-TOLERANT NETWORKING fail
DTNs get around the lack of end-to-end connectivity with an architecture that is based on message switching  as shown in Fig
It is also intended to tolerate links with low reliability and large delays
The architecture is specified in RFC
Contact (working link) Stored bundle Source Storage Sent bundle DTN node Intermittent link (not working) Destination Figure  -
Delay-tolerant networking architecture
In DTN terminology  a message is called a bundle
DTN nodes are equipped with storage  typically persistent storage such as a disk or flash memory
They store bundles until links become available and then forward the bundles
The links work intermittently
A working link is called a contact
In this way  the bundles are relayed via contacts from the source to their destination
The storing and forwarding of bundles at DTN nodes sounds similar to the queueing and forwarding of packets at routers  but there are qualitative differences
In routers in the Internet  queueing occurs for millionds or at most onds
At DTN nodes  bundles may be stored for hours  until a bus arrives in town  while an airplane completes a flight  until a sensor node harvests enough solar energy to run  until a sleeping computer wakes up  and so forth
These examples also point to a ond difference  which is that nodes may move (with a bus or plane) while they hold stored data  and this movement may even be a key part of data delivery
Routers in the Internet are not allowed to move
The whole process of moving bundles might be better known as ââstore-carry-forward
ââ As an example  consider the scenario shown in Fig
-  that was the first use of DTN protocols in space (Wood et al
The source of bundles is an LEO satellite that is recording Earth images as part of the Disaster Monitoring Constellation of satellites
The images must be returned to the collection point
However  the satellite has only intermittent contact with three ground stations as it orbits the Earth
It comes into contact with each ground station in turn
Each of the satellite  ground stations  and collection point act as a DTN node
At each contact  a THE TRANSPORT LAYER
bundle (or a portion of a bundle) is sent to a ground station
The bundles are then sent over a backhaul terrestrial network to the collection point to complete the transfer
Intermittent link (not working) Storage at DTN nodes Satellite Contact Bundle (working link) Ground station Collection point Figure  -
Use of a DTN in space
The primary advantage of the DTN architecture in this example is that it naturally fits the situation of the satellite needing to store images because there is no connectivity at the time the image is taken
There are two further advantages
First  there may be no single contact long enough to send the images
However  they can be spread across the contacts with three ground stations
ond  the use of the link between the satellite and ground station is decoupled from the link over the backhaul network
This means that the satellite download is not limited by a slow terrestrial link
It can proceed at full speed  with the bundle stored at the ground station until it can be relayed to the collection point
An important issue that is not specified by the architecture is how to find good routes via DTN nodes
A route in this path to use
Good routes depend on the nature of the architecture describes when to send data  and also which contacts
Some contacts are known ahead of time
A good example is the motion of heavenly bodies in the space example
For the space experiment  it was known ahead of time when contacts would occur  that the contact intervals ranged from  to   minutes per pass with each ground station  and that the downlink capacity was
Given this knowledge  the transport of a bundle of images can be planned ahead of time
In other cases  the contacts can be predicted  but with less certainty
Examples include buses that make contact with each other in mostly regular ways  due to a timetable  yet with some variation  and the times and amount of off-peak bandwidth in ISP networks  which are predicted from past data
At the other extreme  the contacts are occasional and random
One example is carrying data from user   DELAY-TOLERANT NETWORKING to user on mobile phones depending on which users make contact with each other during the day
When there is unpredictability in contacts  one routing strategy is to send copies of the bundle along different paths in the hope that one of the copies is delivered to the destination before the lifetime is reached
The Bundle Protocol To take a closer look at the operation of DTNs  we will now look at the IETF protocols
DTNs are an emerging kind of network  and experimental DTNs have used different protocols  as there is no requirement that the IETF protocols be used
However  they are at least a good place to start and highlight many of the key issues
The DTN protocol stack is shown in Fig
The key protocol is the Bundle protocol  which is specified in RFC
It is responsible for accepting messages from the application and sending them as one or more bundles via storecarry- forward operations to the destination DTN node
It is also apparent from Fig
In other words  TCP/IP may be used over each contact to move bundles between DTN nodes
This positioning raises the issue of whether the Bundle protocol is a transport layer protocol or an application layer protocol
Just as with RTP  we take the position that  despite running over a transport protocol  the Bundle protocol is providing a transport service to many different applications  and so we cover DTNs in this  ter
Application Bundle Protocol Convergence layer TCP/IP Internet   Other internet Convergence layer Upper layers DTN layer Lower layers Figure  -
Delay-tolerant networking protocol stack
For example  in a space network the links may have very long delays
The round-trip time between Earth and Mars can easily be   minutes depending on the relative position of the planets
Imagine how well TCP acknowledgements and retransmissions will work over that link  especially for relatively short messages
Not well at all
Instead  THE TRANSPORT LAYER
another protocol that uses error-correcting codes might be used
Or in sensor networks that are very resource constrained  a more lightweight protocol than TCP may be used
Since the Bundle protocol is fixed  yet it is intended to run over a variety of transports  there is must be a gap in functionality between the protocols
That gap is the reason for the inclusion of a convergence layer in Fig
The convergence layer is just a glue layer that matches the interfaces of the protocols that it joins
By definition there is a different convergence layer for each different lower layer transport
Convergence layers are commonly found in standards to join new and existing protocols
The format of Bundle protocol messages is shown in Fig
The different fields in these messages tell us some of the key issues that are handled by the Bundle protocol
Bits Type Primary block Payload block Optional blocks Ver
Flags Dest
Source Report Custodian Creation Lifetime Dictionary Flags Length Data variable  Status report Class of service General Bits  variable Figure  -
Bundle protocol message format
Each message consists of a primary block  which can be thought of as a header  a payload block for the data  and optionally other blocks  for example to carry urity parameters
The primary block begins with a Version field (currently  ) followed by a Flags field
Among other functions  the flags encode a class of service to let a source mark its bundles as higher or lower priority  and other handling requests such as whether the destination should acknowledge the bundle
Then come addresses  which highlight three interesting parts of the design
As well as a Destination and Source identifier field  there is a Custodian identifier
The custodian is the party responsible for seeing that the bundle is delivered
In the Internet  the source node is usually the custodian  as it is the node that retransmits if the data is not ultimately delivered to the destination
However  in a DTN  the source node may not always be connected and may have no way of knowing whether the data has been delivered
DTNs deal with this problem using the notion of custody transfer  in which another node  closer to the destination  can assume responsibility for seeing the data safely delivered
For example  if a bundle is stored on an airplane for forwarding at a later time and location  the airplane may become the custodian of the bundle
DELAY-TOLERANT NETWORKING The ond interesting aspect is that these identifiers are not IP addresses
Because the Bundle protocol is intended to work across a variety of transports and internets  it defines its own identifiers
These identifiers are really more like high-level names  such as Web page URLs  than low-level addresses  such as IP addresses
They give DTNs an aspect of application-level routing  such as email delivery or the distribution of software updates
The third interesting aspect is the way the identifiers are encoded
There is also a Report identifier for diagnostic messages
All of the identifiers are encoded as references to a variable length Dictionary field
This provides compression when the custodian or report nodes are the same as the source or the destination
In fact  much of the message format has been designed with both extensibility and efficiency in mind by using a compact representation of variable length fields
The compact representation is important for wireless links and resourceconstrained nodes such as in a sensor network
Next comes a Creation field carrying the time at which the bundle was created  along with a sequence number from the source for ordering  plus a Lifetime field that tells the time at which the bundle data is no longer useful
These fields exist because data may be stored for a long period at DTN nodes and there must be some way to remove stale data from the network
Unlike the Internet  they require that DTN nodes have loosely synchronized clocks
The primary block is completed with the Dictionary field
Then comes the payload block
This block starts with a short Type field that identifies it as a payload  followed by a small set of Flags that describe processing options
Then comes the Data field  preceded by a Length field
Finally  there may be other  optional blocks  such as a block that carries urity parameters
Many aspects of DTNs are being explored in the research community
Good strategies for routing depend on the nature of the contacts  as was mentioned above
Storing data inside the network raises other issues
Now congestion control must consider storage at nodes as another kind of resource that can be depleted
The lack of end-to-end communication also exacerbates urity problems
Before a DTN node takes custody of a bundle  it may want to know that the sender is authorized to use the network and that the bundle is probably wanted by the destination
Solutions to these problems will depend on the kind of DTN  as space networks are different from sensor networks  SUMMARY The transport layer is the key to understanding layered protocols
It provides various services  the most important of which is an end-to-end  reliable  connection- oriented byte stream from sender to receiver
It is accessed through service primitives that permit the establishment  use  and release of connections
A common transport layer interface is the one provided by Berkeley sockets
THE TRANSPORT LAYER
Transport protocols must be able to do connection management over unreliable networks
Connection establishment is complicated by the existence of delayed duplicate packets that can reappear at inopportune moments
To deal with them  three-way handshakes are needed to establish connections
Releasing a connection is easier than establishing one but is still far from trivial due to the two-army problem
Even when the network layer is completely reliable  the transport layer has plenty of work to do
It must handle all the service primitives  manage connections and timers  allocate bandwidth with congestion control  and run a variablesized sliding window for flow control
Congestion control should allocate all of the available bandwidth between competing flows fairly  and it should track changes in the usage of the network
The AIMD control law converges to a fair and efficient allocation
The Internet has two main transport protocols: UDP and TCP
UDP is a connectionless protocol that is mainly a wrapper for IP packets with the additional feature of multiplexing and demultiplexing multiple processes using a single IP address
UDP can be used for client-server interactions  for example  using RPC
It can also be used for building real-time protocols such as RTP
The main Internet transport protocol is TCP
It provides a reliable  bidirectional  congestion-controlled byte stream with a  -byte header on all segments
A great deal of work has gone into optimizing TCP performance  using algorithms from Nagle  Clark  Jacobson  Karn  and others
Network performance is typically dominated by protocol and segment processing overhead  and this situation gets worse at higher speeds
Protocols should be designed to minimize the number of segments and work for large bandwidthdelay paths
For gigabit networks  simple protocols and streamlined processing are called for
Delay-tolerant networking provides a delivery service across networks that have occasional connectivity or long delays across links
Intermediate nodes store  carry  and forward bundles of information so that it is eventually delivered  even if there is no working path from sender to receiver at any time
In our example transport primitives of Fig
Is this strictly necessary? If not  explain how a nonblocking primitive could be used
What advantage would this have over the scheme described in the text?
Primitives of transport service assume asymmetry between the two end points during connection establishment  one end (server) executes LISTEN while the other end (client) executes CONNECT
However  in peer to peer applications such file sharing
PROBLEMS systems
BitTorrent  all end points are peers
There is no server or client functionality
How can transport service primitives may be used to build such peer to peer applications?
In the underlying model of Fig
Suppose that the network layer is percent reliable and never loses packets
What changes  if any  are needed to Fig
In both parts of Fig
Why is this so important?
In the Internet File Server example (Figure  - )  can the connect( ) system call on the client fail for any reason other than listen queue being full on the server? Assume that the network is perfect One criteria for deciding whether to have a server active all the time or have it start on demand using a process server is how frequently the service provided is used
Can you think of any other criteria for making this decision?
Suppose that the clock-driven scheme for generating initial sequence numbers is used with a  -bit wide clock counter
The clock ticks once every m  and the maximum packet lifetime is
How often need resynchronization take place (a) in the worst case? (b) when the data consumes sequence numbers/min?
Why does the maximum packet lifetime  T  have to be large enough to ensure that not only the packet but also its acknowledgements have vanished?
Imagine that a two-way handshake rather than a three-way handshake were used to set up connections
In other words  the third message was not required
Are deadlocks now possible? Give an example or show that none exist Imagine a generalized n-army problem  in which the agreement of any two of the blue armies is sufficient for victory
Does a protocol exist that allows blue to win?
Consider the problem of recovering from host crashes (
If the interval between writing and sending an acknowledgement  or vice versa  can be made relatively small  what are the two best sender-receiver strategies for minimizing the chance of a protocol failure?
In Figure  -   suppose a new flow E is added that takes a path from R  to R  to R
How does the max-min bandwidth allocation change for the five flows?
Discuss the advantages and disadvantages of credits versus sliding window protocols Some other policies for fairness in congestion control are Additive Increase Additive Decrease (AIAD)  Multiplicative Increase Additive Decrease (MIAD)  and Multiplicative Increase Multiplicative Decrease (MIMD)
Discuss these three policies in terms of convergence and stability Why does UDP exist? Would it not have been enough to just let user processes send raw IP packets? THE TRANSPORT LAYER   Consider a simple application-level protocol built on top of UDP that allows a client to retrieve a file from a remote server residing at a well-known address
The client first sends a request with a file name  and the server responds with a sequence of data packets containing different parts of the requested file
To ensure reliability and sequenced delivery  client and server use a stop-and-wait protocol
Ignoring the obvious performance issue  do you see a problem with this protocol? Think carefully about the possibility of processes crashing A client sends a   -byte request to a server located km away over a  -gigabit optical fiber
What is the efficiency of the line during the remote procedure call?
Consider the situation of the previous problem again
Compute the minimum possible response time both for the given  -Gbps line and for a  -Mbps line
What conclusion can you draw?
Both UDP and TCP use port numbers to identify the destination entity when delivering a message
Give two reasons why these protocols invented a new abstract ID (port numbers)  instead of using process IDs  which already existed when these protocols were designed Several RPC implementations provide an option to the client to use RPC implemented over UDP or RPC implemented over TCP
Under what conditions will a client prefer to use RPC over UDP and under what conditions will he prefer to use RPC over TCP?
Consider two networks  N  and N  that have the same average delay between a source A and a destination D
In N  the delay experienced by different packets is unformly distributed with maximum delay being   onds  while in N % of the packets experience less than one ond delay with no limit on maximum delay
Discuss how RTP may be used in these two cases to transmit live audio/video stream What is the total size of the minimum TCP MTU  including TCP and IP overhead but not including data link layer overhead?
Datagram fragmentation and reassembly are handled by IP and are invisible to TCP
Does this mean that TCP does not have to worry about data arriving in the wrong order?
RTP is used to transmit CD-quality audio  which makes a pair of  -bit samples   times/  one sample for each of the stereo channels
How many packets per ond must RTP transmit?
Would it be possible to place the RTP code in the operating system kernel  along with the UDP code? Explain your answer A process on host  has been assigned port p  and a process on host  has been assigned port q
Is it possible for there to be two or more TCP connections between these two ports at the same time?
Does this really add anything? Why or why not?
The maximum payload of a TCP segment is   bytes
Why was such a strange number chosen?
Describe two ways to get into the SYN RCVD state of Fig
The receive window is   KB and the maximum segment size is  KB
How long does it take before the first full window can be sent?
Suppose that the TCP congestion window is set to   KB and a timeout occurs
How big will the window be if the next four transmission bursts are all successful? Assume that the maximum segment size is  KB If the TCP round-trip time  RTT  is currently   m and the following acknowledgements come in after    and   m  respectively  what is the new RTT estimate using the Jacobson algorithm? Use Î± =
A TCP machine is sending full windows of   bytes over a  -Gbps channel that has a  -m one-way delay
What is the maximum throughput achievable? What is the line efficiency?
What is the fastest line speed at which a host can blast out -byte TCP payloads with a   - maximum packet lifetime without having the sequence numbers wrap around? Take TCP  IP  and Ethernet overhead into consideration
Assume that Ethernet frames may be sent continuously To address the limitations of IP version   a major effort had to be undertaken via IETF that resulted in the design of IP version  and there are still is significant reluctance in the adoption of this new version
However  no such major effort is needed to address the limitations of TCP
Explain why this is the case In a network whose max segment is bytes  max segment lifetime is     and has  -bit sequence numbers  what is the maximum data rate per connection?
Suppose that you are measuring the time to receive a segment
When an interrupt occurs  you read out the system clock in millionds
When the segment is fully processed  you read out the clock again
You measure  m    times and  m    times
How long does it take to receive a segment?
A CPU executes instructions at the rate of  MIPS
Data can be copied   bits at a time  with each word copied costing   instructions
If an coming packet has to be copied four times  can this system handle a  -Gbps line? For simplicity  assume that all instructions  even those instructions that read or write memory  run at the full -MIPS rate To get around the problem of sequence numbers wrapping around while old packets still exist  one could use  -bit sequence numbers
However  theoretically  an optical fiber can run at   Tbps
What maximum packet lifetime is required to make sure that future  -Tbps networks do not have wraparound problems even with  -bit sequence numbers? Assume that each byte has its own sequence number  as TCP does In
we calculated that a gigabit line dumps   packets/ on the host  giving it only  instructions to process it and leaving half the CPU time for applications
This calculation assumed a -byte packet
Redo the calculation for an ARPANET-sized packet (   bytes)
In both cases  assume that the packet sizes given include all overhead
THE TRANSPORT LAYER   For a  -Gbps network operating over  km  the delay is the limiting factor  not the bandwidth
Consider a MAN with the average source and destination   km apart
At what data rate does the round-trip delay due to the speed of light equal the transmission delay for a  -KB packet?
Calculate the bandwidth-delay product for the following networks: ( ) T  (
Mbps)  ( ) Ethernet (  Mbps)  ( ) T  (  Mbps)  and ( ) STS-  (   Mbps)
Assume an RTT of m
Recall that a TCP header has   bits reserved for Window Size
What are its implications in light of your calculations?
What is the bandwidth-delay product for a  -Mbps channel on a geostationary satellite? If the packets are all  bytes (including overhead)  how big should the window be in packets?
The file server of Fig
Make the following modifications
(a) Give the client a third argument that specifies a byte range
(b) Add a client flag âw that allows the file to be written to the server One common function that all network protocols need is to manipulate messages
Recall that protocols manipulate messages by adding/striping headers
Some protocols may break a single message into multiple fragments  and later join these multiple fragments back into a single message
To this end  design and implement a message management library that provides support for creating a new message  attaching a header to a message  stripping a header from a message  breaking a message into two messages  combining two messages into a single message  and saving a copy of a message
Your implementation must minimize data copying from one buffer to another as much as possible
It is critical that the operations that manipulate messages do not touch the data in a message  but rather  only manipulate pointers Design and implement a chat system that allows multiple groups of users to chat
A chat coordinator resides at a well-known network address  uses UDP for communication with chat clients  sets up chat servers for each chat session  and maintains a chat session directory
There is one chat server per chat session
A chat server uses TCP for communication with clients
A chat client allows users to start  join  and leave a chat session
Design and implement the coordinator  server  and client code
THE APPLICATION LAYER Having finished all the preliminaries  we now come to the layer where all the applications are found
The layers below the application layer are there to provide transport services  but they do not do real work for users
In this  ter  we will study some real network applications
However  even in the application layer there is a need for support protocols  to allow the applications to function
Accordingly  we will look at an important one of these before starting with the applications themselves
The item in question is DNS  which handles naming within the Internet
After that  we will examine three real applications: electronic mail  the World Wide Web  and multimedia
We will finish the  ter by saying more about content distribution  including by peer-to-peer networks  DNSâTHE DOMAIN NAME SYSTEM Although programs theoretically could refer to Web pages  mailboxes  and other resources by using the network (
IP) addresses of the computers on which they are stored  these addresses are hard for people to remember
Also  browsing a companyâs Web pages from
means that if the company moves the Web server to a different machine with a different IP address  everyone needs to be told the new IP address
Consequently  high-level  readable names were introduced in order to decouple machine names from machine addresses
In    THE APPLICATION LAYER
this way  the companyâs Web server might be known as
regardless of its IP address
Nevertheless  since the network itself understands only numerical addresses  some mechanism is required to convert the names to network addresses
In the following tions  we will study how this mapping is accomplished in the Internet
Way back in the ARPANET days  there was simply a file     that listed all the computer names and their IP addresses
Every night  all the hosts would fetch it from the site at which it was maintained
For a network of a few hundred large timesharing machines  this approach worked reasonably well
However  well before many millions of PCs were connected to the Internet  everyone involved with it realized that this approach could not continue to work forever
For one thing  the size of the file would become too large
However  even more importantly  host name conflicts would occur constantly unless names were centrally managed  something unthinkable in a huge international network due to the load and latency
To solve these problems  DNS (Domain Name System) was invented in
It has been a key part of the Internet ever since
The essence of DNS is the invention of a hierarchical  domain-based naming scheme and a distributed database system for implementing this naming scheme
It is primarily used for mapping host names to IP addresses but can also be used for other purposes
DNS is defined in RFCs     and further elaborated in many others
Very briefly  the way DNS is used is as follows
To map a name onto an IP address  an application program calls a library procedure called the resolver  passing it the name as a parameter
We saw an example of a resolver  gethostbyname  in Fig
The resolver sends a query containing the name to a local DNS server  which looks up the name and returns a response containing the IP address to the resolver  which then returns it to the caller
The query and response messages are sent as UDP packets
Armed with the IP address  the program can then establish a TCP connection with the host or send it UDP packets
The DNS Name Space Managing a large and constantly changing set of names is a nontrivial problem
In the postal system  name management is done by requiring letters to specify (implicitly or explicitly) the country  state or province  city  street address  and name of the addressee
Using this kind of hierarchical addressing ensures that there is no confusion between the Marvin Anderson on Main St
in White Plains
and the Marvin Anderson on Main St
in Austin  Texas
DNS works the same way
For the Internet  the top of the naming hierarchy is managed by an organization called ICANN (Internet Corporation for Assigned Names and Numbers)
ICANN was created for this purpose in  as part of the maturing of the Internet to a worldwide  economic concern
Conceptually  the Internet is divided into   DNSâTHE DOMAIN NAME SYSTEM over top-level domains  where each domain covers many hosts
Each domain is partitioned into subdomains  and these are further partitioned  and so on
All these domains can be represented by a tree  as shown in Fig
The leaves of the tree represent domains that have no subdomains (but do contain machines  of course)
A leaf domain may contain a single host  or it may represent a company and contain thousands of hosts  eng cisco acm ieee eng washington cs robot jack jill ac co csl nec cs uwa keio edu vu oce cs law aero com edu gov museum org net au jp uk us nl Generic Countries
filts fluit Figure  -
A portion of the Internet domain name space
The top-level domains come in two flavors: generic and countries
The generic domains  listed in Fig
Other generic top-level domains will be added in the future
The country domains include one entry for every country  as defined in ISO
Internationalized country domain names that use non-Latin alphabets were introduced in
These domains let people name hosts in Arabic  Cyrillic  Chinese  or other languages
Getting a ond-level domain  such as name-of-   is easy
The top-level domains are run by registrars appointed by ICANN
Getting a name merely requires going to a corresponding registrar (for com in this case) to check if the desired name is available and not somebody elseâs trademark
If there are no problems  the requester pays the registrar a small annual fee and gets the name
However  as the Internet has become more commercial and more international  it has also become more contentious  especially in matters related to naming
This controversy includes ICANN itself
For example  the creation of the xxx domain took several years and court cases to resolve
Is voluntarily placing adult content in its own domain a good or a bad thing? (Some people did not want adult content available at all on the Internet while others wanted to put it all in one domain so nanny filters could easily find and block it from children)
Some of the domains self-organize  while others have restrictions on who can obtain a name  as noted in Fig
But what restrictions are appropriate? Take the pro domain  THE APPLICATION LAYER
Domain Intended use Start date Restricted? com Commercial  No edu Educational institutions  Yes gov Government  Yes int International organizations  Yes mil Military  Yes net Network providers  No org Non-profit organizations  No aero Air transport  Yes biz Businesses  No coop Cooperatives  Yes info Informational  No museum Museums  Yes name People  No pro Professionals  Yes cat Catalan  Yes jobs Employment  Yes mobi Mobile devices  Yes tel Contact details  Yes travel Travel industry  Yes xxx Sex industry  No Figure  -
Generic top-level domains
for example
It is for qualified professionals
But who is a professional? Doctors and lawyers clearly are professionals
But what about freelance photographers  piano teachers  magicians  plumbers  barbers  exterminators  tattoo artists  mercenaries  and prostitutes? Are these occupations eligible? According to whom? There is also money in names
Tuvalu (the country) sold a lease on its tv domain for $  million  all because the country code is well-suited to advertising television sites
Virtually every common (English) word has been taken in the com domain  along with the most common misspellings
Try household articles  animals  plants  body parts  etc
The practice of registering a domain only to turn around and sell it off to an interested party at a much higher price even has a name
It is called cybersquatting
Many companies that were slow off the mark when the Internet era began found their obvious domain names already taken when they tried to acquire them
In general  as long as no trademarks are being violated and no fraud is involved  it is first-come  first-served with names
Nevertheless  policies to resolve naming disputes are still being refined
DNSâTHE DOMAIN NAME SYSTEM Each domain is named by the path upward from it to the (unnamed) root
The components are separated by periods (pronounced ââdotââ)
Thus  the engineering department at Cisco might be
rather than a UNIX-style name such as /com/cisco/eng
Notice that this hierarchical naming means that
does not conflict with a potential use of eng in
which might be used by the English department at the University of Washington
Domain names can be either absolute or relative
An absolute domain name always ends with a period (
)  whereas a relative one does not
Relative names have to be interpreted in some context to uniquely determine their true meaning
In both cases  a named domain refers to a specific node in the tree and all the nodes under it
Domain names are case-insensitive  so edu  Edu  and EDU mean the same thing
Component names can be up to   characters long  and full path names must not exceed characters
In principle  domains can be inserted into the tree in either generic or country domains
For example    could equally well be listed under the us country domain as   In practice  however  most organizations in the United States are under generic domains  and most outside the United States are under the domain of their country
There is no rule against registering under multiple top-level domains
Large companies often do so (
Each domain controls how it allocates the domains under it
For example  Japan has domains   and   that mirror edu and com
The Netherlands does not make this distinction and puts all organizations directly under nl
Thus  all three of the following are university computer science departments:  edu (University of Washington  in the
)   (Vrije Universiteit  in The Netherlands)
(Keio University  in Japan)
To create a new domain  permission is required of the domain in which it will be included
For example  if a VLSI group is started at the University of Washington and wants to be known as
it has to get permission from whoever manages
Similarly  if a new university is chartered  say  the University of Northern South Dakota  it must ask the manager of the edu domain to assign it   (if that is still available)
In this way  name conflicts are avoided and each domain can keep track of all its subdomains
Once a new domain has been created and registered  it can create subdomains  such as    without getting permission from anybody higher up the tree
Naming follows organizational boundaries  not physical networks
For example  if the computer science and electrical engineering departments are located in the same building and share the same LAN  they can nevertheless have distinct THE APPLICATION LAYER
Similarly  even if computer science is split over Babbage Hall and Turing Hall  the hosts in both buildings will normally belong to the same domain
Domain Resource Records Every domain  whether it is a single host or a top-level domain  can have a set of resource records associated with it
These records are the DNS database
For a single host  the most common resource record is just its IP address  but many other kinds of resource records also exist
When a resolver gives a domain name to DNS  what it gets back are the resource records associated with that name
Thus  the primary function of DNS is to map domain names onto resource records
A resource record is a five-tuple
Although they are encoded in binary for efficiency  in most expositions resource records are presented as ASCII text  one line per resource record
The format we will use is as follows: Domain name Time to live Class Type Value The Domain name tells the domain to which this record applies
Normally  many records exist for each domain and each copy of the database holds information about multiple domains
This field is thus the primary search key used to satisfy queries
The order of the records in the database is not significant
The Time to live field gives an indication of how stable the record is
Information that is highly stable is assigned a large value  such as  (the number of onds in  day)
Information that is highly volatile is assigned a small value  such as   (  minute)
We will come back to this point later when we have discussed caching
The third field of every resource record is the Class
For Internet information  it is always IN
For non-Internet information  other codes can be used  but in practice these are rarely seen
The Type field tells what kind of record this is
There are many kinds of DNS records
The important types are listed in Fig
An SOA record provides the name of the primary source of information about the name serverâs zone (described below)  the email address of its administrator  a unique serial number  and various flags and timeouts
The most important record type is the A (Address) record
It holds a  -bit IPv  address of an interface for some host
The corresponding AAAA  or ââquad A ââ record holds a   -bit IPv  address
Every Internet host must have at least one IP address so that other machines can communicate with it
Some hosts have two or more network interfaces  in which case they will have two or more type A or AAAA resource records
Consequently  DNS can return multiple addresses for a single name
A common record type is the MX record
It specifies the name of the host prepared to accept email for the specified domain
It is used because not every   DNSâTHE DOMAIN NAME SYSTEM Type Meaning Value SOA Start of authority Parameters for this zone A IPv  address of a host  -Bit integer AAAA IPv  address of a host   -Bit integer MX Mail exchange Priority  domain willing to accept email NS Name server Name of a server for this domain CNAME Canonical name Domain name PTR Pointer Alias for an IP address SPF Sender policy framework Text encoding of mail sending policy SRV Service Host that provides it TXT Text Descriptive ASCII text Figure  -
The principal DNS resource record types
machine is prepared to accept email
If someone wants to send email to  for example  bill@   the sending host needs to find some mail server located at   that is willing to accept email
The MX record can provide this information
Another important record type is the NS record
It specifies a name server for the domain or subdomain
This is a host that has a copy of the database for a domain
It is used as part of the process to look up names  which we will describe shortly
CNAME records allow aliases to be created
For example  a person familiar with Internet naming in general and wanting to send a message to user paul in the computer science department at
might guess that paul@  will work
Actually  this address will not work  because the domain for
âs computer science department is
However  as a service to people who do not know this
could create a CNAME entry to point people and programs in the right direction
An entry like this one might do the job:    IN CNAME   Like CNAME  PTR points to another name
However  unlike CNAME  which is really just a macro definition (
a mechanism to replace one string by another)  PTR is a regular DNS data type whose interpretation depends on the context in which it is found
In practice  it is nearly always used to associate a name with an IP address to allow lookups of the IP address and return the name of the corresponding machine
These are called reverse lookups
SRV is a newer type of record that allows a host to be identified for a given service in a domain
For example  the Web server for   could be identified as   This record generalizes the MX record that performs the same task but it is just for mail servers
THE APPLICATION LAYER
SPF is also a newer type of record
It lets a domain encode information about what machines in the domain will send mail to the rest of the Internet
This helps receiving machines check that mail is valid
If mail is being received from a machine that calls itself dodgy but the domain records say that mail will only be sent out of the domain by a machine called smtp  chances are that the mail is forged junk mail
Last on the list  TXT records were originally provided to allow domains to identify themselves in arbitrary ways
Nowadays  they usually encode machinereadable information  typically the SPF information
Finally  we have the Value field
This field can be a number  a domain name  or an ASCII string
The semantics depend on the record type
A short description of the Value fields for each of the principal record types is given in Fig
For an example of the kind of information one might find in the DNS database of a domain  see Fig
This figure depicts part of a (hypothetical) database for the   domain shown in Fig
The database contains seven types of resource records
; Authoritative data for
IN SOA star boss (     )
IN MX  zephyr
IN MX  top
IN NS star star  IN A
zephyr  IN A
www  IN CNAME
ftp  IN CNAME
flits  IN A
flits  IN A
flits  IN MX  flits flits  IN MX  zephyr flits  IN MX  top rowboat IN A
IN MX  rowboat IN MX  zephyr little-sister IN A
laserjet IN A
A portion of a possible DNS database for
The first noncomment line of Fig
Then come two entries giving the first   DNSâTHE DOMAIN NAME SYSTEM and ond places to try to deliver email sent to person@
The zephyr (a specific machine) should be tried first
If that fails  the top should be tried as the next choice
The next line identifies the name server for the domain as star
After the blank line (added for readability) come lines giving the IP addresses for the star  zephyr  and top
These are followed by an alias
so that this address can be used without designating a specific machine
Creating this alias allows   to change its World Wide Web server without invalidating the address people use to get to it
A similar argument holds for   The tion for the machine flits lists two IP addresses and three choices are given for handling email sent to   First choice is naturally the flits itself  but if it is down  the zephyr and top are the ond and third choices
The next three lines contain a typical entry for a computer  in this case    The information provided contains the IP address and the primary and ondary mail drops
Then comes an entry for a computer that is not capable of receiving mail itself  followed by an entry that is likely for a printer that is connected to the Internet
Name Servers In theory at least  a single name server could contain the entire DNS database and respond to all queries about it
In practice  this server would be so overloaded as to be useless
Furthermore  if it ever went down  the entire Internet would be crippled
To avoid the problems associated with having only a single source of information  the DNS name space is divided into nonoverlapping zones
One possible way to divide the name space of Fig
Each circled zone contains some part of the tree  eng cisco acm ieee eng washington cs robot jack jill ac co csl nec cs uwa keio edu vu oce cs law aero com edu gov museum org net au jp uk us nl Generic Countries
flits fluit Figure  -
Part of the DNS name space divided into zones (which are circled)
THE APPLICATION LAYER
Where the zone boundaries are placed within a zone is up to that zoneâs administrator
This decision is made in large part based on how many name servers are desired  and where
For example  in Fig
That is a separate zone with its own name servers
Such a decision might be made when a department such as English does not wish to run its own name server  but a department such as Computer Science does
Each zone is also associated with one or more name servers
These are hosts that hold the database for the zone
Normally  a zone will have one primary name server  which gets its information from a file on its disk  and one or more ondary name servers  which get their information from the primary name server
To improve reliability  some of the name servers can be located outside the zone
The process of looking up a name and finding an address is called name resolution
When a resolver has a query about a domain name  it passes the query to a local name server
If the domain being sought falls under the jurisdiction of the name server  such as
falling under    it returns the authoritative resource records
An authoritative record is one that comes from the authority that manages the record and is thus always correct
Authoritative records are in contrast to cached records  which may be out of date
What happens when the domain is remote  such as when
wants to find the IP address of
at UW (University of Washington)? In this case  and if there is no cached information about the domain available locally  the name server begins a remote query
This query follows the process shown in Fig
Step  shows the query that is sent to the local name server
The query contains the domain name sought  the type (A)  and the class(IN)
: query  : query  : edu  :    : query  : query  :    :
: query Local ( ) name server UWCS name server UW name server Edu name server ( - ) Root name server ( - )
Originator Figure  -
Example of a resolver looking up a remote name in   steps
The next step is to start at the top of the name hierarchy by asking one of the root name servers
These name servers have information about each top-level   DNSâTHE DOMAIN NAME SYSTEM domain
This is shown as step  in Fig
To contact a root server  each name server must have information about one or more root name servers
This information is normally present in a system configuration file that is loaded into the DNS cache when the DNS server is started
It is simply a list of NS records for the root and the corresponding A records
There are   root DNS servers  unimaginatively called a-root-  through  -
Each root server could logically be a single computer
However  since the entire Internet depends on the root servers  they are powerful and heavily replicated computers
Most of the servers are present in multiple geographical locations and reached using anycast routing  in which a packet is delivered to the nearest instance of a destination address; we described anycast in
The replication improves reliability and performance
The root name server is unlikely to know the address of a machine at UW  and probably does not know the name server for UW either
But it must know the name server for the edu domain  in which   is located
It returns the name and IP address for that part of the answer in step
The local name server then continues its quest
It sends the entire query to the edu name server ( - )
That name server returns the name server for UW
This is shown in steps  and
Closer now  the local name server sends the query to the UW name server (step  )
If the domain name being sought was in the English department  the answer would be found  as the UW zone includes the English department
But the Computer Science department has chosen to run its own name server
The query returns the name and IP address of the UW Computer Science name server (step  )
Finally  the local name server queries the UW Computer Science name server (step  )
This server is authoritative for the domain    so it must have the answer
It returns the final answer (step  )  which the local name server forwards as a response to
The name has been resolved
You can explore this process using standard tools such as the dig program that is installed on most UNIX systems
For example  typing dig@ -
will send a query for
to the  -  name server and print out the result
This will show you the information obtained in step  in the example above  and you will learn the name and IP address of the UW name servers
There are three technical points to discuss about this long scenario
First  two different query mechanisms are at work in Fig
When the host
sends its query to the local name server  that name server handles the resolution on behalf of flits until it has the desired answer to return
It does not return partial answers
They might be helpful  but they are not what the query was seeking
This mechanism is called a recursive query
THE APPLICATION LAYER
On the other hand  the root name server (and each subsequent name server) does not recursively continue the query for the local name server
It just returns a partial answer and moves on to the next query
The local name server is responsible for continuing the resolution by issuing further queries
This mechanism is called an iterative query
One name resolution can involve both mechanisms  as this example showed
A recursive query may always seem preferable  but many name servers (especially the root) will not handle them
They are too busy
Iterative queries put the burden on the originator
The rationale for the local name server supporting a recursive query is that it is providing a service to hosts in its domain
Those hosts do not have to be configured to run a full name server  just to reach the local one
The ond point is caching
All of the answers  including all the partial answers returned  are cached
In this way  if another   host queries for
the answer will already be known
Even better  if a host queries for a different host in the same domain  say
the query can be sent directly to the authoritative name server
Similarly  queries for other domains in   can start directly from the   name server
Using cached answers greatly reduces the steps in a query and improves performance
The original scenario we sketched is in fact the worst case that occurs when no useful information is cached
However  cached answers are not authoritative  since changes made at   will not be propagated to all the caches in the world that may know about it
For this reason  cache entries should not live too long
This is the reason that the Time to live field is included in each resource record
It tells remote name servers how long to cache records
If a certain machine has had the same IP address for years  it may be safe to cache that information for  day
For more volatile information  it might be safer to purge the records after a few onds or a minute
The third issue is the transport protocol that is used for the queries and responses
DNS messages are sent in UDP packets with a simple format for queries  answers  and name servers that can be used to continue the resolution
We will not go into the details of this format
If no response arrives within a short time  the DNS client repeats the query  trying another server for the domain after a small number of retries
This process is designed to handle the case of the server being down as well as the query or response packet getting lost
A  -bit identifier is included in each query and copied to the response so that a name server can match answers to the corresponding query  even if multiple queries are outstanding at the same time
Even though its purpose is simple  it should be clear that DNS is a large and complex distributed system that is comprised of millions of name servers that work together
It forms a key link between human-readable domain names and the IP addresses of machines
It includes replication and caching for performance and reliability and is designed to be highly robust
DNSâTHE DOMAIN NAME SYSTEM We have not covered urity  but as you might imagine  the ability to change the name-to-address mapping can have devastating consequences if done maliciously
For that reason  urity extensions called DNS have been developed for DNS
We will describe them in   There is also application demand to use names in more flexible ways  for example  by naming content and resolving to the IP address of a nearby host that has the content
This fits the model of searching for and downloading a movie
It is the movie that matters  not the computer that has a copy of it  so all that is wanted is the IP address of any nearby computer that has a copy of the movie
Content distribution networks are one way to accomplish this mapping
We will describe how they build on the DNS later in this  ter  in
ELECTRONIC MAIL Electronic mail  or more commonly email  has been around for over three decades
Faster and cheaper than paper mail  email has been a popular application since the early days of the Internet
Before  it was mostly used in academia
During the s  it became known to the public at large and grew exponentially  to the point where the number of emails sent per day now is vastly more than the number of snail mail (
paper) letters
Other forms of network communication  such as instant messaging and voice-over-IP calls have expanded greatly in use over the past decade  but email remains the workhorse of Internet communication
It is widely used within industry for intracompany communication  for example  to allow far-flung employees all over the world to cooperate on complex projects
Unfortunately  like paper mail  the majority of emailâsome  out of   messagesâis junk mail or spam (McAfee  )
Email  like most other forms of communication  has developed its own conventions and styles
It is very informal and has a low threshold of use
People who would never dream of calling up or even writing a letter to a Very Important Person do not hesitate for a ond to send a sloppily written email to him or her
By eliminating most cues associated with rank  age  and gender  email debates often focus on content  not status
With email  a brilliant idea from a summer student can have more impact than a dumb one from an executive vice president
Email is full of jargon such as BTW (By The Way)  ROTFL (Rolling On The Floor Laughing)  and IMHO (In My Humble Opinion)
Many people also use little ASCII symbols called smileys  starting with the ubiquitous ââ:-)ââ
Rotate the book   degrees clockwise if this symbol is unfamiliar
This symbol and other emoticons help to convey the tone of the message
They have spread to other terse forms of communication  such as instant messaging
The email protocols have evolved during the period of their use  too
The first email systems simply consisted of file transfer protocols  with the convention that the first line of each message (
file) contained the recipientâs address
As time THE APPLICATION LAYER
went on  email diverged from file transfer and many features were added  such as the ability to send one message to a list of recipients
Multimedia capabilities became important in the s to send messages with images and other non-text material
Programs for reading email became much more sophisticated too  shifting from text-based to graphical user interfaces and adding the ability for users to access their mail from their laptops wherever they happen to be
Finally  with the prevalence of spam  mail readers and the mail transfer protocols must now pay attention to finding and removing unwanted email
In our description of email  we will focus on the way that mail messages are moved between users  rather than the look and feel of mail reader programs
Nevertheless  after describing the overall architecture  we will begin with the user-facing part of the email system  as it is familiar to most readers
Architecture and Services In this tion  we will provide an overview of how email systems are organized and what they can do
The architecture of the email system is shown in Fig
It consists of two kinds of subsystems: the user agents  which allow people to read and send email  and the message transfer agents  which move the messages from the source to the destination
We will also refer to message transfer agents informally as mail servers
Message Transfer Agent Message Transfer Agent SMTP Sender User Agent Mailbox Receiver User Agent Email  : Mail submission  : Message transfer  : Final delivery Figure  -
Architecture of the email system
The user agent is a program that provides a graphical interface  or sometimes a text- and command-based interface that lets users interact with the email system
It includes a means to compose messages and replies to messages  display incoming messages  and organize messages by filing  searching  and discarding them
The act of sending new messages into the mail system for delivery is called mail submission
Some of the user agent processing may be done automatically  anticipating what the user wants
For example  incoming mail may be filtered to extract or   ELECTRONIC MAIL deprioritize messages that are likely spam
Some user agents include advanced features  such as arranging for automatic email responses (ââIâm having a wonderful vacation and it will be a while before I get back to youââ)
A user agent runs on the same computer on which a user reads her mail
It is just another program and may be run only some of the time
The message transfer agents are typically system processes
They run in the background on mail server machines and are intended to be always available
Their job is to automatically move email through the system from the originator to the recipient with SMTP (Simple Mail Transfer Protocol)
This is the message transfer step
SMTP was originally specified as RFC and revised to become the current RFC
It sends mail over connections and reports back the delivery status and any errors
Numerous applications exist in which confirmation of delivery is important and may even have legal significance (ââWell  Your Honor  my email system is just not very reliable  so I guess the electronic subpoena just got lost somewhereââ)
Message transfer agents also implement mailing lists  in which an identical copy of a message is delivered to everyone on a list of email addresses
Other advanced features are carbon copies  blind carbon copies  high-priority email  ret (
encrypted) email  alternative recipients if the primary one is not currently available  and the ability for assistants to read and answer their bossesâ email
Linking user agents and message transfer agents are the concepts of mailboxes and a standard format for email messages
Mailboxes store the email that is received for a user
They are maintained by mail servers
User agents simply present users with a view of the contents of their mailboxes
To do this  the user agents send the mail servers commands to manipulate the mailboxes  inspecting their contents  deleting messages  and so on
The retrieval of mail is the final delivery (step  ) in Fig
With this architecture  one user may use different user agents on multiple computers to access one mailbox
Mail is sent between message transfer agents in a standard format
The original format  RFC  has been revised to the current RFC  and extended with support for multimedia content and international text
This scheme is called MIME and will be discussed later
People still refer to Internet email as RFC  though
A key idea in the message format is the distinction between the envelope and its contents
The envelope encapsulates the message
It contains all the information needed for transporting the message  such as the destination address  priority  and urity level  all of which are distinct from the message itself
The message transport agents use the envelope for routing  just as the post office does
The message inside the envelope consists of two separate parts: the header and the body
The header contains control information for the user agents
The body is entirely for the human recipient
None of the agents care much about it
Envelopes and messages are illustrated in Fig
THE APPLICATION LAYER
Daniel Dumkopf   Willow Lane White Plains  NY  United Gizmo Main St Boston  MA  Sept
Yours truly United Gizmo Yours truly United Gizmo Subject: Invoice  Dear Mr
Dumkopf  Our computer records show that you still have not paid the above invoice of $  Please send us a check for $
Dumkopf  Our computer records show that you still have not paid the above invoice of $  Please send us a check for $
Daniel Dumkopf Street:   Willow Lane City: White Plains State: NY Zip code:  Priority: Urgent Encryption: None From: United Gizmo Address: Main St
Location: Boston  MA  Date: Sept
Subject: Invoice  Envelope Message (a) (b) Body Header Envelope  Â¢ Figure  -
Envelopes and messages
(a) Paper mail
(b) Electronic mail
We will examine the pieces of this architecture in more detail by looking at the steps that are involved in sending email from one user to another
This journey starts with the user agent
The User Agent A user agent is a program (sometimes called an email reader) that accepts a variety of commands for composing  receiving  and replying to messages  as well as for manipulating mailboxes
There are many popular user agents  including Google gmail  Microsoft Outlook  Mozilla Thunderbird  and Apple Mail
They can vary greatly in their appearance
Most user agents have a menu- or icondriven graphical interface that requires a mouse  or a touch interface on smaller mobile devices
Older user agents  such as Elm  mh  and Pine  provide text-based interfaces and expect one-character commands from the keyboard
Functionally  these are the same  at least for text messages
The typical elements of a user agent interface are shown in Fig
Your mail reader is likely to be much flashier  but probably has equivalent functions
ELECTRONIC MAIL When a user agent is started  it will usually present a summary of the messages in the userâs mailbox
Often  the summary will have one line for each message in some sorted order
It highlights key fields of the message that are extracted from the message envelope or header
Mail Folders All items Inbox Networks Travel Junk Mail Message summary From trudy Andy djw Amy   guido lazowska lazowska
Subject Not all Trudys are nasty Material on RFID privacy Have you seen this? Request for information Re: Paper acceptance More on that New report out Received Today Today Mar  Mar  Mar  Mar  Mar  Mailbox search ! A
Student Dear Professor  I recently completed my undergraduate studies with distinction at an excellent university
I will be visiting your Message folders Search Graduate studies? Mar  Message Figure  -
Typical elements of the user agent interface
Seven summary lines are shown in the example of Fig
The lines use the From  Subject  and Received fields  in that order  to display who sent the message  what it is about  and when it was received
All the information is formatted in a user-friendly way rather than displaying the literal contents of the message fields  but it is based on the message fields
Thus  people who fail to include a Subject field often discover that responses to their emails tend not to get the highest priority
Many other fields or indications are possible
The icons next to the message subjects in Fig
Many sorting orders are also possible
The most common is to order messages based on the time that they were received  most recent first  with some indication as to whether the message is new or has already been read by the user
The fields in the summary and the sort order can be customized by the user according to her preferences
User agents must also be able to display incoming messages as needed so that people can read their email
Often a short preview of a message is provided  as in Fig
Previews may use small icons or images to describe the contents of the message
Other presentation processing THE APPLICATION LAYER
includes reformatting messages to fit the display  and translating or converting contents to more convenient formats (
digitized speech to recognized text)
After a message has been read  the user can decide what to do with it
This is called message disposition
Options include deleting the message  sending a reply  forwarding the message to another user  and keeping the message for later reference
Most user agents can manage one mailbox for incoming mail with multiple folders for saved mail
The folders allow the user to save message according to sender  topic  or some other category
Filing can be done automatically by the user agent as well  before the user reads the messages
A common example is that the fields and contents of messages are inspected and used  along with feedback from the user about previous messages  to determine if a message is likely to be spam
Many ISPs and companies run software that labels mail as important or spam so that the user agent can file it in the corresponding mailbox
The ISP and company have the advantage of seeing mail for many users and may have lists of known spammers
If hundreds of users have just received a similar message  it is probably spam
By presorting incoming mail as ââprobably legitimateââ and ââprobably spam ââ the user agent can save users a fair amount of work separating the good stuff from the junk
And the most popular spam? It is generated by collections of compromised computers called botnets and its content depends on where you live
Fake diplomas are topical in Asia  and cheap drugs and other dubious product offers are topical in the
Unclaimed Nigerian bank accounts still abound
Pills for enlarging various body parts are common everywhere
Other filing rules can be constructed by users
Each rule specifies a condition and an action
For example  a rule could say that any message received from the boss goes to one folder for immediate reading and any message from a particular mailing list goes to another folder for later reading
Several folders are shown in Fig
The most important folders are the Inbox  for incoming mail not filed elsewhere  and Junk Mail  for messages that are thought to be spam
As well as explicit constructs like folders  user agents now provide rich capabilities to search the mailbox
This feature is also shown in Fig
Search capabilities let users find messages quickly  such as the message about ââwhere to buy Vegemiteââ that someone sent in the last month
Email has come a long way from the days when it was just file transfer
Providers now routinely support mailboxes with up to  GB of stored mail that details a userâs interactions over a long period of time
The sophisticated mail handling of user agents with search and automatic forms of processing is what makes it possible to manage these large volumes of email
For people who send and receive thousands of messages a year  these tools are invaluable
Another useful feature is the ability to automatically respond to messages in some way
One response is to forward incoming email to a different address  for example  a computer operated by a commercial paging service that pages the user   ELECTRONIC MAIL by using radio or satellite and displays the Subject: line on his pager
These autoresponders must run in the mail server because the user agent may not run all the time and may only occasionally retrieve email
Because of these factors  the user agent cannot provide a true automatic response
However  the interface for automatic responses is usually presented by the user agent
A different example of an automatic response is a vacation agent
This is a program that examines each incoming message and sends the sender an insipid reply such as: ââHi
Iâm on vacation
Iâll be back on the  th of August
Talk to you then
ââ Such replies can also specify how to handle urgent matters in the interim  other people to contact for specific problems  etc
Most vacation agents keep track of whom they have sent canned replies to and refrain from sending the same person a ond reply
There are pitfalls with these agents  however
For example  it is not advisable to send a canned reply to a large mailing list
Let us now turn to the scenario of one user sending a message to another user
One of the basic features user agents support that we have not yet discussed is mail composition
It involves creating messages and answers to messages and sending these messages into the rest of the mail system for delivery
Although any text editor can be used to create the body of the message  editors are usually integrated with the user agent so that it can provide assistance with addressing and the numerous header fields attached to each message
For example  when answering a message  the email system can extract the originatorâs address from the incoming email and automatically insert it into the proper place in the reply
Other common features are appending a signature block to the bottom of a message  correcting spelling  and computing digital signatures that show the message is valid
Messages that are sent into the mail system have a standard format that must be created from the information supplied to the user agent
The most important part of the message for transfer is the envelope  and the most important part of the envelope is the destination address
This address must be in a format that the message transfer agents can deal with
The expected form of an address is user@dns-address
Since we studied DNS earlier in this  ter  we will not repeat that material here
However  it is worth noting that other forms of addressing exist
In particular  X
addresses look radically different from DNS addresses
is an ISO standard for message-handling systems that was at one time a competitor to SMTP
SMTP won out handily  though X
systems are still used  mostly outside of the
addresses are composed of attribute=value pairs separated by slashes  for example  /C=US/ST=MASSACHUSETTS/L=CAMBRIDGE/PA=   MEMORIAL DR
/CN=KEN SMITH/ This address specifies a country  state  locality  personal address  and common name (Ken Smith)
Many other attributes are possible  so you can send email to THE APPLICATION LAYER
someone whose exact email address you do not know  provided you know enough other attributes (
company and job title)
Although X
names are considerably less convenient than DNS names  the issue is moot for user agents because they have user-friendly aliases (sometimes called nicknames) that allow users to enter or select a personâs name and get the correct email address
Consequently  it is usually not necessary to actually type in these strange strings
A final point we will touch on for sending mail is mailing lists  which let users send the same message to a list of people with a single command
There are two choices for how the mailing list is maintained
It might be maintained locally  by the user agent
In this case  the user agent can just send a separate message to each intended recipient
Alternatively  the list may be maintained remotely at a message transfer agent
Messages will then be expanded in the message transfer system  which has the effect of allowing multiple users to send to the list
For example  if a group of bird watchers has a mailing list called birders installed on the transfer agent    any message sent to birders@  will be routed to the University of Arizona and expanded into individual messages to all the mailing list members  wherever in the world they may be
Users of this mailing list cannot tell that it is a mailing list
It could just as well be the personal mailbox of Prof
Message Formats Now we turn from the user interface to the format of the email messages themselves
Messages sent by the user agent must be placed in a standard format to be handled by the message transfer agents
First we will look at basic ASCII email using RFC  which is the latest revision of the original Internet message format as described in RFC
After that  we will look at multimedia extensions to the basic format
RFC âThe Internet Message Format Messages consist of a primitive envelope (described as part of SMTP in RFC )  some number of header fields  a blank line  and then the message body
Each header field (logically) consists of a single line of ASCII text containing the field name  a colon  and  for most fields  a value
The original RFC was designed decades ago and did not clearly distinguish the envelope fields from the header fields
Although it has been revised to RFC  completely redoing it was not possible due to its widespread usage
In normal usage  the user agent builds a message and passes it to the message transfer agent  which then uses some of the header fields to construct the actual envelope  a somewhat oldfashioned mixing of message and envelope
ELECTRONIC MAIL The principal header fields related to message transport are listed in Fig
The To: field gives the DNS address of the primary recipient
Having multiple recipients is also allowed
The Cc: field gives the addresses of any ondary recipients
In terms of delivery  there is no distinction between the primary and ondary recipients
It is entirely a psychological difference that may be important to the people involved but is not important to the mail system
The term Cc: (Carbon copy) is a bit dated  since computers do not use carbon paper  but it is well established
The Bcc: (Blind carbon copy) field is like the Cc: field  except that this line is deleted from all the copies sent to the primary and ondary recipients
This feature allows people to send copies to third parties without the primary and ondary recipients knowing this
Header Meaning To: Email address(es) of primary recipient(s) Cc: Email address(es) of ondary recipient(s) Bcc: Email address(es) for blind carbon copies From: Person or people who created the message Sender: Email address of the actual sender Received: Line added by each transfer agent along the route Return-Path: Can be used to identify a path back to the sender Figure  -
RFC  header fields related to message transport
The next two fields  From: and Sender:  tell who wrote and sent the message  respectively
These need not be the same
For example  a business executive may write a message  but her assistant may be the one who actually transmits it
In this case  the executive would be listed in the From: field and the assistant in the Sender: field
The From: field is required  but the Sender: field may be omitted if it is the same as the From: field
These fields are needed in case the message is undeliverable and must be returned to the sender
A line containing Received: is added by each message transfer agent along the way
The line contains the agentâs identity  the date and time the message was received  and other information that can be used for debugging the routing system
The Return-Path: field is added by the final message transfer agent and was intended to tell how to get back to the sender
In theory  this information can be gathered from all the Received: headers (except for the name of the senderâs mailbox)  but it is rarely filled in as such and typically just contains the senderâs address
In addition to the fields of Fig
The most common ones are listed in Fig
Most of these are self-explanatory  so we will not go into all of them in much detail
THE APPLICATION LAYER
Header Meaning Date: The date and time the message was sent Reply-To: Email address to which replies should be sent Message-Id: Unique number for referencing this message later In-Reply-To: Message-Id of the message to which this is a reply References: Other relevant Message-Ids Keywords: User-chosen keywords Subject: Short summary of the message for the one-line display Figure  -
Some fields used in the RFC  message header
The Reply-To: field is sometimes used when neither the person composing the message nor the person sending the message wants to see the reply
For example  a marketing manager may write an email message telling customers about a new product
The message is sent by an assistant  but the Reply-To: field lists the head of the sales department  who can answer questions and take orders
This field is also useful when the sender has two email accounts and wants the reply to go to the other one
The Message-Id: is an automatically generated number that is used to link messages together (
when used in the In-Reply-To: field) and to prevent duplicate delivery
The RFC  document explicitly says that users are allowed to invent optional headers for their own private use
By convention since RFC  these headers start with the string X-
It is guaranteed that no future headers will use names starting with X-  to avoid conflicts between official and private headers
Sometimes wiseguy undergraduates make up fields like X-Fruit-of-the-Day: or X-Disease-of-the-Week:  which are legal  although not always illuminating
After the headers comes the message body
Users can put whatever they want here
Some people terminate their messages with elaborate signatures  including quotations from greater and lesser authorities  political statements  and disclaimers of all kinds (
The XYZ Corporation is not responsible for my opinions; in fact  it cannot even comprehend them)
MIMEâThe Multipurpose Internet Mail Extensions In the early days of the ARPANET  email consisted exclusively of text messages written in English and expressed in ASCII
For this environment  the early RFC format did the job completely: it specified the headers but left the content entirely up to the users
In the s  the worldwide use of the Internet and demand to send richer content through the mail system meant that this approach was no longer adequate
The problems included sending and receiving messages   ELECTRONIC MAIL in languages with accents (
French and German)  non-Latin alphabets (
Hebrew and Russian)  or no alphabets (
Chinese and Japanese)  as well as sending messages not containing text at all (
audio  images  or binary documents and programs)
The solution was the development of MIME (Multipurpose Internet Mail Extensions)
It is widely used for mail messages that are sent across the Internet  as well as to describe content for other applications such as Web browsing
MIME is described in RFCs â    and
The basic idea of MIME is to continue to use the RFC format (the precursor to RFC  the time MIME was proposed) but to add structure to the message body and define encoding rules for the transfer of non-ASCII messages
Not deviating from RFC allowed MIME messages to be sent using the existing mail transfer agents and protocols (based on RFC then  and RFC  now)
All that had to be changed were the sending and receiving programs  which users could do for themselves
MIME defines five new message headers  as shown in Fig
The first of these simply tells the user agent receiving the message that it is dealing with a MIME message  and which version of MIME it uses
Any message not containing a MIME-Version: header is assumed to be an English plaintext message (or at least one using only ASCII characters) and is processed as such
Header Meaning MIME-Version: Identifies the MIME version Content-Description: Human-readable string telling what is in the message Content-Id: Unique identifier Content-Transfer-Encoding: How the body is wrapped for transmission Content-Type: Type and format of the content Figure  -
Message headers added by MIME
The Content-Description: header is an ASCII string telling what is in the message
This header is needed so the recipient will know whether it is worth decoding and reading the message
If the string says ââPhoto of Barbaraâs hamsterââ and the person getting the message is not a big hamster fan  the message will probably be discarded rather than decoded into a high-resolution color photograph
The Content-Id: header identifies the content
It uses the same format as the standard Message-Id: header
The Content-Transfer-Encoding: tells how the body is wrapped for transmission through the network
A key problem at the time MIME was developed was that the mail transfer (SMTP) protocols expected ASCII messages in which no line exceeded  characters
ASCII characters use  bits out of each  -bit byte
Binary data such as executable programs and images use all  bits of each byte  as THE APPLICATION LAYER
do extended character sets
There was no guarantee this data would be transferred safely
Hence  some method of carrying binary data that made it look like a regular ASCII mail message was needed
Extensions to SMTP since the development of MIME do allow  -bit binary data to be transferred  though even today binary data may not always go through the mail system correctly if unencoded
MIME provides five transfer encoding schemes  plus an escape to new schemesâjust in case
The simplest scheme is just ASCII text messages
ASCII characters use  bits and can be carried directly by the email protocol  provided that no line exceeds  characters
The next simplest scheme is the same thing  but using  -bit characters  that is  all values from  up to and including are allowed
Messages using the  -bit encoding must still adhere to the standard maximum line length
Then there are messages that use a true binary encoding
These are arbitrary binary files that not only use all  bits but also do not adhere to the -character line limit
Executable programs fall into this category
Nowadays  mail servers can negotiate to send data in binary (or  -bit) encoding  falling back to ASCII if both ends do not support the extension
The ASCII encoding of binary data is called base  encoding
In this scheme  groups of   bits are broken up into four  -bit units  with each unit being sent as a legal ASCII character
The coding is ââAââ for   ââBââ for   and so on  followed by the   lowercase letters  the   digits  and finally + and / for   and respectively
The == and = sequences indicate that the last group contained only  or   bits  respectively
Carriage returns and line feeds are ignored  so they can be inserted at will in the encoded character stream to keep the lines short enough
Arbitrary binary text can be sent safely using this scheme  albeit inefficiently
This encoding was very popular before binary-capable mail servers were widely deployed
It is still commonly seen
For messages that are almost entirely ASCII but with a few non-ASCII characters  base  encoding is somewhat inefficient
Instead  an encoding known as quoted-printable encoding is used
This is just  -bit ASCII  with all the characters above encoded as an equals sign followed by the characterâs value as two hexadecimal digits
Control characters  some punctuation marks and math symbols  as well as trailing spaces are also so encoded
Finally  when there are valid reasons not to use one of these schemes  it is possible to specify a user-defined encoding in the Content-Transfer-Encoding: header
The last header shown in Fig
It specifies the nature of the message body and has had an impact well beyond email
For instance  content downloaded from the Web is labeled with MIME types so that the browser knows how to present it
So is content sent over streaming media and real-time transports such as voice over IP
Initially  seven MIME types were defined in RFC
Each type has one or more available subtypes
The type and subtype are separated by a slash  as in   ELECTRONIC MAIL ââContent-Type: video/mpegââ
Since then  hundreds of subtypes have been added  along with another type
Additional entries are being added all the time as new types of content are developed
The list of assigned types and subtypes is maintained online by IANA at  /assignments/media-types
The types  along with examples of commonly used subtypes  are given in Fig
Let us briefly go through them  starting with text
The text/plain combination is for ordinary messages that can be displayed as received  with no encoding and no further processing
This option allows ordinary messages to be transported in MIME with only a few extra headers
The text/html subtype was added when the Web became popular (in RFC ) to allow Web pages to be sent in RFC email
A subtype for the eXtensible Markup Language  text/xml  is defined in RFC
XML documents have proliferated with the development of the Web
We will study HTML and XML in
Type Example subtypes Description text plain  html  xml  css Text in various formats image gif  jpeg  tiff Pictures audio basic  mpeg  mp  Sounds video mpeg  mp  quicktime Movies model vrml  D model application octet-stream  pdf  javascript  zip Data produced by applications message http  rfc   Encapsulated message multipart mixed  alternative  parallel  digest Combination of multiple types Figure  -
MIME content types and example subtypes
The next MIME type is image  which is used to transmit still pictures
Many formats are widely used for storing and transmitting images nowadays  both with and without compression
Several of these  including GIF  JPEG  and TIFF  are built into nearly all browsers
Many other formats and corresponding subtypes exist as well
The audio and video types are for sound and moving pictures  respectively
Please note that video may include only the visual information  not the sound
If a movie with sound is to be transmitted  the video and audio portions may have to be transmitted separately  depending on the encoding system used
The first video format defined was the one devised by the modestly named Moving Picture Experts Group (MPEG)  but others have been added since
In addition to audio/basic  a new audio type  audio/mpeg  was added in RFC  to allow people to email MP  audio files
The video/mp  and audio/mp  types signal video and audio data that are stored in the newer MPEG  format
The model type was added after the other content types
It is intended for describing  D model data
However  it has not been widely used to date
THE APPLICATION LAYER
The application type is a catchall for formats that are not covered by one of the other types and that require an application to interpret the data
We have listed the subtypes pdf  javascript  and zip as examples for PDF documents  Java- Script programs  and Zip archives  respectively
User agents that receive this content use a third-party library or external program to display the content; the display may or may not appear to be integrated with the user agent
By using MIME types  user agents gain the extensibility to handle new types of application content as it is developed
This is a significant benefit
On the other hand  many of the new forms of content are executed or interpreted by applications  which presents some dangers
Obviously  running an arbitrary executable program that has arrived via the mail system from ââfriendsââ poses a urity hazard
The program may do all sorts of nasty damage to the parts of the computer to which it has access  especially if it can read and write files and use the network
Less obviously  document formats can pose the same hazards
This is because formats such as PDF are full-blown programming languages in disguise
While they are interpreted and restricted in scope  bugs in the interpreter often allow devious documents to escape the restrictions
Besides these examples  there are many more application subtypes because there are many more applications
As a fallback to be used when no other subtype is known to be more fitting  the octet-stream subtype denotes a sequence of uninterpreted bytes
Upon receiving such a stream  it is likely that a user agent will display it by suggesting to the user that it be copied to a file
Subsequent processing is then up to the user  who presumably knows what kind of content it is
The last two types are useful for composing and manipulating messages themselves
The message type allows one message to be fully encapsulated inside another
This scheme is useful for forwarding email  for example
When a complete RFC message is encapsulated inside an outer message  the rfc   subtype should be used
Similarly  it is common for HTML documents to be encapsulated
And the partial subtype makes it possible to break an encapsulated message into pieces and send them separately (for example  if the encapsulated message is too long)
Parameters make it possible to reassemble all the parts at the destination in the correct order
Finally  the multipart type allows a message to contain more than one part  with the beginning and end of each part being clearly delimited
The mixed subtype allows each part to be a different type  with no additional structure imposed
Many email programs allow the user to provide one or more attachments to a text message
These attachments are sent using the multipart type
In contrast to mixed  the alternative subtype allows the same message to be included multiple times but expressed in two or more different media
For example  a message could be sent in plain ASCII  in HMTL  and in PDF
A properly designed user agent getting such a message would display it according to user preferences
Likely PDF would be the first choice  if that is possible
The ond choice would be HTML
If neither of these were possible  then the flat ASCII   ELECTRONIC MAIL text would be displayed
The parts should be ordered from simplest to most complex to help recipients with pre-MIME user agents make some sense of the message (
even a pre-MIME user can read flat ASCII text)
The alternative subtype can also be used for multiple languages
In this context  the Rosetta Stone can be thought of as an early multipart/alternative message
Of the other two example subtypes  the parallel subtype is used when all parts must be ââviewedââ simultaneously
For example  movies often have an audio channel and a video channel
Movies are more effective if these two channels are played back in parallel  instead of conutively
The digest subtype is used when multiple messages are packed together into a composite message
For example  some discussion groups on the Internet collect messages from subscribers and then send them out to the group periodically as a single multipart/digest message
As an example of how MIME types may be used for email messages  a multimedia message is shown in Fig
Here  a birthday greeting is transmitted in alternative forms as HTML and as an audio file
Assuming the receiver has audio capability  the user agent there will play the sound file
In this example  the sound is carried by reference as a message/external-body subtype  so first the user agent must fetch the sound file   using FTP
If the user agent has no audio capability  the lyrics are displayed on the screen in stony silence
The two parts are delimited by two hyphens followed by a (software-generated) string specified in the boundary parameter
Note that the Content-Type header occurs in three positions within this example
At the top level  it indicates that the message has multiple parts
Within each part  it gives the type and subtype of that part
Finally  within the body of the ond part  it is required to tell the user agent what kind of external file it is to fetch
To indicate this slight difference in usage  we have used lowercase letters here  although all headers are case insensitive
The Content-Transfer-Encoding is similarly required for any external body that is not encoded as  -bit ASCII
Message Transfer Now that we have described user agents and mail messages  we are ready to look at how the message transfer agents relay messages from the originator to the recipient
The mail transfer is done with the SMTP protocol
The simplest way to move messages is to establish a transport connection from the source machine to the destination machine and then just transfer the message
This is how SMTP originally worked
Over the years  however  two different uses of SMTP have been differentiated
The first use is mail submission  step  in the email architecture of Fig
This is the means by which user agents send messages into the mail system for delivery
The ond use is to transfer messages between message transfer agents (step  in Fig
This THE APPLICATION LAYER
From: alice@  To: bob@
MIME-Version:
Message-Id: <   @ > Content-Type: multipart/alternative; boundary=qwertyuiopasdfghjklzxcvbnm Subject: Earth orbits sun integral number of times This is the preamble
The user agent ignores it
Have a nice day
"--qwertyuiopasdfghjklzxcvbnm Content-Type: text/html <p>Happy birthday to you<br> Happy birthday to you<br> Happy birthday dear <b> Bob </b><br> Happy birthday to you</p> --qwertyuiopasdfghjklzxcvbnm Content-Type: message/external-body; access-type=""anon-"
"""; directory=""pub""; name="" "" content-type: audio/basic content-transfer-encoding: base  --qwertyuiopasdfghjklzxcvbnm-- Figure  -"
A multipart message containing HTML and audio alternatives
sequence delivers mail all the way from the sending to the receiving message transfer agent in one hop
Final delivery is accomplished with different protocols that we will describe in the next tion
In this tion  we will describe the basics of the SMTP protocol and its extension mechanism
Then we will discuss how it is used differently for mail submission and message transfer
SMTP (Simple Mail Transfer Protocol) and Extensions Within the Internet  email is delivered by having the sending computer establish a TCP connection to port   of the receiving computer
Listening to this port is a mail server that speaks SMTP (Simple Mail Transfer Protocol)
This server accepts incoming connections  subject to some urity checks  and accepts messages for delivery
If a message cannot be delivered  an error report containing the first part of the undeliverable message is returned to the sender
SMTP is a simple ASCII protocol
This is not a weakness but a feature
Using ASCII text makes protocols easy to develop  test  and debug
They can be   ELECTRONIC MAIL tested by sending commands manually  and records of the messages are easy to read
Most application-level Internet protocols now work this way (
We will walk through a simple message transfer between mail servers that delivers a message
After establishing the TCP connection to port the sending machine  operating as the client  waits for the receiving machine  operating as the server  to talk first
The server starts by sending a line of text giving its identity and telling whether it is prepared to receive mail
If it is not  the client releases the connection and tries again later
If the server is willing to accept email  the client announces whom the email is coming from and whom it is going to
If such a recipient exists at the destination  the server gives the client the go-ahead to send the message
Then the client sends the message and the server acknowledges it
No checksums are needed because TCP provides a reliable byte stream
If there is more email  that is now sent
When all the email has been exchanged in both directions  the connection is released
A sample dialog for sending the message of Fig
The lines sent by the client (
the sender) are marked C:
Those sent by the server (
the receiver) are marked S:
The first command from the client is indeed meant to be HELO
Of the various four-character abbreviations for HELLO  this one has numerous advantages over its biggest competitor
Why all the commands had to be four characters has been lost in the mists of time
Such commands are allowed to send a single message to multiple receivers
Each one is individually acknowledged or rejected
Even if some recipients are rejected (because they do not exist at the destination)  the message can be sent to the other ones
Finally  although the syntax of the four-character commands from the client is rigidly specified  the syntax of the replies is less rigid
Only the numerical code really counts
Each implementation can put whatever string it wants after the code
The basic SMTP works well  but it is limited in several respects
It does not include authentication
This means that the FROM command in the example could give any sender address that it pleases
This is quite useful for sending spam
Another limitation is that SMTP transfers ASCII messages  not binary data
This is why the base  MIME content transfer encoding was needed
However  with that encoding the mail transmission uses bandwidth inefficiently  which is an issue for large messages
A third limitation is that SMTP sends messages in the clear
It has no encryption to provide a measure of privacy against prying eyes
To allow these and many other problems related to message processing to be addressed  SMTP was revised to have an extension mechanism
This mechanism is a mandatory part of the RFC  standard
The use of SMTP with extensions is called ESMTP (Extended SMTP)
THE APPLICATION LAYER
SMTP service ready C: HELO   S:   says hello to
C: MAIL FROM: <alice@ > S: sender ok C: RCPT TO: <bob@
"> S: recipient ok C: DATA S: Send mail; end with """
""" on a line by itself C: From: alice@  C: To: bob@"
C: MIME-Version:
C: Message-Id: <   @
> C: Content-Type: multipart/alternative; boundary=qwertyuiopasdfghjklzxcvbnm C: Subject: Earth orbits sun integral number of times C: C: This is the preamble
The user agent ignores it
Have a nice day
"C: C: --qwertyuiopasdfghjklzxcvbnm C: Content-Type: text/html C: C: <p>Happy birthday to you C: Happy birthday to you C: Happy birthday dear <bold> Bob </bold> C: Happy birthday to you C: C: --qwertyuiopasdfghjklzxcvbnm C: Content-Type: message/external-body; C: access-type=""anon-ftp""; C: site="""
"""; C: directory=""pub""; C: name="" "" C: C: content-type: audio/basic C: content-transfer-encoding: base  C: --qwertyuiopasdfghjklzxcvbnm C:"
S: message accepted C: QUIT S:
closing connection Figure  -
Sending a message from alice@  to bob@  Clients wanting to use an extension send an EHLO message instead of HELO initially
If this is rejected  the server is a regular SMTP server  and the client should proceed in the usual way
If the EHLO is accepted  the server replies with the extensions that it supports
The client may then use any of these extensions
Several common extensions are shown in Fig
The figure gives the keyword   ELECTRONIC MAIL as used in the extension mechanism  along with a description of the new functionality
We will not go into extensions in further detail
Keyword Description AUTH Client authentication BINARYMIME Server accepts binary messages CHUNKING Server accepts large messages in chunks SIZE Check message size before trying to send STARTTLS Switch to ure transport (TLS; see
) UTF SMTP Internationalized addresses Figure  -
Some SMTP extensions
To get a better feel for how SMTP and some of the other protocols described in this  ter work  try them out
In all cases  first go to a machine connected to the Internet
On a UNIX (or Linux) system  in a shell  type telnet     substituting the DNS name of your ISPâs mail server for
On a Windows XP system  click on Start  then Run  and type the command in the dialog box
On a Vista or Windows  machine  you may have to first install the telnet program (or equivalent) and then start it yourself
This command will establish a telnet (
TCP) connection to port   on that machine
Port   is the SMTP port; see Fig
You will probably get a response something like this: Trying
Connected to   Escape character is âË]âcom Smail #  ready at Thu Sept   :  + The first three lines are from telnet  telling you what it is doing
The last line is from the SMTP server on the remote machine  announcing its willingness to talk to you and accept email
To find out what commands it accepts  type HELP From this point on  a command sequence such as the one in Fig
Mail Submission Originally  user agents ran on the same computer as the sending message transfer agent
In this setting  all that is required to send a message is for the user agent to talk to the local mail server  using the dialog that we have just described
However  this setting is no longer the usual case
THE APPLICATION LAYER
User agents often run on laptops  home PCs  and mobile phones
They are not always connected to the Internet
Mail transfer agents run on ISP and company servers
They are always connected to the Internet
This difference means that a user agent in Boston may need to contact its regular mail server in Seattle to send a mail message because the user is traveling
By itself  this remote communication poses no problem
It is exactly what the TCP/IP protocols are designed to support
However  an ISP or company usually does not want any remote user to be able to submit messages to its mail server to be delivered elsewhere
The ISP or company is not running the server as a public service
In addition  this kind of open mail relay attracts spammers
This is because it provides a way to launder the original sender and thus make the message more difficult to identify as spam
Given these considerations  SMTP is normally used for mail submission with the AUTH extension
This extension lets the server check the credentials (username and password) of the client to confirm that the server should be providing mail service
There are several other differences in the way SMTP is used for mail submission
For example  port is used in preference to port   and the SMTP server can check and correct the format of the messages sent by the user agent
For more information about the restricted use of SMTP for mail submission  please see RFC
Message Transfer Once the sending mail transfer agent receives a message from the user agent  it will deliver it to the receiving mail transfer agent using SMTP
To do this  the sender uses the destination address
Consider the message in Fig
-   addressed to bob@  To what mail server should the message be delivered? To determine the correct mail server to contact  DNS is consulted
In the previous tion  we described how DNS contains multiple types of records  including the MX  or mail exchanger  record
In this case  a DNS query is made for the MX records of the domain   This query returns an ordered list of the names and IP addresses of one or more mail servers
The sending mail transfer agent then makes a TCP connection on port   to the IP address of the mail server to reach the receiving mail transfer agent  and uses SMTP to relay the message
The receiving mail transfer agent will then place mail for the user bob in the correct mailbox for Bob to read it at a later time
This local delivery step may involve moving the message among computers if there is a large mail infrastructure
With this delivery process  mail travels from the initial to the final mail transfer agent in a single hop
There are no intermediate servers in the message transfer stage
It is possible  however  for this delivery process to occur multiple times
One example that we have described already is when a message transfer agent   ELECTRONIC MAIL implements a mailing list
In this case  a message is received for the list
It is then expanded as a message to each member of the list that is sent to the individual member addresses
As another example of relaying  Bob may have graduated from
and also be reachable via the address bob@
Rather than reading mail on multiple accounts  Bob can arrange for mail sent to this address to be forwarded to bob@
In this case  mail sent to bob@  will undergo two deliveries
First  it will be sent to the mail server for
Then  it will be sent to the mail server for   Each of these legs is a complete and separate delivery as far as the mail transfer agents are concerned
Another consideration nowadays is spam
Nine out of ten messages sent today are spam (McAfee  )
Few people want more spam  but it is hard to avoid because it masquerades as regular mail
Before accepting a message  additional checks may be made to reduce the opportunities for spam
The message for Bob was sent from alice@
The receiving mail transfer agent can look up the sending mail transfer agent in DNS
This lets it check that the IP address of the other end of the TCP connection matches the DNS name
More generally  the receiving agent may look up the sending domain in DNS to see if it has a mail sending policy
This information is often given in the TXT and SPF records
It may indicate that other checks can be made
For example  mail sent from   may always be sent from the host   If the sending mail transfer agent is not june  there is a problem
If any of these checks fail  the mail is probably being forged with a fake sending address
In this case  it is discarded
However  passing these checks does not imply that mail is not spam
The checks merely ensure that the mail seems to be coming from the region of the network that it purports to come from
The idea is that spammers should be forced to use the correct sending address when they send mail
This makes spam easier to recognize and delete when it is unwanted
Final Delivery Our mail message is almost delivered
It has arrived at Bobâs mailbox
All that remains is to transfer a copy of the message to Bobâs user agent for display
This is step  in the architecture of Fig
This task was straightforward in the early Internet  when the user agent and mail transfer agent ran on the same machine as different processes
The mail transfer agent simply wrote new messages to the end of the mailbox file  and the user agent simply checked the mailbox file for new mail
Nowadays  the user agent on a PC  laptop  or mobile  is likely to be on a different machine than the ISP or company mail server
Users want to be able to access their mail remotely  from wherever they are
They want to access email from work  from their home PCs  from their laptops when on business trips  and from cybercafes when on so-called vacation
They also want to be able to work offline  THE APPLICATION LAYER
then reconnect to receive incoming mail and send outgoing mail
Moreover  each user may run several user agents depending on what computer it is convenient to use at the moment
Several user agents may even be running at the same time
In this setting  the job of the user agent is to present a view of the contents of the mailbox  and to allow the mailbox to be remotely manipulated
Several different protocols can be used for this purpose  but SMTP is not one of them
SMTP is a push-based protocol
It takes a message and connects to a remote server to transfer the message
Final delivery cannot be achieved in this manner both because the mailbox must continue to be stored on the mail transfer agent and because the user agent may not be connected to the Internet at the moment that SMTP attempts to relay messages
IMAPâThe Internet Message Access Protocol One of the main protocols that is used for final delivery is IMAP (Internet Message Access Protocol)
Version  of the protocol is defined in RFC
To use IMAP  the mail server runs an IMAP server that listens to port
The user agent runs an IMAP client
The client connects to the server and begins to issue commands from those listed in Fig
First  the client will start a ure transport if one is to be used (in order to keep the messages and commands confidential)  and then log in or otherwise authenticate itself to the server
Once logged in  there are many commands to list folders and messages  fetch messages or even parts of messages  mark messages with flags for later deletion  and organize messages into folders
To avoid confusion  please note that we use the term ââfolderââ here to be consistent with the rest of the material in this tion  in which a user has a single mailbox made up of multiple folders
However  in the IMAP specification  the term mailbox is used instead
One user thus has many IMAP mailboxes  each of which is typically presented to the user as a folder
IMAP has many other features  too
It has the ability to address mail not by message number  but by using attributes (
give me the first message from Alice)
Searches can be performed on the server to find the messages that satisfy certain criteria so that only those messages are fetched by the client
IMAP is an improvement over an earlier final delivery protocol  POP  (Post Office Protocol  version  )  which is specified in RFC
POP  is a simpler protocol but supports fewer features and is less ure in typical usage
Mail is usually downloaded to the user agent computer  instead of remaining on the mail server
This makes life easier on the server  but harder on the user
It is not easy to read mail on multiple computers  plus if the user agent computer breaks  all email may be lost permanently
Nonetheless  you will still find POP  in use
Proprietary protocols can also be used because the protocol runs between a mail server and user agent that can be supplied by the same company
Microsoft Exchange is a mail system with a proprietary protocol
ELECTRONIC MAIL Command Description CAPABILITY List server capabilities STARTTLS Start ure transport (TLS; see
) LOGIN Log on to server AUTHENTICATE Log on with other method SELECT Select a folder EXAMINE Select a read-only folder CREATE Create a folder DELETE Delete a folder RENAME Rename a folder SUBSCRIBE Add folder to active set UNSUBSCRIBE Remove folder from active set LIST List the available folders LSUB List the active folders STATUS Get the status of a folder APPEND Add a message to a folder CHECK Get a checkpoint of a folder FETCH Get messages from a folder SEARCH Find messages in a folder STORE Alter message flags COPY Make a copy of a message in a folder EXPUNGE Remove messages flagged for deletion UID Issue commands using unique identifiers NOOP Do nothing CLOSE Remove flagged messages and close folder LOGOUT Log out and close connection Figure  -
IMAP (version  ) commands
Webmail An increasingly popular alternative to IMAP and SMTP for providing email service is to use the Web as an interface for sending and receiving mail
Widely used Webmail systems include Google Gmail  Microsoft Hotmail and Yahoo! Mail
Webmail is one example of software (in this case  a mail user agent) that is provided as a service using the Web
In this architecture  the provider runs mail servers as usual to accept messages for users with SMTP on port
However  the user agent is different
Instead of THE APPLICATION LAYER
being a standalone program  it is a user interface that is provided via Web pages
This means that users can use any browser they like to access their mail and send new messages
We have not yet studied the Web  but a brief description that you might come back to is as follows
When the user goes to the email Web page of the provider  a form is presented in which the user is asked for a login name and password
The login name and password are sent to the server  which then validates them
If the login is successful  the server finds the userâs mailbox and builds a Web page listing the contents of the mailbox on the fly
The Web page is then sent to the browser for display
Many of the items on the page showing the mailbox are clickable  so messages can be read  deleted  and so on
To make the interface responsive  the Web pages will often include JavaScript programs
These programs are run locally on the client in response to local events (
mouse clicks) and can also download and upload messages in the background  to prepare the next message for display or a new message for submission
In this model  mail submission happens using the normal Web protocols by posting data to a URL
The Web server takes care of injecting messages into the traditional mail delivery system that we have described
For urity  the standard Web protocols can be used as well
These protocols concern themselves with encrypting Web pages  not whether the content of the Web page is a mail message  THE WORLD WIDE WEB The Web  as the World Wide Web is popularly known  is an architectural framework for accessing linked content spread out over millions of machines all over the Internet
In   years it went from being a way to coordinate the design of high-energy physics experiments in Switzerland to the application that millions of people think of as being ââThe Internet
ââ Its enormous popularity stems from the fact that it is easy for beginners to use and provides access with a rich graphical interface to an enormous wealth of information on almost every conceivable subject  from aardvarks to Zulus
The Web began in  at CERN  the European Center for Nuclear Research
The initial idea was to help large teams  often with members in half a dozen or more countries and time zones  collaborate using a constantly changing collection of reports  blueprints  drawings  photos  and other documents produced by experiments in particle physics
The proposal for a web of linked documents came from CERN physicist Tim Berners-Lee
The first (text-based) prototype was operational   months later
A public demonstration given at the Hypertext â  conference caught the attention of other researchers  which led Marc Andreessen at the University of Illinois to develop the first graphical browser
It was called Mosaic and released in February
THE WORLD WIDE WEB The rest  as they say  is now history
Mosaic was so popular that a year later Andreessen left to form a company  Netscape Communications Corp
whose goal was to develop Web software
For the next three years  Netscape Navigator and Microsoftâs Internet Explorer engaged in a ââbrowser war ââ each one trying to capture a larger share of the new market by frantically adding more features (and thus more bugs) than the other one
Through the s and s  Web sites and Web pages  as Web content is called  grew exponentially until there were millions of sites and billions of pages
A small number of these sites became tremendously popular
Those sites and the companies behind them largely define the Web as people experience it today
Examples include: a bookstore (Amazon  started in  market capitalization $  billion)  a flea market (eBay   $  B)  search (Google   $  B)  and social networking (Facebook   private company valued at more than $  B)
The period through  when many Web companies became worth hundreds of millions of dollars overnight  only to go bust practically the next day when they turned out to be hype  even has a name
It is called the dot com era
New ideas are still striking it rich on the Web
Many of them come from students
For example  Mark Zuckerberg was a Harvard student when he started Facebook  and Sergey Brin and Larry Page were students at Stanford when they started Google
Perhaps you will come up with the next big thing
In  CERN and
signed an agreement setting up the W C (World Wide Web Consortium)  an organization devoted to further developing the Web  standardizing protocols  and encouraging interoperability between sites
Berners- Lee became the director
Since then  several hundred universities and companies have joined the consortium
Although there are now more books about the Web than you can shake a stick at  the best place to get up-to-date information about the Web is (naturally) on the Web itself
The consortiumâs home page is at
Interested readers are referred there for links to pages covering all of the consortiumâs numerous documents and activities
Architectural Overview From the usersâ point of view  the Web consists of a vast  worldwide collection of content in the form of Web pages  often just called pages for short
Each page may contain links to other pages anywhere in the world
Users can follow a link by clicking on it  which then takes them to the page pointed to
This process can be repeated indefinitely
The idea of having one page point to another  now called hypertext  was invented by a visionary
professor of electrical engineering  Vannevar Bush  in  (Bush  )
This was long before the Internet was invented
In fact  it was before commercial computers existed although several universities had produced crude prototypes that filled large rooms and had less power than a modern pocket calculator
THE APPLICATION LAYER
Pages are generally viewed with a program called a browser
Firefox  Internet Explorer  and Chrome are examples of popular browsers
The browser fetches the page requested  interprets the content  and displays the page  properly formatted  on the screen
The content itself may be a mix of text  images  and formatting commands  in the manner of a traditional document  or other forms of content such as video or programs that produce a graphical interface with which users can interact
A picture of a page is shown on the top-left side of Fig
It is the page for the Computer Science & Engineering department at the University of Washington
This page shows text and graphical elements (that are mostly too small to read)
Some parts of the page are associated with links to other pages
A piece of text  icon  image  and so on associated with another page is called a hyperlink
To follow a link  the user places the mouse cursor on the linked portion of the page area (which causes the cursor to change shape) and clicks
Following a link is simply a way of telling the browser to fetch another page
In the early days of the Web  links were highlighted with underlining and colored text so that they would stand out
Nowadays  the creators of Web pages have ways to control the look of linked regions  so a link might appear as an icon or change its appearance when the mouse passes over it
It is up to the creators of the page to make the links visually distinct  to provide a usable interface
HTTP Request Database Web page Hyperlink Web browser Document
Program HTTP Response Web server   google-  Figure  -
Architecture of the Web
THE WORLD WIDE WEB Students in the department can learn more by following a link to a page with information especially for them
This link is accessed by clicking in the circled area
The browser then fetches the new page and displays it  as partially shown in the bottom left of Fig
Dozens of other pages are linked off the first page besides this example
Every other page can be comprised of content on the same machine(s) as the first page  or on machines halfway around the globe
The user cannot tell
Page fetching is done by the browser  without any help from the user
Thus  moving between machines while viewing content is seamless
The basic model behind the display of pages is also shown in Fig
The browser is displaying a Web page on the client machine
Each page is fetched by sending a request to one or more servers  which respond with the contents of the page
The request-response protocol for fetching pages is a simple text-based protocol that runs over TCP  just as was the case for SMTP
It is called HTTP (HyperText Transfer Protocol)
The content may simply be a document that is read off a disk  or the result of a database query and program execution
The page is a static page if it is a document that is the same every time it is displayed
In contrast  if it was generated on demand by a program or contains a program it is a dynamic page
A dynamic page may present itself differently each time it is displayed
For example  the front page for an electronic store may be different for each visitor
If a bookstore customer has bought mystery novels in the past  upon visiting the storeâs main page  the customer is likely to see new thrillers prominently displayed  whereas a more culinary-minded customer might be greeted with new cookbooks
How the Web site keeps track of who likes what is a story to be told shortly
But briefly  the answer involves cookies (even for culinarily challenged visitors)
In the figure  the browser contacts three servers to fetch the two pages        and google-
The content from these different servers is integrated for display by the browser
Display entails a range of processing that depends on the kind of content
Besides rendering text and graphics  it may involve playing a video or running a script that presents its own user interface as part of the page
In this case  the   server supplies the main page  the   server supplies an embedded video  and the google-  server supplies nothing that the user can see but tracks visitors to the site
We will have more to say about trackers later
The Client Side Let us now examine the Web browser side in Fig
In essence  a browser is a program that can display a Web page and catch mouse clicks to items on the displayed page
When an item is selected  the browser follows the hyperlink and fetches the page selected
THE APPLICATION LAYER
When the Web was first created  it was immediately apparent that having one page point to another Web page required mechanisms for naming and locating pages
In particular  three questions had to be answered before a selected page could be displayed:
What is the page called?
Where is the page located?
How can the page be accessed? If every page were somehow assigned a unique name  there would not be any ambiguity in identifying pages
Nevertheless  the problem would not be solved
Consider a parallel between people and pages
In the United States  almost everyone has a social urity number  which is a unique identifier  as no two people are supposed to have the same one
Nevertheless  if you are armed only with a social urity number  there is no way to find the ownerâs address  and certainly no way to tell whether you should write to the person in English  Spanish  or Chinese
The Web has basically the same problems
The solution chosen identifies pages in a way that solves all three problems at once
Each page is assigned a URL (Uniform Resource Locator) that effectively serves as the pageâs worldwide name
URLs have three parts: the protocol (also known as the scheme)  the DNS name of the machine on which the page is located  and the path uniquely indicating the specific page (a file to read or program to run on the machine)
In the general case  the path has a hierarchical name that models a file directory structure
However  the interpretation of the path is up to the server; it may or may not reflect the actual directory structure
As an example  the URL of the page shown in Fig
-  is http://
/  This URL consists of three parts: the protocol (http)  the DNS name of the host (
)  and the path name ( )
When a user clicks on a hyperlink  the browser carries out a series of steps in order to fetch the page pointed to
Let us trace the steps that occur when our example link is selected:
The browser determines the URL (by seeing what was selected) The browser asks DNS for the IP address of the server
DNS replies with
The browser makes a TCP connection to
on port the well-known port for the HTTP protocol It sends over an HTTP request asking for the page /
THE WORLD WIDE WEB
server sends the page as an HTTP response  for example  by sending the file /  If the page includes URLs that are needed for display  the browser fetches the other URLs using the same process
In this case  the URLs include multiple embedded images also fetched from
an embedded video from    and a script from google-  The browser displays the page /  as it appears in Fig
Many browsers display which step they are currently executing in a status line at the bottom of the screen
In this way  when the performance is poor  the user can see if it is due to DNS not responding  a server not responding  or simply page transmission over a slow or congested network
The URL design is open-ended in the sense that it is straightforward to have browsers use multiple protocols to get at different kinds of resources
In fact  URLs for various other protocols have been defined
Slightly simplified forms of the common ones are listed in Fig
Name Used for Example http Hypertext (HTML) http://
/~rob/ https Hypertext with urity https:// /accounts/ ftp FTP ftp://
/pub/minix/README file Local file file:///usr/suzanne/  mailto Sending email mailto:JohnUser@  rtsp Streaming media rtsp:// /  sip Multimedia calls sip:eve@  about Browser information about:plugins Figure  -
Some common URL schemes
Let us briefly go over the list
The http protocol is the Webâs native language  the one spoken by Web servers
HTTP stands for HyperText Transfer Protocol
We will examine it in more detail later in this tion
The ftp protocol is used to access files by FTP  the Internetâs file transfer protocol
FTP predates the Web and has been in use for more than three decades
The Web makes it easy to obtain files placed on numerous FTP servers throughout the world by providing a simple  clickable interface instead of a command- line interface
This improved access to information is one reason for the spectacular growth of the Web
THE APPLICATION LAYER
It is possible to access a local file as a Web page by using the file protocol  or more simply  by just naming it
This approach does not require having a server
Of course  it works only for local files  not remote ones
The mailto protocol does not really have the flavor of fetching Web pages  but is useful anyway
It allows users to send email from a Web browser
Most browsers will respond when a mailto link is followed by starting the userâs mail agent to compose a message with the address field already filled in
The rtsp and sip protocols are for establishing streaming media sessions and audio and video calls
Finally  the about protocol is a convention that provides information about the browser
For example  following the about:plugins link will cause most browsers to show a page that lists the MIME types that they handle with browser extensions called plug-ins
In short  the URLs have been designed not only to allow users to navigate the Web  but to run older protocols such as FTP and email as well as newer protocols for audio and video  and to provide convenient access to local files and browser information
This approach makes all the specialized user interface programs for those other services unnecessary and integrates nearly all Internet access into a single program: the Web browser
If it were not for the fact that this idea was thought of by a British physicist working a research lab in Switzerland  it could easily pass for a plan dreamed up by some software companyâs advertising department
Despite all these nice properties  the growing use of the Web has turned up an inherent weakness in the URL scheme
A URL points to one specific host  but sometimes it is useful to reference a page without simultaneously telling where it is
For example  for pages that are heavily referenced  it is desirable to have multiple copies far apart  to reduce the network traffic
There is no way to say: ââI want page xyz  but I do not care where you get it
ââ To solve this kind of problem  URLs have been generalized into URIs (Uniform Resource Identifiers)
Some URIs tell how to locate a resource
These are the URLs
Other URIs tell the name of a resource but not where to find it
These URIs are called URNs (Uniform Resource Names)
The rules for writing URIs are given in RFC  while the different URI schemes in use are tracked by IANA
There are many different kinds of URIs besides the schemes listed in Fig
MIME Types To be able to display the new page (or any page)  the browser has to understand its format
To allow all browsers to understand all Web pages  Web pages are written in a standardized language called HTML
It is the lingua franca of the Web (for now)
We will discuss it in detail later in this  ter
THE WORLD WIDE WEB Although a browser is basically an HTML interpreter  most browsers have numerous buttons and features to make it easier to navigate the Web
Most have a button for going back to the previous page  a button for going forward to the next page (only operative after the user has gone back from it)  and a button for going straight to the userâs preferred start page
Most browsers have a button or menu item to set a bookmark on a given page and another one to display the list of bookmarks  making it possible to revisit any of them with only a few mouse clicks
As our example shows  HTML pages can contain rich content elements and not simply text and hypertext
For added generality  not all pages need contain HTML
A page may consist of a video in MPEG format  a document in PDF format  a photograph in JPEG format  a song in MP  format  or any one of hundreds of other file types
Since standard HTML pages may link to any of these  the browser has a problem when it hits a page it does not know how to interpret
Rather than making the browsers larger and larger by building in interpreters for a rapidly growing collection of file types  most browsers have chosen a more general solution
When a server returns a page  it also returns some additional information about the page
This information includes the MIME type of the page (see Fig
Pages of type text/html are just displayed directly  as are pages in a few other built-in types
If the MIME type is not one of the built-in ones  the browser consults its table of MIME types to determine how to display the page
This table associates MIME types with viewers
There are two possibilities: plug-ins and helper applications
A plug-in is a third-party code module that is installed as an extension to the browser  as illustrated in Fig
Common examples are plug-ins for PDF  Flash  and Quicktime to render documents and play audio and video
Because plug-ins run inside the browser  they have access to the current page and can modify its appearance
Process Helper Browser Plug-in Browser application Process Process (a) (b) Figure  -
(a) A browser plug-in
(b) A helper application
Each browser has a set of procedures that all plug-ins must implement so the browser can call the plug-ins
For example  there is typically a procedure the THE APPLICATION LAYER
browserâs base code calls to supply the plug-in with data to display
This set of procedures is the plug-inâs interface and is browser specific
In addition  the browser makes a set of its own procedures available to the plug-in  to provide services to plug-ins
Typical procedures in the browser interface are for allocating and freeing memory  displaying a message on the browserâs status line  and querying the browser about parameters
Before a plug-in can be used  it must be installed
The usual installation procedure is for the user to go to the plug-inâs Web site and download an installation file
Executing the installation file unpacks the plug-in and makes the appropriate calls to register the plug-inâs MIME type with the browser and associate the plug-in with it
Browsers usually come preloaded with popular plug-ins
The other way to extend a browser is make use of a helper application
This is a complete program  running as a separate process
It is illustrated in Fig
Since the helper is a separate program  the interface is at armâs length from the browser
It usually just accepts the name of a scratch file where the content file has been stored  opens the file  and displays the contents
Typically  helpers are large programs that exist independently of the browser  for example  Microsoft Word or PowerPoint
Many helper applications use the MIME type application
As a consequence  a considerable number of subtypes have been defined for them to use  for example  application/ -powerpoint for PowerPoint files
vnd denotes vendor-specific formats
In this way  a URL can point directly to a PowerPoint file  and when the user clicks on it  PowerPoint is automatically started and handed the content to be displayed
Helper applications are not restricted to using the application MIME type
Adobe Photoshop uses image/x-photoshop  for example
Consequently  browsers can be configured to handle a virtually unlimited number of document types with no changes to themselves
Modern Web servers are often configured with hundreds of type/subtype combinations and new ones are often added every time a new program is installed
A source of conflicts is that multiple plug-ins and helper applications are available for some subtypes  such as video/mpeg
What happens is that the last one to register overwrites the existing association with the MIME type  capturing the type for itself
As a consequence  installing a new program may change the way a browser handles existing types
Browsers can also open local files  with no network in sight  rather than fetching them from remote Web servers
However  the browser needs some way to determine the MIME type of the file
The standard method is for the operating system to associate a file extension with a MIME type
In a typical configuration  opening   will open it in the browser using an application/pdf plug-in and opening   will open it in Word as the application/msword helper
Here  too  conflicts can arise  since many programs are willingâno  make that eagerâto handle  say  mpg
During installation  programs intended for sophisticated users often display checkboxes for the MIME types and extensions   THE WORLD WIDE WEB they are prepared to handle to allow the user to select the appropriate ones and thus not overwrite existing associations by accident
Programs aimed at the consumer market assume that the user does not have a clue what a MIME type is and simply grab everything they can without regard to what previously installed programs have done
The ability to extend the browser with a large number of new types is convenient but can also lead to trouble
When a browser on a Windows PC fetches a file with the extension exe  it realizes that this file is an executable program and therefore has no helper
The obvious action is to run the program
However  this could be an enormous urity hole
All a malicious Web site has to do is produce a Web page with pictures of  say  movie stars or sports heroes  all of which are linked to a virus
A single click on a picture then causes an unknown and potentially hostile executable program to be fetched and run on the userâs machine
To prevent unwanted guests like this  Firefox and other browsers come configured to be cautious about running unknown programs automatically  but not all users understand what choices are safe rather than convenient
The Server Side So much for the client side
Now let us take a look at the server side
As we saw above  when the user types in a URL or clicks on a line of hypertext  the browser parses the URL and interprets the part between http:// and the next slash as a DNS name to look up
Armed with the IP address of the server  the browser establishes a TCP connection to port   on that server
Then it sends over a command containing the rest of the URL  which is the path to the page on that server
The server then returns the page for the browser to display
To a first approximation  a simple Web server is similar to the server of Fig
That server is given the name of a file to look up and return via the network
In both cases  the steps that the server performs in its main loop are:
Accept a TCP connection from a client (a browser) Get the path to the page  which is the name of the file requested Get the file (from disk) Send the contents of the file to the client Release the TCP connection
Modern Web servers have more features  but in essence  this is what a Web server does for the simple case of content that is contained in a file
For dynamic content  the third step may be replaced by the execution of a program (determined from the path) that returns the contents
However  Web servers are implemented with a different design to serve many requests per ond
One problem with the simple design is that accessing files is THE APPLICATION LAYER
often the bottleneck
Disk reads are very slow compared to program execution  and the same files may be read repeatedly from disk using operating system calls
Another problem is that only one request is processed at a time
The file may be large  and other requests will be blocked while it is transferred
One obvious improvement (used by all Web servers) is to maintain a cache in memory of the n most recently read files or a certain number of gigabytes of content
Before going to disk to get a file  the server checks the cache
If the file is there  it can be served directly from memory  thus eliminating the disk access
Although effective caching requires a large amount of main memory and some extra processing time to check the cache and manage its contents  the savings in time are nearly always worth the overhead and expense
To tackle the problem of serving a single request at a time  one strategy is to make the server multithreaded
In one design  the server consists of a front-end module that accepts all incoming requests and k processing modules  as shown in Fig
The k +  threads all belong to the same process  so the processing modules all have access to the cache within the processâ address space
When a request comes in  the front end accepts it and builds a short record describing it
It then hands the record to one of the processing modules
Processing module (thread) Front end Cache Disk Request Client Response Server Figure  -
A multithreaded Web server with a front end and processing modules
The processing module first checks the cache to see if the file needed is there
If so  it updates the record to include a pointer to the file in the record
If it is not there  the processing module starts a disk operation to read it into the cache (possibly discarding some other cached file(s) to make room for it)
When the file comes in from the disk  it is put in the cache and also sent back to the client
The advantage of this scheme is that while one or more processing modules are blocked waiting for a disk or network operation to complete (and thus consuming no CPU time)  other modules can be actively working on other requests
With k processing modules  the throughput can be as much as k times higher than with a single-threaded server
Of course  when the disk or network is the limiting   THE WORLD WIDE WEB factor  it is necessary to have multiple disks or a faster network to get any real improvement over the single-threaded model
Modern Web servers do more than just accept path names and return files
In fact  the actual processing of each request can get quite complicated
For this reason  in many servers each processing module performs a series of steps
The front end passes each incoming request to the first available module  which then carries it out using some subset of the following steps  depending on which ones are needed for that particular request
These steps occur after the TCP connection and any ure transport mechanism (such as SSL/TLS  which will be described in
) have been established Resolve the name of the Web page requested Perform access control on the Web page Check the cache Fetch the requested page from disk or run a program to build it Determine the rest of the response (
the MIME type to send) Return the response to the client Make an entry in the server log
Step  is needed because the incoming request may not contain the actual name of a file or program as a literal string
It may contain built-in shortcuts that need to be translated
As a simple example  the URL http://
/ has an empty file name
It has to be expanded to some default file name that is usually
Another common rule is to map ~user/ onto userâs Web directory
These rules can be used together
Thus  the home page of one of the authors (AST) can be reached at http://
/~ast/ even though the actual file name is   in a certain default directory
Also  modern browsers can specify configuration information such as the browser software and the userâs default language (
Italian or English)
This makes it possible for the server to select a Web page with small pictures for a mobile device and in the preferred language  if available
In general  name expansion is not quite so trivial as it might at first appear  due to a variety of conventions about how to map paths to the file directory and programs
Step  checks to see if any access restrictions associated with the page are met
Not all pages are available to the general public
Determining whether a client can fetch a page may depend on the identity of the client (
as given by usernames and passwords) or the location of the client in the DNS or IP space
For example  a page may be restricted to users inside a company
How this is THE APPLICATION LAYER
accomplished depends on the design of the server
For the popular Apache server  for instance  the convention is to place a file called  that lists the access restrictions in the directory where the restricted page is located
Steps  and  involve getting the page
Whether it can be taken from the cache depends on processing rules
For example  pages that are created by running programs cannot always be cached because they might produce a different result each time they are run
Even files should occasionally be checked to see if their contents have changed so that the old contents can be removed from the cache
If the page requires a program to be run  there is also the issue of setting the program parameters or input
These data come from the path or other parts of the request
Step  is about determining other parts of the response that accompany the contents of the page
The MIME type is one example
It may come from the file extension  the first few words of the file or program output  a configuration file  and possibly other sources
Step  is returning the page across the network
To increase performance  a single TCP connection may be used by a client and server for multiple page fetches
This reuse means that some logic is needed to map a request to a shared connection and to return each response so that it is associated with the correct request
Step  makes an entry in the system log for administrative purposes  along with keeping any other important statistics
Such logs can later be mined for valuable information about user behavior  for example  the order in which people access the pages
Cookies Navigating the Web as we have described it so far involves a series of independent page fetches
There is no concept of a login session
The browser sends a request to a server and gets back a file
Then the server forgets that it has ever seen that particular client
This model is perfectly adequate for retrieving publicly available documents  and it worked well when the Web was first created
However  it is not suited for returning different pages to different users depending on what they have already done with the server
This behavior is needed for many ongoing interactions with Web sites
For example  some Web sites (
newspapers) require clients to register (and possibly pay money) to use them
This raises the question of how servers can distinguish between requests from users who have previously registered and everyone else
A ond example is from e-commerce
If a user wanders around an electronic store  tossing items into her virtual shopping cart from time to time  how does the server keep track of the contents of the cart? A third example is customized Web portals such as Yahoo!
Users can set up a personalized   THE WORLD WIDE WEB detailed initial page with only the information they want (
their stocks and their favorite sports teams)  but how can the server display the correct page if it does not know who the user is? At first glance  one might think that servers could track users by observing their IP addresses
However  this idea does not work
Many users share computers  especially at home  and the IP address merely identifies the computer  not the user
Even worse  many companies use NAT  so that outgoing packets bear the same IP address for all users
That is  all of the computers behind the NAT box look the same to the server
And many ISPs assign IP addresses to customers with DHCP
The IP addresses change over time  so to a server you might suddenly look like your neighbor
For all of these reasons  the server cannot use IP addresses to track users
This problem is solved with an oft-critized mechanism called cookies
The name derives from ancient programmer slang in which a program calls a procedure and gets something back that it may need to present later to get some work done
In this sense  a UNIX file descriptor or a Windows object handle can be considered to be a cookie
Cookies were first implemented in the Netscape browser in  and are now specified in RFC
When a client requests a Web page  the server can supply additional information in the form of a cookie along with the requested page
The cookie is a rather small  named string (of at most  KB) that the server can associate with a browser
This association is not the same thing as a user  but it is much closer and more useful than an IP address
Browsers store the offered cookies for an interval  usually in a cookie directory on the clientâs disk so that the cookies persist across browser invocations  unless the user has disabled cookies
Cookies are just strings  not executable programs
In principle  a cookie could contain a virus  but since cookies are treated as data  there is no official way for the virus to actually run and do damage
However  it is always possible for some hacker to exploit a browser bug to cause activation
A cookie may contain up to five fields  as shown in Fig
The Domain tells where the cookie came from
Browsers are supposed to check that servers are not lying about their domain
Each domain should store no more than   cookies per client
The Path is a path in the serverâs directory structure that identifies which parts of the serverâs file tree may use the cookie
It is often /  which means the whole tree
The Content field takes the form name = value
Both name and value can be anything the server wants
This field is where the cookieâs content is stored
The Expires field specifies when the cookie expires
If this field is absent  the browser discards the cookie when it exits
Such a cookie is called a nonpersistent cookie
If a time and date are supplied  the cookie is said to be a persistent cookie and is kept until it expires
Expiration times are given in Greenwich Mean Time
To remove a cookie from a clientâs hard disk  a server just sends it again  but with an expiration time in the past
THE APPLICATION LAYER
Domain Path Content Expires ure toms-  / CustomerID=   -  - :  Yes jills-  / Cart= - ; - ; -   - - :  No   / Prefs=Stk:CSCO+ORCL;Spt:Jets  -  - :  No   / UserID=    -  - :  No Figure  -
Some examples of cookies
Finally  the ure field can be set to indicate that the browser may only return the cookie to a server using a ure transport  namely SSL/TLS (which we will describe in
This feature is used for e-commerce  banking  and other ure applications
We have now seen how cookies are acquired  but how are they used? Just before a browser sends a request for a page to some Web site  it checks its cookie directory to see if any cookies there were placed by the domain the request is going to
If so  all the cookies placed by that domain  and only that domain  are included in the request message
When the server gets them  it can interpret them any way it wants to
Let us examine some possible uses for cookies
When the client returns next week to throw away some more money  the browser sends over the cookie so the server knows who it is
Armed with the customer ID  the server can look up the customerâs record in a database and use this information to build an appropriate Web page to display
Depending on the customerâs known gambling habits  this page might consist of a poker hand  a listing of todayâs horse races  or a slot machine
The ond cookie came from jills-
The scenario here is that the client is wandering around the store  looking for good things to buy
When she finds a bargain and clicks on it  the server adds it to her shopping cart (maintained on the server) and also builds a cookie containing the product code of the item and sends the cookie back to the client
As the client continues to wander around the store by clicking on new pages  the cookie is returned to the server on every new page request
As more purchases accumulate  the server adds them to the cookie
Finally  when the client clicks on PROCEED TO CHECKOUT  the cookie  now containing the full list of purchases  is sent along with the request
In this way  the server knows exactly what the customer wants to buy
The third cookie is for a Web portal
When the customer clicks on a link to the portal  the browser sends over the cookie
This tells the portal to build a page containing the stock prices for Cisco and Oracle  and the New York Jetsâ football results
Since a cookie can be up to  KB  there is plenty of room for more detailed preferences concerning newspaper headlines  local weather  special offers  etc
THE WORLD WIDE WEB A more controversial use of cookies is to track the online behavior of users
This lets Web site operators understand how users navigate their sites  and advertisers build up profiles of the ads or sites a particular user has viewed
The controversy is that users are typically unaware that their activity is being tracked  even with detailed profiles and across seemingly unrelated Web sites
Nonetheless  Web tracking is big business
DoubleClick  which provides and tracks ads  is ranked among the busiest Web sites in the world by the Web monitoring company Alexa
Google Analytics  which tracks site usage for operators  is used by more than half of the busiest    sites on the Web
It is easy for a server to track user activity with cookies
Suppose a server wants to keep track of how many unique visitors it has had and how many pages each visitor looked at before leaving the site
When the first request comes in  there will be no accompanying cookie  so the server sends back a cookie containing Counter =
Subsequent page views on that site will send the cookie back to the server
Each time the counter is incremented and sent back to the client
By keeping track of the counters  the server can see how many people give up after seeing the first page  how many look at two pages  and so on
Tracking the browsing behavior of users across sites is only slightly more complicated
It works like this
An advertising agency  say  Sneaky Ads  contacts major Web sites and places ads for its clientsâ products on their pages  for which it pays the site owners a fee
Instead  of giving the sites the ad as a GIF file to place on each page  it gives them a URL to add to each page
Each URL it hands out contains a unique number in the path  such as http:// /    When a user first visits a page  P  containing such an ad  the browser fetches the HTML file
Then the browser inspects the HTML file and sees the link to the image file at    so it sends a request there for the image
A GIF file containing an ad is returned  along with a cookie containing a unique user ID    in Fig
Sneaky records the fact that the user with this ID visited page P
This is easy to do since the path requested (   ) is referenced only on page P
Of course  the actual ad may appear on thousands of pages  but each time with a different name
Sneaky probably collects a fraction of a penny from the product manufacturer each time it ships out the ad
Later  when the user visits another Web page containing any of Sneakyâs ads  the browser first fetches the HTML file from the server
Then it sees the link to  say  http:// /    on the page and requests that file
Since it already has a cookie from the domain    the browser includes Sneakyâs cookie containing the userâs ID
Sneaky now knows a ond page the user has visited
In due course  Sneaky can build up a detailed profile of the userâs browsing habits  even though the user has never clicked on any of the ads
Of course  it does not yet have the userâs name (although it does have his IP address  which THE APPLICATION LAYER
may be enough to deduce the name from other databases)
However  if the user ever supplies his name to any site cooperating with Sneaky  a complete profile along with a name will be available for sale to anyone who wants to buy it
The sale of this information may be profitable enough for Sneaky to place more ads on more Web sites and thus collect more information
And if Sneaky wants to be supersneaky  the ad need not be a classical banner ad
An ââadââ consisting of a single pixel in the background color (and thus invisible) has exactly the same effect as a banner ad: it requires the browser to go fetch the  Ã  -pixel GIF image and send it all cookies originating at the pixelâs domain
Cookies have become a focal point for the debate over online privacy because of tracking behavior like the above
The most insidious part of the whole business is that many users are completely unaware of this information collection and may even think they are safe because they do not click on any of the ads
For this reason  cookies that track users across sites are considered by many to be spyware
Have a look at the cookies that are already stored by your browser
Most browsers will display this information along with the current privacy preferences
You might be surprised to find names  email addresses  or passwords as well as opaque identifiers
Hopefully  you will not find credit card numbers  but the potential for abuse is clear
To maintain a semblance of privacy  some users configure their browsers to reject all cookies
However  this can cause problems because many Web sites will not work properly without cookies
Alternatively  most browsers let users block third-party cookies
A third-party cookie is one from a different site than the main page that is being fetched  for example  the   cookie that is used when interacting with page P on a completely different Web site
Blocking these cookies helps to prevent tracking across Web sites
Browser extensions can also be installed to provide fine-grained control over how cookies are used (or  rather  not used)
As the debate continues  many companies are developing privacy policies that limit how they will share information to prevent abuse
Of course  the policies are simply how the companies say they will handle information
For example: ââWe may use the information collected from you in the conduct of our businessâââwhich might be selling the information
Static Web Pages The basis of the Web is transferring Web pages from server to client
In the simplest form  Web pages are static
That is  they are just files sitting on some server that present themselves in the same way each time they are fetched and viewed
Just because they are static does not mean that the pages are inert at the browser  however
A page containing a video can be a static Web page
As mentioned earlier  the lingua franca of the Web  in which most pages are written  is HTML
The home pages of teachers are usually static HTML pages
THE WORLD WIDE WEB The home pages of companies are usually dynamic pages put together by a Web design company
In this tion  we will take a brief look at static HTML pages as a foundation for later material
Readers already familiar with HTML can skip ahead to the next tion  where we describe dynamic content and Web services
HTMLâThe HyperText Markup Language HTML (HyperText Markup Language) was introduced with the Web
It allows users to produce Web pages that include text  graphics  video  pointers to other Web pages  and more
HTML is a markup language  or language for describing how documents are to be formatted
The term ââmarkupââ comes from the old days when copyeditors actually marked up documents to tell the printerâ in those days  a human beingâwhich fonts to use  and so on
Markup languages thus contain explicit commands for formatting
For example  in HTML  <b> means start boldface mode  and </b> means leave boldface mode
LaTeX and TeX are other examples of markup languages that are well known to most academic authors
The key advantage of a markup language over one with no explicit markup is that it separates content from how it should be presented
Writing a browser is then straightforward: the browser simply has to understand the markup commands and apply them to the content
Embedding all the markup commands within each HTML file and standardizing them makes it possible for any Web browser to read and reformat any Web page
That is crucial because a page may have been produced in a  Ã  window with  -bit color on a high-end computer but may have to be displayed in a Ã window on a mobile phone
While it is certainly possible to write documents like this with any plain text editor  and many people do  it is also possible to use word processors or special HTML editors that do most of the work (but correspondingly give the user less direct control over the details of the final result)
A simple Web page written in HTML and its presentation in a browser are given in Fig
A Web page consists of a head and a body  each enclosed by <html> and </html> tags (formatting commands)  although most browsers do not complain if these tags are missing
As can be seen in Fig
-  (a)  the head is bracketed by the <head> and </head> tags and the body is bracketed by the <body> and </body> tags
The strings inside the tags are called directives
Most  but not all  HTML tags have this format
That is  they use <something> to mark the beginning of something and </something> to mark its end
Tags can be in either lowercase or uppercase
Thus  <head> and <HEAD> mean the same thing  but lower case is best for compatibility
Actual layout of the HTML document is irrelevant
HTML parsers ignore extra spaces and carriage returns since they have to reformat the text to make it fit the current display area
Consequently  white space can be added at will to make HTML documents more THE APPLICATION LAYER
readable  something most of them are badly in need of
As another consequence  blank lines cannot be used to separate paragraphs  as they are simply ignored
An explicit tag is required
Some tags have (named) parameters  called attributes
For example  the <img> tag in Fig
It has two attributes  src and alt
The first attribute gives the URL for the image
The HTML standard does not specify which image formats are permitted
In practice  all browsers support GIF and JPEG files
Browsers are free to support other formats  but this extension is a two-edged sword
If a user is accustomed to a browser that supports  say  TIFF files  he may include these in his Web pages and later be surprised when other browsers just ignore all of his wonderful art
The ond attribute gives alternate text to use if the image cannot be displayed
For each tag  the HTML standard gives a list of what the permitted parameters  if any  are  and what they mean
Because each parameter is named  the order in which the parameters are given is not significant
Technically  HTML documents are written in the ISO -  Latin-  character set  but for users whose keyboards support only ASCII  escape sequences are present for the special characters  such as e`
The list of special characters is given in the standard
All of them begin with an ampersand and end with a semicolon
For example  &nbsp; produces a space  &egrave; produces e` and &eacute; produces eÂ´
Since <  >  and & have special meanings  they can be expressed only with their escape sequences  &lt;  &gt;  and &amp;  respectively
The main item in the head is the title  delimited by <title> and </title>
Certain kinds of metainformation may also be present  though none are present in our example
The title itself is not displayed on the page
Some browsers use it to label the pageâs window
Several headings are used in Fig
Each heading is generated by an <hn> tag  where n is a digit in the range  to
Thus  <h > is the most important heading; <h > is the least important one
It is up to the browser to render these appropriately on the screen
Typically  the lower-numbered headings will be displayed in a larger and heavier font
The browser may also choose to use different colors for each level of heading
Usually  <h > headings are large and boldface with at least one blank line above and below
In contrast  <h > headings are in a smaller font with less space above and below
The tags <b> and <i> are used to enter boldface and italics mode  respectively
The <hr> tag forces a break and draws a horizontal line across the display
The <p> tag starts a paragraph
The browser might display this by inserting a blank line and some indentation  for example
Interestingly  the </p> tag that exists to mark the end of a paragraph is often omitted by lazy HTML programmers
HTML provides various mechanisms for making lists  including nested lists
Unordered lists  like the ones in Fig
There is also an <ol> tag to starts an ordered list
The   THE WORLD WIDE WEB <html> <head> <title> AMALGAMATED WIDGET  INC
"</title> </head> <body> <h > Welcome to AWIâs Home Page </h > <img src=""http:// /images/ "" ALT=""AWI Logo""> <br> We are so happy that you have chosen to visit <b> Amalgamated Widgetâs</b> home page"
We hope <i> you </i> will find all the information you need here
<p>Below we have links to information about our many fine products
You can order electronically (by WWW)  by telephone  or by email
"</p> <hr> <h > Product information </h > <ul> <li> <a href=""http:// /products/big""> Big widgets </a> </li> <li> <a href=""http:// /products/little""> Little widgets </a> </li> </ul> <h > Contact information </h > <ul> <li> By telephone:  -  -WIDGETS </li> <li> By email: info@amalgamated-  </li> </ul> </body> </html> (a) Welcome to AWI's Home Page We are so happy that you have chosen to visit Amalgamated Widget's home page"
We hope you will find all the information you need here
Below we have links to information about our many fine products
You can order electronically (by WWW)  by telephone  or by email
Product Information
Big widgets
Little widgets Contact information
By telephone:  -  -WIDGETS
By email: info@amalgamated-  (b) Figure  -
(a) The HTML for a sample Web page
(b) The formatted page
THE APPLICATION LAYER
individual items in unordered lists often appear with bullets ( ) in front of them
Items in ordered lists are numbered by the browser
Finally  we come to hyperlinks
Examples of these are seen in Fig
-  using the <a> (anchor) and </a> tags
The <a> tag has various parameters  the most important of which is href the linked URL
The text between the <a> and </a> is displayed
If it is selected  the hyperlink is followed to a new page
It is also permitted to link other elements
For example  an image can be given between the <a> and </a> tags using <img>
In this case  the image is displayed and clicking on it activates the hyperlink
There are many other HTML tags and attributes that we have not seen in this simple example
For instance  the <a> tag can take a parameter name to plant a hyperlink  allowing a hyperlink to point to the middle of a page
This is useful  for example  for Web pages that start out with a clickable table of contents
By clicking on an item in the table of contents  the user jumps to the corresponding tion of the same page
An example of a different tag is <br>
It forces the browser to break and start a new line
Probably the best way to understand tags is to look at them in action
To do this  you can pick a Web page and look at the HTML in your browser to see how the page was put together
Most browsers have a VIEW SOURCE menu item (or something similar)
Selecting this item displays the current pageâs HTML source  instead of its formatted output
We have sketched the tags that have existed from the early Web
HTML keeps evolving
refers to the version of HTML used with the introduction of the Web
HTML versions
appeared in rapid succession in the space of only a few years as the Web exploded
After HTML
a period of almost ten years passed before the path to standarization of the next major version  HTML
became clear
Because it is a major upgrade that consolidates the ways that browsers handle rich content  the HTML
effort is ongoing and not expected to produce a standard before  at the earliest
Standards notwithstanding  the major browsers already support HTML
functionality
The progression through HTML versions is all about adding new features that people wanted but had to handle in nonstandard ways (
plug-ins) until they became standard
For example  HTML
did not have tables
They were added in HTML   An HTML table consists of one or more rows  each consisting of one or more table cells that can contain a wide range of material (
text  images  other tables)
Before HTML
authors needing a table had to resort to ad hoc methods  such as including an image showing the table
more new features were added
These included accessibility features for handicapped users  object embedding (a generalization of the <img> tag so other objects can also be embedded in pages)  support for scripting languages (to allow dynamic content)  and more
THE WORLD WIDE WEB Item HTML
Hyperlinks x x x x x Images x x x x x Lists x x x x x Active maps & images x x x x Forms x x x x Equations x x x Toolbars x x x Tables x x x Accessibility features x x Object embedding x x Style sheets x x Scripting x x Video and audio x Inline vector graphics x XML representation x Background threads x Browser storage x Drawing canvas x Figure  -
Some differences between HTML versions
includes many features to handle the rich media that are now routinely used on the Web
Video and audio can be included in pages and played by the browser without requiring the user to install plug-ins
Drawings can be built up in the browser as vector graphics  rather than using bitmap image formats (like JPEG and GIF) There is also more support for running scripts in browsers  such as background threads of computation and access to storage
All of these features help to support Web pages that are more like traditional applications with a user interface than documents
This is the direction the Web is heading
Input and Forms There is one important capability that we have not discussed yet: input
was basically one-way
Users could fetch pages from information providers  but it was difficult to send information back the other way
It quickly became apparent that there was a need for two-way traffic to allow orders for products to be placed via Web pages  registration cards to be filled out online  search terms to be entered  and much  much more
THE APPLICATION LAYER
Sending input from the user to the server (via the browser) requires two kinds of support
First  it requires that HTTP be able to carry data in that direction
We describe how this is done in a later tion; it uses the POST method
The ond requirement is to be able to present user interface elements that gather and package up the input
Forms were included with this functionality in HTML   Forms contain boxes or buttons that allow users to fill in information or make choices and then send the information back to the pageâs owner
Forms are written just like other parts of HTML  as seen in the example of Fig
Note that forms are still static content
They exhibit the same behavior regardless of who is using them
Dynamic content  which we will cover later  provides more sophisticated ways to gather input by sending a program whose behavior may depend on the browser environment
Like all forms  this one is enclosed between the <form> and </form> tags
The attributes of this tag tell what to do with the data that are input  in this case using the POST method to send the data to the specified URL
Text not enclosed in a tag is just displayed
All the usual tags (
<b>) are allowed in a form to let the author of the page control the look of the form on the screen
Three kinds of input boxes are used in this form  each of which uses the <input> tag
It has a variety of parameters for determining the size  nature  and usage of the box displayed
The most common forms are blank fields for accepting user text  boxes that can be checked  and submit buttons that cause the data to be returned to the server
The first kind of input box is a text box that follows the text ââNameââ
The box is   characters wide and expects the user to type in a string  which is then stored in the variable customer
The next line of the form asks for the userâs street address characters wide
Then comes a line asking for the city  state  and country
Since no <p> tags are used between these fields  the browser displays them all on one line (instead of as separate paragraphs) if they will fit
As far as the browser is concerned  the one paragraph contains just six items: three strings alternating with three boxes
The next line asks for the credit card number and expiration date
Transmitting credit card numbers over the Internet should only be done when adequate urity measures have been taken
We will discuss some of these in   Following the expiration date  we encounter a new feature: radio buttons
These are used when a choice must be made among two or more alternatives
The intellectual model here is a car radio with half a dozen buttons for choosing stations
Clicking on one button turns off all the other ones in the same group
The visual presentation is up to the browser
Widget size also uses two radio buttons
The two groups are distinguished by their name parameter  not by static scoping using something like <radiobutton>
</radiobutton>
The value parameters are used to indicate which radio button was pushed
For example  depending on which credit card options the user has chosen  the variable cc will be set to either the string ââmastercardââ or the string ââvisacardââ
"THE WORLD WIDE WEB <html> <head> <title> AWI CUSTOMER ORDERING FORM </title> </head> <body> <h > Widget Order Form </h > <form ACTION=""http:// /cgi-bin/ "" method=POST> <p> Name <input name=""customer"" size=  > </p> <p> Street address <input name=""address"" size=  > </p> <p> City <input name=""city"" size=  > State <input name=""state"" size = > Country <input name=""country"" size=  > </p> <p> Credit card # <input name=""cardno"" size=  > Expires <input name=""expires"" size= > M/C <input name=""cc"" type=radio value=""mastercard""> VISA <input name=""cc"" type=radio value=""visacard""> </p> <p> Widget size Big <input name=""product"" type=radio value=""expensive""> Little <input name=""product"" type=radio value=""cheap""> Ship by express courier <input name=""express"" type=checkbox> </p> <p><input type=submit value=""Submit order""> </p> Thank you for ordering an AWI widget  the best widget money can buy! </form> </body> </html> (a) Widget Order Form Name Street address City Credit card # Widget size Big Thank you for ordering an AWI widget  the best widget money can buy! Little Ship by express courier Expires M/C Visa State Country Submit order (b) Figure  -"
(a) The HTML for an order form
(b) The formatted page
After the two sets of radio buttons  we come to the shipping option  represented by a box of type checkbox
It can be either on or off
Unlike radio buttons  where exactly one out of the set must be chosen  each box of type checkbox can be on or off  independently of all the others
THE APPLICATION LAYER
Finally  we come to the submit button
The value string is the label on the button and is displayed
When the user clicks the submit button  the browser packages the collected information into a single long line and sends it back to the server to the URL provided as part of the <form> tag
A simple encoding is used
The & is used to separate fields and + is used to represent space
For our example form  the line might look like the contents of Fig
customer=John+Doe&address=  +Main+St
&city=White+Plains& state=NY&country=USA&cardno=  &expires= /  &cc=mastercard& product=cheap&express=on Figure  -
A possible response from the browser to the server with information filled in by the user
The string is sent back to the server as one line
(It is broken into three lines here because the page is not wide enough
) It is up to the server to make sense of this string  most likely by passing the information to a program that will process it
We will discuss how this can be done in the next tion
There are also other types of input that are not shown in this simple example
Two other types are password and textarea
A password box is the same as a text box (the default type that need not be named)  except that the characters are not displayed as they are typed
A textarea box is also the same as a text box  except that it can contain multiple lines
For long lists from which a choice must be made  the <select> and </select> tags are provided to bracket a list of alternatives
This list is often rendered as a drop-down menu
The semantics are those of radio buttons unless the multiple parameter is given  in which case the semantics are those of checkboxes
Finally  there are ways to indicate default or initial values that the user can change
For example  if a text box is given a value field  the contents are displayed in the form for the user to edit or erase
CSSâCascading Style Sheets The original goal of HTML was to specify the structure of the document  not its appearance
For example  <h > Deborahâs Photos </h > instructs the browser to emphasize the heading  but does not say anything about the typeface  point size  or color
That is left up to the browser  which knows the properties of the display (
how many pixels it has)
"However  many Web page designers wanted absolute control over how their pages appeared  so new tags were added to HTML to control appearance  such as <font face=""helvetica"" size=""  "" color=""red""> Deborahâs Photos </font>   THE WORLD WIDE WEB Also  ways were added to control positioning on the screen accurately"
The trouble with this approach is that it is tedious and produces bloated HTML that is not portable
Although a page may render perfectly in the browser it is developed on  it may be a complete mess in another browser or another release of the same browser or at a different screen resolution
A better alternative is the use of style sheets
Style sheets in text editors allow authors to associate text with a logical style instead of a physical style  for example  ââinitial paragraphââ instead of ââitalic text
ââ The appearance of each style is defined separately
In this way  if the author decides to change the initial paragraphs from  -point italics in blue to  -point boldface in shocking pink  all it requires is changing one definition to convert the entire document
CSS (Cascading Style Sheets) introduced style sheets to the Web with HTML
though widespread use and browser support did not take off until
CSS defines a simple language for describing rules that control the appearance of tagged content
Let us look at an example
Suppose that AWI wants snazzy Web pages with navy text in the Arial font on an off-white background  and level headings that are an extra   % and  % larger than the text for each level  respectively
The CSS definition in Fig
body {background-color:linen; color:navy; font-family:Arial;} h  {font-size:  %;} h  {font-size:  %;} Figure  -
CSS example
As can be seen  the style definitions can be compact
Each line selects an element to which it applies and gives the values of properties
The properties of an element apply as defaults to all other HTML elements that it contains
Thus  the style for body sets the style for paragraphs of text in the body
There are also convenient shorthands for color names (
Any style parameters that are not defined are filled with defaults by the browser
This behavior makes style sheet definitions optional; some reasonable presentation will occur without them
Style sheets can be placed in an HTML file (
using the <style> tag)  but it is more common to place them in a separate file and reference them
For example  the <head> tag of the AWI page can be modified to refer to a style sheet in the file   as shown in Fig
The example also shows the MIME type of CSS files to be text/css
<head> <title> AMALGAMATED WIDGET  INC
"</title> <link rel=""stylesheet"" type=""text/css"" href="" "" /> </head> Figure  -"
Including a CSS style sheet
THE APPLICATION LAYER
This strategy has two advantages
First  it lets one set of styles be applied to many pages on a Web site
This organization lends a consistent appearance to pages even if they were developed by different authors at different times  and allows the look of the entire site to be changed by editing one CSS file and not the HTML
This method can be compared to an #include file in a C program: changing one macro definition there changes it in all the program files that include the header
The ond advantage is that the HTML files that are downloaded are kept small
This is because the browser can download one copy of the CSS file for all pages that reference it
It does not need to download a new copy of the definitions along with each Web page
Dynamic Web Pages and Web Applications The static page model we have used so far treats pages as multimedia documents that are conveniently linked together
It was a fitting model in the early days of the Web  as vast amounts of information were put online
Nowadays  much of the excitement around the Web is using it for applications and services
Examples include buying products on e-commerce sites  searching library catalogs  exploring maps  reading and sending email  and collaborating on documents
These new uses are like traditional application software (
mail readers and word processors)
The twist is that these applications run inside the browser  with user data stored on servers in Internet data centers
They use Web protocols to access information via the Internet  and the browser to display a user interface
The advantage of this approach is that users do not need to install separate application programs  and user data can be accessed from different computers and backed up by the service operator
It is proving so successful that it is rivaling traditional application software
Of course  the fact that these applications are offered for free by large providers helps
This model is the prevalent form of cloud computing  in which computing moves off individual desktop computers and into shared clusters of servers in the Internet
To act as applications  Web pages can no longer be static
Dynamic content is needed
For example  a page of the library catalog should reflect which books are currently available and which books are checked out and are thus not available
Similarly  a useful stock market page would allow the user to interact with the page to see stock prices over different periods of time and compute profits and losses
As these examples suggest  dynamic content can be generated by programs running on the server or in the browser (or in both places)
In this tion  we will examine each of these two cases in turn
The general situation is as shown in Fig
For example  consider a map service that lets the user enter a street address and presents a corresponding map of the location
Given a request for a location  the Web server must use a program to create a page that shows the map for the location from a database of streets and other geographic information
This action is shown as steps  through
The request (step   THE WORLD WIDE WEB  ) causes a program to run on the server
The program consults a database to generate the appropriate page (step  ) and returns it to the browser (step  )
Program Program Web browser Web server  Web page  Program DB Figure  -
Dynamic pages
There is more to dynamic content  however
The page that is returned may itself contain programs that run in the browser
In our map example  the program would let the user find routes and explore nearby areas at different levels of detail
It would update the page  zooming in or out as directed by the user (step  )
To handle some interactions  the program may need more data from the server
In this case  the program will send a request to the server (step  ) that will retrieve more information from the database (step  ) and return a response (step  )
The program will then continue updating the page (step  )
The requests and responses happen in the background; the user may not even be aware of them because the page URL and title typically do not change
By including client-side programs  the page can present a more responsive interface than with server-side programs alone
Server-Side Dynamic Web Page Generation Let us look at the case of server-side content generation in more detail
A simple situation in which server-side processing is necessary is the use of forms
Consider the user filling out the AWI order form of Fig
When the user clicks  a request is sent to the server at the URL specified with the form (a POST to http:// /cgi-bin/  in this case) along with the contents of the form as filled in by the user
These data must be given to a program or script to process
Thus  the URL identifies the program to run; the data are provided to the program as input
In this case  processing would involve entering the order in AWIâs internal system  updating customer records  and charging the credit card
The page returned by this request will depend on what happens during the processing
It is not fixed like a static page
If the order succeeds  the page returned might give the expected shipping date
If it is unsuccessful  the returned page might say that widgets requested are out of stock or the credit card was not valid for some reason
THE APPLICATION LAYER
Exactly how the server runs a program instead of retrieving a file depends on the design of the Web server
It is not specified by the Web protocols themselves
This is because the interface can be proprietary and the browser does not need to know the details
As far as the browser is concerned  it is simply making a request and fetching a page
Nonetheless  standard APIs have been developed for Web servers to invoke programs
The existence of these interfaces makes it easier for developers to extend different servers with Web applications
We will briefly look at two APIs to give you a sense of what they entail
The first API is a method for handling dynamic page requests that has been available since the beginning of the Web
It is called the CGI (Common Gateway Interface) and is defined in RFC
CGI provides an interface to allow Web servers to talk to back-end programs and scripts that can accept input (
from forms) and generate HTML pages in response
These programs may be written in whatever language is convenient for the developer  usually a scripting language for ease of development
Pick Python  Ruby  Perl or your favorite language
By convention  programs invoked via CGI live in a directory called cgi-bin  which is visible in the URL
The server maps a request to this directory to a program name and executes that program as a separate process
It provides any data sent with the request as input to the program
The output of the program gives a Web page that is returned to the browser
In our example  the program   is invoked with input from the form encoded as shown in Fig
It will parse the parameters and process the order
A useful convention is that the program will return the HTML for the order form if no form input is provided
In this way  the program will be sure to know the representation of the form
The ond API we will look at is quite different
The approach here is to embed little scripts inside HTML pages and have them be executed by the server itself to generate the page
A popular language for writing these scripts is PHP (PHP: Hypertext Preprocessor)
To use it  the server has to understand PHP  just as a browser has to understand CSS to interpret Web pages with style sheets
Usually  servers identify Web pages containing PHP from the file extension php rather than html or htm
PHP is simpler to use than CGI
As an example of how it works with forms  see the example in Fig
The top part of this figure contains a normal HTML page with a simple form in it
This time  the <form> tag specifies that action
php is to be invoked to handle the parameters when the user submits the form
The page displays two text boxes  one with a request for a name and one with a request for an age
After the two boxes have been filled in and the form submitted  the server parses the Fig
It then starts to process the action
php file  shown in Fig
During the processing of this file    THE WORLD WIDE WEB the PHP commands are executed
If the user filled in ââBarbaraââ and ââ  ââ in the boxes  the HTML file sent back will be the one given in Fig
Thus  handling forms becomes extremely simple using PHP
"<html> <body> <form action="" "" method=""post""> <p> Please enter your name: <input type=""text"" name=""name""> </p> <p> Please enter your age: <input type=""text"" name=""age""> </p> <input type=""submit""> </form> </body> </html> (a) <html> <body> <h > Reply: </h > Hello <?php echo $name; ?>"
Prediction: next year you will be <?php echo $age +  ; ?> </body> </html> (b) <html> <body> <h > Reply: </h > Hello Barbara
Prediction: next year you will be   </body> </html> (c) Figure  -
(a) A Web page containing a form
(b) A PHP script for handling the output of the form
(c) Output from the PHP script when the inputs are ââBarbaraââ and ââ  ââ  respectively
Although PHP is easy to use  it is actually a powerful programming language for interfacing the Web and a server database
It has variables  strings  arrays  and most of the control structures found in C  but much more powerful I/O than just printf
PHP is open source code  freely available  and widely used
It was designed specifically to work well with Apache  which is also open source and is the worldâs most widely used Web server
For more information about PHP  see Valade (   )
We have now seen two different ways to generate dynamic HTML pages: CGI scripts and embedded PHP
There are several others to choose from
JSP (JavaServer Pages) is similar to PHP  except that the dynamic part is written in THE APPLICATION LAYER
the Java programming language instead of in PHP
Pages using this technique have the file extension
(Active Server Pages ) is Microsoftâs version of PHP and JavaServer Pages
It uses programs written in Microsoftâs proprietary  networked application framework for generating the dynamic content
Pages using this technique have the extension
The choice among these three techniques usually has more to do with politics (open source vs
Microsoft) than with technology  since the three languages are roughly comparable
Client-Side Dynamic Web Page Generation PHP and CGI scripts solve the problem of handling input and interactions with databases on the server
They can all accept incoming information from forms  look up information in one or more databases  and generate HTML pages with the results
What none of them can do is respond to mouse movements or interact with users directly
For this purpose  it is necessary to have scripts embedded in HTML pages that are executed on the client machine rather than the server machine
Starting with HTML
such scripts are permitted using the tag <script>
The technologies used to produce these interactive Web pages are broadly referred to as dynamic HTML The most popular scripting language for the client side is JavaScript  so we will now take a quick look at it
Despite the similarity in names  JavaScript has almost nothing to do with the Java programming language
Like other scripting languages  it is a very high-level language
For example  in a single line of JavaScript it is possible to pop up a dialog box  wait for text input  and store the resulting string in a variable
High-level features like this make JavaScript ideal for designing interactive Web pages
On the other hand  the fact that it is mutating faster than a fruit fly trapped in an X-ray machine makes it extremely difficult to write JavaScript programs that work on all platforms  but maybe some day it will stabilize
As an example of a program in JavaScript  consider that of Fig
Like that of Fig
The body is almost the same as the PHP example  the main difference being the declaration of the Submit button and the assignment statement in it
This assignment statement tells the browser to invoke the response script on a button click and pass it the form as a parameter
What is completely new here is the declaration of the JavaScript function response in the head of the HTML file  an area normally reserved for titles  background colors  and so on
This function extracts the value of the name field from the form and stores it in the variable person as a string
It also extracts the value of the age field  converts it to an integer by using the eval function  adds  to it  and stores the result in years
"Then it opens a document for output  does four   THE WORLD WIDE WEB <html> <head> <script language=""javascript"" type=""text/javascript""> function response(test form) { var person = test  ; var years = eval(test  ) +  ;  ();  (""<html> <body>"");  (""Hello "" + person + """
"<br>"");  (""Prediction: next year you will be "" + years + """
""");  (""</body> </html>"");  (); } </script> </head> <body> <form> Please enter your name: <input type=""text"" name=""name""> <p> Please enter your age: <input type=""text"" name=""age""> <p> <input type=""button"" value=""submit"" onclick=""response( )""> </form> </body> </html> Figure  -"
Use of JavaScript for processing a form
writes to it using the writeln method  and closes the document
The document is an HTML file  as can be seen from the various HTML tags in it
The browser then displays the document on the screen
It is very important to understand that while PHP and JavaScript look similar in that they both embed code in HTML files  they are processed totally differently
In the PHP example of Fig
The server loads the PHP file and executes the PHP script that is embedded in to produce a new HTML page
That page is sent back to the browser for display
The browser cannot even be sure that it was produced by a program
This processing is shown as steps  to  in Fig
In the JavaScript example of Fig
All the work is done locally  inside the browser
There is no contact with the server
This processing is shown as steps  and  in Fig
As a consequence  the result is displayed virtually instantaneously  whereas with PHP there can be a delay of several onds before the resulting HTML arrives at the client
THE APPLICATION LAYER
Server (a) PHP module Browser User    Server (b) Browser User JavaScript Figure  -
(a) Server-side scripting with PHP
(b) Client-side scripting with JavaScript
This difference does not mean that JavaScript is better than PHP
Their uses are completely different
PHP (and  by implication  JSP and ASP) is used when interaction with a database on the server is needed
JavaScript (and other clientside languages we will mention  such as VBScript) is used when the interaction is with the user at the client computer
It is certainly possible to combine them  as we will see shortly
JavaScript is not the only way to make Web pages highly interactive
An alternative on Windows platforms is VBScript  which is based on Visual Basic
Another popular method across platforms is the use of applets
These are small Java programs that have been compiled into machine instructions for a virtual computer called the JVM (Java Virtual Machine)
Applets can be embedded in HTML pages (between <applet> and </applet>) and interpreted by JVM-capable browsers
Because Java applets are interpreted rather than directly executed  the Java interpreter can prevent them from doing Bad Things
At least in theory
In practice  applet writers have found a nearly endless stream of bugs in the Java I/O libraries to exploit
Microsoftâs answer to Sunâs Java applets was allowing Web pages to hold ActiveX controls  which are programs compiled to x  machine language and executed on the bare hardware
This feature makes them vastly faster and more flexible than interpreted Java applets because they can do anything a program can do
When Internet Explorer sees an ActiveX control in a Web page  it downloads it  verifies its identity  and executes it
However  downloading and running foreign programs raises enormous urity issues  which we will discuss in   Since nearly all browsers can interpret both Java programs and JavaScript  a designer who wants to make a highly interactive Web page has a choice of at least two techniques  and if portability to multiple platforms is not an issue  ActiveX in addition
As a general rule  JavaScript programs are easier to write  Java applets execute faster  and ActiveX controls run fastest of all
Also  since all browsers implement exactly the same JVM but no two browsers implement the same version of JavaScript  Java applets are more portable than JavaScript programs
For more information about JavaScript  there are many books  each with many (often with more than ) pages
See  for example  Flanagan (   )
THE WORLD WIDE WEB AJAXâAsynchronous JavaScript and XML Compelling Web applications need responsive user interfaces and seamless access to data stored on remote Web servers
Scripting on the client (
with JavaScript) and the server (
with PHP) are basic technologies that provide pieces of the solution
These technologies are commonly used with several other key technologies in a combination called AJAX (Asynchronous JAvascript and Xml)
Many full-featured Web applications  such as Googleâs Gmail  Maps  and Docs  are written with AJAX
AJAX is somewhat confusing because it is not a language
It is a set of technologies that work together to enable Web applications that are every bit as responsive and powerful as traditional desktop applications
The technologies are:
HTML and CSS to present information as pages DOM (Document Object Model) to change parts of pages while they are viewed XML (eXtensible Markup Language) to let programs exchange application data with the server An asynchronous way for programs to send and retrieve XML data JavaScript as a language to bind all this functionality together
As this is quite a collection  we will go through each piece to see what it contributes
We have already seen HTML and CSS
They are standards for describing content and how it should be displayed
Any program that can produce HTML and CSS can use a Web browser as a display engine
DOM (Document Object Model) is a representation of an HTML page that is accessible to programs
This representation is structured as a tree that reflects the structure of the HTML elements
For instance  the DOM tree of the HTML in Fig
At the root is an html element that represents the entire HTML block
This element is the parent of the body element  which is in turn parent to a form element
The form has two attributes that are drawn to the right-hand side  one for the form method (a POST ) and one for the form action (the URL to request)
This element has three children  reflecting the two paragraph tags and one input tag that are contained within the form
At the bottom of the tree are leaves that contain either elements or literals  such as text strings
The significance of the DOM model is that it provides programs with a straightforward way to change parts of the page
There is no need to rewrite the entire page
Only the node that contains the change needs to be replaced
When this change is made  the browser will correspondingly update the display
For example  if an image on part of the page is changed in DOM  the browser will update that image without changing the other parts of the page
We have already seen DOM in action when the JavaScript example of Fig
html body Attributes to the right form action = â â method = âpostâ p p âPlease enter your name:â type = âsubmitâ input input type = âtxtâ name = âageâ Child elements below Elements âPlease enter your age:â input type = âtxtâ name = âageâ Figure  -
The DOM tree for the HTML in Fig
document element to cause new lines of text to appear at the bottom of the browser window
The DOM is a powerful method for producing pages that can evolve
The third technology  XML (eXtensible Markup Language)  is a language for specifying structured content
HTML mixes content with formatting because it is concerned with the presentation of information
However  as Web applications become more common  there is an increasing need to separate structured content from its presentation
For example  consider a program that searches the Web for the best price for some book
It needs to analyze many Web pages looking for the itemâs title and price
With Web pages in HTML  it is very difficult for a program to figure out where the title is and where the price is
For this reason  the W C developed XML (Bray et al
) to allow Web content to be structured for automated processing
Unlike HTML  there are no defined tags for XML
Each user can define her own tags
A simple example of an XML document is given in Fig
It defines a structure called book list  which is a list of books
Each book has three fields  the title  author  and year of publication
These structures are extremely simple
It is permitted to have structures with repeated fields (
multiple authors)  optional fields (
URL of the audio book)  and alternative fields (
URL of a bookstore if it is in print or URL of an auction site if it is out of print)
In this example  each of the three fields is an indivisible entity  but it is also permitted to further subdivide the fields
For example  the author field could have been done as follows to give finer-grained control over searching and formatting: <author> <first name> George </first name> <last name> Zipf </last name> </author> Each field can be subdivided into subfields and subsubfields  arbitrarily deeply
"THE WORLD WIDE WEB <?xml version="""
""" ?> <book list> <book> <title> Human Behavior and the Principle of Least Effort </title> <author> George Zipf </author> <year>  </year> </book> <book> <title> The Mathematical Theory of Communication </title> <author> Claude E"
Shannon </author> <author> Warren Weaver </author> <year>  </year> </book> <book> <title> Nineteen Eighty-Four </title> <author> George Orwell </author> <year>  </year> </book> </book list> Figure  -
A simple XML document
All the file of Fig
It is well suited for transporting information between programs running in browsers and servers  but it says nothing about how to display the document as a Web page
To do that  a program that consumes the information and judges  to be a fine year for books might output HTML in which the titles are marked up as italic text
Alternatively  a language called XSLT (eXtensible Stylesheet Language Transformations)  can be used to define how XML should be transformed into HTML
XSLT is like CSS  but much more powerful
We will spare you the details
The other advantage of expressing data in XML  instead of HTML  is that it is easier for programs to analyze
HTML was originally written manually (and often is still) so a lot of it is a bit sloppy
Sometimes the closing tags  like </p>  are left out
Other tags do not have a matching closing tag  like <br>
Still other tags may be nested improperly  and the case of tag and attribute names can vary
Most browsers do their best to work out what was probably intended
XML is stricter and cleaner in its definition
Tag names and attributes are always lowercase  tags must always be closed in the reverse of the order that they were opened (or indicate clearly if they are an empty tag with no corresponding close)  and attribute values must be enclosed in quotation marks
This precision makes parsing easier and unambiguous
HTML is even being defined in terms of XML
This approach is called XHTML (eXtended HyperText Markup Language)
Basically  it is a Very THE APPLICATION LAYER
Picky version of HTML
XHTML pages must strictly conform to the XML rules  otherwise they are not accepted by the browser
No more shoddy Web pages and inconsistencies across browsers
As with XML  the intent is to produce pages that are better for programs (in this case Web applications) to process
While XHTML has been around since  it has been slow to catch on
People who produce HTML do not see why they need XHTML  and browser support has lagged
is being defined so that a page can be represented as either HTML or XHTML to aid the transition
Eventually  XHTML should replace HTML  but it will be a long time before this transition is complete
XML has also proved popular as a language for communication between programs
When this communication is carried by the HTTP protocol (described in the next tion) it is called a Web service
In particular  SOAP (Simple Object Access Protocol) is a way of implementing Web services that performs RPC between programs in a language- and system-independent way
The client just constructs the request as an XML message and sends it to the server  using the HTTP protocol
The server sends back a reply as an XML-formatted message
In this way  applications on heterogeneous platforms can communicate
Getting back to AJAX  our point is simply that XML is a useful format to exchange data between programs running in the browser and the server
However  to provide a responsive interface in the browser while sending or receiving data  it must be possible for scripts to perform asynchronous I/O that does not block the display while awaiting the response to a request
For example  consider a map that can be scrolled in the browser
When it is notified of the scroll action  the script on the map page may request more map data from the server if the view of the map is near the edge of the data
The interface should not freeze while those data are fetched
Such an interface would win no user awards
Instead  the scrolling should continue smoothly
When the data arrive  the script is notified so that it can use the data
If all goes well  new map data will be fetched before it is needed
Modern browsers have support for this model of communication
The final piece of the puzzle is a scripting language that holds AJAX together by providing access to the above list of technologies
In most cases  this language is JavaScript  but there are alternatives such as VBScript
We presented a simple example of JavaScript earlier
Do not be fooled by this simplicity
JavaScript has many quirks  but it is a full-blown programming language  with all the power of C or Java
It has variables  strings  arrays  objects  functions  and all the usual control structures
It also has interfaces specific to the browser and Web pages
JavaScript can track mouse motion over objects on the screen  which makes it easy to make a menu suddenly appear and leads to lively Web pages
It can use DOM to access pages  manipulate HTML and XML  and perform asynchronous HTTP communication
Before leaving the subject of dynamic pages  let us briefly summarize the technologies we have covered so far by relating them on a single figure
Complete Web pages can be generated on the fly by various scripts on the server   THE WORLD WIDE WEB machine
The scripts can be written in server extension languages like PHP  JSP  or    or run as separate CGI processes and thus be written in any language
These options are shown in Fig
Server machine CGI script Helper application Client machine Web browser process PHP ASP JSP Java Script Plug-ins interpreter HTML / CSS / XML interpreter Java virtual machine VB Script interpreter XML HTML/CSS etc
Web browser process Figure  -
Various technologies used to generate dynamic pages
Once these Web pages are received by the browser  they are treated as normal pages in HTML  CSS and other MIME types and just displayed
Plug-ins that run in the browser and helper applications that run outside of the browser can be installed to extend the MIME types that are supported by the browser
Dynamic content generation is also possible on the client side
The programs that are embedded in Web pages can be written in JavaScript  VBScript  Java  and other languages
These programs can perform arbitrary computations and update the display
With AJAX  programs in Web pages can asynchronously exchange XML and other kinds of data with the server
This model supports rich Web applications that look just like traditional applications  except that they run inside the browser and access information that is stored at servers on the Internet
HTTPâThe HyperText Transfer Protocol Now that we have an understanding of Web content and applications  it is time to look at the protocol that is used to transport all this information between Web servers and clients
It is HTTP (HyperText Transfer Protocol)  as specified in RFC
HTTP is a simple request-response protocol that normally runs over TCP
It specifies what messages clients may send to servers and what responses they get back in return
The request and response headers are given in ASCII  just like in SMTP
The contents are given in a MIME-like format  also like in SMTP
This simple model was partly responsible for the early success of the Web because it made development and deployment straightforward
In this tion  we will look at the more important properties of HTTP as it is used nowadays
However  before getting into the details we will note that the way THE APPLICATION LAYER
it is used in the Internet is evolving
HTTP is an application layer protocol because it runs on top of TCP and is closely associated with the Web
That is why we are covering it in this  ter
However  in another sense HTTP is becoming more like a transport protocol that provides a way for processes to communicate content across the boundaries of different networks
These processes do not have to be a Web browser and Web server
A media player could use HTTP to talk to a server and request album information
Antivirus software could use HTTP to download the latest updates
Developers could use HTTP to fetch project files
Consumer electronics products like digital photo frames often use an embedded HTTP server as an interface to the outside world
Machine-to-machine communication increasingly runs over HTTP
For example  an airline server might use SOAP (an XML RPC over HTTP) to contact a car rental server and make a car reservation  all as part of a vacation package
These trends are likely to continue  along with the expanding use of HTTP
Connections The usual way for a browser to contact a server is to establish a TCP connection to port   on the serverâs machine  although this procedure is not formally required
The value of using TCP is that neither browsers nor servers have to worry about how to handle long messages  reliability  or congestion control
All of these matters are handled by the TCP implementation
Early in the Web  with HTTP
after the connection was established a single request was sent over and a single response was sent back
Then the TCP connection was released
In a world in which the typical Web page consisted entirely of HTML text  this method was adequate
Quickly  the average Web page grew to contain large numbers of embedded links for content such as icons and other eye candy
Establishing a separate TCP connection to transport each single icon became a very expensive way to operate
This observation led to HTTP
which supports persistent connections
With them  it is possible to establish a TCP connection  send a request and get a response  and then send additional requests and get additional responses
This strategy is also called connection reuse
By amortizing the TCP setup  startup  and release costs over multiple requests  the relative overhead due to TCP is reduced per request
It is also possible to pipeline requests  that is  send request  before the response to request  has arrived
The performance difference between these three cases is shown in Fig
Part (a) shows three requests  one after the other and each in a separate connection
Let us suppose that this represents a Web page with two embedded images on the same server
The URLs of the images are determined as the main page is fetched  so they are fetched after the main page
Nowadays  a typical page has around   other objects that must be fetched to present it  but that would make our figure far too big so we will use only two embedded objects
THE WORLD WIDE WEB (a) (b) (c) Pipelined requests Connection setup HTTP Response HTTP Request Connection setup Connection setup Time Connection setup Connection setup Figure  -
HTTP with (a) multiple connections and sequential requests
(b) A persistent connection and sequential requests
(c) A persistent connection and pipelined requests
That is  the TCP connection is opened at the beginning  then the same three requests are sent  one after the other as before  and only then is the connection closed
Observe that the fetch completes more quickly
There are two reasons for the speedup
First  time is not wasted setting up additional connections
Each TCP connection requires at least one round-trip time to establish
ond  the transfer of the same images proceeds more quickly
Why is this? It is because of TCP congestion control
At the start of a connection  TCP uses the slow-start procedure to increase the throughput until it learns the behavior of the network path
The consequence of this warmup period is that multiple short TCP connections take disproportionately longer to transfer information than one longer TCP connection
Finally  in Fig
-  (c)  there is one persistent connection and the requests are pipelined
Specifically  the ond and third requests are sent in rapid succession as soon as enough of the main page has been retrieved to identify that the images must be fetched
The responses for these requests follow eventually
This method cuts down the time that the server is idle  so it further improves performance
Persistent connections do not come for free  however
A new issue that they raise is when to close the connection
A connection to a server should stay open while the page loads
What then? There is a good chance that the user will click on a link that requests another page from the server
If the connection remains open  the next request can be sent immediately
However  there is no guarantee that the client will make another request of the server any time soon
In practice  THE APPLICATION LAYER
clients and servers usually keep persistent connections open until they have been idle for a short time (
onds) or they have a large number of open connections and need to close some
The observant reader may have noticed that there is one combination that we have left out so far
It is also possible to send one request per TCP connection  but run multiple TCP connections in parallel
This parallel connection method was widely used by browsers before persistent connections
It has the same disadvantage as sequential connectionsâextra overheadâbut much better performance
This is because setting up and ramping up the connections in parallel hides some of the latency
In our example  connections for both of the embedded images could be set up at the same time
However  running many TCP connections to the same server is discouraged
The reason is that TCP performs congestion control for each connection independently
As a consequence  the connections compete against each other  causing added packet loss  and in aggregate are more aggressive users of the network than an individual connection
Persistent connections are superior and used in preference to parallel connections because they avoid overhead and do not suffer from congestion problems
Methods Although HTTP was designed for use in the Web  it was intentionally made more general than necessary with an eye to future object-oriented uses
For this reason  operations  called methods  other than just requesting a Web page are supported
This generality is what permitted SOAP to come into existence
Each request consists of one or more lines of ASCII text  with the first word on the first line being the name of the method requested
The built-in methods are listed in Fig
The names are case sensitive  so GET is allowed but not get
Method Description GET Read a Web page HEAD Read a Web pageâs header POST Append to a Web page PUT Store a Web page DELETE Remove the Web page TRACE Echo the incoming request CONNECT Connect through a proxy OPTIONS Query options for a page Figure  -
The built-in HTTP request methods
The GET method requests the server to send the page
(When we say ââpageââ we mean ââobjectââ in the most general case  but thinking of a page as the contents   THE WORLD WIDE WEB of a file is sufficient to understand the concepts
) The page is suitably encoded in MIME
The vast majority of requests to Web servers are GETs
The usual form of GET is GET filename HTTP/
where filename names the page to be fetched and
is the protocol version
The HEAD method just asks for the message header  without the actual page
This method can be used to collect information for indexing purposes  or just to test a URL for validity
The POST method is used when forms are submitted
Both it and GET are also used for SOAP Web services
Like GET  it bears a URL  but instead of simply retrieving a page it uploads data to the server (
the contents of the form or RPC parameters)
The server then does something with the data that depends on the URL  conceptually appending the data to the object
The effect might be to purchase an item  for example  or to call a procedure
Finally  the method returns a page indicating the result
The remaining methods are not used much for browsing the Web
The PUT method is the reverse of GET: instead of reading the page  it writes the page
This method makes it possible to build a collection of Web pages on a remote server
The body of the request contains the page
It may be encoded using MIME  in which case the lines following the PUT might include authentication headers  to prove that the caller indeed has permission to perform the requested operation
DELETE does what you might expect: it removes the page  or at least it indicates that the Web server has agreed to remove the page
As with PUT  authentication and permission play a major role here
The TRACE method is for debugging
It instructs the server to send back the request
This method is useful when requests are not being processed correctly and the client wants to know what request the server actually got
The CONNECT method lets a user make a connection to a Web server through an intermediate device  such as a Web cache
The OPTIONS method provides a way for the client to query the server for a page and obtain the methods and headers that can be used with that page
Every request gets a response consisting of a status line  and possibly additional information (
all or part of a Web page)
The status line contains a three-digit status code telling whether the request was satisfied and  if not  why not
The first digit is used to divide the responses into five major groups  as shown in Fig
The  xx codes are rarely used in practice
The  xx codes mean that the request was handled successfully and the content (if any) is being returned
The  xx codes tell the client to look elsewhere  either using a different URL or in its own cache (discussed later)
The  xx codes mean the request failed due to a client error such an invalid request or a nonexistent page
Finally  the  xx errors mean the server itself has an internal problem  either due to an error in its code or to a temporary overload
THE APPLICATION LAYER
Code Meaning Examples  xx Information = server agrees to handle clientâs request  xx Success = request succeeded; = no content present  xx Redirection = page moved; = cached page still valid  xx Client error = forbidden page; = page not found  xx Server error = internal server error; = try again later Figure  -
The status code response groups
Message Headers The request line (
the line with the GET method) may be followed by additional lines with more information
They are called request headers
This information can be compared to the parameters of a procedure call
Responses may also have response headers
Some headers can be used in either direction
A selection of the more important ones is given in Fig
This list is not short  so as you might imagine there is often a variety of headers on each request and response
The User-Agent header allows the client to inform the server about its browser implementation (
and Chrome/
This information is useful to let servers tailor their responses to the browser  since different browsers can have widely varying capabilities and behaviors
The four Accept headers tell the server what the client is willing to accept in the event that it has a limited repertoire of what is acceptable
The first header specifies the MIME types that are welcome (
text/html)
The ond gives the character set (
ISO-   -  or Unicode- - )
The third deals with compression methods (
The fourth indicates a natural language (
If the server has a choice of pages  it can use this information to supply the one the client is looking for
If it is unable to satisfy the request  an error code is returned and the request fails
The If-Modified-Since and If-None-Match headers are used with caching
They let the client ask for a page to be sent only if the cached copy is no longer valid
We will describe caching shortly
The Host header names the server
It is taken from the URL
This header is mandatory
It is used because some IP addresses may serve multiple DNS names and the server needs some way to tell which host to hand the request to
The Authorization header is needed for pages that are protected
In this case  the client may have to prove it has a right to see the page requested
This header is used for that case
The client uses the misspelled Referer header to give the URL that referred to the URL that is now requested
Most often this is the URL of the previous page
THE WORLD WIDE WEB Header Type Contents User-Agent Request Information about the browser and its platform Accept Request The type of pages the client can handle Accept-Charset Request The character sets that are acceptable to the client Accept-Encoding Request The page encodings the client can handle Accept-Language Request The natural languages the client can handle If-Modified-Since Request Time and date to check freshness If-None-Match Request Previously sent tags to check freshness Host Request The serverâs DNS name Authorization Request A list of the clientâs credentials Referer Request The previous URL from which the request came Cookie Request Previously set cookie sent back to the server Set-Cookie Response Cookie for the client to store Server Response Information about the server Content-Encoding Response How the content is encoded (
gzip) Content-Language Response The natural language used in the page Content-Length Response The pageâs length in bytes Content-Type Response The pageâs MIME type Content-Range Response Identifies a portion of the pageâs content Last-Modified Response Time and date the page was last changed Expires Response Time and date when the page stops being valid Location Response Tells the client where to send its request Accept-Ranges Response Indicates the server will accept byte range requests Date Both Date and time the message was sent Range Both Identifies a portion of a page Cache-Control Both Directives for how to treat caches ETag Both Tag for the contents of the page Upgrade Both The protocol the sender wants to switch to Figure  -
Some HTTP message headers
This header is particularly useful for tracking Web browsing  as it tells servers how a client arrived at the page
Although cookies are dealt with in RFC  rather than RFC  they also have headers
The Set-Cookie header is how servers send cookies to clients
The client is expected to save the cookie and return it on subsequent requests to the server by using the Cookie header
(Note that there is a more recent specification for cookies with newer headers  RFC  but this has largely been rejected by industry and is not widely implemented
) THE APPLICATION LAYER
Many other headers are used in responses
The Server header allows the server to identify its software build if it wishes
The next five headers  all starting with Content-  allow the server to describe properties of the page it is sending
The Last-Modified header tells when the page was last modified  and the Expires header tells for how long the page will remain valid
Both of these headers play an important role in page caching
The Location header is used by the server to inform the client that it should try a different URL
This can be used if the page has moved or to allow multiple URLs to refer to the same page (possibly on different servers)
It is also used for companies that have a main Web page in the com domain but redirect clients to a national or regional page based on their IP addresses or preferred language
If a page is very large  a small client may not want it all at once
Some servers will accept requests for byte ranges  so the page can be fetched in multiple small units
The Accept-Ranges header announces the serverâs willingness to handle this type of partial page request
Now we come to headers that can be used in both directions
The Date header can be used in both directions and contains the time and date the message was sent  while the Range header tells the byte range of the page that is provided by the response
The ETag header gives a short tag that serves as a name for the content of the page
It is used for caching
The Cache-Control header gives other explicit instructions about how to cache (or  more usually  how not to cache) pages
Finally  the Upgrade header is used for switching to a new communication protocol  such as a future HTTP protocol or a ure transport
It allows the client to announce what it can support and the server to assert what it is using
Caching People often return to Web pages that they have viewed before  and related Web pages often have the same embedded resources
Some examples are the images that are used for navigation across the site  as well as common style sheets and scripts
It would be very wasteful to fetch all of these resources for these pages each time they are displayed because the browser already has a copy
Squirreling away pages that are fetched for subsequent use is called caching
The advantage is that when a cached page can be reused  it is not necessary to repeat the transfer
HTTP has built-in support to help clients identify when they can safely reuse pages
This support improves performance by reducing both network traffic and latency
The trade-off is that the browser must now store pages  but this is nearly always a worthwhile trade-off because local storage is inexpensive
The pages are usually kept on disk so that they can be used when the browser is run at a later date
The difficult issue with HTTP caching is how to determine that a previously cached copy of a page is the same as the page would be if it was fetched again
THE WORLD WIDE WEB This determination cannot be made solely from the URL
For example  the URL may give a page that displays the latest news item
The contents of this page will be updated frequently even though the URL stays the same
Alternatively  the contents of the page may be a list of the gods from Greek and Roman mythology
This page should change somewhat less rapidly
HTTP uses two strategies to tackle this problem
They are shown in Fig
The first strategy is page validation (step  )
The cache is consulted  and if it has a copy of a page for the requested URL that is known to be fresh (
still valid)  there is no need to fetch it anew from the server
Instead  the cached page can be returned directly
The Expires header returned when the cached page was originally fetched and the current date and time can be used to make this determination
a: Not modified Web browser Cache Web server  : Request  : Check expiry  : Conditional GET  b: Response  : Response Program Figure  -
HTTP caching
However  not all pages come with a convenient Expires header that tells when the page must be fetched again
After all  making predictions is hardâespecially about the future
In this case  the browser may use heuristics
For example  if the page has not been modified in the past year (as told by the Last-Modified header) it is a fairly safe bet that it will not change in the next hour
There is no guarantee  however  and this may be a bad bet
For example  the stock market might have closed for the day so that the page will not change for hours  but it will change rapidly once the next trading session starts
Thus  the cacheability of a page may vary wildly over time
For this reason  heuristics should be used with care  though they often work well in practice
Finding pages that have not expired is the most beneficial use of caching because it means that the server does not need to be contacted at all
Unfortunately  it does not always work
Servers must use the Expires header conservatively  since they may be unsure when a page will be updated
Thus  the cached copies may still be fresh  but the client does not know
The ond strategy is used in this case
It is to ask the server if the cached copy is still valid
This request is a conditional GET  and it is shown in Fig
If the server knows that the cached copy is still valid  it can send a short reply to say so (step  a)
Otherwise  it must send the full response (step  b)
THE APPLICATION LAYER
More header fields are used to let the server check whether a cached copy is still valid
The client has the time a cached page was last updated from the Last- Modified header
It can send this time to the server using the If-Modified-Since header to ask for the page only if it has been changed in the meantime
Alternatively  the server may return an ETag header with a page
This header gives a tag that is a short name for the content of the page  like a checksum but better
(It can be a cryptographic hash  which we will describe in  ) The client can validate cached copies by sending the server an If-None-Match header listing the tags of the cached copies
If any of the tags match the content that the server would respond with  the corresponding cached copy may be used
This method can be used when it is not convenient or useful to determine freshness
For example  a server may return different content for the same URL depending on what languages and MIME types are preferred
In this case  the modification date alone will not help the server to determine if the cached page is fresh
Finally  note that both of these caching strategies are overridden by the directives carried in the Cache-Control header
These directives can be used to restrict caching (
no-cache) when it is not appropriate
An example is a dynamic page that will be different the next time it is fetched
Pages that require authorization are also not cached
There is much more to caching  but we only have the space to make two important points
First  caching can be performed at other places besides in the browser
In the general case  HTTP requests can be routed through a series of caches
The use of a cache external to the browser is called proxy caching
Each added level of caching can help to reduce requests further up the chain
It is common for organizations such as ISPs and companies to run proxy caches to gain the benefits of caching pages across different users
We will discuss proxy caching with the broader topic of content distribution in   at the end of this  ter
ond  caches provide an important boost to performance  but not as much as one might hope
The reason is that  while there are certainly popular documents on the Web  there are also a great many unpopular documents that people fetch  many of which are also very long (
The ââlong tailââ of unpopular documents take up space in caches  and the number of requests that can be handled from the cache grows only slowly with the size of the cache
Web caches are always likely to be able to handle less than half of the requests
See Breslau et al
(   ) for more information
Experimenting with HTTP Because HTTP is an ASCII protocol  it is quite easy for a person at a terminal (as opposed to a browser) to directly talk to Web servers
All that is needed is a TCP connection to port   on the server
Readers are encouraged to experiment with the following command sequence
It will work in most UNIX shells and the command window on Windows (once the telnet program is enabled)
THE WORLD WIDE WEB telnet     GET /  HTTP/
Host:   This sequence of commands starts up a telnet (
TCP) connection to port   on IETFâs Web server
Then comes the GET command naming the path of the URL and the protocol
Try servers and URLs of your choosing
The next line is the mandatory Host header
A blank line following the last header is mandatory
It tells the server that there are no more request headers
The server will then send the response
Depending on the server and the URL  many different kinds of headers and pages can be observed
The Mobile Web The Web is used from most every type of computer  and that includes mobile phones
Browsing the Web over a wireless network while mobile can be very useful
It also presents technical problems because much Web content was designed for flashy presentations on desktop computers with broadband connectivity
In this tion we will describe how Web access from mobile devices  or the mobile Web  is being developed
Compared to desktop computers at work or at home  mobile phones present several difficulties for Web browsing:
Relatively small screens preclude large pages and large images Limited input capabilities make it tedious to enter URLs or other lengthy input Network bandwidth is limited over wireless links  particularly on cellular ( G) networks  where it is often expensive too Connectivity may be intermittent Computing power is limited  for reasons of battery life  size  heat dissipation  and cost
These difficulties mean that simply using desktop content for the mobile Web is likely to deliver a frustrating user experience
Early approaches to the mobile Web devised a new protocol stack tailored to wireless devices with limited capabilities
WAP (Wireless Application Protocol) is the most well-known example of this strategy
The WAP effort was started in  by major mobile phone vendors that included Nokia  Ericsson  and Motorola
However  something unexpected happened along the way
Over the next decade  network bandwidth and device capabilities grew tremendously with the deployment of  G data services and mobile phones with larger color displays  THE APPLICATION LAYER
faster processors  and
wireless capabilities
All of a sudden  it was possible for mobiles to run simple Web browsers
There is still a gap between these mobiles and desktops that will never close  but many of the technology problems that gave impetus to a separate protocol stack have faded
The approach that is increasingly used is to run the same Web protocols for mobiles and desktops  and to have Web sites deliver mobile-friendly content when the user happens to be on a mobile device
Web servers are able to detect whether to return desktop or mobile versions of Web pages by looking at the request headers
The User-Agent header is especially useful in this regard because it identifies the browser software
Thus  when a Web server receives a request  it may look at the headers and return a page with small images  less text  and simpler navigation to an iPhone and a full-featured page to a user on a laptop
W C is encouraging this approach in several ways
One way is to standardize best practices for mobile Web content
A list of   such best practices is provided in the first specification (Rabin and McCathieNevile  )
Most of these practices take sensible steps to reduce the size of pages  including by the use of compression  since the costs of communication are higher than those of computation  and by maximizing the effectiveness of caching
This approach encourages sites  especially large sites  to create mobile Web versions of their content because that is all that is required to capture mobile Web users
To help those users along  there is also a logo to indicate pages that can be viewed (well) on the mobile Web
Another useful tool is a stripped-down version of HTML called XHTML Basic
This language is a subset of XHTML that is intended for use by mobile phones  televisions  PDAs  vending machines  pagers  cars  game machines  and even watches
For this reason  it does not support style sheets  scripts  or frames  but most of the standard tags are there
They are grouped into   modules
Some are required; some are optional
All are defined in XML
The modules and some example tags are listed in Fig
However  not all pages will be designed to work well on the mobile Web
Thus  a complementary approach is the use of content transformation or transcoding
In this approach  a computer that sits between the mobile and the server takes requests from the mobile  fetches content from the server  and transforms it to mobile Web content
A simple transformation is to reduce the size of large images by reformatting them at a lower resolution
Many other small but useful transformations are possible
Transcoding has been used with some success since the early days of the mobile Web
See  for example  Fox et al
However  when both approaches are used there is a tension between the mobile content decisions that are made by the server and by the transcoder
For instance  a Web site may select a particular combination of image and text for a mobile Web user  only to have a transcoder change the format of the image
Our discussion so far has been about content  not protocols  as it is the content that is the biggest problem in realizing the mobile Web
However  we will briefly mention the issue of protocols
The HTTP  TCP  and IP protocols used by the   THE WORLD WIDE WEB Module Req
? Function Example tags Structure Yes Doc
structure body  head  html  title Text Yes Information br  code  dfn  em  hn  kbd  p  strong Hypertext Yes Hyperlinks a List Yes Itemized lists dl  dt  dd  ol  ul  li Forms No Fill-in forms form  input  label  option  textarea Tables No Rectangular tables caption  table  td  th  tr Image No Pictures img Object No Applets  maps  etc
object  param Meta-information No Extra info meta Link No Similar to <a> link Base No URL starting point base Figure  -
The XHTML Basic modules and tags
Web may consume a significant amount of bandwidth on protocol overheads such as headers
To tackle this problem  WAP and other solutions defined special-purpose protocols
This turns out to be largely unecessary
Header compression technologies  such as ROHC (RObust Header Compression) described in
can reduce the overheads of these protocols
In this way  it is possible to have one set of protocols (HTTP  TCP  IP) and use them over either high- or low- bandwidth links
Use over the low-bandwidth links simply requires that header compression be turned on
Web Search To finish our description of the Web  we will discuss what is arguably the most successful Web application: search
In  Sergey Brin and Larry Page  then graduate students at Stanford  formed a startup called Google to build a better Web search engine
They were armed with the then-radical idea that a search algorithm that counted how many times each page was pointed to by other pages was a better measure of its importance than how many times it contained the key words being sought
For instance  many pages link to the main Cisco page  which makes this page more important to a user searching for ââCiscoââ than a page outside of the company that happens to use the word ââCiscoââ many times
They were right
It did prove possible to build a better search engine  and people flocked to it
Backed by venture capital  Google grew tremendously
It became a public company in  with a market capitalization of $  billion
By  it was estimated to run more than one million servers in data centers throughout the world
THE APPLICATION LAYER
In one sense  search is simply another Web application  albeit one of the most mature Web applications because it has been under development since the early days of the Web
However  Web search has proved indispensible in everyday usage
Over one billion Web searches are estimated to be done each day
People looking for all manner of information use search as a starting point
For example  to find out where to buy Vegemite in Seattle  there is no obvious Web site to use as a starting point
But chances are that a search engine knows of a page with the desired information and can quickly direct you to the answer
To perform a Web search in the traditional manner  the user directs her browser to the URL of a Web search site
The major search sites include Google  Yahoo!  and Bing
Next  the user submits search terms using a form
This act causes the search engine to perform a query on its database for relevant pages or images  or whatever kind of resource is being searched for  and return the result as a dynamic page
The user can then follow links to the pages that have been found
Web search is an interesting topic for discussion because it has implications for the design and use of networks
First  there is the question of how Web search finds pages
The Web search engine must have a database of pages to run a query
Each HTML page may contain links to other pages  and everything interesting (or at least searchable) is linked somewhere
This means that it is theoretically possible to start with a handful of pages and find all other pages on the Web by doing a traversal of all pages and links
This process is called Web crawling
All Web search engines use Web crawlers
One issue with crawling is the kind of pages that it can find
Fetching static documents and following links is easy
However  many Web pages contain programs that display different pages depending on user interaction
An example is an online catalog for a store
The catalog may contain dynamic pages created from a product database and queries for different products
This kind of content is different from static pages that are easy to traverse
How do Web crawlers find these dynamic pages? The answer is that  for the most part  they do not
This kind of hidden content is called the deep Web
How to search the deep Web is an open problem that researchers are now tackling
See  for example  madhavan et al
There are also conventions by which sites make a page (known as  ) to tell crawlers what parts of the sites should or should not be visited
A ond consideration is how to process all of the crawled data
To let indexing algorithms be run over the mass of data  the pages must be stored
Estimates vary  but the main search engines are thought to have an index of tens of billions of pages taken from the visible part of the Web
The average page size is estimated at KB
These figures mean that a crawled copy of the Web takes on the order of   petabytes or  Ã  bytes to store
While this is a truly huge number  it is also an amount of data that can comfortably be stored and processed in Internet data centers (Chang et al
For example  if disk storage costs $  /TB  then  Ã TB costs $    which is not exactly a huge amount for companies the size of Google  Microsoft  and Yahoo!
And while the Web is   THE WORLD WIDE WEB expanding  disk costs are dropping dramatically  so storing the entire Web may continue to be feasible for large companies for the foreseeable future
Making sense of this data is another matter
You can appreciate how XML can help programs extract the structure of the data easily  while ad hoc formats will lead to much guesswork
There is also the issue of conversion between formats  and even translation between languages
But even knowing the structure of data is only part of the problem
The hard bit is to understand what it means
This is where much value can be unlocked  starting with more relevant result pages for search queries
The ultimate goal is to be able to answer questions  for example  where to buy a cheap but decent toaster oven in your city
A third aspect of Web search is that it has come to provide a higher level of naming
There is no need to remember a long URL if it is just as reliable (or perhaps more) to search for a Web page by a personâs name  assuming that you are better at remembering names than URLs
This strategy is increasingly successful
In the same way that DNS names relegated IP addresses to computers  Web search is relegating URLs to computers
Also in favor of search is that it corrects spelling and typing errors  whereas if you type in a URL wrong  you get the wrong page
Finally  Web search shows us something that has little to do with network design but much to do with the growth of some Internet services: there is much money in advertising
Advertising is the economic engine that has driven the growth of Web search
The main change from print advertising is the ability to target advertisements depending on what people are searching for  to increase the relevance of the advertisements
Variations on an auction mechanism are used to match the search query to the most valuable advertisement (Edelman et al
This new model has given rise to new problems  of course  such as click fraud  in which programs imitate users and click on advertisements to cause payments that have not been fairly earned  STREAMING AUDIO AND VIDEO Web applications and the mobile Web are not the only exciting developments in the use of networks
For many people  audio and video are the holy grail of networking
When the word ââmultimediaââ is mentioned  both the propellerheads and the suits begin salivating as if on cue
The former see immense technical challenges in providing voice over IP and video-on-demand to every computer
The latter see equally immense profits in it
While the idea of sending audio and video over the Internet has been around since the s at least  it is only since roughly  that real-time audio and real-time video traffic has grown with a vengeance
Real-time traffic is different from Web traffic in that it must be played out at some predetermined rate to be useful
After all  watching a video in slow motion with fits and starts is not most THE APPLICATION LAYER
peopleâs idea of fun
In contrast  the Web can have short interruptions  and page loads can take more or less time  within limits  without it being a major problem
Two things happened to enable this growth
First  computers have became much more powerful and are equipped with microphones and cameras so that they can input  process  and output audio and video data with ease
ond  a flood of Internet bandwidth has come to be available
Long-haul links in the core of the Internet run at many gigabits/  and broadband and
wireless reaches users at the edge of the Internet
These developments allow ISPs to carry tremendous levels of traffic across their backbones and mean that ordinary users can connect to the Internet   â times faster than with a  -kbps telephone modem
The flood of bandwidth caused audio and video traffic to grow  but for different reasons
Telephone calls take up relatively little bandwidth (in principle   kbps but less when compressed) yet telephone service has traditionally been expensive
Companies saw an opportunity to carry voice traffic over the Internet using existing bandwidth to cut down on their telephone bills
Startups such as Skype saw a way to let customers make free telephone calls using their Internet connections
Upstart telephone companies saw a cheap way to carry traditional voice calls using IP networking equipment
The result was an explosion of voice data carried over Internet networks that is called voice over IP or Internet telephony
Unlike audio  video takes up a large amount of bandwidth
Reasonable quality Internet video is encoded with compression at rates of around  Mbps  and a typical DVD movie is  GB of data
Before broadband Internet access  sending movies over the network was prohibitive
Not so any more
With the spread of broadband  it became possible for the first time for users to watch decent  streamed video at home
People love to do it
Around a quarter of the Internet users on any given day are estimated to visit YouTube  the popular video sharing site
The movie rental business has shifted to online downloads
And the sheer size of videos has changed the overall makeup of Internet traffic
The majority of Internet traffic is already video  and it is estimated that  % of Internet traffic will be video within a few years (Cisco  )
Given that there is enough bandwidth to carry audio and video  the key issue for designing streaming and conferencing applications is network delay
Audio and video need real-time presentation  meaning that they must be played out at a predetermined rate to be useful
Long delays mean that calls that should be interactive no longer are
This problem is clear if you have ever talked on a satellite phone  where the delay of up to half a ond is quite distracting
For playing music and movies over the network  the absolute delay does not matter  because it only affects when the media starts to play
But the variation in delay  called jitter  still matters
It must be masked by the player or the audio will sound unintelligible and the video will look jerky
In this tion  we will discuss some strategies to handle the delay problem  as well as protocols for setting up audio and video sessions
After an introduction to   STREAMING AUDIO AND VIDEO digital audio and video  our presentation is broken into three cases for which different designs are used
The first and easiest case to handle is streaming stored media  like watching a video on YouTube
The next case in terms of difficulty is streaming live media
Two examples are Internet radio and IPTV  in which radio and television stations broadcast to many users live on the Internet
The last and most difficult case is a call as might be made with Skype  or more generally an interactive audio and video conference
As an aside  the term multimedia is often used in the context of the Internet to mean video and audio
Literally  multimedia is just two or more media
That definition makes this book a multimedia presentation  as it contains text and graphics (the figures)
However  that is probably not what you had in mind  so we use the term ââmultimediaââ to imply two or more continuous media  that is  media that have to be played during some well-defined time interval
The two media are normally video with audio  that is  moving pictures with sound
Many people also refer to pure audio  such as Internet telephony or Internet radio  as multimedia as well  which it is clearly not
Actually  a better term for all these cases is streaming media
Nonetheless  we will follow the herd and consider real-time audio to be multimedia as well
Digital Audio An audio (sound) wave is a one-dimensional acoustic (pressure) wave
When an acoustic wave enters the ear  the eardrum vibrates  causing the tiny bones of the inner ear to vibrate along with it  sending nerve pulses to the brain
These pulses are perceived as sound by the listener
In a similar way  when an acoustic wave strikes a microphone  the microphone generates an electrical signal  representing the sound amplitude as a function of time
The frequency range of the human ear runs from   Hz to   Hz
Some animals  notably dogs  can hear higher frequencies
The ear hears loudness logarithmically  so the ratio of two sounds with power A and B is conventionally expressed in dB (decibels) as the quantity   log  (A /B)
If we define the lower limit of audibility (a sound pressure of about   Î¼Pascals) for a  -kHz sine wave as  dB  an ordinary conversation is about   dB and the pain threshold is about dB
The dynamic range is a factor of more than  million
The ear is surprisingly sensitive to sound variations lasting only a few millionds
The eye  in contrast  does not notice changes in light level that last only a few millionds
The result of this observation is that jitter of only a few millionds during the playout of multimedia affects the perceived sound quality much more than it affects the perceived image quality
Digital audio is a digital representation of an audio wave that can be used to recreate it
Audio waves can be converted to digital form by an ADC (Analogto- Digital Converter)
An ADC takes an electrical voltage as input and generates a binary number as output
THE APPLICATION LAYER
To represent this signal digitally  we can sample it every ÎT onds  as shown by the bar heights in Fig
If a sound wave is not a pure sine wave but a linear superposition of sine waves where the highest frequency component present is f  the Nyquist theorem (see
) states that it is sufficient to make samples at a frequency  f
Sampling more often is of no value since the higher frequencies that such sampling could detect are not present
T T T T T (a) (b) (c) T Figure  -
(a) A sine wave
(b) Sampling the sine wave
(c) Quantizing the samples to  bits
The reverse process takes digital values and produces an analog electrical voltage
It is done by a DAC (Digital-to-Analog Converter)
A loudspeaker can then convert the analog voltage to acoustic waves so that people can hear sounds
Digital samples are never exact
The samples of Fig
-  (c) allow only nine values  from â
in steps of   An  -bit sample would allow distinct values
A  -bit sample would allow   distinct values
The error introduced by the finite number of bits per sample is called the quantization noise
If it is too large  the ear detects it
Two well-known examples where sampled sound is used are the telephone and audio compact discs
Pulse code modulation  as used within the telephone system  uses  -bit samples made  times per ond
The scale is nonlinear to minimize perceived distortion  and with only  samples/  frequencies above  kHz are lost
In North America and Japan  the Î¼-law encoding is used
In Europe and internationally  the A-law encoding is used
Each encoding gives a data rate of   bps
Audio CDs are digital with a sampling rate of   samples/  enough to capture frequencies up to   Hz  which is good enough for people but bad for canine music lovers
The samples are   bits each and are linear over the range of amplitudes
Note that  -bit samples allow only   distinct values  even though the dynamic range of the ear is more than  million
Thus  even though CD-quality audio is much better than telephone-quality audio  using only   bits per sample introduces noticeable quantization noise (although the full dynamic range is not coveredâCDs are not supposed to hurt)
Some fanatic audiophiles   STREAMING AUDIO AND VIDEO still prefer  -RPM LP records to CDs because records do not have a Nyquist frequency cutoff at   kHz and have no quantization noise
(But they do have scratches unless handled very carefully) With   samples/ of   bits each  uncompressed CD-quality audio needs a bandwidth of
kbps for monaural and
Mbps for stereo
Audio Compression Audio is often compressed to reduce bandwidth needs and transfer times  even though audio data rates are much lower than video data rates
All compression systems require two algorithms: one for compressing the data at the source  and another for decompressing it at the destination
In the literature  these algorithms are referred to as the encoding and decoding algorithms  respectively
We will use this terminology too
Compression algorithms exhibit certain asymmetries that are important to understand
Even though we are considering audio first  these asymmetries hold for video as well
For many applications  a multimedia document will only be encoded once (when it is stored on the multimedia server) but will be decoded thousands of times (when it is played back by customers)
This asymmetry means that it is acceptable for the encoding algorithm to be slow and require expensive hardware provided that the decoding algorithm is fast and does not require expensive hardware
The operator of a popular audio (or video) server might be quite willing to buy a cluster of computers to encode its entire library  but requiring customers to do the same to listen to music or watch movies is not likely to be a big success
Many practical compression systems go to great lengths to make decoding fast and simple  even at the price of making encoding slow and complicated
On the other hand  for live audio and video  such as a voice-over-IP calls  slow encoding is unacceptable
Encoding must happen on the fly  in real time
Consequently  real-time multimedia uses different algorithms or parameters than stored audio or videos on disk  often with appreciably less compression
A ond asymmetry is that the encode/decode process need not be invertible
That is  when compressing a data file  transmitting it  and then decompressing it  the user expects to get the original back  accurate down to the last bit
With multimedia  this requirement does not exist
It is usually acceptable to have the audio (or video) signal after encoding and then decoding be slightly different from the original as long as it sounds (or looks) the same
When the decoded output is not exactly equal to the original input  the system is said to be lossy
If the input and output are identical  the system is lossless
Lossy systems are important because accepting a small amount of information loss normally means a huge payoff in terms of the compression ratio possible
Historically  long-haul bandwidth in the telephone network was very expensive  so there is a substantial body of work on vocoders (short for ââvoice codersââ) that compress audio for the special case of speech
Human speech tends to be in THE APPLICATION LAYER
the   -Hz to -Hz range and is produced by a mechanical process that depends on the speakerâs vocal tract  tongue  and jaw
Some vocoders make use of models of the vocal system to reduce speech to a few parameters (
the sizes and shapes of various cavities) and a data rate of as little as
How these vocoders work is beyond the scope of this book  however
We will concentrate on audio as sent over the Internet  which is typically closer to CD-quality
It is also desirable to reduce the data rates for this kind of audio
Mbps  stereo audio would tie up many broadband links  leaving less room for video and other Web traffic
Its data rate with compression can be reduced by an order of magnitude with little to no perceived loss of quality
Compression and decompression require signal processing
Fortunately  digitized sound and movies can be easily processed by computers in software
In fact  dozens of programs exist to let users record  display  edit  mix  and store media from multiple sources
This has led to large amounts of music and movies being available on the Internetânot all of it legalâwhich has resulted in numerous lawsuits from the artists and copyright owners
Many audio compression algorithms have been developed
Probably the most popular formats are MP  (MPEG audio layer  ) and AAC (Advanced Audio Coding) as carried in MP  (MPEG- ) files
To avoid confusion  note that MPEG provides audio and video compression
MP  refers to the audio compression portion (part  ) of the MPEG-  standard  not the third version of MPEG
In fact  no third version of MPEG was released  only MPEG-  MPEG-  and MPEG-
AAC is the successor to MP  and the default audio encoding used in MPEG-
MPEG-  allows both MP  and AAC audio
Is that clear now? The nice thing about standards is that there are so many to choose from
And if you do not like any of them  just wait a year or two
Audio compression can be done in two ways
In waveform coding  the signal is transformed mathematically by a Fourier transform into its frequency components
we showed an example function of time and its Fourier amplitudes in Fig
The amplitude of each component is then encoded in a minimal way
The goal is to reproduce the waveform fairly accurately at the other end in as few bits as possible
The other way  perceptual coding  exploits certain flaws in the human auditory system to encode a signal in such a way that it sounds the same to a human listener  even if it looks quite different on an oscilloscope
Perceptual coding is based on the science of psychoacousticsâhow people perceive sound
Both MP  and AAC are based on perceptual coding
The key property of perceptual coding is that some sounds can mask other sounds
Imagine you are broadcasting a live flute concert on a warm summer day
Then all of a sudden  out of the blue  a crew of workmen nearby turn on their jackhammers and start tearing up the street
No one can hear the flute any more
Its sounds have been masked by the jackhammers
For transmission purposes  it is now sufficient to encode just the frequency band used by the jackhammers   STREAMING AUDIO AND VIDEO because the listeners cannot hear the flute anyway
This is called frequency maskingâthe ability of a loud sound in one frequency band to hide a softer sound in another frequency band that would have been audible in the absence of the loud sound
In fact  even after the jackhammers stop  the flute will be inaudible for a short period of time because the ear turns down its gain when they start and it takes a finite time to turn it up again
This effect is called temporal masking
To make these effects more quantitative  imagine experiment
A person in a quiet room puts on headphones connected to a computerâs sound card
The computer generates a pure sine wave at Hz at low  but gradually increasing  power
The subject is instructed to strike a key when she hears the tone
The computer records the current power level and then repeats the experiment at Hz  Hz  and all the other frequencies up to the limit of human hearing
When averaged over many people  a log-log graph of how much power it takes for a tone to be audible looks like that of Fig
A direct consequence of this curve is that it is never necessary to encode any frequencies whose power falls below the threshold of audibility
For example  if the power at Hz were   dB in Fig
Masking signal at Hz Threshold of audibility
Frequency (kHz) Power (dB)       Frequency (kHz) (a) (b) Masked signal Threshold of audibility Power (dB)
(a) The threshold of audibility as a function of frequency
(b) The masking effect
Now consider experiment
The computer runs experiment  again  but this time with a constant-amplitude sine wave at  say  Hz superimposed on the test frequency
What we discover is that the threshold of audibility for frequencies near Hz is raised  as shown in Fig
The consequence of this new observation is that by keeping track of which signals are being masked by more powerful signals in nearby frequency bands  we can omit more and more frequencies in the encoded signal  saving bits
Even after a powerful signal stops in some frequency band  knowledge of its temporal masking properties allows us to continue to omit the masked frequencies for some time interval as the ear recovers
The THE APPLICATION LAYER
essence of MP  and AAC is to Fourier-transform the sound to get the power at each frequency and then transmit only the unmasked frequencies  encoding these in as few bits as possible
With this information as background  we can now see how the encoding is done
The audio compression is done by sampling the waveform at a rate from  to   kHz for AAC  often at
kHz  to mimic CD sound
Sampling can be done on one (mono) or two (stereo) channels
Next  the output bit rate is chosen
MP  can compress a stereo rock ân roll CD down to   kbps with little perceptible loss in quality  even for rock ân roll fans with no hearing loss
For a piano concert  AAC with at least kbps is needed
The difference is because the signalto- noise ratio for rock ân roll is much higher than for a piano concert (in an engineering sense  anyway)
It is also possible to choose lower output rates and accept some loss in quality
The samples are processed in small batches
Each batch is passed through a bank of digital filters to get frequency bands
The frequency information is fed into a psychoacoustic model to determine the masked frequencies
Then the available bit budget is divided among the bands  with more bits allocated to the bands with the most unmasked spectral power  fewer bits allocated to unmasked bands with less spectral power  and no bits allocated to masked bands
Finally  the bits are encoded using Huffman encoding  which assigns short codes to numbers that appear frequently and long codes to those that occur infrequently
There are many more details for the curious reader
For more information  see Brandenburg (   )
Digital Video Now that we know all about the ear  it is time to move on to the eye
(No  this tion is not followed by one on the nose
) The human eye has the property that when an image appears on the retina  the image is retained for some number of millionds before decaying
If a sequence of images is drawn at   images/  the eye does not notice that it is looking at discrete images
All video systems exploit this principle to produce moving pictures
The simplest digital representation of video is a sequence of frames  each consisting of a rectangular grid of picture elements  or pixels
Each pixel can be a single bit  to represent either black or white
However  the quality of such a system is awful
Try using your favorite image editor to convert the pixels of a color image to black and white (and not shades of gray)
The next step up is to use  bits per pixel to represent gray levels
This scheme gives high-quality ââblack-and-whiteââ video
For color video  many systems use  bits for each of the red  green and blue (RGB) primary color components
This representation is possible because any color can be constructed from a linear superposition of red  green  and blue with the appropriate intensities
With   STREAMING AUDIO AND VIDEO   bits per pixel  there are about   million colors  which is more than the human eye can distinguish
On color LCD computer monitors and televisions  each discrete pixel is made up of closely spaced red  green and blue subpixels
Frames are displayed by setting the intensity of the subpixels  and the eye blends the color components
Common frame rates are   frames/ (inherited from  mm motion-picture film) frames/ (inherited from NTSC
televisions)  and   frames/ (inherited from the PAL television system used in nearly all the rest of the world)
(For the truly picky  NTSC color television runs at
The original black-and-white system ran at   frames/  but when color was introduced  the engineers needed a bit of extra bandwidth for signaling so they reduced the frame rate to   NTSC videos intended for computers really use
) PAL was invented after NTSC and really uses
To make this story complete  a third system  AM  is used in France  Francophone Africa  and Eastern Europe
It was first introduced into Eastern Europe by then Communist East Germany so the East German people could not watch West German (PAL) television lest they get Bad Ideas
But many of these countries are switching to PAL
Technology and politics at their best
Actually  for broadcast television frames/ is not quite good enough for smooth motion so the images are split into two fields  one with the odd-numbered scan lines and one with the even-numbered scan lines
The two (half-resolution) fields are broadcast sequentially  giving almost   (NTSC) or exactly   (PAL) fields/  a system known as interlacing
Videos intended for viewing on a computer are progressive  that is  do not use interlacing because computer monitors have buffers on their graphics cards  making it possible for the CPU to put a new image in the buffer   times/ but have the graphics card redraw the screen   or even times/ to eliminate flicker
Analog television sets do not have a frame buffer the way computers do
When an interlaced video with rapid movement is displayed on a computer  short horizontal lines will be visible near sharp edges  an effect known as combing
The frame sizes used for video sent over the Internet vary widely for the simple reason that larger frames require more bandwidth  which may not always be available
Low-resolution video might be by pixels  and ââfull-screenââ video is by pixels
These dimensions approximate those of early computer monitors and NTSC television  respectively
The aspect ratio  or width to height ratio  of  :  is the same as a standard television
HDTV (High-Definition TeleVision) videos can be downloaded with  by pixels
These ââwidescreenââ images have an aspect ratio of  :  to more closely match the  :  aspect ratio of film
For comparison  standard DVD video is usually by pixels  and video on Blu-ray discs is usually HDTV at  by pixels
On the Internet  the number of pixels is only part of the story  as media players can present the same image at different sizes
Video is just another window on a computer screen that can be blown up or shrunk down
The role of more THE APPLICATION LAYER
pixels is to increase the quality of the image  so that it does not look blurry when it is expanded
However  many monitors can show images (and hence videos) with even more pixels than even HDTV
Video Compression It should be obvious from our discussion of digital video that compression is critical for sending video over the Internet
Even a standard-quality video with by pixel frames bits of color information per pixel  and   frames/ takes over Mbps
This far exceeds the bandwidth by which most company offices are connected to the Internet  let alone home users  and this is for a single video stream
Since transmitting uncompressed video is completely out of the question  at least over wide area networks  the only hope is that massive compression is possible
Fortunately  a large body of research over the past few decades has led to many compression techniques and algorithms that make video transmission feasible
Many formats are used for video that is sent over the Internet  some proprietary and some standard
The most popular encoding is MPEG in its various forms
It is an open standard found in files with mpg and mp  extensions  as well as in other container formats
In this tion  we will look at MPEG to study how video compression is accomplished
To begin  we will look at the compression of still images with JPEG
A video is just a sequence of images (plus sound)
One way to compress video is to encode each image in succession
To a first approximation  MPEG is just the JPEG encoding of each frame  plus some extra features for removing the redundancy across frames
The JPEG Standard The JPEG (Joint Photographic Experts Group) standard for compressing continuous-tone still pictures (
photographs) was developed by photographic experts working under the joint auspices of ITU  ISO  and IEC  another standards body
It is widely used (look for files with the extension jpg) and often provides compression ratios of  :  or better for natural images
JPEG is defined in International Standard
Really  it is more like a shopping list than a single algorithm  but of the four modes that are defined only the lossy sequential mode is relevant to our discussion
Furthermore  we will concentrate on the way JPEG is normally used to encode  -bit RGB video images and will leave out some of the options and details for the sake of simplicity
The algorithm is illustrated in Fig
Step  is block preparation
For the sake of specificity  let us assume that the JPEG input is a Ã RGB image with   bits/pixel  as shown in Fig
RGB is not the best color model to use for compression
The eye is much more sensitive to the luminance  or brightness  of video signals than the chrominance  or color  of video signals
Thus  we   STREAMING AUDIO AND VIDEO first compute the luminance  Y  and the two chrominances  Cb and Cr  from the R  G  and B components
The following formulas are used for  -bit values that range from  to   : Y =   +
B Block preparation Discrete cosine transform Quantization Differential quantization Runlength encoding Statistical output encoding Input Output Figure  -
Steps in JPEG lossy sequential encoding
Separate matrices are constructed for Y  Cb  and Cr
Next  square blocks of four pixels are averaged in the Cb and Cr matrices to reduce them to Ã
This reduction is lossy  but the eye barely notices it since the eye responds to luminance more than to chrominance
Nevertheless  it compresses the total amount of data by a factor of two
Now is subtracted from each element of all three matrices to put  in the middle of the range
Finally  each matrix is divided up into  Ã  blocks
The Y matrix has  blocks; the other two have  blocks each  as shown in Fig
(a) (b) Cr RGB Y Cb     Block Block  -Bit pixel  -Bit pixel Figure  -
(a) RGB input data
(b) After block preparation
Step  of JPEG encoding is to apply a DCT (Discrete Cosine Transformation) to each of the  blocks separately
The output of each DCT is an  Ã  matrix of DCT coefficients
DCT element (   ) is the average value of the block
The other elements tell how much spectral power is present at each spatial frequency
Normally  these elements decay rapidly with distance from the origin  (   )  as suggested by Fig
Once the DCT is complete  JPEG encoding moves on to step   called quantization  in which the less important DCT coefficients are wiped out
This (lossy) THE APPLICATION LAYER
Y/Cb/Cr Amplitude DCT x Fx y Fy (a) (b) Figure  -
(a) One block of the Y matrix
(b) The DCT coefficients
transformation is done by dividing each of the coefficients in the  Ã  DCT matrix by a weight taken from a table
If all the weights are   the transformation does nothing
However  if the weights increase sharply from the origin  higher spatial frequencies are dropped quickly
An example of this step is given in Fig
Here we see the initial DCT matrix  the quantization table  and the result obtained by dividing each DCT element by the corresponding quantization table element
The values in the quantization table are not part of the JPEG standard
Each application must supply its own  allowing it to control the loss-compression trade-off
DCT coefficients Quantization table Quantized coefficients                                          Figure  -
Computation of the quantized DCT coefficients
Step  reduces the (   ) value of each block (the one in the upper-left corner) by replacing it with the amount it differs from the corresponding element in the previous block
Since these elements are the averages of their respective blocks  they should change slowly  so taking the differential values should reduce most of them to small values
No differentials are computed from the other values
STREAMING AUDIO AND VIDEO Step  linearizes the   elements and applies run-length encoding to the list
Scanning the block from left to right and then top to bottom will not concentrate the zeros together  so a zigzag scanning pattern is used  as shown in Fig
In this example  the zigzag pattern produces   conutive  s at the end of the matrix
This string can be reduced to a single count saying there are   zeros  a technique known as run-length encoding
The order in which the quantized values are transmitted
Now we have a list of numbers that represent the image (in transform space)
Step  Huffman-encodes the numbers for storage or transmission  assigning common numbers shorter codes than uncommon ones
JPEG may seem complicated  but that is because it is complicated
Still  the benefits of up to  :  compression are worth it
Decoding a JPEG image requires running the algorithm backward
JPEG is roughly symmetric: decoding takes as long as encoding
This property is not true of all compression algorithms  as we shall now see
The MPEG Standard Finally  we come to the heart of the matter: the MPEG (Motion Picture Experts Group) standards
Though there are many proprietary algorithms  these standards define the main algorithms used to compress videos
They have been international standards since
Because movies contain both images and sound  MPEG can compress both audio and video
We have already examined audio compression and still image compression  so let us now examine video compression
The MPEG-  standard (which includes MP  audio) was first published in  and is still widely used
Its goal was to produce video-recorder-quality output that was compressed  :  to rates of around  Mbps
This video is suitable for THE APPLICATION LAYER
broad Internet use on Web sites
Do not worry if you do not remember video recordersâMPEG-  was also used for storing movies on CDs when they existed
If you do not know what CDs are  we will have to move on to MPEG-
The MPEG-  standard  released in  was designed for compressing broadcast-quality video
It is very common now  as it is used as the basis for video encoded on DVDs (which inevitably finds its way onto the Internet) and for digital broadcast television (as DVB)
DVD quality video is typically encoded at rates of  â  Mbps
The MPEG-  standard has two video formats
The first format  released in  encodes video with an object-based representation
This allows for the mixing of natural and synthetic images and other kinds of media  for example  a weatherperson standing in front of a weather map
With this structure  it is easy to let programs interact with movie data
The ond format  released in  is known as H
or AVC (Advanced Video Coding)
Its goal is to encode video at half the rate of earlier encoders for the same quality level  all the better to support the transmission of video over networks
This encoder is used for HDTV on most Blu-ray discs
The details of all these standards are many and varied
The later standards also have many more features and encoding options than the earlier standards
However  we will not go into the details
For the most part  the gains in video compression over time have come from numerous small improvements  rather than fundamental shifts in how video is compressed
Thus  we will sketch the overall concepts
MPEG compresses both audio and video
Since the audio and video encoders work independently  there is an issue of how the two streams get synchronized at the receiver
The solution is to have a single clock that outputs timestamps of the current time to both encoders
These timestamps are included in the encoded output and propagated all the way to the receiver  which can use them to synchronize the audio and video streams
MPEG video compression takes advantage of two kinds of redundancies that exist in movies: spatial and temporal
Spatial redundancy can be utilized by simply coding each frame separately with JPEG
This approach is occasionally used  especially when random access to each frame is needed  as in editing video productions
In this mode  JPEG levels of compression are achieved
Additional compression can be achieved by taking advantage of the fact that conutive frames are often almost identical
This effect is smaller than it might first appear since many movie directors cut between scenes every  or  onds (time a movie fragment and count the scenes)
Nevertheless  runs of   or more highly similar frames offer the potential of a major reduction over simply encoding each frame separately with JPEG
For scenes in which the camera and background are stationary and one or two actors are moving around slowly  nearly all the pixels will be identical from frame to frame
Here  just subtracting each frame from the previous one and running   STREAMING AUDIO AND VIDEO JPEG on the difference would do fine
However  for scenes where the camera is panning or zooming  this technique fails badly
What is needed is some way to compensate for this motion
This is precisely what MPEG does; it is the main difference between MPEG and JPEG
MPEG output consists of three kinds of frames:
I- (Intracoded) frames: self-contained compressed still pictures P- (Predictive) frames: block-by-block difference with the previous frames B- (Bidirectional) frames: block-by-block differences between previous and future frames
I-frames are just still pictures
They can be coded with JPEG or something similar
It is valuable to have I-frames appear in the output stream periodically (
once or twice per ond) for three reasons
First  MPEG can be used for a multicast transmission  with viewers tuning in at will
If all frames depended on their predecessors going back to the first frame  anybody who missed the first frame could never decode any subsequent frames
ond  if any frame were received in error  no further decoding would be possible: everything from then on would be unintelligble junk
Third  without I-frames  while doing a fast forward or rewind the decoder would have to calculate every frame passed over so it would know the full value of the one it stopped on
P-frames  in contrast  code interframe differences
They are based on the idea of macroblocks  which cover  for example Ã   pixels in luminance space and  Ã  pixels in chrominance space
A macroblock is encoded by searching the previous frame for it or something only slightly different from it
An example of where P-frames would be useful is given in Fig
Here we see three conutive frames that have the same background  but differ in the position of one person
The macroblocks containing the background scene will match exactly  but the macroblocks containing the person will be offset in position by some unknown amount and will have to be tracked down
Three conutive frames
The MPEG standards do not specify how to search  how far to search  or how good a match has to be in order to count
This is up to each implementation
For THE APPLICATION LAYER
example  an implementation might search for a macroblock at the current position in the previous frame  and all other positions offset Â±Îx in the x direction and Â±Îy in the y direction
For each position  the number of matches in the luminance matrix could be computed
The position with the highest score would be declared the winner  provided it was above some predefined threshold
Otherwise  the macroblock would be said to be missing
Much more sophisticated algorithms are also possible  of course
If a macroblock is found  it is encoded by taking the difference between its current value and the one in the previous frame (for luminance and both chrominances)
These difference matrices are then subjected to the discrete cosine transformation  quantization  run-length encoding  and Huffman encoding  as usual
The value for the macroblock in the output stream is then the motion vector (how far the macroblock moved from its previous position in each direction)  followed by the encoding of its difference
If the macroblock is not located in the previous frame  the current value is encoded  just as in an I-frame
Clearly  this algorithm is highly asymmetric
An implementation is free to try every plausible position in the previous frame if it wants to  in a desperate attempt to locate every last macroblock  no matter where it has moved to
This approach will minimize the encoded MPEG stream at the expense of very slow encoding
This approach might be fine for a one-time encoding of a film library but would be terrible for real-time videoconferencing
Similarly  each implementation is free to decide what constitutes a ââfoundââ macroblock
This freedom allows implementers to compete on the quality and speed of their algorithms  but always produce compliant MPEG output
So far  decoding MPEG is straightforward
Decoding I-frames is similar to decoding JPEG images
Decoding P-frames requires the decoder to buffer the previous frames so it can build up the new one in a separate buffer based on fully encoded macroblocks and macroblocks containing differences from the previous frames
The new frame is assembled macroblock by macroblock
B-frames are similar to P-frames  except that they allow the reference macroblock to be in either previous frames or succeeding frames
This additional freedom allows for improved motion compensation
It is useful  for example  when objects pass in front of  or behind  other objects
To do B-frame encoding  the encoder needs to hold a sequence of frames in memory at once: past frames  the current frame being encoded  and future frames
Decoding is similarly more complicated and adds some delay
This is because a given B-frame cannot be decoded until the successive frames on which it depends are decoded
Thus  although Bframes give the best compression  they are not always used due to their greater complexity and buffering requirements
The MPEG standards contain many enhancements to these techniques to achieve excellent levels of compression
AVC can be used to compress video at ratios in excess of  :  which reduces network bandwidth requirements by the same factor
For more information on AVC  see Sullivan and Wiegand (   )
STREAMING AUDIO AND VIDEO    Streaming Stored Media Let us now move on to network applications
Our first case is streaming media that is already stored in files
The most common example of this is watching videos over the Internet
This is one form of VoD (Video on Demand)
Other forms of video on demand use a provider network that is separate from the Internet to deliver the movies (
the cable network)
In the next tion  we will look at streaming live media  for example  broadcast IPTV and Internet radio
Then we will look at the third case of real-time conferencing
An example is a voice-over-IP call or video conference with Skype
These three cases place increasingly stringent requirements on how we can deliver the audio and video over the network because we must pay increasing attention to delay and jitter
The Internet is full of music and video sites that stream stored media files
Actually  the easiest way to handle stored media is not to stream it
Imagine you want to create an online movie rental site to compete with Appleâs iTunes
A regular Web site will let users download and then watch videos (after they pay  of course)
The sequence of steps is shown in Fig
We will spell them out to contrast them with the next example
: Save media  : Media response (HTTP)  : Media request (HTTP) Browser Client Media player Web server Server  : Play media Disk Disk Figure  -
Playing media over the Web via simple downloads
The browser goes into action when the user clicks on a movie
In step   it sends an HTTP request for the movie to the Web server to which the movie is linked
In step   the server fetches the movie (which is just a file in MP  or some other format) and sends it back to the browser
Using the MIME type  for example  video/mp  the browser looks up how it is supposed to display the file
In this case  it is with a media player that is shown as a helper application  though it could also be a plug-in
The browser saves the entire movie to a scratch file on disk in step
It then starts the media player  passing it the name of the scratch file
Finally  in step  the media player starts reading the file and playing the movie
In principle  this approach is completely correct
It will play the movie
There is no real-time network issue to address either because the download is simply a THE APPLICATION LAYER
file download
The only trouble is that the entire video must be transmitted over the network before the movie starts
Most customers do not want to wait an hour for their ââvideo on demand
ââ This model can be problematic even for audio
Imagine previewing a song before purchasing an album
If the song is  MB  which is a typical size for an MP  song  and the broadband connectivity is  Mbps  the user will be greeted by half a minute of silence before the preview starts
This model is unlikely to sell many albums
To get around this problem without changing how the browser works  sites can use the design shown in Fig
The page linked to the movie is not the actual movie file
Instead  it is what is called a metafile  a very short file just naming the movie (and possibly having other key descriptors)
A simple metafile might be only one line of ASCII text and look like this: rtsp://joes-movie-server/movie-     The browser gets the page as usual  now a one-line file  in steps  and
Then it starts the media player and hands it the one-line file in step   all as usual
The media player reads the metafile and sees the URL of where to get the movie
It contacts joes-video-server and asks for the movie in step
The movie is then streamed back to the media player in step
The advantage of this arrangement is that the media player starts quickly  after only a very short metafile is downloaded
Once this happens  the browser is not in the loop any more
The media is sent directly to the media player  which can start showing the movie before the entire file has been downloaded
: Media request (RTSP)  : Metafile response (HTTP)  : Metafile request (HTTP) Browser Client Media player Web server Server Media server Server  : Handoff metafile  : Media response (via TCP or UDP) Disk Figure  -
Streaming media using the Web and a media server
We have shown two servers in Fig
In fact  it is generally not even   STREAMING AUDIO AND VIDEO an HTTP server  but a specialized media server
In this example  the media server uses RTSP (Real Time Streaming Protocol)  as indicated by the scheme name rtsp
The media player has four major jobs to do:
Manage the user interface Handle transmission errors Decompress the content Eliminate jitter
Most media players nowadays have a glitzy user interface  sometimes simulating a stereo unit  with buttons  knobs  sliders  and visual displays
Often there are interchangeable front panels  called skins  that the user can drop onto the player
The media player has to manage all this and interact with the user
The other jobs are related and depend on the network protocols
We will go through each one in turn  starting with handling transmission errors
Dealing with errors depends on whether a TCP-based transport like HTTP is used to transport the media  or a UDP-based transport like RTP is used
Both are used in practice
If a TCP-based transport is being used then there are no errors for the media player to correct because TCP already provides reliability by using retransmissions
This is an easy way to handle errors  at least for the media player  but it does complicate the removal of jitter in a later step
Alternatively  a UDP-based transport like RTP can be used to move the data
We studied it in   With these protocols  there are no retransmissions
Thus  packet loss due to congestion or transmission errors will mean that some of the media does not arrive
It is up to the media player to deal with this problem
Let us understand the difficulty we are up against
The loss is a problem because customers do not like large gaps in their songs or movies
However  it is not as much of a problem as loss in a regular file transfer because the loss of a small amount of media need not degrade the presentation for the user
For video  the user is unlikely to notice if there are occasionally   new frames in some ond instead of   new frames
For audio  short gaps in the playout can be masked with sounds close in time
The user is unlikely to detect this substitution unless they are paying very close attention
The key to the above reasoning  however  is that the gaps are very short
Network congestion or a transmission error will generally cause an entire packet to be lost  and packets are often lost in small bursts
Two strategies can be used to reduce the impact of packet loss on the media that is lost: FEC and interleaving
We will describe each in turn
FEC (Forward Error Correction) is simply the error-correcting coding that we studied in
applied at the application level
Parity across packets provides an example (Shacham and McKenny  )
For every four data packets THE APPLICATION LAYER
that are sent  a fifth parity packet can be constructed and sent
This is shown in Fig
-  with packets A  B  C  and D
The parity packet  P  contains redundant bits that are the parity or exclusive-OR sums of the bits in each of the four data packets
Hopefully  all of the packets will arrive for most groups of five packets
When this happens  the parity packet is simply discarded at the receiver
Or  if only the parity packet is lost  no harm is done
Lost packet Construct parity: Client Media player Media server Server Repair loss: Parity packet B = P A + + C + D A B C D P P = A B + + C + D Disk Figure  -
Using a parity packet to repair loss
Occasionally  however  a data packet may be lost during transmission  as B is in Fig
The media player receives only three data packets  A  C  and D  plus the parity packet  P
By design  the bits in the missing data packet can be reconstructed from the parity bits
To be specific  using ââ+ââ to represent exclusive-OR or modulo  addition  B can be reconstructed as B = P + A + C + D by the properties of exclusive-OR (
X + Y + Y = X)
FEC can reduce the level of loss seen by the media player by repairing some of the packet losses  but it only works up to a certain level
If two packets in a group of five are lost  there is nothing we can do to recover the data
The other property to note about FEC is the cost that we have paid to gain this protection
Every four packets have become five packets  so the bandwidth requirements for the media are  % larger
The latency of decoding has increased too  as we may need to wait until the parity packet has arrived before we can reconstruct a data packet that came before it
There is also one clever trick in the technique above
we described parity as providing error detection
Here we are providing error-correction
How can it do both? The answer is that in this case it is known which packet was lost
The lost data is called an erasure
when we considered a frame that was received with some bits in error  we did not know which bit was errored
This case is harder to deal with than erasures
Thus  with erasures parity can provide error correction  and without erasures parity can only provide error detection
We will see another unexpected benefit of parity soon  when we get to multicast scenarios
The ond strategy is called interleaving
This approach is based on mixing up or interleaving the order of the media before transmission and unmixing or   STREAMING AUDIO AND VIDEO deinterleaving it on reception
That way  if a packet (or burst of packets) is lost  the loss will be spread out over time by the unmixing
It will not result in a single  large gap when the media is played out
For example  a packet might contain stereo samples  each containing a pair of  -bit numbers  normally good for  m of music
If the samples were sent in order  a lost packet would represent a  m gap in the music
Instead  the samples are transmitted as shown in Fig
All the even samples for a  -m interval are sent in one packet  followed by all the odd samples in the next one
The loss of packet  now does not represent a  -m gap in the music  but the loss of every other sample for   m
This loss can be handled easily by having the media player interpolate using the previous and succeeding samples
The result is lower temporal resolution for   m  but not a noticeable time gap in the media
Even time sample Legend Odd time sample  (b) Packet (a)  Time (m)   Lost This packet contains even time samples  This packet contains odd time samples Figure  -
When packets carry alternate samples  the loss of a packet reduces the temporal resolution rather than creating a gap in time
This interleaving scheme above only works with uncompressed sampling
However  interleaving (over short periods of time  not individual samples) can also be applied after compression as long as there is a way to find sample boundaries in the compressed stream
RFC  gives a scheme that works with compressed audio
Interleaving is an attractive technique when it can be used because it needs no additional bandwidth  unlike FEC
However  interleaving adds to the latency  just like FEC  because of the need to wait for a group of packets to arrive (so they can be de-interleaved)
The media playerâs third job is decompressing the content
Although this task is computationally intensive  it is fairly straightforward
The thorny issue is how to decode media if the network protocol does not correct transmission errors
In many compression schemes  later data cannot be decompressed until the earlier data has been decompressed  because the later data is encoded relative to the earlier data
For a UDP-based transport  there can be packet loss
Thus  the encoding THE APPLICATION LAYER
process must be designed to permit decoding despite packet loss
This requirement is why MPEG uses I-  P- and B-frames
Each I-frame can be decoded independently of the other frames to recover from the loss of any earlier frames
The fourth job is to eliminate jitter  the bane of all real-time systems
The general solution that we described in
is to use a playout buffer
All streaming systems start by buffering  â   worth of media before starting to play  as shown in Fig
Playing drains media regularly from the buffer so that the audio is clear and the video is smooth
The startup delay gives the buffer a chance to fill to the low-water mark
The idea is that data should now arrive regularly enough that the buffer is never completely emptied
If that were to happen  the media playout would stall
The value of buffering is that if the data are sometimes slow to arrive due to congestion  the buffered media will allow the playout to continue normally until new media arrive and the buffer is replenished
Buffer Lowwater mark Highwater mark Media player Media server Client machine Server machine Figure  -
The media player buffers input from the media server and plays from the buffer rather than directly from the network
How much buffering is needed  and how fast the media server sends media to fill up the buffer  depend on the network protocols
There are many possibilities
The largest factor in the design is whether a UDP-based transport or a TCP-based transport is used
Suppose that a UDP-based transport like RTP is used
Further suppose that there is ample bandwidth to send packets from the media server to the media player with little loss  and little other traffic in the network
In this case  packets can be sent at the exact rate that the media is being played
Each packet will transit the network and  after a propagation delay  arrive at about the right time for the media player to present the media
Very little buffering is needed  as there is no variability in delay
If interleaving or FEC is used  more buffering is needed for at least the group of packets over which the interleaving or FEC is performed
However  this adds only a small amount of buffering
Unfortunately  this scenario is unrealistic in two respects
First  bandwidth varies over network paths  so it is usually not clear to the media server whether there will be sufficient bandwidth before it tries to stream the media
A simple solution is to encode media at multiple resolutions and let each user choose a   STREAMING AUDIO AND VIDEO resolution that is supported by his Internet connectivity
Often there are just two levels: high quality  say  encoded at
Mbps or better  and low quality  say encoded at kbps or less
ond  there will be some jitter  or variation in how long it takes media samples to cross the network
This jitter comes from two sources
There is often an appreciable amount of competing traffic in the networkâsome of which can come from multitasking users themselves browsing the Web while ostensibly watching a streamed movie)
This traffic will cause fluctuations in when the media arrives
Moreover  we care about the arrival of video frames and audio samples  not packets
With compression  video frames in particular may be larger or smaller depending on their content
An action sequence will typically take more bits to encode than a placid landscape
If the network bandwidth is constant  the rate of media delivery versus time will vary
The more jitter  or variation in delay  from these sources  the larger the low-water mark of the buffer needs to be to avoid underrun
Now suppose that a TCP-based transport like HTTP is used to send the media
By performing retransmissions and waiting to deliver packets until they are in order  TCP will increase the jitter that is observed by the media player  perhaps significantly
The result is that a larger buffer and higher low-water mark are needed
However  there is an advantage
TCP will send data as fast as the network will carry it
Sometimes media may be delayed if loss must be repaired
But much of the time  the network will be able to deliver media faster than the player consumes it
In these periods  the buffer will fill and prevent future underruns
If the network is significantly faster than the average media rate  as is often the case  the buffer will fill rapidly after startup such that emptying it will soon cease to be a concern
With TCP  or with UDP and a transmission rate that exceeds the playout rate  a question is how far ahead of the playout point the media player and media server are willing to proceed
Often they are willing to download the entire file
However  proceeding far ahead of the playout point performs work that is not yet needed  may require significant storage  and is not necessary to avoid buffer underruns
When it is not wanted  the solution is for the media player to define a high-water mark in the buffer
Basically  the server just pumps out data until the buffer is filled to the high-water mark
Then the media player tells it to pause
Since data will continue to pour in until the server has gotten the pause request  the distance between the high-water mark and the end of the buffer has to be greater than the bandwidth-delay product of the network
After the server has stopped  the buffer will begin to empty
When it hits the low-water mark  the media player tells the media server to start again
To avoid underrun  the low-water mark must also take the bandwidth-delay product of the network into account when asking the media server to resume sending the media
To start and stop the flow of media  the media player needs a remote control for it
This is what RTSP provides
It is defined in RFC  and provides the THE APPLICATION LAYER
mechanism for the player to control the server
As well as starting and stopping the stream  it can seek back or forward to a position  play specified intervals  and play at fast or slow speeds
It does not provide for the data stream  though  which is usually RTP over UDP or RTP over HTTP over TCP
The main commands provided by RTSP are listed in Fig
They have a simple text format  like HTTP messages  and are usually carried over TCP
RTSP can run over UDP too  since each command is acknowledged (and so can be resent if it is not acknowledged)
Command Server action DESCRIBE List media parameters SETUP Establish a logical channel between the player and the server PLAY Start sending data to the client RECORD Start accepting data from the client PAUSE Temporarily stop sending data TEARDOWN Release the logical channel Figure  -
RTSP commands from the player to the server
Even though TCP would seem a poor fit to real-time traffic  it is often used in practice
The main reason is that it is able to pass through firewalls more easily than UDP  especially when run over the HTTP port
Most administrators configure firewalls to protect their networks from unwelcome visitors
They almost always allow TCP connections from remote port   to pass through for HTTP and Web traffic
Blocking that port quickly leads to unhappy campers
However  most other ports are blocked  including for RSTP and RTP  which use ports and  amongst others
Thus  the easiest way to get streaming media through the firewall is for the Web site to pretend it is an HTTP server sending a regular HTTP response  at least to the firewall
There are some other advantages of TCP  too
Because it provides reliability  TCP gives the client a complete copy of the media
This makes it easy for a user to rewind to a previously viewed playout point without concern for lost data
Finally  TCP will buffer as much of the media as possible as quickly as possible
When buffer space is cheap (which it is when the disk is used for storage)  the media player can download the media while the user watches
Once the download is complete  the user can watch uninterrupted  even if he loses connectivity
This property is helpful for mobiles because connectivity can change rapidly with motion
The disadvantage of TCP is the added startup latency (because of TCP startup) and also a higher low-water mark
However  this is rarely much of a penalty as long as the network bandwidth exceeds the media rate by a large factor
STREAMING AUDIO AND VIDEO    Streaming Live Media It is not only recorded videos that are tremendously popular on the Web
Live media streaming is very popular too
Once it became possible to stream audio and video over the Internet  commercial radio and TV stations got the idea of broadcasting their content over the Internet as well as over the air
Not so long after that  college stations started putting their signals out over the Internet
Then college students started their own Internet broadcasts
Today  people and companies of all sizes stream live audio and video
The area is a hotbed of innovation as the technologies and standards evolve
Live streaming is used for an online presence by major television stations
This is called IPTV (IP TeleVision)
It is also used to broadcast radio stations like the BBC
This is called Internet radio
Both IPTV and Internet radio reach audiences worldwide for events ranging from fashion shows to World Cup soccer and test matches live from the Melbourne Cricket Ground
Live streaming over IP is used as a technology by cable providers to build their own broadcast systems
And it is widely used by low-budget operations from adult sites to zoos
With current technology  virtually anyone can start live streaming quickly and with little expense
One approach to live streaming is to record programs to disk
Viewers can connect to the serverâs archives  pull up any program  and download it for listening
A podcast is an episode retrieved in this manner
For scheduled events  it is also possible to store content just after it is broadcast live  so the archive is only running  say  half an hour or less behind the live feed
In fact  this approach is exactly the same as that used for the streaming media we just discussed
It is easy to do  all the techniques we have discussed work for it  and viewers can pick and choose among all the programs in the archive
A different approach is to broadcast live over the Internet
Viewers tune in to an ongoing media stream  just like turning on the television
However  media players provide the added features of letting the user pause or rewind the playout
The live media will continue to be streamed and will be buffered by the player until the user is ready for it
From the browserâs point of view  it looks exactly like the case of streaming stored media
It does not matter to the player whether the content comes from a file or is being sent live  and usually the player will not be able to tell (except that it is not possible to skip forward with a live stream)
Given the similarity of mechanism  much of our previous discussion applies  but there are also some key differences
Importantly  there is still the need for buffering at the client side to smooth out jitter
In fact  a larger amount of buffering is often needed for live streaming (independent of the consideration that the user may pause playback)
When streaming from a file  the media can be pushed out at a rate that is greater than the playback rate
This will build up a buffer quickly to compensate for network jitter (and the player will stop the stream if it does not want to buffer more data)
In contrast  live media streaming is always transmitted at precisely the rate it is THE APPLICATION LAYER
generated  which is the same as the rate at which it is played back
It cannot be sent faster than this
As a consequence  the buffer must be large enough to handle the full range of network jitter
In practice  a  â  ond startup delay is usually adequate  so this is not a large problem
The other important difference is that live streaming events usually have hundreds or thousands of simultaneous viewers of the same content
Under these circumstances  the natural solution for live streaming is to use multicasting
This is not the case for streaming stored media because the users typically stream different content at any given time
Streaming to many users then consists of many individual streaming sessions that happen to occur at the same time
A multicast streaming scheme works as follows
The server sends each media packet once using IP multicast to a group address
The network delivers a copy of the packet to each member of the group
All of the clients who want to receive the stream have joined the group
The clients do this using IGMP  rather than sending an RTSP message to the media server
This is because the media server is already sending the live stream (except before the first user joins)
What is needed is to arrange for the stream to be received locally
Since multicast is a one-to-many delivery service  the media is carried in RTP packets over a UDP transport
TCP only operates between a single sender and a single receiver
Since UDP does not provide reliability  some packets may be lost
To reduce the level of media loss to an acceptable level  we can use FEC and interleaving  as before
In the case of FEC  there is a beneficial interaction with multicast that is shown in the parity example of Fig
When the packets are multicast  different clients may lose different packets
For example  client  has lost packet B  client  lost the parity packet P  client  lost D  and client  did not lose any packets
However  even though three different packets are lost across the clients  each client can recover all of the data packets in this example
All that is required is that each client lose no more than one packet  whichever one it may be  so that the missing packet can be recovered by a parity computation
Nonnenmacher et al
(   ) describe how this idea can be used to boost reliability
For a server with a large number of clients  multicast of media in RTP and UDP packets is clearly the most efficient way to operate
Otherwise  the server must transmit N streams when it has N clients  which will require a very large amount of network bandwidth at the server for large streaming events
It may surprise you to learn that the Internet does not work like this in practice
What usually happens is that each user establishes a separate TCP connection to the server  and the media is streamed over that connection
To the client  this is the same as streaming stored media
And as with streaming stored media  there are several reasons for this seemingly poor choice
The first reason is that IP multicast is not broadly available on the Internet
Some ISPs and networks support it internally  but it is usually not available across network boundaries as is needed for wide-area streaming
The other reasons are   STREAMING AUDIO AND VIDEO Different packets lost Client  Server P D C B A Multicast Parity packet RTP/UDP data packet Client  Client  Client  P D C B A P D C B A P D C B A P D C B A Figure  -
Multicast streaming media with a parity packet
the same advantages of TCP over UDP as discussed earlier
Streaming with TCP will reach nearly all clients on the Internet  particularly when disguised as HTTP to pass through firewalls  and reliable media delivery allows users to rewind easily
There is one important case in which UDP and multicast can be used for streaming  however: within a provider network
For example  a cable company might decide to broadcast TV channels to customer set-top boxes using IP technology instead of traditional video broadcasts
The use of IP to distribute broadcast video is broadly called IPTV  as discussed above
Since the cable company has complete control of its own network  it can engineer it to support IP multicast and have sufficient bandwidth for UDP-based distribution
All of this is invisible to the customer  as the IP technology exists within the walled garden of the provider
It looks just like cable TV in terms of service  but it is IP underneath  with the set-top box being a computer running UDP and the TV set being simply a monitor attached to the computer
Back to the Internet case  the disadvantage of live streaming over TCP is that the server must send a separate copy of the media for each client
This is feasible for a moderate number of clients  especially for audio
The trick is to place the server at a location with good Internet connectivity so that there is sufficient bandwidth
Usually this means renting a server in a data center from a hosting provider  not using a server at home with only broadband Internet connectivity
There is a very competitive hosting market  so this need not be expensive
In fact  it is easy for anybody  even a student  to set up and operate a streaming media server such as an Internet radio station
The main components of this THE APPLICATION LAYER
station are illustrated in Fig
The basis of the station is an ordinary PC with a decent sound card and microphone
Popular software is used to capture audio and encode it in various formats  for example  MP  and media players are used to listen to the audio as usual
Studentâs PC Microphone Media Audio player capture plug-in Codec plug-in TCP connections to listeners Media server Internet Figure  -
A student radio station
The audio stream captured on the PC is then fed over the Internet to a media server with good network connectivity  either as podcasts for stored file streaming or for live streaming
The server handles the task of distributing the media via large numbers of TCP connections
It also presents a front-end Web site with pages about the station and links to the content that is available for streaming
There are commercial software packages for managing all the pieces  as well as open source packages such as icecast
However  for a very large number of clients  it becomes infeasible to use TCP to send media to each client from a single server
There is simply not enough bandwidth to the one server
For large streaming sites  the streaming is done using a set of servers that are geographically spread out  so that a client can connect to the nearest server
This is a content distribution network that we will study at the end of the  ter
Real-Time Conferencing Once upon a time  voice calls were carried over the public switched telephone network  and network traffic was primarily voice traffic  with a little bit of data traffic here and there
Then came the Internet  and the Web
The data traffic grew and grew  until by  there was as much data traffic as voice traffic (since voice is now digitized  both can be measured in bits)
By  the volume of data traffic was an order of magnitude more than the volume of voice traffic and still growing exponentially  with voice traffic staying almost flat
The consequence of this growth has been to flip the telephone network on its head
Voice traffic is now carried using Internet technologies  and represents only   STREAMING AUDIO AND VIDEO a tiny fraction of the network bandwidth
This disruptive technology is known as voice over IP  and also as Internet telephony
Voice-over-IP is used in several forms that are driven by strong economic factors
(English translation: it saves money so people use it
) One form is to have what look like regular (old-fashioned?) telephones that plug into the Ethernet and send calls over the network
Pehr Anderson was an undergraduate student at
when he and his friends prototyped this design for a class project
They got a ââBââ grade
Not content  he started a company called NBX in  pioneered this kind of voice over IP  and sold it to  Com for $  million three years later
Companies love this approach because it lets them do away with separate telephone lines and make do with the networks that they have already
Another approach is to use IP technology to build a long-distance telephone network
In countries such as the
this network can be accessed for competitive long-distance service by dialing a special prefix
Voice samples are put into packets that are injected into the network and pulled out of the packets when they leave it
Since IP equipment is much cheaper than telecommunications equipment this leads to cheaper services
As an aside  the difference in price is not entirely technical
For many decades  telephone service was a regulated monopoly that guaranteed the phone companies a fixed percentage profit over their costs
Not surprisingly  this led them to run up costs  for example  by having lots and lots of redundant hardware  justified in the name of better reliability (the telephone system was only allowed to be down for a total of  hours every   years  or  min/year on average)
This effect was often referred to as the ââgold-plated telephone pole syndrome
ââ Since deregulation  the effect has decreased  of course  but legacy equipment still exists
The IT industry never had any history operating like this  so it has always been lean and mean
However  we will concentrate on the form of voice over IP that is likely the most visible to users: using one computer to call another computer
This form became commonplace as PCs began shipping with microphones  speakers  cameras  and CPUs fast enough to process media  and people started connecting to the Internet from home at broadband rates
A well-known example is the Skype software that was released starting in
Skype and other companies also provide gateways to make it easy to call regular telephone numbers as well as computers with IP addresses
As network bandwidth increased  video calls joined voice calls
Initially  video calls were in the domain of companies
Videoconferencing systems were designed to exchange video between two or more locations enabling executives at different locations to see each other while they held their meetings
However  with good broadband Internet connectivity and video compression software  home users can also videoconference
Tools such as Skype that started as audio-only now routinely include video with the calls so that friends and family across the world can see as well as hear each other
THE APPLICATION LAYER
From our point of view  Internet voice or video calls are also a media streaming problem  but one that is much more constrained than streaming a stored file or a live event
The added constraint is the low latency that is needed for a two-way conversation
The telephone network allows a one-way latency of up to m for acceptable usage  after which delay begins to be perceived as annoying by the participants
(International calls may have a latency of up to m  by which point they are far from a positive user experience
) This low latency is difficult to achieve
Certainly  buffering  â  onds of media is not going to work (as it would for broadcasting a live sports event)
Instead  video and voice-over-IP systems must be engineered with a variety of techniques to minimize latency
This goal means starting with UDP as the clear choice rather than TCP  because TCP retransmissions introduce at least one round-trip worth of delay
Some forms of latency cannot be reduced  however  even with UDP
For example  the distance between Seattle and Amsterdam is close to  km
The speed-of-light propagation delay for this distance in optical fiber is   m
Good luck beating that
In practice  the propagation delay through the network will be longer because it will cover a larger distance (the bits do not follow a great circle route) and have transmission delays as each IP router stores and forwards a packet
This fixed delay eats into the acceptable delay budget
Another source of latency is related to packet size
Normally  large packets are the best way to use network bandwidth because they are more efficient
However  at an audio sampling rate of   kbps  a  -KB packet would take m to fill (and even longer if the samples are compressed)
This delay would consume most of the overall delay budget
In addition  if the  -KB packet is sent over a broadband access link that runs at just  Mbps  it will take  m to transmit
Then add another  m for the packet to go over the broadband link at the other end
Clearly  large packets will not work
Instead  voice-over-IP systems use short packets to reduce latency at the cost of bandwidth efficiency
They batch audio samples in smaller units  commonly   m
At   kbps  this is bytes of data  less with compression
However  by definition the delay from this packetization will be   m
The transmission delay will be smaller as well because the packet is shorter
In our example  it would reduce to around  m
By using short packets  the minimum one-way delay for a Seattle-to-Amsterdam packet has been reduced from an unacceptable m (  + +  ) to an acceptable   m (  +   +  )
We have not even talked about the software overhead  but it  too  will eat up some of the delay budget
This is especially true for video  since compression is usually needed to fit video into the available bandwidth
Unlike streaming from a stored file  there is no time to have a computationally intensive encoder for high levels of compression
The encoder and the decoder must both run quickly
Buffering is still needed to play out the media samples on time (to avoid unintelligible audio or jerky video)  but the amount of buffering must be kept very small since the time remaining in our delay budget is measured in millionds
STREAMING AUDIO AND VIDEO When a packet takes too long to arrive  the player will skip over the missing samples  perhaps playing ambient noise or repeating a frame to mask the loss to the user
There is a trade-off between the size of the buffer used to handle jitter and the amount of media that is lost
A smaller buffer reduces latency but results in more loss due to jitter
Eventually  as the size of the buffer shrinks  the loss will become noticeable to the user
Observant readers may have noticed that we have said nothing about the network layer protocols so far in this tion
The network can reduce latency  or at least jitter  by using quality of service mechanisms
The reason that this issue has not come up before is that streaming is able to operate with substantial latency  even in the live streaming case
If latency is not a major concern  a buffer at the end host is sufficient to handle the problem of jitter
However  for real-time conferencing  it is usually important to have the network reduce delay and jitter to help meet the delay budget
The only time that it is not important is when there is so much network bandwidth that everyone gets good service
we described two quality of service mechanisms that help with this goal
One mechanism is DS (Differentiated Services)  in which packets are marked as belonging to different classes that receive different handling within the network
The appropriate marking for voice-over-IP packets is low delay
In practice  systems set the DS codepoint to the well-known value for the Expedited Forwarding class with Low Delay type of service
This is especially useful over broadband access links  as these links tend to be congested when Web traffic or other traffic competes for use of the link
Given a stable network path  delay and jitter are increased by congestion
Every  -KB packet takes  m to send over a  -Mbps link  and a voice-over-IP packet will incur these delays if it is sitting in a queue behind Web traffic
However  with a low delay marking the voice-over-IP packets will jump to the head of the queue  bypassing the Web packets and lowering their delay
The ond mechanism that can reduce delay is to make sure that there is sufficient bandwidth
If the available bandwidth varies or the transmission rate fluctuates (as with compressed video) and there is sometimes not sufficient bandwidth  queues will build up and add to the delay
This will occur even with DS
To ensure sufficient bandwidth  a reservation can be made with the network
This capability is provided by integrated services
Unfortunately  it is not widely deployed
Instead  networks are engineered for an expected traffic level or network customers are provided with service-level agreements for a given traffic level
Applications must operate below this level to avoid causing congestion and introducing unnecessary delays
For casual videoconferencing at home  the user may choose a video quality as a proxy for bandwidth needs  or the software may test the network path and select an appropriate quality automatically
Any of the above factors can cause the latency to become unacceptable  so real-time conferencing requires that attention be paid to all of them
For an overview of voice over IP and analysis of these factors  see Goode (   )
THE APPLICATION LAYER
Now that we have discussed the problem of latency in the media streaming path  we will move on to the other main problem that conferencing systems must address
This problem is how to set up and tear down calls
We will look at two protocols that are widely used for this purpose  H
Skype is another important system  but its inner workings are proprietary
One thing that was clear to everyone before voice and video calls were made over the Internet was that if each vendor designed its own protocol stack  the system would never work
To avoid this problem  a number of interested parties got together under ITU auspices to work out standards
In  ITU issued recommendation H
entitled ââVisual Telephone Systems and Equipment for Local Area Networks Which Provide a Non-Guaranteed Quality of Service
ââ Only the telephone industry would think of such a name
It was quickly changed to ââPacket- based Multimedia Communications Systemsââ in the  revision
was the basis for the first widespread Internet conferencing systems
It remains the most widely deployed solution  in its seventh version as of
is more of an architectural overview of Internet telephony than a specific protocol
It references a large number of specific protocols for speech coding  call setup  signaling  data transport  and other areas rather than specifying these things itself
The general model is depicted in Fig
At the center is a gateway that connects the Internet to the telephone network
It speaks the H
protocols on the Internet side and the PSTN protocols on the telephone side
The communicating devices are called terminals
A LAN may have a gatekeeper  which controls the end points under its jurisdiction  called a zone
Internet Gatekeeper Telephone network Zone Terminal Gateway Figure  -
architectural model for Internet telephony
A telephone network needs a number of protocols
To start with  there is a protocol for encoding and decoding audio and video
Standard telephony representations of a single voice channel as   kbps of digital audio ( samples of  bits per ond) are defined in ITU recommendation G All H
systems   STREAMING AUDIO AND VIDEO must support G Other encodings that compress speech are permitted  but not required
They use different compression algorithms and make different tradeoffs between quality and bandwidth
For video  the MPEG forms of video compression that we described above are supported  including H Since multiple compression algorithms are permitted  a protocol is needed to allow the terminals to negotiate which one they are going to use
This protocol is called H It also negotiates other aspects of the connection such as the bit rate
RTCP is need for the control of the RTP channels
Also required is a protocol for establishing and releasing connections  providing dial tones  making ringing sounds  and the rest of the standard telephony
is used here
The terminals need a protocol for talking to the gatekeeper (if present) as well
For this purpose  H
The PC-to-gatekeeper channel it manages is called the RAS (Registration/Admission/Status ) channel
This channel allows terminals to join and leave the zone  request and return bandwidth  and provide status updates  among other things
Finally  a protocol is needed for the actual data transmission
RTP over UDP is used for this purpose
It is managed by RTCP  as usual
The positioning of all these protocols is shown in Fig
Link layer protocol IP Audio G
xx RTP Physical layer protocol UDP TCP Video H
(Signaling) H
(Call Control) Control Figure  -
protocol stack
To see how these protocols fit together  consider the case of a PC terminal on a LAN (with a gatekeeper) calling a remote telephone
The PC first has to discover the gatekeeper  so it broadcasts a UDP gatekeeper discovery packet to port
When the gatekeeper responds  the PC learns the gatekeeperâs IP address
Now the PC registers with the gatekeeper by sending it a RAS message in a UDP packet
After it has been accepted  the PC sends the gatekeeper a RAS admission message requesting bandwidth
Only after bandwidth has been granted may call setup begin
The idea of requesting bandwidth in advance is to allow the gatekeeper to limit the number of calls
It can then avoid oversubscribing the outgoing line in order to help provide the necessary quality of service
THE APPLICATION LAYER
As an aside  the telephone system does the same thing
When you pick up the receiver  a signal is sent to the local end office
If the office has enough spare capacity for another call  it generates a dial tone
If not  you hear nothing
Nowadays  the system is so overdimensioned that the dial tone is nearly always instantaneous  but in the early days of telephony  it often took a few onds
So if your grandchildren ever ask you ââWhy are there dial tones?ââ now you know
Except by then  probably telephones will no longer exist
The PC now establishes a TCP connection to the gatekeeper to begin call setup
Call setup uses existing telephone network protocols  which are connection oriented  so TCP is needed
In contrast  the telephone system has nothing like RAS to allow telephones to announce their presence  so the H
designers were free to use either UDP or TCP for RAS  and they chose the lower-overhead UDP
Now that it has bandwidth allocated  the PC can send a Q
SETUP message over the TCP connection
This message specifies the number of the telephone being called (or the IP address and port  if a computer is being called)
The gatekeeper responds with a Q
CALL PROCEEDING message to acknowledge correct receipt of the request
The gatekeeper then forwards the SETUP message to the gateway
The gateway  which is half computer  half telephone switch  then makes an ordinary telephone call to the desired (ordinary) telephone
The end office to which the telephone is attached rings the called telephone and also sends back a Q
ALERT message to tell the calling PC that ringing has begun
When the person at the other end picks up the telephone  the end office sends back a Q
CONNECT message to signal the PC that it has a connection
Once the connection has been established  the gatekeeper is no longer in the loop  although the gateway is  of course
Subsequent packets bypass the gatekeeper and go directly to the gatewayâs IP address
At this point  we just have a bare tube running between the two parties
This is just a physical layer connection for moving bits  no more
Neither side knows anything about the other one
protocol is now used to negotiate the parameters of the call
It uses the H
control channel  which is always open
Each side starts out by announcing its capabilities  for example  whether it can handle video (H
can handle video) or conference calls  which codecs it supports  etc
Once each side knows what the other one can handle  two unidirectional data channels are set up and a codec and other parameters are assigned to each one
Since each side may have different equipment  it is entirely possible that the codecs on the forward and reverse channels are different
After all negotiations are complete  data flow can begin using RTP
It is managed using RTCP  which plays a role in congestion control
If video is present  RTCP handles the audio/video synchronization
The various channels are shown in Fig
When either party hangs up  the Q
call signaling channel is used to tear down the connection after the call has been completed in order to free up resources no longer needed
STREAMING AUDIO AND VIDEO Data control channel (RTCP) Reverse data channel (RTP) Forward data channel (RTP) Call control channel (H
) Call signaling channel (Q
) Caller Callee Figure  -
Logical channels between the caller and callee during a call
When the call is terminated  the calling PC contacts the gatekeeper again with a RAS message to release the bandwidth it has been assigned
Alternatively  it can make another call
We have not said anything about quality of service as part of H
even though we have said it is an important part of making real-time conferencing a success
The reason is that QoS falls outside the scope of H If the underlying network is capable of producing a stable  jitter-free connection from the calling PC to the gateway  the QoS on the call will be good; otherwise  it will not be
However  any portion of the call on the telephone side will be jitter-free  because that is how the telephone network is designed
SIPâThe Session Initiation Protocol H
was designed by ITU
Many people in the Internet community saw it as a typical telco product: large  complex  and inflexible
Consequently  IETF set up a committee to design a simpler and more modular way to do voice over IP
The major result to date is SIP (Session Initiation Protocol)
The latest version is described in RFC  which was written in
This protocol describes how to set up Internet telephone calls  video conferences  and other multimedia connections
which is a complete protocol suite  SIP is a single module  but it has been designed to interwork well with existing Internet applications
For example  it defines telephone numbers as URLs  so that Web pages can contain them  allowing a click on a link to initiate a telephone call (the same way the mailto scheme allows a click on a link to bring up a program to send an email message)
SIP can establish two-party sessions (ordinary telephone calls)  multiparty sessions (where everyone can hear and speak)  and multicast sessions (one sender  many receivers)
The sessions may contain audio  video  or data  the latter being useful for multiplayer real-time games  for example
SIP just handles setup  management  and termination of sessions
Other protocols  such as RTP/RTCP  are THE APPLICATION LAYER
also used for data transport
SIP is an application-layer protocol and can run over UDP or TCP  as required
SIP supports a variety of services  including locating the callee (who may not be at his home machine) and determining the calleeâs capabilities  as well as handling the mechanics of call setup and termination
In the simplest case  SIP sets up a session from the callerâs computer to the calleeâs computer  so we will examine that case first
Telephone numbers in SIP are represented as URLs using the sip scheme  for example  sip:ilse@  for a user named Ilse at the host specified by the DNS name
SIP URLs may also contain IPv  addresses  IPv  addresses  or actual telephone numbers
The SIP protocol is a text-based protocol modeled on HTTP
One party sends a message in ASCII text consisting of a method name on the first line  followed by additional lines containing headers for passing parameters
Many of the headers are taken from MIME to allow SIP to interwork with existing Internet applications
The six methods defined by the core specification are listed in Fig
Method Description INVITE Request initiation of a session ACK Confirm that a session has been initiated BYE Request termination of a session OPTIONS Query a host about its capabilities CANCEL Cancel a pending request REGISTER Inform a redirection server about the userâs current location Figure  -
SIP methods
To establish a session  the caller either creates a TCP connection with the callee and sends an INVITE message over it or sends the INVITE message in a UDP packet
In both cases  the headers on the ond and subsequent lines describe the structure of the message body  which contains the callerâs capabilities  media types  and formats
If the callee accepts the call  it responds with an HTTP-type reply code (a three-digit number using the groups of Fig
-   for acceptance)
Following the reply-code line  the callee also may supply information about its capabilities  media types  and formats
Connection is done using a three-way handshake  so the caller responds with an ACK message to finish the protocol and confirm receipt of the message
Either party may request termination of a session by sending a message with the BYE method
When the other side acknowledges it  the session is terminated
The OPTIONS method is used to query a machine about its own capabilities
It is typically used before a session is initiated to find out if that machine is even capable of voice over IP or whatever type of session is being contemplated
STREAMING AUDIO AND VIDEO The REGISTER method relates to SIPâs ability to track down and connect to a user who is away from home
This message is sent to a SIP location server that keeps track of who is where
That server can later be queried to find the userâs current location
The operation of redirection is illustrated in Fig
Here  the caller sends the INVITE message to a proxy server to hide the possible redirection
The proxy then looks up where the user is and sends the INVITE message there
It then acts as a relay for the subsequent messages in the three-way handshake
The LOOKUP and REPLY messages are not part of SIP; any convenient protocol can be used  depending on what kind of location server is used
OK  OK  INVITE  LOOKUP  REPLY  INVITE  ACK  ACK Caller Callee Location server Proxy  Data Figure  -
Use of a proxy server and redirection with SIP
SIP has a variety of other features that we will not describe here  including call waiting  call screening  encryption  and authentication
It also has the ability to place calls from a computer to an ordinary telephone  if a suitable gateway between the Internet and telephone system is available
Comparison of H
and SIP Both H
and SIP allow two-party and multiparty calls using both computers and telephones as end points
Both support parameter negotiation  encryption  and the RTP/RTCP protocols
A summary of their similarities and differences is given in Fig
Although the feature sets are similar  the two protocols differ widely in philosophy
is a typical  heavyweight  telephone-industry standard  specifying the complete protocol stack and defining precisely what is allowed and what is forbidden
This approach leads to very well defined protocols in each layer  easing the task of interoperability
The price paid is a large  complex  and rigid standard that is difficult to adapt to future applications
In contrast  SIP is a typical Internet protocol that works by exchanging short lines of ASCII text
It is a lightweight module that interworks well with other Internet protocols but less well with existing telephone system signaling protocols
THE APPLICATION LAYER
SIP Designed by ITU IETF Compatibility with PSTN Yes Largely Compatibility with Internet Yes  over time Yes Architecture Monolithic Modular Completeness Full protocol stack SIP just handles setup Parameter negotiation Yes Yes Call signaling Q
over TCP SIP over TCP or UDP Message format Binary ASCII Media transport RTP/RTCP RTP/RTCP Multiparty calls Yes Yes Multimedia conferences Yes No Addressing URL or phone number URL Call termination Explicit or TCP release Explicit or timeout Instant messaging No Yes Encryption Yes Yes Size of standards  pages pages Implementation Large and complex Moderate  but issues Status Widespread  esp
video Alternative  esp
voice Figure  -
Comparison of H
Because the IETF model of voice over IP is highly modular  it is flexible and can be adapted to new applications easily
The downside is that is has suffered from ongoing interoperability problems as people try to interpret what the standard means  CONTENT DELIVERY The Internet used to be all about communication  like the telephone network
Early on  academics would communicate with remote machines  logging in over the network to perform tasks
People have used email to communicate with each other for a long time  and now use video and voice over IP as well
Since the Web grew up  however  the Internet has become more about content than communication
Many people use the Web to find information  and there is a tremendous amount of peer-to-peer file sharing that is driven by access to movies  music  and programs
The switch to content has been so pronounced that the majority of Internet bandwidth is now used to deliver stored videos
CONTENT DELIVERY Because the task of distributing content is different from that of communication  it places different requirements on the network
For example  if Sally wants to talk to Jitu  she may make a voice-over-IP call to his mobile
The communication must be with a particular computer; it will do no good to call Paulâs computer
But if Jitu wants to watch his teamâs latest cricket match  he is happy to stream video from whichever computer can provide the service
He does not mind whether the computer is Sallyâs or Paulâs  or  more likely  an unknown server in the Internet
That is  location does not matter for content  except as it affects performance (and legality)
The other difference is that some Web sites that provide content have become tremendously popular
YouTube is a prime example
It allows users to share videos of their own creation on every conceivable topic
Many people want to do this
The rest of us want to watch
With all of these bandwidth-hungry videos  it is estimated that YouTube accounts for up to  % of Internet traffic today
No single server is powerful or reliable enough to handle such a startling level of demand
Instead  YouTube and other large content providers build their own content distribution networks
These networks use data centers spread around the world to serve content to an extremely large number of clients with good performance and availability
The techniques that are used for content distribution have been developed over time
Early in the growth of the Web  its popularity was almost its undoing
More demands for content led to servers and networks that were frequently overloaded
Many people began to call the WWW the World Wide Wait
In response to consumer demand  very large amounts of bandwidth were provisioned in the core of the Internet  and faster broadband connectivity was rolled out at the edge of the network
This bandwidth was key to improving performance  but it is only part of the solution
To reduce the endless delays  researchers also developed different architectures to use the bandwidth for distributing content
One architecture is a CDN (Content Distribution Network)
In it  a provider sets up a distributed collection of machines at locations inside the Internet and uses them to serve content to clients
This is the choice of the big players
An alternative architecture is a P P (Peer-to-Peer) network
In it  a collection of computers pool their resources to serve content to each other  without separately provisioned servers or any central point of control
This idea has captured peopleâs imagination because  by acting together  many little players can pack an enormous punch
In this tion  we will look at the problem of distributing content on the Internet and some of the solutions that are used in practice
After briefly discussing content popularity and Internet traffic  we will describe how to build powerful Web servers and use caching to improve performance for Web clients
Then we will come to the two main architectures for distributing content: CDNs and P P networks
There design and properties are quite different  as we will see
THE APPLICATION LAYER
Content and Internet Traffic To design and engineer networks that work well  we need an understanding of the traffic that they must carry
With the shift to content  for example  servers have migrated from company offices to Internet data centers that provide large numbers of machines with excellent network connectivity
To run even a small server nowadays  it is easier and cheaper to rent a virtual server hosted in an Internet data center than to operate a real machine in a home or office with broadband connectivity to the Internet
Fortunately  there are only two facts about Internet traffic that is it essential to know
The first fact is that it changes quickly  not only in the details but in the overall makeup
Before  most traffic was traditional FTP file transfer (for moving programs and data sets between computers) and email
Then the Web arrived and grew exponentially
Web traffic left FTP and email traffic in the dust long before the dot com bubble of
Starting around  P P file sharing for music and then movies took off
By  most Internet traffic was P P traffic  leaving the Web in the dust
Sometime in the late s  video streamed using content distribution methods by sites like YouTube began to exceed P P traffic
By  Cisco predicts that  % of all Internet traffic will be video in one form or another (Cisco  )
It is not always traffic volume that matters
For instance  while voice-over-IP traffic boomed even before Skype started in  it will always be a minor blip on the chart because the bandwidth requirements of audio are two orders of magnitude lower than for video
However  voice-over-IP traffic stresses the network in other ways because it is sensitive to latency
As another example  online social networks have grown furiously since Facebook started in
In  for the first time  Facebook reached more users on the Web per day than Google
Even putting the traffic aside (and there is an awful lot of traffic)  online social networks are important because they are changing the way that people interact via the Internet
The point we are making is that seismic shifts in Internet traffic happen quickly  and with some regularity
What will come next? Please check back in the  th edition of this book and we will let you know
The ond essential fact about Internet traffic is that it is highly skewed
Many properties with which we are familiar are clustered around an average
For instance  most adults are close to the average height
There are some tall people and some short people  but few very tall or very short people
For these kinds of properties  it is possible to design for a range that is not very large but nonetheless captures the majority of the population
Internet traffic is not like this
For a long time  it has been known that there are a small number of Web sites with massive traffic and a vast number of Web site with much smaller traffic
This feature has become part of the language of networking
Early papers talked about traffic in terms of packet trains  the idea   CONTENT DELIVERY being that express trains with a large number of packets would suddenly travel down a link (Jain and Routhier  )
This was formalized as the notion of selfsimilarity  which for our purposes can be thought of as network traffic that exhibits many short and many long gaps even when viewed at different time scales (Leland et al
Later work spoke of long traffic flows as elephants and short traffic flows as mice
The idea is that there are only a few elephants and many mice  but the elephants matter because they are so big
Returning to Web content  the same sort of skew is evident
Experience with video rental stores  public libraries  and other such organizations shows that not all items are equally popular
Experimentally  when N movies are available  the fraction of all requests for the kth most popular one is approximately C /k
Here  C is computed to normalize the sum to   namely  C =  /(  +  /  +  /  +  /  +  /  +
+  /N) Thus  the most popular movie is seven times as popular as the number seven movie
This result is known as Zipfâs law (Zipf  )
It is named after George Zipf  a professor of linguistics at Harvard University who noted that the frequency of a wordâs usage in a large body of text is inversely proportional to its rank
For example  the  th most common word is used twice as much as the  th most common word and three times as much as the   th most common word
A Zipf distribution is shown in Fig
It captures the notion that there are a small number of popular items and a great many unpopular items
To recognize distributions of this form  it is convenient to plot the data on a log scale on both axes  as shown in Fig
The result should be a straight line
(a)  Relative Frequency Rank   Relative Frequency Rank â   â    (b) Figure  -
Zipf distribution (a) On a linear scale
(b) On a log-log scale
When people looked at the popularity of Web pages  it also turned out to roughly follow Zipfâs law (Breslau et al
A Zipf distribution is one example in a family of distributions known as power laws
Power laws are evident THE APPLICATION LAYER
in many human phenomena  such as the distribution of city populations and of wealth
They have the same propensity to describe a few large players and a great many smaller players  and they too appear as a straight line on a log-log plot
It was soon discovered that the topology of the Internet could be roughly described with power laws (Faloutsos et al
Next  researchers began plotting every imaginable property of the Internet on a log scale  observing a straight line  and shouting: ââPower law!ââ However  what matters more than a straight line on a log-log plot is what these distributions mean for the design and use of networks
Given the many forms of content that have Zipf or power law distributions  it seems fundamental that Web sites on the Internet are Zipf-like in popularity
This in turn means that an average site is not a useful representation
Sites are better described as either popular or unpopular
Both kinds of sites matter
The popular sites obviously matter  since a few popular sites may be responsible for most of the traffic on the Internet
Perhaps surprisingly  the unpopular sites can matter too
This is because the total amount of traffic directed to the unpopular sites can add up to a large fraction of the overall traffic
The reason is that there are so many unpopular sites
The notion that  collectively  many unpopular choices can matter has been popularized by books such as The Long Tail (Anderson  a)
Curves showing decay like that of Fig
In particular  situations in which the rate of decay is proportional to how much material is left (such as with unstable radioactive atoms) exhibit exponential decay  which drops off much faster than Zipfâs Law
The number of items  say atoms  left after time t is usually expressed as e ât /Î±  where the constant Î± determines how fast the decay is
The difference between exponential decay and Zipfâs Law is that with exponential decay  it is safe to ignore the end of tail but with Zipfâs Law the total weight of the tail is significant and cannot be ignored
To work effectively in this skewed world  we must be able to build both kinds of Web sites
Unpopular sites are easy to handle
By using DNS  many different sites may actually point to the same computer in the Internet that runs all of the sites
On the other hand  popular sites are difficult to handle
There is no single computer even remotely powerful enough  and using a single computer would make the site inaccessible for millions of users if it fails
To handle these sites  we must build content distribution systems
We will start on that quest next
Server Farms and Web Proxies The Web designs that we have seen so far have a single server machine talking to multiple client machines
To build large Web sites that perform well  we can speed up processing on either the server side or the client side
On the server side  more powerful Web servers can be built with a server farm  in which a cluster of computers acts as a single server
On the client side  better performance can   CONTENT DELIVERY be achieved with better caching techniques
In particular  proxy caches provide a large shared cache for a group of clients
We will describe each of these techniques in turn
However  note that neither technique is sufficient to build the largest Web sites
Those popular sites require the content distribution methods that we describe in the following tions  which combine computers at many different locations
Server Farms No matter how much bandwidth one machine has  it can only serve so many Web requests before the load is too great
The solution in this case is to use more than one computer to make a Web server
This leads to the server farm model of Fig
Front end Backend database Internet access Clients Server farm Servers Balances load across servers Figure  -
A server farm
The difficulty with this seemingly simple model is that the set of computers that make up the server farm must look like a single logical Web site to clients
If they do not  we have just set up different Web sites that run in parallel
There are several possible solutions to make the set of servers appear to be one Web site
All of the solutions assume that any of the servers can handle a request from any client
To do this  each server must have a copy of the Web site
The servers are shown as connected to a common back-end database by a dashed line for this purpose
One solution is to use DNS to spread the requests across the servers in the server farm
When a DNS request is made for the Web URL  the DNS server returns a rotating list of the IP addresses of the servers
Each client tries one IP address  typically the first on the list
The effect is that different clients contact different servers to access the same Web site  just as intended
The DNS method is at the heart of CDNs  and we will revisit it later in this tion
The other solutions are based on a front end that sprays incoming requests over the pool of servers in the server farm
This happens even when the client THE APPLICATION LAYER
contacts the server farm using a single destination IP address
The front end is usually a link-layer switch or an IP router  that is  a device that handles frames or packets
All of the solutions are based on it (or the servers) peeking at the network  transport  or application layer headers and using them in nonstandard ways
A Web request and response are carried as a TCP connection
To work correctly  the front end must distribute all of the packets for a request to the same server
A simple design is for the front end to broadcast all of the incoming requests to all of the servers
Each server answers only a fraction of the requests by prior agreement
For example servers might look at the source IP address and reply to the request only if the last  bits of the source IP address match their configured selectors
Other packets are discarded
While this is wasteful of incoming bandwidth  often the responses are much longer than the request  so it is not nearly as inefficient as it sounds
In a more general design  the front end may inspect the IP  TCP  and HTTP headers of packets and arbitrarily map them to a server
The mapping is called a load balancing policy as the goal is to balance the workload across the servers
The policy may be simple or complex
A simple policy might be to use the servers one after the other in turn  or round-robin
With this approach  the front end must remember the mapping for each request so that subsequent packets that are part of the same request will be sent to the same server
Also  to make the site more reliable than a single server  the front end should notice when servers have failed and stop sending them requests
Much like NAT  this general design is perilous  or at least fragile  in that we have just created a device that violates the most basic principle of layered protocols: each layer must use its own header for control purposes and may not inspect and use information from the payload for any purpose
But people design such systems anyway and when they break in the future due to changes in higher layers  they tend to be surprised
The front end in this case is a switch or router  but it may take action based on transport layer information or higher
Such a box is called a middlebox because it interposes itself in the middle of a network path in which it has no business  according to the protocol stack
In this case  the front end is best considered an internal part of a server farm that terminates all layers up to the application layer (and hence can use all of the header information for those layers)
Nonetheless  as with NAT  this design is useful in practice
The reason for looking at TCP headers is that it is possible to do a better job of load balancing than with IP information alone
For example  one IP address may represent an entire company and make many requests
It is only by looking at TCP or higherlayer information that these requests can be mapped to different servers
The reason for looking at the HTTP headers is somewhat different
Many Web interactions access and update databases  such as when a customer looks up her most recent purchase
The server that fields this request will have to query the back-end database
It is useful to direct subsequent requests from the same user to   CONTENT DELIVERY the same server  because that server has already cached information about the user
The simplest way to cause this to happen is to use Web cookies (or other information to distinguish the user) and to inspect the HTTP headers to find the cookies
As a final note  although we have described this design for Web sites  a server farm can be built for other kinds of servers as well
An example is servers streaming media over UDP
The only change that is required is for the front end to be able to load balance these requests (which will have different protocol header fields than Web requests)
Web Proxies Web requests and responses are sent using HTTP
In   we described how browsers can cache responses and reuse them to answer future requests
Various header fields and rules are used by the browser to determine if a cached copy of a Web page is still fresh
We will not repeat that material here
Caching improves performance by shortening the response time and reducing the network load
If the browser can determine that a cached page is fresh by itself  the page can be fetched from the cache immediately  with no network traffic at all
However  even if the browser must ask the server for confirmation that the page is still fresh  the response time is shortened and the network load is reduced  especially for large pages  since only a small message needs to be sent
However  the best the browser can do is to cache all of the Web pages that the user has previously visited
From our discussion of popularity  you may recall that as well as a few popular pages that many people visit repeatedly  there are many  many unpopular pages
In practice  this limits the effectiveness of browser caching because there are a large number of pages that are visited just once by a given user
These pages always have to be fetched from the server
One strategy to make caches more effective is to share the cache among multiple users
That way  a page already fetched for one user can be returned to another user when that user makes the same request
Without browser caching  both users would need to fetch the page from the server
Of course  this sharing cannot be done for encrypted traffic  pages that require authentication  and uncacheable pages (
current stock prices) that are returned by programs
Dynamic pages created by programs  especially  are a growing case for which caching is not effective
Nonetheless  there are plenty of Web pages that are visible to many users and look the same no matter which user makes the request (
A Web proxy is used to share a cache among users
A proxy is an agent that acts on behalf of someone else  such as the user
There are many kinds of proxies
For instance  an ARP proxy replies to ARP requests on behalf of a user who is elsewhere (and cannot reply for himself)
A Web proxy fetches Web requests on behalf of its users
It normally provides caching of the Web responses  and since it is shared across users it has a substantially larger cache than a browser
THE APPLICATION LAYER
When a proxy is used  the typical setup is for an organization to operate one Web proxy for all of its users
The organization might be a company or an ISP
Both stand to benefit by speeding up Web requests for its users and reducing its bandwidth needs
While flat pricing  independent of usage  is common for end users  most companies and ISPs are charged according to the bandwidth that they use
This setup is shown in Fig
To use the proxy  each browser is configured to make page requests to the proxy instead of to the pageâs real server
If the proxy has the page  it returns the page immediately
If not  it fetches the page from the server  adds it to the cache for future use  and returns it to the client that requested it
Clients Servers Browser cache Organization Proxy cache Internet Figure  -
A proxy cache between Web browsers and Web servers
As well as sending Web requests to the proxy instead of the real server  clients perform their own caching using its browser cache
The proxy is only consulted after the browser has tried to satisfy the request from its own cache
That is  the proxy provides a ond level of caching
Further proxies may be added to provide additional levels of caching
Each proxy (or browser) makes requests via its upstream proxy
Each upstream proxy caches for the downstream proxies (or browsers)
Thus  it is possible for browsers in a company to use a company proxy  which uses an ISP proxy  which contacts Web servers directly
However  the single level of proxy caching we have shown in Fig
The problem again is the long tail of popularity
Studies of Web traffic have shown that shared caching is especially beneficial until the number of users reaches about the size of a small company (say  people)
As the number of people grows larger  the benefits of sharing a cache become marginal because of the unpopular requests that cannot be cached due to lack of storage space (Wolman et al
Web proxies provide additional benefits that are often a factor in the decision to deploy them
One benefit is to filter content
The administrator may configure   CONTENT DELIVERY the proxy to blacklist sites or otherwise filter the requests that it makes
For example  many administrators frown on employees watching YouTube videos (or worse yet  pornography) on company time and set their filters accordingly
Another benefit of having proxies is privacy or anonymity  when the proxy shields the identity of the user from the server
Content Delivery Networks Server farms and Web proxies help to build large sites and to improve Web performance  but they are not sufficient for truly popular Web sites that must serve content on a global scale
For these sites  a different approach is needed
CDNs (Content Delivery Networks) turn the idea of traditional Web caching on its head
Instead  of having clients look for a copy of the requested page in a nearby cache  it is the provider who places a copy of the page in a set of nodes at different locations and directs the client to use a nearby node as the server
An example of the path that data follows when it is distributed by a CDN is shown in Fig
It is a tree
The origin server in the CDN distributes a copy of the content to other nodes in the CDN  in Sydney  Boston  and Amsterdam  in this example
This is shown with dashed lines
Clients then fetch pages from the nearest node in the CDN
This is shown with solid lines
In this way  the clients in Sydney both fetch the page copy that is stored in Sydney; they do not both fetch the page from the origin server  which may be in Europe
CDN origin server CDN node Sydney Boston Amsterdam Distribution to CDN nodes Page fetch Worldwide clients Figure  -
CDN distribution tree
Using a tree structure has three virtues
First  the content distribution can be scaled up to as many clients as needed by using more nodes in the CDN  and more levels in the tree when the distribution among CDN nodes becomes the bottleneck
No matter how many clients there are  the tree structure is efficient
The origin server is not overloaded because it talks to the many clients via the tree THE APPLICATION LAYER
of CDN nodes; it does not have to answer each request for a page by itself
ond  each client gets good performance by fetching pages from a nearby server instead of a distant server
This is because the round-trip time for setting up a connection is shorter  TCP slow-start ramps up more quickly because of the shorter round-trip time  and the shorter network path is less likely to pass through regions of congestion in the Internet
Finally  the total load that is placed on the network is also kept at a minimum
If the CDN nodes are well placed  the traffic for a given page should pass over each part of the network only once
This is important because someone pays for network bandwidth  eventually
The idea of using a distribution tree is straightforward
What is less simple is how to organize the clients to use this tree
For example  proxy servers would seem to provide a solution
Looking at Fig
However  this strategy falls short in practice  for three reasons
The first reason is that the clients in a given part of the network probably belong to different organizations  so they are probably using different Web proxies
Recall that caches are not usually shared across organizations because of the limited benefit of caching over a large number of clients  and for urity reasons too
ond  there can be multiple CDNs  but each client uses only a single proxy cache
Which CDN should a client use as its proxy? Finally  perhaps the most practical issue of all is that Web proxies are configured by clients
They may or may not be configured to benefit content distribution by a CDN  and there is little that the CDN can do about it
Another simple way to support a distribution tree with one level is to use mirroring
In this approach  the origin server replicates content over the CDN nodes as before
The CDN nodes in different network regions are called mirrors
The Web pages on the origin server contain explicit links to the different mirrors  usually telling the user their location
This design lets the user manually select a nearby mirror to use for downloading content
A typical use of mirroring is to place a large software package on mirrors located in  for example  the East and West coasts of the
Asia  and Europe
Mirrored sites are generally completely static  and the choice of sites remains stable for months or years
They are a tried and tested technique
However  they depend on the user to do the distribution as the mirrors are really different Web sites  even if they are linked together
The third approach  which overcomes the difficulties of the previous two approaches  uses DNS and is called DNS redirection
Suppose that a client wants to fetch a page with the URL http:// /
To fetch the page  the browser will use DNS to resolve   to an IP address
This DNS lookup proceeds in the usual manner
By using the DNS protocol  the browser learns the IP address of the name server for    then contacts the name server to ask it to resolve
Now comes the really clever bit
The name server is run by the CDN
Instead  of returning the same IP address for each request  it will look at the IP address of the client making the request and return   CONTENT DELIVERY different answers
The answer will be the IP address of the CDN node that is nearest the client
That is  if a client in Sydney asks the CDN name server to resolve    the name server will return the IP address of the Sydney CDN node  but if a client in Amsterdam makes the same request  the name server will return the IP address of the Amsterdam CDN node instead
This strategy is perfectly legal according to the semantics of DNS
We have previously seen that name servers may return changing lists of IP addresses
After the name resolution  the Sydney client will fetch the page directly from the Sydney CDN node
Further pages on the same ââserverââ will be fetched directly from the Sydney CDN node as well because of DNS caching
The overall sequence of steps is shown in Fig
CDN origin server  : Query DNS CDN DNS server Amsterdam CDN node Sydney CDN node  : âContact Sydneyâ âContact Amsterdamâ  : Fetch page  : Distribute content Sydney clients Amsterdam clients Figure  -
Directing clients to nearby CDN nodes using DNS
A complex question in the above process is what it means to find the nearest CDN node  and how to go about it
To define nearest  it is not really geography that matters
There are at least two factors to consider in mapping a client to a CDN node
One factor is the network distance
The client should have a short and high-capacity network path to the CDN node
This situation will produce quick downloads
CDNs use a map they have previously computed to translate between the IP address of a client and its network location
The CDN node that is selected might be the one at the shortest distance as the crow flies  or it might not
What matters is some combination of the length of the network path and any capacity limits along it
The ond factor is the load that is already being carried by the CDN node
If the CDN nodes are overloaded  they will deliver slow responses  just like the overloaded Web server that we sought to avoid in the first place
Thus  it may be necessary to balance the load across the CDN nodes  mapping some clients to nodes that are slightly further away but more lightly loaded
The techniques for using DNS for content distribution were pioneered by Akamai starting in  when the Web was groaning under the load of its early THE APPLICATION LAYER
Akamai was the first major CDN and became the industry leader
Probably even more clever than the idea of using DNS to connect clients to nearby nodes was the incentive structure of their business
Companies pay Akamai to deliver their content to clients  so that they have responsive Web sites that customers like to use
The CDN nodes must be placed at network locations with good connectivity  which initially meant inside ISP networks
For the ISPs  there is a benefit to having a CDN node in their networks  namely that the CDN node cuts down the amount of upstream network bandwidth that they need (and must pay for)  just as with proxy caches
In addition  the CDN node improves responsiveness for the ISPâs customers  which makes the ISP look good in their eyes  giving them a competitive advantage over ISPs that do not have a CDN node
These benefits (at no cost to the ISP) makes installing a CDN node a no brainer for the ISP
Thus  the content provider  the ISP  and the customers all benefit and the CDN makes money
Since  other companies have gotten into the business  so it is now a competitive industry with multiple providers
As this description implies  most companies do not build their own CDN
Instead  they use the services of a CDN provider such as Akamai to actually deliver their content
To let other companies use the service of a CDN  we need to add one last step to our picture
After the contract is signed for a CDN to distribute content on behalf of a Web site owner  the owner gives the CDN the content
This content is pushed to the CDN nodes
In addition  the owner rewrites any of its Web pages that link to the content
Instead of linking to the content on their Web site  the pages link to the content via the CDN
As an example of how this scheme works  consider the source code for Fluffy Videoâs Web page  given in Fig
After preprocessing  it is transformed to Fig
-  (b) and placed on Fluffy Videoâs server as  /
When a user types in the URL   to his browser  DNS returns the IP address of Fluffy Videoâs own Web site  allowing the main (HTML) page to be fetched in the normal way
When the user clicks on any of the hyperlinks  the browser asks DNS to look up
This lookup contacts the CDNâs DNS server  which returns the IP address of the nearby CDN node
The browser then sends a regular HTTP request to the CDN node  for example  for /fluffyvideo/
The URL identifies the page to return  starting the path with fluffyvideo so that the CDN node can separate requests for the different companies that it serves
Finally  the video is returned and the user sees cute fluffy animals
The strategy behind this split of content hosted by the CDN and entry pages hosted by the content owner is that it gives the content owner control while letting the CDN move the bulk of the data
Most entry pages are tiny  being just HTML text
These pages often link to large files  such as videos and images
It is precisely these large files that are served by the CDN  even though the use of a CDN is completely transparent to users
The site looks the same  but performs faster
CONTENT DELIVERY <html> <head> <title> Fluffy Video </title> </head> <body> <h > Fluffy Videoâs Product List </h > <p> Click below for free samples
"</p> <a href="" ""> Koalas Today </a> <br> <a href="" ""> Funny Kangaroos </a> <br> <a href="" ""> Nice Wombats </a> <br> </body> </html> (a) <html> <head> <title> Fluffy Video </title> </head> <body> <h > Fluffy Videoâs Product List </h > <p> Click below for free samples"
"</p> <a href=""http:// /fluffyvideo/ ""> Koalas Today </a> <br> <a href=""http:// /fluffyvideo/ ""> Funny Kangaroos </a> <br> <a href=""http:// /fluffyvideo/ ""> Nice Wombats </a> <br> </body> </html> (b) Figure  -"
(a) Original Web page
(b) Same page after linking to the CDN
There is another advantage for sites using a shared CDN
The future demand for a Web site can be difficult to predict
Frequently  there are surges in demand known as flash crowds
Such a surge may happen when the latest product is released  there is a fashion show or other event  or the company is otherwise in the news
Even a Web site that was a previously unknown  unvisited backwater can suddenly become the focus of the Internet if it is newsworthy and linked from popular sites
Since most sites are not prepared to handle massive increases in traffic  the result is that many of them crash when traffic surges
Case in point
Normally the Florida retary of Stateâs Web site is not a busy place  although you can look up information about Florida corporations  notaries  and cultural affairs  as well as information about voting and elections there
For some odd reason  on Nov
(the date of the
presidential election with Bush vs
Gore)  a whole lot of people were suddenly interested in the election results page of this site
The site suddenly became one of the busiest Web sites in the world and naturally crashed as a result
If it had been using a CDN  it would probably have survived
By using a CDN  a site has access to a very large content-serving capacity
The largest CDNs have tens of thousands of servers deployed in countries all over the world
Since only a small number of sites will be experiencing a flash crowd THE APPLICATION LAYER
at any one time (by definition)  those sites may use the CDNâs capacity to handle the load until the storm passes
That is  the CDN can quickly scale up a siteâs serving capacity
The preceding discussion above is a simplified description of how Akamai works
There are many more details that matter in practice
The CDN nodes pictured in our example are normally clusters of machines
DNS redirection is done with two levels: one to map clients to the approximate network location  and another to spread the load over nodes in that location
Both reliability and performance are concerns
To be able to shift a client from one machine in a cluster to another  DNS replies at the ond level are given with short TTLs so that the client will repeat the resolution after a short while
Finally  while we have concentrated on distributing static objects like images and videos  CDNs can also support dynamic page creation  streaming media  and more
For more information about CDNs  see Dilley et al
Peer-to-Peer Networks Not everyone can set up a -node CDN at locations around the world to distribute their content
(Actually  it is not hard to rent  virtual machines around the globe because of the well-developed and competitive hosting industry
However  setting up a CDN only starts with getting the nodes
) Luckily  there is an alternative for the rest of us that is simple to use and can distribute a tremendous amount of content
It is a P P (Peer-to-Peer) network
P P networks burst onto the scene starting in
The first widespread application was for mass crime:   million Napster users were exchanging copyrighted songs without the copyright ownersâ permission until Napster was shut down by the courts amid great controversy
Nevertheless  peer-to-peer technology has many interesting and legal uses
Other systems continued development  with such great interest from users that P P traffic quickly eclipsed Web traffic
Today  BitTorrent is the most popular P P protocol
It is used so widely to share (licensed and public domain) videos  as well as other content  that it accounts for a large fraction of all Internet traffic
We will look at it in this tion
The basic idea of a P P (Peer-to-Peer) file-sharing network is that many computers come together and pool their resources to form a content distribution system
The computers are often simply home computers
They do not need to be machines in Internet data centers
The computers are called peers because each one can alternately act as a client to another peer  fetching its content  and as a server  providing content to other peers
What makes peer-to-peer systems interesting is that there is no dedicated infrastructure  unlike in a CDN
Everyone participates in the task of distributing content  and there is often no central point of control
Many people are excited about P P technology because it is seen as empowering the little guy
The reason is not only that it takes a large company to run a   CONTENT DELIVERY CDN  while anyone with a computer can join a P P network
It is that P P networks have a formidable capacity to distribute content that can match the largest of Web sites
Consider a P P network made up of N average users  each with broadband connectivity at  Mbps
The aggregate upload capacity of the P P network  or rate at which the users can send traffic into the Internet  is N Mbps
The download capacity  or rate at which the users can receive traffic  is also N Mbps
Each user can upload and download at the same time  too  because they have a  -Mbps link in each direction
It is not obvious that this should be true  but it turns out that all of the capacity can be used productively to distribute content  even for the case of sharing a single copy of a file with all the other users
To see how this can be so  imagine that the users are organized into a binary tree  with each non-leaf user sending to two other users
The tree will carry the single copy of the file to all the other users
To use the upload bandwidth of as many users as possible at all times (and hence distribute the large file with low latency)  we need to pipeline the network activity of the users
Imagine that the file is divided into  pieces
Each user can receive a new piece from somewhere up the tree and send the previously received piece down the tree at the same time
This way  once the pipeline is started  after a small number of pieces (equal to the depth of the tree) are sent  all non-leaf users will be busy uploading the file to other users
Since there are approximately N/  non-leaf users  the upload bandwidth of this tree is N/  Mbps
We can repeat this trick and create another tree that uses the other N/  Mbps of upload bandwidth by swapping the roles of leaf and non-leaf nodes
Together  this construction uses all of the capacity
This argument means that P P networks are self-scaling
Their usable upload capacity grows in tandem with the download demands that can be made by their users
They are always ââlarge enoughââ in some sense  without the need for any dedicated infrastructure
In contrast  the capacity of even a large Web site is fixed and will either be too large or too small
Consider a site with only clusters  each capable of   Gbps
This enormous capacity does not help when there are a small number of users
The site cannot get information to N users at a rate faster than N Mbps because the limit is at the users and not the Web site
And when there are more than one million  -Mbps users  the Web site cannot pump out data fast enough to keep all the users busy downloading
That may seem like a large number of users  but large BitTorrent networks (
Pirate Bay) claim to have more than   users
That is more like   terabits/ in terms of our example! You should take these back-of-the-envelope numbers with a grain (or better yet  a metric ton) of salt because they oversimplify the situation
A significant challenge for P P networks is to use bandwidth well when users can come in all shapes and sizes  and have different download and upload capacities
Nevertheless  these numbers do indicate the enormous potential of P P
THE APPLICATION LAYER
There is another reason that P P networks are important
CDNs and other centrally run services put the providers in a position of having a trove of personal information about many users  from browsing preferences and where people shop online  to peopleâs locations and email addresses
This information can be used to provide better  more personalized service  or it can be used to intrude on peopleâs privacy
The latter may happen either intentionallyâsay as part of a new productâ or through an accidental disclosure or compromise
With P P systems  there can be no single provider that is capable of monitoring the entire system
This does not mean that P P systems will necessarily provide privacy  as users are trusting each other to some extent
It only means that they can provide a different form of privacy than centrally managed systems
P P systems are now being explored for services beyond file sharing (
storage  streaming)  and time will tell whether this advantage is significant
P P technology has followed two related paths as it has been developed
On the more practical side  there are the systems that are used every day
The most well known of these systems are based on the BitTorrent protocol
On the more academic side  there has been intense interest in DHT (Distributed Hash Table) algorithms that let P P systems perform well as a whole  yet rely on no centralized components at all
We will look at both of these technologies
BitTorrent The BitTorrent protocol was developed by Brahm Cohen in  to let a set of peers share files quickly and easily
There are dozens of freely available clients that speak this protocol  just as there are many browsers that speak the HTTP protocol to Web servers
The protocol is available as an open standard at
In a typical peer-to-peer system  like that formed with BitTorrent  the users each have some information that may be of interest to other users
This information may be free software  music  videos  photographs  and so on
There are three problems that need to be solved to share content in this setting:
How does a peer find other peers that have the content it wants to download?
How is content replicated by peers to provide high-speed downloads for everyone?
How do peers encourage each other to upload content to others as well as download content for themselves? The first problem exists because not all peers will have all of the content  at least initially
The approach taken in BitTorrent is for every content provider to create a content description called a torrent
The torrent is much smaller than the   CONTENT DELIVERY content  and is used by a peer to verify the integrity of the data that it downloads from other peers
Other users who want to download the content must first obtain the torrent  say  by finding it on a Web page advertising the content
The torrent is just a file in a specified format that contains two key kinds of information
One kind is the name of a tracker  which is a server that leads peers to the content of the torrent
The other kind of information is a list of equal-sized pieces  or chunks  that make up the content
Different chunk sizes can be used for different torrents  typically   KB to KB
The torrent file contains the name of each chunk  given as a   -bit SHA-  hash of the chunk
We will cover cryptographic hashes such as SHA-  in   For now  you can think of a hash as a longer and more ure checksum
Given the size of chunks and hashes  the torrent file is at least three orders of magnitude smaller than the content  so it can be transferred quickly
To download the content described in a torrent  a peer first contacts the tracker for the torrent
The tracker is a server that maintains a list of all the other peers that are actively downloading and uploading the content
This set of peers is called a swarm
The members of the swarm contact the tracker regularly to report that they are still active  as well as when they leave the swarm
When a new peer contacts the tracker to join the swarm  the tracker tells it about other peers in the swarm
Getting the torrent and contacting the tracker are the first two steps for downloading content  as shown in Fig
Seed peer Unchoked peers Tracker Torrent Peer  : Get torrent metafile  : Get peers from tracker  : Trade chunks with peers Source of content Figure  -
BitTorrent
The ond problem is how to share content in a way that gives rapid downloads
When a swarm is first formed  some peers must have all of the chunks that make up the content
These peers are called seeders
Other peers that join the swarm will have no chunks; they are the peers that are downloading the content
While a peer participates in a swarm  it simultaneously downloads chunks that it is missing from other peers  and uploads chunks that it has to other peers who THE APPLICATION LAYER
This trading is shown as the last step of content distribution in Fig
Over time  the peer gathers more chunks until it has downloaded all of the content
The peer can leave the swarm (and return) at any time
Normally a peer will stay for a short period after finishes its own download
With peers coming and going  the rate of churn in a swarm can be quite high
For the above method to work well  each chunk should be available at many peers
If everyone were to get the chunks in the same order  it is likely that many peers would depend on the seeders for the next chunk
This would create a bottleneck
Instead  peers exchange lists of the chunks they have with each other
Then they select rare chunks that are hard to find to download
The idea is that downloading a rare chunk will make a copy of it  which will make the chunk easier for other peers to find and download
If all peers do this  after a short while all chunks will be widely available
The third problem is perhaps the most interesting
CDN nodes are set up exclusively to provide content to users
P P nodes are not
They are usersâ computers  and the users may be more interested in getting a movie than helping other users with their downloads
Nodes that take resources from a system without contributing in kind are called free-riders or leechers
If there are too many of them  the system will not function well
Earlier P P systems were known to host them (Saroiu et al
) so BitTorrent sought to minimize them
The approach taken in BitTorrent clients is to reward peers who show good upload behavior
Each peer randomly samples the other peers  retrieving chunks from them while it uploads chunks to them
The peer continues to trade chunks with only a small number of peers that provide the highest download performance  while also randomly trying other peers to find good partners
Randomly trying peers also allows newcomers to obtain initial chunks that they can trade with other peers
The peers with which a node is currently exchanging chunks are said to be unchoked
Over time  this algorithm is intended to match peers with comparable upload and download rates with each other
The more a peer is contributing to the other peers  the more it can expect in return
Using a set of peers also helps to saturate a peerâs download bandwidth for high performance
Conversely  if a peer is not uploading chunks to other peers  or is doing so very slowly  it will be cut off  or choked  sooner or later
This strategy discourages antisocial behavior in which peers free-ride on the swarm
The choking algorithm is sometimes described as implementing the tit-for-tat strategy that encourages cooperation in repeated interactions
However  it does not prevent clients from gaming the system in any strong sense (Piatek et al
Nonetheless  attention to the issue and mechanisms that make it more difficult for casual users to free-ride have likely contributed to the success of Bit- Torrent
As you can see from our discussion  BitTorrent comes with a rich vocabulary
There are torrents  swarms  leechers  seeders  and trackers  as well as snubbing    CONTENT DELIVERY choking  lurking  and more
For more information see the short paper on Bit- Torrent (Cohen  ) and look on the Web starting with
DHTsâDistributed Hash Tables The emergence of P P file sharing networks around  sparked much interest in the research community
The essence of P P systems is that they avoid the centrally managed structures of CDNs and other systems
This can be a significant advantage
Centrally managed components become a bottleneck as the system grows very large and are a single point of failure
Central components can also be used as a point of control (
to shut off the P P network)
However  the early P P systems were only partly decentralized  or  if they were fully decentralized  they were inefficient
The traditional form of BitTorrent that we just described uses peer-to-peer transfers and a centralized tracker for each swarm
It is the tracker that turns out to be the hard part to decentralize in a peer-to-peer system
The key problem is how to find out which peers have specific content that is being sought
For example  each user might have one or more data items such as songs  photographs  programs  files  and so on that other users might want to read
How do the other users find them? Making one index of who has what is simple  but it is centralized
Having every peer keep its own index does not help
True  it is distributed
However  it requires so much work to keep the indexes of all peers up to date (as content is moved about the system) that it is not worth the effort
The question tackled by the research community was whether it was possible to build P P indexes that were entirely distributed but performed well
By perform well  we mean three things
First  each node keeps only a small amount of information about other nodes
This property means that it will not be expensive to keep the index up to date
ond  each node can look up entries in the index quickly
Otherwise  it is not a very useful index
Third  each node can use the index at the same time  even as other nodes come and go
This property means the performance of the index grows with the number of nodes
The answer is to the question was: ââYes
ââ Four different solutions were invented in
They are Chord (Stoica et al
)  CAN (Ratnasamy et al
)  Pastry (Rowstron and Druschel  )  and Tapestry (Zhao et al
Other solutions were invented soon afterwards  including Kademlia  which is used in practice (Maymounkov and Mazieres  )
The solutions are known as DHTs (Distributed Hash Tables) because the basic functionality of an index is to map a key to a value
This is like a hash table  and the solutions are distributed versions  of course
DHTs do their work by imposing a regular structure on the communication between the nodes  as we will see
This behavior is quite different than that of traditional P P networks that use whatever connections peers happen to make
THE APPLICATION LAYER
For this reason  DHTs are called structured P P networks
Traditional P P protocols build unstructured P P networks
The DHT solution that we will describe is Chord
As a scenario  consider how to replace the centralized tracker traditionally used in BitTorrent with a fully-distributed tracker
Chord can be used to solve this problem
In this scenario  the overall index is a listing of all of the swarms that a computer may join to download content
The key used to look up the index is the torrent description of the content
It uniquely identifies a swarm from which content can be downloaded as the hashes of all the content chunks
The value stored in the index for each key is the list of peers that comprise the swarm
These peers are the computers to contact to download the content
A person wanting to download content such as a movie has only the torrent description
The question the DHT must answer is how  lacking a central database  does a person find out which peers (out of the millions of BitTorrent nodes) to download the movie from? A Chord DHT consists of n participating nodes
They are nodes running Bit- Torrent in our scenario
Each node has an IP address by which it may be contacted
The overall index is spread across the nodes
This implies that each node stores bits and pieces of the index for use by other nodes
The key part of Chord is that it navigates the index using identifiers in a virtual space  not the IP addresses of nodes or the names of content like movies
Conceptually  the identifiers are simply m-bit numbers that can be arranged in ascending order into a ring
To turn a node address into an identifier  it is mapped to an m-bit number using a hash function  hash
Chord uses SHA-  for hash
This is the same hash that we mentioned when describing BitTorrent
We will look at it when we discuss cryptography in   For now  suffice it to say that it is just a function that takes a variable-length byte string as an argument and produces a highly random   -bit number
Thus  we can use it to convert any IP address to a   -bit number called the node identifier
-  (a)  we show the node identifier circle for m =
(Just ignore the arcs in the middle for the moment
) Some of the identifiers correspond to nodes  but most do not
In this example  the nodes with identifiers    and   correspond to actual nodes and are shaded in the figure; the rest do not exist
Let us now define the function successor(k) as the node identifier of the first actual node following k around the circle  clockwise
For example  successor ( ) =   successor ( ) = and successor (  ) =
A key is also produced by hashing a content name with hash (
SHA- ) to generate a   -bit number
In our scenario  the content name is the torrent
Thus  in order to convert torrent (the torrent description file) to its key  we compute key = hash(torrent )
This computation is just a local procedure call to hash
To start a new a swarm  a node needs to insert a new key-value pair consisting of (torrent  my-IP-address) into the index
To accomplish this  the node asks successor(hash(torrent )) to store my-IP-address
In this way  the index is distributed over the nodes at random
For fault tolerance  p different hash functions   CONTENT DELIVERY                    Node  's finger table   Start IP addr of successor    Node  's finger table    Start IP addr of successor    Node  's finger table    Start IP addr of successor (a) (b) Node identifier Actual node Figure  -
(a) A set of   node identifiers arranged in a circle
The shaded ones correspond to actual machines
The arcs show the fingers from nodes  and
The labels on the arcs are the table indices
(b) Examples of the finger tables
could be used to store the data at p nodes  but we will not consider the subject of fault tolerance further here
Some time after the DHT is constructed  another node wants to find a torrent so that it can join the swarm and download content
A node looks up torrent by first hashing it to get key  and ond using successor (key) to find the IP address of the node storing the corresponding value
The value is the list of peers in the swarm; the node can add its IP address to the list and contact the other peers to download content with the BitTorrent protocol
The first step is easy; the ond one is not easy
To make it possible to find the IP address of the node corresponding to a certain key  each node is required to THE APPLICATION LAYER
maintain certain administrative data structures
One of these is the IP address of its successor node along the node identifier circle
For example  in Fig
Lookup can now proceed as follows
The requesting node sends a packet to its successor containing its IP address and the key it is looking for
The packet is propagated around the ring until it locates the successor to the node identifier being sought
That node checks to see if it has any information matching the key  and if so  returns it directly to the requesting node  whose IP address it has
However  linearly searching all the nodes is very inefficient in a large peerto- peer system since the mean number of nodes required per search is n/
To greatly speed up the search  each node also maintains what Chord calls a finger table
The finger table has m entries  indexed by  through m â   each one pointing to a different actual node
Each of the entries has two fields: start and the IP address of successor(start )  as shown for three example nodes in Fig
The values of the fields for entry i at a node with identifier k are: start = k +  i (modulo  m) IP address of successor(start [i ]) Note that each node stores the IP addresses of a relatively small number of nodes and that most of these are fairly close by in terms of node identifier
Using the finger table  the lookup of key at node k proceeds as follows
If key falls between k and successor (k)  the node holding information about key is successor (k) and the search terminates
Otherwise  the finger table is searched to find the entry whose start field is the closest predecessor of key
A request is then sent directly to the IP address in that finger table entry to ask it to continue the search
Since it is closer to key but still below it  chances are good that it will be able to return the answer with only a small number of additional queries
In fact  since every lookup halves the remaining distance to the target  it can be shown that the average number of lookups is log n
As a first example  consider looking up key =  at node
Since node  knows that  lies between it and its successor the desired node is  and the search terminates  returning node  âs IP address
As a ond example  consider looking up key =   at node
Since   does not lie between  and   the finger table is consulted
The closest predecessor to   is   so the request is forwarded to the IP address of  âs entry  namely  that of node
Node   also does not know the answer itself  so it looks for the node most closely preceding   and finds which yields the IP address of node
A query is then sent there
Node   observes that   lies between it and its successor (  )  so it returns the IP address of   to the caller  which works its way back to node
Since nodes join and leave all the time  Chord needs a way to handle these operations
We assume that when the system began operation it was small enough that the nodes could just exchange information directly to build the first circle and   CONTENT DELIVERY finger tables
After that  an automated procedure is needed
When a new node  r  wants to join  it must contact some existing node and ask it to look up the IP address of successor (r) for it
Next  the new node then asks successor (r) for its predecessor
The new node then asks both of these to insert r in between them in the circle
For example  if   in Fig
-  wants to join  it asks any node to look up successor (  )  which is
Then it asks   for its predecessor (  )
After it tells both of those about its existence uses   as its successor and   uses   as its predecessor
In addition  node   hands over those keys in the range  â   which now belong to
At this point is fully inserted
However  many finger tables are now wrong
To correct them  every node runs a background process that periodically recomputes each finger by calling successor
When one of these queries hits a new node  the corresponding finger entry is updated
When a node leaves gracefully  it hands its keys over to its successor and informs its predecessor of its departure so the predecessor can link to the departing nodeâs successor
When a node crashes  a problem arises because its predecessor no longer has a valid successor
To alleviate this problem  each node keeps track not only of its direct successor but also its s direct successors  to allow it to skip over up to s â  conutive failed nodes and reconnect the circle if disaster strikes
There has been a tremendous amount of research on DHTs since they were invented
To give you an idea of just how much research  let us pose a question: what is the most-cited networking paper of all time? You will find it difficult to come up with a paper that is cited more than the seminal Chord paper (Stoica et al
Despite this veritable mountain of research  applications of DHTs are only slowly beginning to emerge
Some BitTorrent clients use DHTs to provide a fully distributed tracker of the kind that we described
Large commercial cloud services such as Amazonâs Dynamo also incorporate DHT techniques (DeCandia et al
)  SUMMARY Naming in the ARPANET started out in a very simple way: an ASCII text file listed the names of all the hosts and their corresponding IP addresses
Every night all the machines downloaded this file
But when the ARPANET morphed into the Internet and exploded in size  a far more sophisticated and dynamic naming scheme was required
The one used now is a hierarchical scheme called the Domain Name System
It organizes all the machines on the Internet into a set of trees
At the top level are the well-known generic domains  including com and edu  as well as about country domains
DNS is implemented as a distributed database with servers all over the world
By querying a DNS server  a process THE APPLICATION LAYER
can map an Internet domain name onto the IP address used to communicate with a computer for that domain
Email is the original killer app of the Internet
It is still widely used by everyone from small children to grandparents
Most email systems in the world use the mail system now defined in RFCs  and
Messages have simple ASCII headers  and many kinds of content can be sent using MIME
Mail is submitted to message transfer agents for delivery and retrieved from them for presentation by a variety of user agents  including Web applications
Submitted mail is delivered using SMTP  which works by making a TCP connection from the sending message transfer agent to the receiving one
The Web is the application that most people think of as being the Internet
Originally  it was a system for seamlessly linking hypertext pages (written in HTML) across machines
The pages are downloaded by making a TCP connection from the browser to a server and using HTTP
Nowadays  much of the content on the Web is produced dynamically  either at the server (
with PHP) or in the browser (
with JavaScript)
When combined with back-end databases  dynamic server pages allow Web applications such as e-commerce and search
Dynamic browser pages are evolving into full-featured applications  such as email  that run inside the browser and use the Web protocols to communicate with remote servers
Caching and persistent connections are widely used to enhance Web performance
Using the Web on mobile devices can be challenging  despite the growth in the bandwidth and processing power of mobiles
Web sites often send tailored versions of pages with smaller images and less complex navigation to devices with small displays
The Web protocols are increasingly being used for machine-to-machine communication
XML is preferred to HTML as a description of content that is easy for machines to process
SOAP is an RPC mechanism that sends XML messages using HTTP
Digital audio and video have been key drivers for the Internet since
The majority of Internet traffic today is video
Much of it is streamed from Web sites over a mix of protocols (including RTP/UDP and RTP/HTTP/TCP)
Live media is streamed to many consumers
It includes Internet radio and TV stations that broadcast all manner of events
Audio and video are also used for real-time conferencing
Many calls use voice over IP  rather than the traditional telephone network  and include videoconferencing
There are a small number of tremendously popular Web sites  as well as a very large number of less popular ones
To serve the popular sites  content distribution networks have been deployed
CDNs use DNS to direct clients to a nearby server; the servers are placed in data centers all around the world
Alternatively  P P networks let a collection of machines share content such as movies among themselves
They provide a content distribution capacity that scales with the number of machines in the P P network and which can rival the largest of sites  PROBLEMS PROBLEMS
Many business computers have three distinct and worldwide unique identifiers
What are they?
Consider a situation in which a cyberterrorist makes all the DNS servers in the world crash simultaneously
How does this change oneâs ability to use the Internet?
DNS uses UDP instead of TCP
If a DNS packet is lost  there is no automatic recovery
Does this cause a problem  and if so  how is it solved?
John wants to have an original domain name and uses a randomized program to generate a ondary domain name for him
He wants to register this domain name in the com generic domain
The domain name that was generated is characters long
Will the com registrar allow this domain name to be registered?
Can a machine with a single DNS name have multiple IP addresses? How could this occur?
The number of companies with a Web site has grown explosively in recent years
As a result  thousands of companies are registered in the com domain  causing a heavy load on the top-level server for this domain
Suggest a way to alleviate this problem without changing the naming scheme (
without introducing new top-level domain names)
It is permitted that your solution requires changes to the client code Some email systems support a Content Return: header field
It specifies whether the body of a message is to be returned in the event of nondelivery
Does this field belong to the envelope or to the header?
Electronic mail systems need directories so peopleâs email addresses can be looked up
To build such directories  names should be broken up into standard components (
first name  last name) to make searching possible
Discuss some problems that must be solved for a worldwide standard to be acceptable A large law firm  which has many employees  provides a single email address for each employee
Each employeeâs email address is <login>@
However  the firm did not explicitly define the format of the login
Thus  some employees use their first names as their login names  some use their last names  some use their initials  etc
The firm now wishes to make a fixed format  for example:  @   that can be used for the email addresses of all its employees
How can this be done without rocking the boat too much?
A binary file is  bytes long
How long will it be if encoded using base  encoding  with a CR+LF pair inserted after every bytes sent and at the end?
Name five MIME types not listed in this book
You can check your browser or the Internet for information
PROBLEMS   Suppose that you want to send an MP  file to a friend  but your friendâs ISP limits the size of each incoming message to  MB and the MP  file is  MB
Is there a way to handle this situation by using RFC  and MIME?
Suppose that John just set up an auto-forwarding mechanism on his work email address  which receives all of his business-related emails  to forward them to his personal email address  which he shares with his wife
Johnâs wife was unaware of this  and activated a vacation agent on their personal account
Because John forwarded his email  he did not set up a vacation daemon on his work machine
What happens when an email is received at Johnâs work email address?
In any standard  such as RFC  a precise grammar of what is allowed is needed so that different implementations can interwork
Even simple items have to be defined carefully
The SMTP headers allow white space between the tokens
Give two plausible alternative definitions of white space between tokens Is the vacation agent part of the user agent or the message transfer agent? Of course  it is set up using the user agent  but does the user agent actually send the replies? Explain your answer In a simple version of the Chord algorithm for peer-to-peer lookup  searches do not use the finger table
Instead  they are linear around the circle  in either direction
Can a node accurately predict which direction it should search in? Discuss your answer IMAP allows users to fetch and download email from a remote mailbox
Does this mean that the internal format of mailboxes has to be standardized so any IMAP program on the client side can read the mailbox on any mail server? Discuss your answer Consider the Chord circle of Fig
Suppose that node   suddenly goes online
Which of the finger tables shown in the figure are affected? how?
Does Webmail use POP  IMAP  or neither? If one of these  why was that one chosen? If neither  which one is it closer to in spirit?
When Web pages are sent out  they are prefixed by MIME headers
Is it possible that when a user clicks on a link with Firefox  a particular helper is started  but clicking on the same link in Internet Explorer causes a completely different helper to be started  even though the MIME type returned in both cases is identical? Explain your answer Although it was not mentioned in the text  an alternative form for a URL is to use the IP address instead of its DNS name
Use this information to explain why a DNS name cannot end with a digit Imagine that someone in the math department at Stanford has just written a new document including a proof that he wants to distribute by FTP for his colleagues to review
He puts the program in the FTP directory ftp/pub/forReview/
What is the URL for this program likely to be?
A disadvantage of this scheme is that cookies are limited to  KB  so if the preferences are
PROBLEMS extensive  for example  many stocks  sports teams  types of news stories  weather for multiple cities  specials in numerous product categories  and more  the  -KB limit may be reached
Design an alternative way to keep track of preferences that does not have this problem Sloth Bank wants to make online banking easy for its lazy customers  so after a customer signs up and is authenticated by a password  the bank returns a cookie containing a customer ID number
In this way  the customer does not have to identify himself or type a password on future visits to the online bank
What do you think of this idea? Will it work? Is it a good idea?
(a) Consider the following HTML tag: <h  title=ââthis is the headerââ> HEADER  </h > Under what conditions does the browser use the TITLE attribute  and how? (b) How does the TITLE attribute differ from the ALT attribute?
How do you make an image clickable in HTML? Give an example Write an HTML page that includes a link to the email address username@DomainName
What happens when a user clicks this link?
Write an XML page for a university registrar listing multiple students  each having a name  an address  and a GPA For each of the following applications  tell whether it would be ( ) possible and ( ) better to use a PHP script or JavaScript  and why: (a) Displaying a calendar for any requested month since September
(b) Displaying the schedule of flights from Amsterdam to New York
(c) Graphing a polynomial from user-supplied coefficients Write a program in JavaScript that accepts an integer greater than  and tells whether it is a prime number
Note that JavaScript has if and while statements with the same syntax as C and Java
The modulo operator is %
"If you need the square root of x  use   (x) An HTML page is as follows: <html> <body> <a href="" - / ""> Click here for info </a> </body> </html> If the user clicks on the hyperlink  a TCP connection is opened and a series of lines is sent to the server"
List all the lines sent The If-Modified-Since header can be used to check whether a cached page is still valid
Requests can be made for pages containing images  sound  video  and so on  as well as HTML
Do you think the effectiveness of this technique is better or worse for JPEG images as compared to HTML? Think carefully about what ââeffectivenessââ means and explain your answer On the day of a major sporting event  such as the championship game in some popular sport  many people go to the official Web site
Is this a flash crowd in the same sense as the  Florida presidential election? Why or why not? PROBLEMS   Does it make sense for a single ISP to function as a CDN? If so  how would that work? If not  what is wrong with the idea?
Assume that compression is not used for audio CDs
How many MB of data must the compact disc contain in order to be able to play two hours of music?
-  (c)  quantization noise occurs due to the use of  -bit samples to represent nine signal values
The first sample  at   is exact  but the next few are not
What is the percent error for the samples at  / /   and  /  of the period?
Could a psychoacoustic model be used to reduce the bandwidth needed for Internet telephony? If so  what conditions  if any  would have to be met to make it work? If not  why not?
An audio streaming server has a one-way ââdistanceââ of m to a media player
It outputs at  Mbps
If the media player has a  -MB buffer  what can you say about the position of the low-water mark and the high-water mark?
Does voice over IP have the same problems with firewalls that streaming audio does? Discuss your answer What is the bit rate for transmitting uncompressed  Ã pixel color frames with   bits/pixel at   frames/?
Can a  -bit error in an MPEG frame affect more than the frame in which the error occurs? Explain your answer Consider a  -customer video server  where each customer watches three movies per month
Two-thirds of the movies are served at
How many movies does the server have to transmit at once during this time period? If each movie requires  Mbps  how many OC-  connections does the server need to the network?
Suppose that Zipfâs law holds for accesses to a  -movie video server
If the server holds the most popular  movies in memory and the remaining  on disk  give an expression for the fraction of all references that will be to memory
Write a little program to evaluate this expression numerically Some cybersquatters have registered domain names that are misspellings of common corporate sites  for example
Make a list of at least five such domains Numerous people have registered DNS names that consist of    where word is a common word
For each of the following categories  list five such Web sites and briefly summarize what it is (
belongs to a gastroenterologist on Long Island)
Here is the list of categories: animals  foods  household objects  and body parts
For the last category  please stick to body parts above the waist Rewrite the server of Fig
-  as a true Web server using the GET command for HTTP   It should also accept the Host message
The server should maintain a cache of files recently fetched from the disk and serve requests from the cache when possible
NETWORK URITY For the first few decades of their existence  computer networks were primarily used by university researchers for sending email and by corporate employees for sharing printers
Under these conditions  urity did not get a lot of attention
But now  as millions of ordinary citizens are using networks for banking  shopping  and filing their tax returns  and weakness after weakness has been found  network urity has become a problem of massive proportions
In this  ter  we will study network urity from several angles  point out numerous pitfalls  and discuss many algorithms and protocols for making networks more ure
urity is a broad topic and covers a multitude of sins
In its simplest form  it is concerned with making sure that nosy people cannot read  or worse yet  retly modify messages intended for other recipients
It is concerned with people trying to access remote services that they are not authorized to use
It also deals with ways to tell whether that message purportedly from the IRS ââPay by Friday  or elseââ is really from the IRS and not from the Mafia
urity also deals with the problems of legitimate messages being captured and replayed  and with people later trying to deny that they sent certain messages
Most urity problems are intentionally caused by malicious people trying to gain some benefit  get attention  or harm someone
A few of the most common perpetrators are listed in Fig
It should be clear from this list that making a network ure involves a lot more than just keeping it free of programming errors
It involves outsmarting often intelligent  dedicated  and sometimes wellfunded adversaries
It should also be clear that measures that will thwart casual    NETWORK URITY
attackers will have little impact on the serious ones
Police records show that the most damaging attacks are not perpetrated by outsiders tapping a phone line but by insiders bearing a grudge
urity systems should be designed accordingly
Adversary Goal Student To have fun snooping on peopleâs email Cracker To test out someoneâs urity system; steal data Sales rep To claim to represent all of Europe  not just Andorra Corporation To discover a competitorâs strategic marketing plan Ex-employee To get revenge for being fired Accountant To embezzle money from a company Stockbroker To deny a promise made to a customer by email Identity thief To steal credit card numbers for sale Government To learn an enemyâs military or industrial rets Terrorist To steal biological warfare rets Figure  -
Some people who may cause urity problems  and why
Network urity problems can be divided roughly into four closely intertwined areas: recy  authentication  nonrepudiation  and integrity control
recy  also called confidentiality  has to do with keeping information out of the grubby little hands of unauthorized users
This is what usually comes to mind when people think about network urity
Authentication deals with determining whom you are talking to before revealing sensitive information or entering into a business deal
Nonrepudiation deals with signatures: how do you prove that your customer really placed an electronic order for ten million left-handed doohickeys at   cents each when he later claims the price was   cents? Or maybe he claims he never placed any order
Finally  integrity control has to do with how you can be sure that a message you received was really the one sent and not something that a malicious adversary modified in transit or concocted
All these issues (recy  authentication  nonrepudiation  and integrity control) occur in traditional systems  too  but with some significant differences
Integrity and recy are achieved by using registered mail and locking documents up
Robbing the mail train is harder now than it was in Jesse Jamesâ day
Also  people can usually tell the difference between an original paper document and a photocopy  and it often matters to them
As a test  make a photocopy of a valid check
Try cashing the original check at your bank on Monday
Now try cashing the photocopy of the check on Tuesday
Observe the difference in the bankâs behavior
With electronic checks  the original and the copy are indistinguishable
It may take a while for banks to learn how to handle this
People authenticate other people by various means  including recognizing their faces  voices  and handwriting
Proof of signing is handled by signatures on letterhead paper  raised seals  and so on
Tampering can usually be detected by handwriting  ink  and paper experts
None of these options are available electronically
Clearly  other solutions are needed
Before getting into the solutions themselves  it is worth spending a few moments considering where in the protocol stack network urity belongs
There is probably no one single place
Every layer has something to contribute
In the physical layer  wiretapping can be foiled by enclosing transmission lines (or better yet  optical fibers) in sealed tubes containing an inert gas at high pressure
Any attempt to drill into a tube will release some gas  reducing the pressure and triggering an alarm
Some military systems use this technique
In the data link layer  packets on a point-to-point line can be encrypted as they leave one machine and decrypted as they enter another
All the details can be handled in the data link layer  with higher layers oblivious to what is going on
This solution breaks down when packets have to traverse multiple routers  however  because packets have to be decrypted at each router  leaving them vulnerable to attacks from within the router
Also  it does not allow some sessions to be protected (
those involving online purchases by credit card) and others not
Nevertheless  link encryption  as this method is called  can be added to any network easily and is often useful
In the network layer  firewalls can be installed to keep good packets and bad packets out
IP urity also functions in this layer
In the transport layer  entire connections can be encrypted end to end  that is  process to process
For maximum urity  end-to-end urity is required
Finally  issues such as user authentication and nonrepudiation can only be handled in the application layer
Since urity does not fit neatly into any layer  it does not fit into any  ter of this book
For this reason  it rates its own  ter
While this  ter is long  technical  and essential  it is also quasi-irrelevant for the moment
It is well documented that most urity failures at banks  for example  are due to lax urity procedures and incompetent employees  numerous implementation bugs that enable remote break-ins by unauthorized users  and socalled social engineering attacks  where customers are tricked into revealing their account details
All of these urity problems are more prevalent than clever criminals tapping phone lines and then decoding encrypted messages
If a person can walk into a random branch of a bank with an ATM slip he found on the street claiming to have forgotten his PIN and get a new one on the spot (in the name of good customer relations)  all the cryptography in the world will not prevent abuse
In this respect  Ross Andersonâs (   a) book is a real eye-opener  as it documents hundreds of examples of urity failures in numerous industries  nearly all of them due to what might politely be called sloppy business practices or inattention to urity
Nevertheless  the technical foundation on which e-commerce is built when all of these other factors are done well is cryptography
Except for physical layer urity  nearly all network urity is based on cryptographic principles
For this reason  we will begin our study of urity by NETWORK URITY
examining cryptography in some detail
In   we will look at some of the basic principles
Then we will examine in detail how these concepts can be used to achieve urity in networks
We will conclude with some brief thoughts about technology and society
Before starting  one last thought is in order: what is not covered
We have tried to focus on networking issues  rather than operating system and application issues  although the line is often hard to draw
For example  there is nothing here about user authentication using biometrics  password urity  buffer overflow attacks  Trojan horses  login spoofing  code injection such as cross-site scripting  viruses  worms  and the like
All of these topics are covered at length in
of Modern Operating Systems (Tanenbaum  )
The interested reader is referred to that book for the systems aspects of urity
Now let us begin our journey  CRYPTOGRAPHY Cryptography comes from the Greek words for ââret writing
ââ It has a long and colorful history going back thousands of years
In this tion  we will just sketch some of the highlights  as background information for what follows
For a complete history of cryptography  Kahnâs (   ) book is recommended reading
For a comprehensive treatment of modern urity and cryptographic algorithms  protocols  and applications  and related material  see Kaufman et al
For a more mathematical approach  see Stinson (   )
For a less mathematical approach  see Burnett and Paine (   )
Professionals make a distinction between ciphers and codes
A cipher is a character-for-character or bit-for-bit transformation  without regard to the linguistic structure of the message
In contrast  a code replaces one word with another word or symbol
Codes are not used any more  although they have a glorious history
The most successful code ever devised was used by the
armed forces during World War II in the Pacific
They simply had Navajo Indians talking to each other using specific Navajo words for military terms  for example chay-dagahi- nail-tsaidi (literally: tortoise killer) for antitank weapon
The Navajo language is highly tonal  exceedingly complex  and has no written form
And not a single person in Japan knew anything about it
In September  the San Diego Union described the code by saying ââFor three years  wherever the Marines landed  the Japanese got an earful of strange gurgling noises interspersed with other sounds resembling the call of a Tibetan monk and the sound of a hot water bottle being emptied
ââ The Japanese never broke the code and many Navajo code talkers were awarded high military honors for extraordinary service and bravery
The fact that the
broke the Japanese code but the Japanese never broke the Navajo code played a crucial role in the American victories in the Pacific
CRYPTOGRAPHY    Introduction to Cryptography Historically  four groups of people have used and contributed to the art of cryptography: the military  the diplomatic corps  diarists  and lovers
Of these  the military has had the most important role and has shaped the field over the centuries
Within military organizations  the messages to be encrypted have traditionally been given to poorly paid  low-level code clerks for encryption and transmission
The sheer volume of messages prevented this work from being done by a few elite specialists
Until the advent of computers  one of the main constraints on cryptography had been the ability of the code clerk to perform the necessary transformations  often on a battlefield with little equipment
An additional constraint has been the difficulty in switching over quickly from one cryptographic method to another one  since this entails retraining a large number of people
However  the danger of a code clerk being captured by the enemy has made it essential to be able to change the cryptographic method instantly if need be
These conflicting requirements have given rise to the model of Fig
Encryption method  E Passive intruder just listens Active intruder can alter messages Plaintext  P Plaintext  P Decryption method  D Encryption key  K Decryption key  K Ciphertext  C = EK(P) Intruder Figure  -
The encryption model (for a symmetric-key cipher)
The messages to be encrypted  known as the plaintext  are transformed by a function that is parameterized by a key
The output of the encryption process  known as the ciphertext  is then transmitted  often by messenger or radio
We assume that the enemy  or intruder  hears and accurately copies down the complete ciphertext
However  unlike the intended recipient  he does not know what the decryption key is and so cannot decrypt the ciphertext easily
Sometimes the intruder can not only listen to the communication channel (passive intruder) but can also record messages and play them back later  inject his own messages  or modify legitimate messages before they get to the receiver (active intruder)
The art of NETWORK URITY
breaking ciphers  known as cryptanalysis  and the art of devising them (cryptography) are collectively known as cryptology
It will often be useful to have a notation for relating plaintext  ciphertext  and keys
We will use C = EK(P) to mean that the encryption of the plaintext P using key K gives the ciphertext C
Similarly  P = DK(C) represents the decryption of C to get the plaintext again
It then follows that DK(EK(P)) = P This notation suggests that E and D are just mathematical functions  which they are
The only tricky part is that both are functions of two parameters  and we have written one of the parameters (the key) as a subscript  rather than as an argument  to distinguish it from the message
A fundamental rule of cryptography is that one must assume that the cryptanalyst knows the methods used for encryption and decryption
In other words  the cryptanalyst knows how the encryption method  E  and decryption  D  of Fig
The amount of effort necessary to invent  test  and install a new algorithm every time the old method is compromised (or thought to be compromised) has always made it impractical to keep the encryption algorithm ret
Thinking it is ret when it is not does more harm than good
This is where the key enters
The key consists of a (relatively) short string that selects one of many potential encryptions
In contrast to the general method  which may only be changed every few years  the key can be changed as often as required
Thus  our basic model is a stable and publicly known general method parameterized by a ret and easily changed key
The idea that the cryptanalyst knows the algorithms and that the recy lies exclusively in the keys is called Kerckhoffâs principle  named after the Flemish military cryptographer Auguste Kerckhoff who first stated it in  (Kerckhoff  )
Thus  we have Kerckhoffâs principle: All algorithms must be public; only the keys are ret The nonrecy of the algorithm cannot be emphasized enough
Trying to keep the algorithm ret  known in the trade as urity by obscurity  never works
Also  by publicizing the algorithm  the cryptographer gets free consulting from a large number of academic cryptologists eager to break the system so they can publish papers demonstrating how smart they are
If many experts have tried to break the algorithm for a long time after its publication and no one has succeeded  it is probably pretty solid
Since the real recy is in the key  its length is a major design issue
Consider a simple combination lock
The general principle is that you enter digits in sequence
Everyone knows this  but the key is ret
A key length of two digits means that there are possibilities
A key length of three digits means  possibilities  and a key length of six digits means a million
The longer the key  the higher the work factor the cryptanalyst has to deal with
The work factor for breaking the system by exhaustive search of the key space is exponential in the   CRYPTOGRAPHY key length
recy comes from having a strong (but public) algorithm and a long key
To prevent your kid brother from reading your email   -bit keys will do
For routine commercial use  at least bits should be used
To keep major governments at bay  keys of at least bits  preferably more  are needed
From the cryptanalystâs point of view  the cryptanalysis problem has three principal variations
When he has a quantity of ciphertext and no plaintext  he is confronted with the ciphertext-only problem
The cryptograms that appear in the puzzle tion of newspapers pose this kind of problem
When the cryptanalyst has some matched ciphertext and plaintext  the problem is called the known plaintext problem
Finally  when the cryptanalyst has the ability to encrypt pieces of plaintext of his own choosing  we have the chosen plaintext problem
Newspaper cryptograms could be broken trivially if the cryptanalyst were allowed to ask such questions as ââWhat is the encryption of ABCDEFGHIJKL?ââ Novices in the cryptography business often assume that if a cipher can withstand a ciphertext-only attack  it is ure
This assumption is very naive
In many cases  the cryptanalyst can make a good guess at parts of the plaintext
For example  the first thing many computers say when you call them up is ââlogin:ââ
Equipped with some matched plaintext-ciphertext pairs  the cryptanalystâs job becomes much easier
To achieve urity  the cryptographer should be conservative and make sure that the system is unbreakable even if his opponent can encrypt arbitrary amounts of chosen plaintext
Encryption methods have historically been divided into two categories: substitution ciphers and transposition ciphers
We will now deal with each of these briefly as background information for modern cryptography
Substitution Ciphers In a substitution cipher  each letter or group of letters is replaced by another letter or group of letters to disguise it
One of the oldest known ciphers is the Caesar cipher  attributed to Julius Caesar
With this method  a becomes D  b becomes E  c becomes F
and z becomes C
For example  attack becomes DWWDFN
In our examples  plaintext will be given in lowercase letters  and ciphertext in uppercase letters
A slight generalization of the Caesar cipher allows the ciphertext alphabet to be shifted by k letters  instead of always three
In this case  k becomes a key to the general method of circularly shifted alphabets
The Caesar cipher may have fooled Pompey  but it has not fooled anyone since
The next improvement is to have each of the symbols in the plaintext  say  the   letters for simplicity  map onto some other letter
For example  a b c d e f g h i j k l mn o p q r s t u vwx y z QWE R T Y U I O P A S D F G H J K L Z X C V B NM plaintext: ciphertext: NETWORK URITY
The general system of symbol-for-symbol substitution is called a monoalphabetic substitution cipher  with the key being the  -letter string corresponding to the full alphabet
For the key just given  the plaintext attack would be transformed into the ciphertext QZZQEA
At first glance this might appear to be a safe system because although the cryptanalyst knows the general system (letter-for-letter substitution)  he does not know which of the  !â¼â¼  Ã  possible keys is in use
In contrast with the Caesar cipher  trying all of them is not a promising approach
Even at  n per solution  a million computer chips working in parallel would take   years to try all the keys
Nevertheless  given a surprisingly small amount of ciphertext  the cipher can be broken easily
The basic attack takes advantage of the statistical properties of natural languages
In English  for example  e is the most common letter  followed by t  o  a  n  i  etc
The most common two-letter combinations  or digrams  are th  in  er  re  and an
The most common three-letter combinations  or trigrams  are the  ing  and  and ion
A cryptanalyst trying to break a monoalphabetic cipher would start out by counting the relative frequencies of all letters in the ciphertext
Then he might tentatively assign the most common one to e and the next most common one to t
He would then look at trigrams to find a common one of the form tXe  which strongly suggests that X is h
Similarly  if the pattern thYt occurs frequently  the Y probably stands for a
With this information  he can look for a frequently occurring trigram of the form aZW  which is most likely and
By making guesses at common letters  digrams  and trigrams and knowing about likely patterns of vowels and consonants  the cryptanalyst builds up a tentative plaintext  letter by letter
Another approach is to guess a probable word or phrase
For example  consider the following ciphertext from an accounting firm (blocked into groups of five characters): CTBMN BYCTC BTJDS QXBNS GSTJC BTSWX CTQTZ CQVUJ QJSGS TJQZZ MNQJS VLNSX VSZJU JDSTS JQUUS JUBXJ DSKSU JSNTK BGAQJ ZBGYQ TLCTZ BNYBN QJSW A likely word in a message from an accounting firm is financial
Using our knowledge that financial has a repeated letter (i)  with four other letters between their occurrences  we look for repeated letters in the ciphertext at this spacing
We find   hits  at positions        and
However  only two of these and have the next letter (corresponding to n in the plaintext) repeated in the proper place
Of these two  only   also has the a correctly positioned  so we know that financial begins at position
From this point on  deducing the key is easy by using the frequency statistics for English text and looking for nearly complete words to finish off
CRYPTOGRAPHY    Transposition Ciphers Substitution ciphers preserve the order of the plaintext symbols but disguise them
Transposition ciphers  in contrast  reorder the letters but do not disguise them
Figure  -  depicts a common transposition cipher  the columnar transposition
The cipher is keyed by a word or phrase not containing any repeated letters
In this example  MEGABUCK is the key
The purpose of the key is to order the columns  with column  being under the key letter closest to the start of the alphabet  and so on
The plaintext is written horizontally  in rows  padded to fill the matrix if need be
The ciphertext is read out by columns  starting with the column whose key letter is the lowest
M E G A B U C K    p l e a s e t r Plaintext pleasetransferonemilliondollarsto myswissbankaccountsixtwotwo Ciphertext AFLLSKSOSELAWAIATOOSSCTCLNMOMANT ESILYNTWRNNTSOWDPAEDOBUOERIRICXB a n s f e r o n e m i l l i o n d o l l a r s t o m y s w i s s b a n k a c c o u n t s i x t w o t w o a b c d Figure  -
A transposition cipher
To break a transposition cipher  the cryptanalyst must first be aware that he is dealing with a transposition cipher
By looking at the frequency of E  T  A  O  I  N  etc
it is easy to see if they fit the normal pattern for plaintext
If so  the cipher is clearly a transposition cipher  because in such a cipher every letter represents itself  keeping the frequency distribution intact
The next step is to make a guess at the number of columns
In many cases  a probable word or phrase may be guessed at from the context
For example  suppose that our cryptanalyst suspects that the plaintext phrase milliondollars occurs somewhere in the message
Observe that digrams MO  IL  LL  LA  IR  and OS occur in the ciphertext as a result of this phrase wrapping around
The ciphertext letter O follows the ciphertext letter M (
they are vertically adjacent in column  ) because they are separated in the probable phrase by a distance equal to the key length
If a key of length seven had been used  the digrams MD  IO  LL  LL  IA  OR  and NS would have occurred instead
In fact  for each key length  a different set of digrams is produced in the ciphertext
By hunting for the various possibilities  the cryptanalyst can often easily determine the key length
NETWORK URITY
The remaining step is to order the columns
When the number of columns  k  is small  each of the k(k â  ) column pairs can be examined in turn to see if its digram frequencies match those for English plaintext
The pair with the best match is assumed to be correctly positioned
Now each of the remaining columns is tentatively tried as the successor to this pair
The column whose digram and trigram frequencies give the best match is tentatively assumed to be correct
The next column is found in the same way
The entire process is continued until a potential ordering is found
Chances are that the plaintext will be recognizable at this point (
if milloin occurs  it is clear what the error is)
Some transposition ciphers accept a fixed-length block of input and produce a fixed-length block of output
These ciphers can be completely described by giving a list telling the order in which the characters are to be output
For example  the cipher of Fig
Its output is          In other words  the fourth input character  a  is the first to be output  followed by the twelfth  f  and so on
One-Time Pads Constructing an unbreakable cipher is actually quite easy; the technique has been known for decades
First choose a random bit string as the key
Then convert the plaintext into a bit string  for example  by using its ASCII representation
Finally  compute the XOR (eXclusive OR) of these two strings  bit by bit
The resulting ciphertext cannot be broken because in a sufficiently large sample of ciphertext  each letter will occur equally often  as will every digram  every trigram  and so on
This method  known as the one-time pad  is immune to all present and future attacks  no matter how much computational power the intruder has
The reason derives from information theory: there is simply no information in the message because all possible plaintexts of the given length are equally likely
An example of how one-time pads are used is given in Fig
First  message   ââI love you
ââ is converted to  -bit ASCII
Then a one-time pad  pad   is chosen and XORed with the message to get the ciphertext
A cryptanalyst could try all possible one-time pads to see what plaintext came out for each one
For example  the one-time pad listed as pad  in the figure could be tried  resulting in plaintext   ââElvis livesââ  which may or may not be plausible (a subject beyond the scope of this book)
In fact  for every  -character ASCII plaintext  there is a one-time pad that generates it
That is what we mean by saying there is no information in the ciphertext: you can get any message of the correct length out of it
One-time pads are great in theory but have a number of disadvantages in practice
To start with  the key cannot be memorized  so both sender and receiver must carry a written copy with them
If either one is subject to capture  written keys are clearly undesirable
Additionally  the total amount of data that can be transmitted is limited by the amount of key available
If the spy strikes it rich and discovers a wealth of data  he may find himself unable to transmit them back to   CRYPTOGRAPHY Message  :              Pad  :              Ciphertext:              Pad  :              Plaintext  :              Figure  -
The use of a one-time pad for encryption and the possibility of getting any possible plaintext from the ciphertext by the use of some other pad
headquarters because the key has been used up
Another problem is the sensitivity of the method to lost or inserted characters
If the sender and receiver get out of synchronization  all data from then on will appear garbled
With the advent of computers  the one-time pad might potentially become practical for some applications
The source of the key could be a special DVD that contains several gigabytes of information and  if transported in a DVD movie box and prefixed by a few minutes of video  would not even be suspicious
Of course  at gigabit network speeds  having to insert a new DVD every    could become tedious
And the DVDs must be personally carried from the sender to the receiver before any messages can be sent  which greatly reduces their practical utility
Quantum Cryptography Interestingly  there may be a solution to the problem of how to transmit the one-time pad over the network  and it comes from a very unlikely source: quantum mechanics
This area is still experimental  but initial tests are promising
If it can be perfected and be made efficient  virtually all cryptography will eventually be done using one-time pads since they are provably ure
Below we will briefly explain how this method  quantum cryptography  works
In particular  we will describe a protocol called BB  after its authors and publication year (Bennet and Brassard  )
Suppose that a user  Alice  wants to establish a one-time pad with a ond user  Bob
Alice and Bob are called principals  the main characters in our story
For example  Bob is a banker with whom Alice would like to do business
The names ââAliceââ and ââBobââ have been used for the principals in virtually every paper and book on cryptography since Ron Rivest introduced them many years ago (Rivest et al
Cryptographers love tradition
If we were to use ââAndyââ and ââBarbaraââ as the principals  no one would believe anything in this  ter
If Alice and Bob could establish a one-time pad  they could use it to communicate urely
The question is: how can they establish it without previously exchanging DVDs? We can assume that Alice and Bob are at the opposite ends NETWORK URITY
of an optical fiber over which they can send and receive light pulses
However  an intrepid intruder  Trudy  can cut the fiber to splice in an active tap
Trudy can read all the bits sent in both directions
She can also send false messages in both directions
The situation might seem hopeless for Alice and Bob  but quantum cryptography can shed some new light on the subject
Quantum cryptography is based on the fact that light comes in little packets called photons  which have some peculiar properties
Furthermore  light can be polarized by being passed through a polarizing filter  a fact well known to both sunglasses wearers and photographers
If a beam of light (
a stream of photons) is passed through a polarizing filter  all the photons emerging from it will be polarized in the direction of the filterâs axis (
vertically)
If the beam is now passed through a ond polarizing filter  the intensity of the light emerging from the ond filter is proportional to the square of the cosine of the angle between the axes
If the two axes are perpendicular  no photons get through
The absolute orientation of the two filters does not matter; only the angle between their axes counts
To generate a one-time pad  Alice needs two sets of polarizing filters
Set one consists of a vertical filter and a horizontal filter
This choice is called a rectilinear basis
A basis (plural: bases) is just a coordinate system
The ond set of filters is the same  except rotated   degrees  so one filter runs from the lower left to the upper right and the other filter runs from the upper left to the lower right
This choice is called a diagonal basis
Thus  Alice has two bases  which she can rapidly insert into her beam at will
In reality  Alice does not have four separate filters  but a crystal whose polarization can be switched electrically to any of the four allowed directions at great speed
Bob has the same equipment as Alice
The fact that Alice and Bob each have two bases available is essential to quantum cryptography
For each basis  Alice now assigns one direction as  and the other as
In the example presented below  we assume she chooses vertical to be  and horizontal to be
Independently  she also chooses lower left to upper right as  and upper left to lower right as
She sends these choices to Bob as plaintext
Now Alice picks a one-time pad  for example based on a random number generator (a complex subject all by itself)
She transfers it bit by bit to Bob  choosing one of her two bases at random for each bit
To send a bit  her photon gun emits one photon polarized appropriately for the basis she is using for that bit
For example  she might choose bases of diagonal  rectilinear  rectilinear  diagonal  rectilinear  etc
To send her one-time pad of    with these bases  she would send the photons shown in Fig
Given the one-time pad and the sequence of bases  the polarization to use for each bit is uniquely determined
Bits sent one photon at a time are called qubits
Bob does not know which bases to use  so he picks one at random for each arriving photon and just uses it  as shown in Fig
If he picks the correct basis  he gets the correct bit
If he picks the incorrect basis  he gets a random bit   CRYPTOGRAPHY Trudy's pad (g) x  x  x x x ?  x ? ?  x ?    x No Yes No Yes No No No Yes Yes No Yes Yes Yes No Yes No Bit number Data Trudy's bases (f) Onetime pad (e) Correct basis? (d) What Bob gets (c) Bob's bases (b) What Alice sends (a)          Figure  -
An example of quantum cryptography
because if a photon hits a filter polarized at   degrees to its own polarization  it randomly jumps to the polarization of the filter or to a polarization perpendicular to the filter  with equal probability
This property of photons is fundamental to quantum mechanics
Thus  some of the bits are correct and some are random  but Bob does not know which are which
Bobâs results are depicted in Fig
How does Bob find out which bases he got right and which he got wrong? He simply tells Alice which basis he used for each bit in plaintext and she tells him which are right and which are wrong in plaintext  as shown in Fig
From this information  both of them can build a bit string from the correct guesses  as shown in Fig
On the average  this bit string will be half the length of the original bit string  but since both parties know it  they can use it as a one-time pad
All Alice has to do is transmit a bit string slightly more than twice the desired length  and she and Bob will have a one-time pad of the desired length
But wait a minute
We forgot Trudy
Suppose that she is curious about what Alice has to say and cuts the fiber  inserting her own detector and transmitter
Unfortunately for her  she does not know which basis to use for each photon either
The best she can do is pick one at random for each photon  just as Bob does
An example of her choices is shown in Fig
When Bob later reports (in plaintext) which bases he used and Alice tells him (in plaintext) which ones are NETWORK URITY
correct  Trudy now knows when she got it right and when she got it wrong
But she knows from Aliceâs reply in Fig
For four of these bits (    and  )  she guessed right and captured the correct bit
For the other four (  and  )  she guessed wrong and does not know the bit transmitted
Thus  Bob knows the one-time pad starts with  from Fig
- (e) but all Trudy has is  ? ?? ?  from Fig
Of course  Alice and Bob are aware that Trudy may have captured part of their one-time pad  so they would like to reduce the information Trudy has
They can do this by performing a transformation on it
For example  they could divide the one-time pad into blocks of  bits  square each one to form a -bit number  and use the concatenation of these -bit numbers as the one-time pad
With her partial knowledge of the bit string transmitted  Trudy has no way to generate its square and so has nothing
The transformation from the original one-time pad to a different one that reduces Trudyâs knowledge is called privacy amplification
In practice  complex transformations in which every output bit depends on every input bit are used instead of squaring
Poor Trudy
Not only does she have no idea what the one-time pad is  but her presence is not a ret either
After all  she must relay each received bit to Bob to trick him into thinking he is talking to Alice
The trouble is  the best she can do is transmit the qubit she received  using the polarization she used to receive it  and about half the time she will be wrong  causing many errors in Bobâs one-time pad
When Alice finally starts sending data  she encodes it using a heavy forwarderror- correcting code
From Bobâs point of view  a  -bit error in the one-time pad is the same as a  -bit transmission error
Either way  he gets the wrong bit
If there is enough forward error correction  he can recover the original message despite all the errors  but he can easily count how many errors were corrected
If this number is far more than the expected error rate of the equipment  he knows that Trudy has tapped the line and can act accordingly (
tell Alice to switch to a radio channel  call the police  etc
If Trudy had a way to clone a photon so she had one photon to inspect and an identical photon to send to Bob  she could avoid detection  but at present no way to clone a photon perfectly is known
And even if Trudy could clone photons  the value of quantum cryptography to establish onetime pads would not be reduced
Although quantum cryptography has been shown to operate over distances of   km of fiber  the equipment is complex and expensive
Still  the idea has promise
For more information about quantum cryptography  see Mullins (   )
Two Fundamental Cryptographic Principles Although we will study many different cryptographic systems in the pages ahead  two principles underlying all of them are important to understand
Pay attention
You violate them at your peril
CRYPTOGRAPHY Redundancy The first principle is that all encrypted messages must contain some redundancy  that is  information not needed to understand the message
An example may make it clear why this is needed
Consider a mail-order company  The Couch Potato (TCP)  with   products
Thinking they are being very efficient  TCPâs programmers decide that ordering messages should consist of a  - byte customer name followed by a  -byte data field (  byte for the quantity and  bytes for the product number)
The last  bytes are to be encrypted using a very long key known only by the customer and TCP
At first  this might seem ure  and in a sense it is because passive intruders cannot decrypt the messages
Unfortunately  it also has a fatal flaw that renders it useless
Suppose that a recently fired employee wants to punish TCP for firing her
Just before leaving  she takes the customer list with her
She works through the night writing a program to generate fictitious orders using real customer names
Since she does not have the list of keys  she just puts random numbers in the last  bytes  and sends hundreds of orders off to TCP
When these messages arrive  TCPâs computer uses the customersâ name to locate the key and decrypt the message
Unfortunately for TCP  almost every  - byte message is valid  so the computer begins printing out shipping instructions
While it might seem odd for a customer to order sets of childrenâs swings or sandboxes  for all the computer knows  the customer might be planning to open a chain of franchised playgrounds
In this way  an active intruder (the exemployee) can cause a massive amount of trouble  even though she cannot understand the messages her computer is generating
This problem can be solved by the addition of redundancy to all messages
For example  if order messages are extended to   bytes  the first  of which must be zeros  this attack no longer works because the ex-employee can no longer generate a large stream of valid messages
The moral of the story is that all messages must contain considerable redundancy so that active intruders cannot send random junk and have it be interpreted as a valid message
However  adding redundancy makes it easier for cryptanalysts to break messages
Suppose that the mail-order business is highly competitive  and The Couch Potatoâs main competitor  The Sofa Tuber  would dearly love to know how many sandboxes TCP is selling so it taps TCPâs phone line
In the original scheme with  -byte messages  cryptanalysis was nearly impossible because after guessing a key  the cryptanalyst had no way of telling whether it was right because almost every message was technically legal
With the new  -byte scheme  it is easy for the cryptanalyst to tell a valid message from an invalid one
Thus  we have Cryptographic principle  : Messages must contain some redundancy In other words  upon decrypting a message  the recipient must be able to tell whether it is valid by simply inspecting the message and perhaps performing a NETWORK URITY
simple computation
This redundancy is needed to prevent active intruders from sending garbage and tricking the receiver into decrypting the garbage and acting on the ââplaintext
ââ However  this same redundancy makes it much easier for passive intruders to break the system  so there is some tension here
Furthermore  the redundancy should never be in the form of n  s at the start or end of a message  since running such messages through some cryptographic algorithms gives more predictable results  making the cryptanalystsâ job easier
A CRC polynomial is much better than a run of  s since the receiver can easily verify it  but it generates more work for the cryptanalyst
Even better is to use a cryptographic hash  a concept we will explore later
For the moment  think of it as a better CRC
Getting back to quantum cryptography for a moment  we can also see how redundancy plays a role there
Due to Trudyâs interception of the photons  some bits in Bobâs one-time pad will be wrong
Bob needs some redundancy in the incoming messages to determine that errors are present
One very crude form of redundancy is repeating the message two times
If the two copies are not identical  Bob knows that either the fiber is very noisy or someone is tampering with the transmission
Of course  sending everything twice is overkill; a Hamming or Reed-Solomon code is a more efficient way to do error detection and correction
But it should be clear that some redundancy is needed to distinguish a valid message from an invalid message  especially in the face of an active intruder
Freshness The ond cryptographic principle is that measures must be taken to ensure that each message received can be verified as being fresh  that is  sent very recently
This measure is needed to prevent active intruders from playing back old messages
If no such measures were taken  our ex-employee could tap TCPâs phone line and just keep repeating previously sent valid messages
Thus  Cryptographic principle  : Some method is needed to foil replay attacks One such measure is including in every message a timestamp valid only for  say onds
The receiver can then just keep messages around for   onds and compare newly arrived messages to previous ones to filter out duplicates
Messages older than   onds can be thrown out  since any replays sent more than   onds later will be rejected as too old
Measures other than timestamps will be discussed later  SYMMETRIC-KEY ALGORITHMS Modern cryptography uses the same basic ideas as traditional cryptography (transposition and substitution)  but its emphasis is different
Traditionally  cryptographers have used simple algorithms
Nowadays  the reverse is true: the object   SYMMETRIC-KEY ALGORITHMS is to make the encryption algorithm so complex and involuted that even if the cryptanalyst acquires vast mounds of enciphered text of his own choosing  he will not be able to make any sense of it at all without the key
The first class of encryption algorithms we will study in this  ter are called symmetric-key algorithms because they use the same key for encryption and decryption
In particular  we will focus on block ciphers  which take an n-bit block of plaintext as input and transform it using the key into an n-bit block of ciphertext
Cryptographic algorithms can be implemented in either hardware (for speed) or software (for flexibility)
Although most of our treatment concerns the algorithms and protocols  which are independent of the actual implementation  a few words about building cryptographic hardware may be of interest
Transpositions and substitutions can be implemented with simple electrical circuits
Figure  -  (a) shows a device  known as a P-box (P stands for permutation)  used to effect a transposition on an  -bit input
If the  bits are designated from top to bottom as  the output of this particular P-box is
By appropriate internal wiring  a P-box can be made to perform any transposition and do it at practically the speed of light since no computation is involved  just signal propagation
This design follows Kerckhoffâs principle: the attacker knows that the general method is permuting the bits
What he does not know is which bit goes where
S  S  P  P  P  P  S  S  S  S  S  S  Product cipher (c) S-box Decoder:  to  Encoder:  to  (b) P-box (a) S  S  S  S  Figure  -
Basic elements of product ciphers
(c) Product
Substitutions are performed by S-boxes  as shown in Fig
In this example  a  -bit plaintext is entered and a  -bit ciphertext is output
The  -bit input selects one of the eight lines exiting from the first stage and sets it to  ; all the other lines are
The ond stage is a P-box
The third stage encodes the selected input line in binary again
With the wiring shown  if the eight octal numbers  were input one after another  the output sequence would be
In other words   has been replaced by  has been replaced by   etc
Again  by appropriate wiring of the P-box inside the S-box  any substitution can be accomplished
Furthermore  such a device can be built in hardware to achieve great speed  since encoders and decoders have only one or two (subnanoond) gate delays and the propagation time across the P-box may well be less than  pico
NETWORK URITY
The real power of these basic elements only becomes apparent when we cascade a whole series of boxes to form a product cipher  as shown in Fig
In this example input lines are transposed (
permuted) by the first stage (P )
In the ond stage  the input is broken up into four groups of  bits  each of which is substituted independently of the others (S  to S )
This arrangement shows a method of approximating a larger S-box from multiple  smaller S-boxes
It is useful because small S-boxes are practical for a hardware implementation (
an  -bit S-box can be realized as a   -entry lookup table)  but large Sboxes become unwieldy to build (
a  -bit S-box would at a minimum need =  crossed wires in its middle stage)
Although this method is less general  it is still powerful
By inclusion of a sufficiently large number of stages in the product cipher  the output can be made to be an exceedingly complicated function of the input
Product ciphers that operate on k-bit inputs to produce k-bit outputs are very common
Typically  k is   to
A hardware implementation usually has at least   physical stages  instead of just  as in Fig
A software implementation is programmed as a loop with at least eight iterations  each one performing S-box-type substitutions on subblocks of the  - to   -bit data block  followed by a permutation that mixes the outputs of the S-boxes
Often there is a special initial permutation and one at the end as well
In the literature  the iterations are called rounds
DESâThe Data Encryption Standard In January  the
Government adopted a product cipher developed by IBM as its official standard for unclassified information
This cipher  DES (Data Encryption Standard)  was widely adopted by the industry for use in urity products
It is no longer ure in its original form  but in a modified form it is still useful
We will now explain how DES works
An outline of DES is shown in Fig
Plaintext is encrypted in blocks of   bits  yielding   bits of ciphertext
The algorithm  which is parameterized by a  -bit key  has   distinct stages
The first stage is a key-independent transposition on the  -bit plaintext
The last stage is the exact inverse of this transposition
The stage prior to the last one exchanges the leftmost   bits with the rightmost   bits
The remaining   stages are functionally identical but are parameterized by different functions of the key
The algorithm has been designed to allow decryption to be done with the same key as encryption  a property needed in any symmetric-key algorithm
The steps are just run in the reverse order
The operation of one of these intermediate stages is illustrated in Fig
Each stage takes two  -bit inputs and produces two  -bit outputs
The left output is simply a copy of the right input
The right output is the bitwise XOR of the left input and a function of the right input and the key for this stage  Ki
Pretty much all the complexity of the algorithm lies in this function
SYMMETRIC-KEY ALGORITHMS (a) (b) Initial transposition Iteration   Li-  â f(Ri -  Ki)  -Bit plaintext L i-  Ri-   -Bit ciphertext   bits Li   bits Ri Iteration  Iteration -Bit key  -Bit swap Inverse transposition Figure  -
The Data Encryption Standard
(a) General outline
(b) Detail of one iteration
The circled + means exclusive OR
The function consists of four steps  carried out in sequence
First  a  -bit number  E  is constructed by expanding the  -bit Ri â  according to a fixed transposition and duplication rule
ond  E and Ki are XORed together
This output is then partitioned into eight groups of  bits each  each of which is fed into a different S-box
Each of the   possible inputs to an S-box is mapped onto a  - bit output
Finally  these  Ã  bits are passed through a P-box
In each of the   iterations  a different key is used
Before the algorithm starts  a  -bit transposition is applied to the key
Just before each iteration  the key is partitioned into two  -bit units  each of which is rotated left by a number of bits dependent on the iteration number
Ki is derived from this rotated key by applying yet another  -bit transposition to it
A different  -bit subset of the   bits is extracted and permuted on each round
A technique that is sometimes used to make DES stronger is called whitening
It consists of XORing a random  -bit key with each plaintext block before feeding it into DES and then XORing a ond  -bit key with the resulting ciphertext before transmitting it
Whitening can easily be removed by running the NETWORK URITY
reverse operations (if the receiver has the two whitening keys)
Since this technique effectively adds more bits to the key length  it makes an exhaustive search of the key space much more time consuming
Note that the same whitening key is used for each block (
there is only one whitening key)
DES has been enveloped in controversy since the day it was launched
It was based on a cipher developed and patented by IBM  called Lucifer  except that IBMâs cipher used a   -bit key instead of a  -bit key
Federal Government wanted to standardize on one cipher for unclassified use  it ââinvitedââ IBM to ââdiscussââ the matter with NSA  the
Governmentâs code-breaking arm  which is the worldâs largest employer of mathematicians and cryptologists
NSA is so ret that an industry joke goes: Q: What does NSA stand for? A: No Such Agency
Actually  NSA stands for National urity Agency
After these discussions took place  IBM reduced the key from bits to   bits and decided to keep ret the process by which DES was designed
Many people suspected that the key length was reduced to make sure that NSA could just break DES  but no organization with a smaller budget could
The point of the ret design was supposedly to hide a back door that could make it even easier for NSA to break DES
When an NSA employee discreetly told IEEE to cancel a planned conference on cryptography  that did not make people any more comfortable
NSA denied everything
In  two Stanford cryptography researchers  Diffie and Hellman (   )  designed a machine to break DES and estimated that it could be built for   million dollars
Given a small piece of plaintext and matched ciphertext  this machine could find the key by exhaustive search of the   -entry key space in under  day
Nowadays  the game is up
Such a machine exists  is for sale  and costs less than $  to make (Kumar et al
Triple DES As early as  IBM realized that the DES key length was too short and devised a way to effectively increase it  using triple encryption (Tuchman  )
The method chosen  which has since been incorporated in International Standard  is illustrated in Fig
Here  two keys and three stages are used
In the first stage  the plaintext is encrypted using DES in the usual way with K
In the ond stage  DES is run in decryption mode  using K  as the key
Finally  another DES encryption is done with K
This design immediately gives rise to two questions
First  why are only two keys used  instead of three? ond  why is EDE (Encrypt Decrypt Encrypt) used  instead of EEE (Encrypt Encrypt Encrypt)? The reason that two keys are used is that even the most paranoid of cryptographers believe that bits is   SYMMETRIC-KEY ALGORITHMS K  E K  D K  P E C K  D K  E (a) (b) K  C D P Figure  -
(a) Triple encryption using DES
(b) Decryption
adequate for routine commercial applications for the time being
(And among cryptographers  paranoia is considered a feature  not a bug
) Going to bits would just add the unnecessary overhead of managing and transporting another key for little real gain
The reason for encrypting  decrypting  and then encrypting again is backward compatibility with existing single-key DES systems
Both the encryption and decryption functions are mappings between sets of  -bit numbers
From a cryptographic point of view  the two mappings are equally strong
By using EDE  however  instead of EEE  a computer using triple encryption can speak to one using single encryption by just setting K  = K
This property allows triple encryption to be phased in gradually  something of no concern to academic cryptographers but of considerable importance to IBM and its customers
AESâThe Advanced Encryption Standard As DES began approaching the end of its useful life  even with triple DES  NIST (National Institute of Standards and Technology)  the agency of the
of Commerce charged with approving standards for the
Federal Government  decided that the government needed a new cryptographic standard for unclassified use
NIST was keenly aware of all the controversy surrounding DES and well knew that if it just announced a new standard  everyone knowing anything about cryptography would automatically assume that NSA had built a back door into it so NSA could read everything encrypted with it
Under these conditions  probably no one would use the standard and it would have died quietly
So  NIST took a surprisingly different approach for a government bureaucracy: it sponsored a cryptographic bake-off (contest)
In January  researchers from all over the world were invited to submit proposals for a new standard  to be called AES (Advanced Encryption Standard)
The bake-off rules were:
The algorithm must be a symmetric block cipher The full design must be public Key lengths of  and bits must be supported
NETWORK URITY
Both software and hardware implementations must be possible The algorithm must be public or licensed on nondiscriminatory terms
Fifteen serious proposals were made  and public conferences were organized in which they were presented and attendees were actively encouraged to find flaws in all of them
In August  NIST selected five finalists  primarily on the basis of their urity  efficiency  simplicity  flexibility  and memory requirements (important for embedded systems)
More conferences were held and more potshots taken
In October  NIST announced that it had selected Rijndael  by Joan Daemen and Vincent Rijmen
The name Rijndael  pronounced Rhine-doll (more or less)  is derived from the last names of the authors: Rijmen + Daemen
In November  Rijndael became the AES
Government standard  published as FIPS (Federal Information Processing Standard)
Due to the extraordinary openness of the competition  the technical properties of Rijndael  and the fact that the winning team consisted of two young Belgian cryptographers (who were unlikely to have built in a back door just to please NSA)  Rijndael has become the worldâs dominant cryptographic cipher
AES encryption and decryption is now part of the instruction set for some microprocessors (
Rijndael supports key lengths and block sizes from bits to bits in steps of   bits
The key length and block length may be chosen independently
However  AES specifies that the block size must be bits and the key length must be  or bits
It is doubtful that anyone will ever use   -bit keys  so de facto  AES has two variants: a   -bit block with a   -bit key and a   -bit block with a   -bit key
In our treatment of the algorithm  we will examine only the   /   case because this is likely to become the commercial norm
A   -bit key gives a key space of  â¼â¼  Ã  keys
Even if NSA manages to build a machine with  billion parallel processors  each being able to evaluate one key per picoond  it would take such a machine about  years to search the key space
By then the sun will have burned out  so the folks then present will have to read the results by candlelight
Rijndael From a mathematical perspective  Rijndael is based on Galois field theory  which gives it some provable urity properties
However  it can also be viewed as C code  without getting into the mathematics
Like DES  Rijndael uses substitution and permutations  and it also uses multiple rounds
The number of rounds depends on the key size and block size  being   for   -bit keys with   -bit blocks and moving up to   for the largest key or the largest block
However  unlike DES  all operations involve entire bytes  to   SYMMETRIC-KEY ALGORITHMS allow for efficient implementations in both hardware and software
An outline of the code is given in Fig
Note that this code is for the purpose of illustration
Good implementations of urity code will follow additional practices  such as zeroing out sensitive memory after it has been used
See  for example  Ferguson et al
#define LENGTH   /* # bytes in data block or key */ #define NROWS  /* number of rows in state */ #define NCOLS  /* number of columns in state */ #define ROUNDS   /* number of iterations */ typedef unsigned char byte; /* unsigned  -bit integer */ rijndael(byte plaintext[LENGTH]  byte ciphertext[LENGTH]  byte key[LENGTH]) { int r; /* loop index */ byte state[NROWS][NCOLS]; /* current state */ struct {byte k[NROWS][NCOLS];} rk[ROUNDS +  ]; /* round keys */ expand key(key  rk); /* construct the round keys */ copy plaintext to state(state  plaintext); /* init current state */ xor roundkey into state(state  rk[ ]); /* XOR key into state */ for (r =  ; r <= ROUNDS; r++) { substitute(state); /* apply S-box to each byte */ rotate rows(state); /* rotate row i by i bytes */ if (r < ROUNDS) mix columns(state); /* mix function */ xor roundkey into state(state  rk[r]); /* XOR key into state */ } copy state to ciphertext(ciphertext  state); /* return result */ } Figure  -
An outline of Rijndael in C
The function rijndael has three parameters
They are: plaintext  an array of   bytes containing the input data; ciphertext  an array of   bytes where the enciphered output will be returned; and key  the  -byte key
During the calculation  the current state of the data is maintained in a byte array  state  whose size is NROWS Ã NCOLS
For   -bit blocks  this array is  Ã  bytes
With   bytes  the full   -bit data block can be stored
The state array is initialized to the plaintext and modified by every step in the computation
In some steps  byte-for-byte substitution is performed
In others  the bytes are permuted within the array
Other transformations are also used
At the end  the contents of the state are returned as the ciphertext
The code starts out by expanding the key into   arrays of the same size as the state
They are stored in rk  which is an array of structs  each containing a state array
One of these will be used at the start of the calculation and the other   will be used during the   rounds  one per round
The calculation of the round NETWORK URITY
keys from the encryption key is too complicated for us to get into here
Suffice it to say that the round keys are produced by repeated rotation and XORing of various groups of key bits
For all the details  see Daemen and Rijmen (   )
The next step is to copy the plaintext into the state array so it can be processed during the rounds
It is copied in column order  with the first  bytes going into column   the next  bytes going into column   and so on
Both the columns and the rows are numbered starting at   although the rounds are numbered starting at
This initial setup of the   byte arrays of size  Ã  is illustrated in Fig
state rk[ ] rk[ ] rk[ ] rk[ ] rk[ ] rk[ ] rk[ ] rk[ ] rk[ ] rk[ ] rk[  ]   -Bit plaintext   -Bit encryption key Round keys Figure  -
Creating the state and rk arrays
There is one more step before the main computation begins: rk[ ] is XORed into state  byte for byte
In other words  each of the   bytes in state is replaced by the XOR of itself and the corresponding byte in rk[ ]
Now it is time for the main attraction
The loop executes   iterations  one per round  transforming state on each iteration
The contents of each round is produced in four steps
Step  does a byte-for-byte substitution on state
Each byte in turn is used as an index into an S-box to replace its value by the contents of that S-box entry
This step is a straight monoalphabetic substitution cipher
Unlike DES  which has multiple S-boxes  Rijndael has only one S-box
Step  rotates each of the four rows to the left
Row  is rotated  bytes (
not changed)  row  is rotated  byte  row  is rotated  bytes  and row  is rotated  bytes
This step diffuses the contents of the current data around the block  analogous to the permutations of Fig
Step  mixes up each column independently of the other ones
The mixing is done using matrix multiplication in which the new column is the product of the old column and a constant matrix  with the multiplication done using the finite Galois field  GF(  )
Although this may sound complicated  an algorithm exists that allows each element of the new column to be computed using two table lookups and three XORs (Daemen and Rijmen   Appendix E)
SYMMETRIC-KEY ALGORITHMS Finally  step  XORs the key for this round into the state array for use in the next round
Since every step is reversible  decryption can be done just by running the algorithm backward
However  there is also a trick available in which decryption can be done by running the encryption algorithm using different tables
The algorithm has been designed not only for great urity  but also for great speed
A good software implementation on a  -GHz machine should be able to achieve an encryption rate of Mbps  which is fast enough to encrypt over MPEG-  videos in real time
Hardware implementations are faster still
Cipher Modes Despite all this complexity  AES (or DES  or any block cipher for that matter) is basically a monoalphabetic substitution cipher using big characters (  -bit characters for AES and  -bit characters for DES)
Whenever the same plaintext block goes in the front end  the same ciphertext block comes out the back end
If you encrypt the plaintext abcdefgh times with the same DES key  you get the same ciphertext times
An intruder can exploit this property to help subvert the cipher
Electronic Code Book Mode To see how this monoalphabetic substitution cipher property can be used to partially defeat the cipher  we will use (triple) DES because it is easier to depict  -bit blocks than   -bit blocks  but AES has exactly the same problem
The straightforward way to use DES to encrypt a long piece of plaintext is to break it up into conutive  -byte (  -bit) blocks and encrypt them one after another with the same key
The last piece of plaintext is padded out to   bits  if need be
This technique is known as ECB mode (Electronic Code Book mode) in analogy with old-fashioned code books where each plaintext word was listed  followed by its ciphertext (usually a five-digit decimal number)
This file consists of conutive  -byte records  one per employee  in the format shown:   bytes for the name   bytes for the position  and  bytes for the bonus
Each of the sixteen  -byte blocks (numbered from  to  ) is encrypted by (triple) DES
Leslie just had a fight with the boss and is not expecting much of a bonus
Kim  in contrast  is the bossâ favorite  and everyone knows this
Leslie can get access to the file after it is encrypted but before it is sent to the bank
Can Leslie rectify this unfair situation  given only the encrypted file? No problem at all
All Leslie has to do is make a copy of the  th ciphertext block (which contains Kimâs bonus) and use it to replace the fourth ciphertext block (which contains Leslieâs bonus)
Even without knowing what the  th NETWORK URITY
Name Position Bonus Bytes   D a v i s  B o b b i e J a n i t o r $  C o l l i n s  K i m M a n a g e r $  B l a c k  R o b i n B o s s $  A d a m s  L e s l i e C l e r k $ Figure  -
The plaintext of a file encrypted as   DES blocks
block says  Leslie can expect to have a much merrier Christmas this year
(Copying the eighth ciphertext block is also a possibility  but is more likely to be detected; besides  Leslie is not a greedy person
) Cipher Block Chaining Mode To thwart this type of attack  all block ciphers can be chained in various ways so that replacing a block the way Leslie did will cause the plaintext decrypted starting at the replaced block to be garbage
One way of chaining is cipher block chaining
In this method  shown in Fig
Consequently  the same plaintext block no longer maps onto the same ciphertext block  and the encryption is no longer a big monoalphabetic substitution cipher
The first block is XORed with a randomly chosen IV (Initialization Vector)  which is transmitted (in plaintext) along with the ciphertext
(a) (b) + E IV Key Key IV P  C  + E P  C  E P  C  E P  C  D C  P  D C  P  D C  P  D Decryption box Encryption box Exclusive OR C  P  + + + + + + Figure  -
Cipher block chaining
(a) Encryption
(b) Decryption
We can see how cipher block chaining mode works by examining the example of Fig
We start out by computing C  = E(P  XOR IV)
Then we compute C  = E(P  XOR C )  and so on
Decryption also uses XOR to reverse the process  with P  = IV XOR D(C )  and so on
Note that the encryption of block i is a   SYMMETRIC-KEY ALGORITHMS function of all the plaintext in blocks  through i â   so the same plaintext generates different ciphertext depending on where it occurs
A transformation of the type Leslie made will result in nonsense for two blocks starting at Leslieâs bonus field
To an astute urity officer  this peculiarity might suggest where to start the ensuing investigation
Cipher block chaining also has the advantage that the same plaintext block will not result in the same ciphertext block  making cryptanalysis more difficult
In fact  this is the main reason it is used
Cipher Feedback Mode However  cipher block chaining has the disadvantage of requiring an entire  -bit block to arrive before decryption can begin
For byte-by-byte encryption  cipher feedback mode using (triple) DES is used  as shown in Fig
For AES  the idea is exactly the same  only a   -bit shift register is used
In this figure  the state of the encryption machine is shown after bytes  through  have been encrypted and sent
When plaintext byte   arrives  as illustrated in Fig
The leftmost byte of that ciphertext is extracted and XORed with P
That byte is transmitted on the transmission line
In addition  the shift register is shifted left  bits  causing C  to fall off the left end  and C  is inserted in the position just vacated at the right end by C
(a) Key P  C  C  C  E  -bit shift register C  C  C  C  C  C  C  C  Encryption box Select leftmost byte Exclusive OR (b) Key C  P  E  -bit shift register C  C  C  C  C  C  C  C  + Encryption box Select leftmost byte + Figure  -
Cipher feedback mode
(a) Encryption
(b) Decryption
Note that the contents of the shift register depend on the entire previous history of the plaintext  so a pattern that repeats multiple times in the plaintext will be encrypted differently each time in the ciphertext
As with cipher block chaining  an initialization vector is needed to start the ball rolling
NETWORK URITY
Decryption with cipher feedback mode works the same way as encryption
In particular  the content of the shift register is encrypted  not decrypted  so the selected byte that is XORed with C  to get P  is the same one that was XORed with P  to generate C  in the first place
As long as the two shift registers remain identical  decryption works correctly
This is illustrated in Fig
A problem with cipher feedback mode is that if one bit of the ciphertext is accidentally inverted during transmission  the  bytes that are decrypted while the bad byte is in the shift register will be corrupted
Once the bad byte is pushed out of the shift register  correct plaintext will once again be generated
Thus  the effects of a single inverted bit are relatively localized and do not ruin the rest of the message  but they do ruin as many bits as the shift register is wide
Stream Cipher Mode Nevertheless  applications exist in which having a  -bit transmission error mess up   bits of plaintext is too large an effect
For these applications  a fourth option  stream cipher mode  exists
It works by encrypting an initialization vector  using a key to get an output block
The output block is then encrypted  using the key to get a ond output block
This block is then encrypted to get a third block  and so on
The (arbitrarily large) sequence of output blocks  called the keystream  is treated like a one-time pad and XORed with the plaintext to get the ciphertext  as shown in Fig
Note that the IV is used only on the first step
After that  the output is encrypted
Also note that the keystream is independent of the data  so it can be computed in advance  if need be  and is completely insensitive to transmission errors
Decryption is shown in Fig
E (a) Key Plaintext Ciphertext Keystream Encryption box IV + E (b) Key Ciphertext Plaintext Keystream Encryption box IV + Figure  -
A stream cipher
(a) Encryption
(b) Decryption
Decryption occurs by generating the same keystream at the receiving side
Since the keystream depends only on the IV and the key  it is not affected by transmission errors in the ciphertext
Thus  a  -bit error in the transmitted ciphertext generates only a  -bit error in the decrypted plaintext
SYMMETRIC-KEY ALGORITHMS It is essential never to use the same (key  IV) pair twice with a stream cipher because doing so will generate the same keystream each time
Using the same keystream twice exposes the ciphertext to a keystream reuse attack
Imagine that the plaintext block  P  is encrypted with the keystream to get P  XOR K
Later  a ond plaintext block  Q  is encrypted with the same keystream to get Q  XOR K
An intruder who captures both of these ciphertext blocks can simply XOR them together to get P  XOR Q  which eliminates the key
The intruder now has the XOR of the two plaintext blocks
If one of them is known or can be guessed  the other can also be found
In any event  the XOR of two plaintext streams can be attacked by using statistical properties of the message
For example  for English text  the most common character in the stream will probably be the XOR of two spaces  followed by the XOR of space and the letter ââeââ  etc
In short  equipped with the XOR of two plaintexts  the cryptanalyst has an excellent chance of deducing both of them
Counter Mode One problem that all the modes except electronic code book mode have is that random access to encrypted data is impossible
For example  suppose a file is transmitted over a network and then stored on disk in encrypted form
This might be a reasonable way to operate if the receiving computer is a notebook computer that might be stolen
Storing all critical files in encrypted form greatly reduces the damage due to ret information leaking out in the event that the computer falls into the wrong hands
However  disk files are often accessed in nonsequential order  especially files in databases
With a file encrypted using cipher block chaining  accessing a random block requires first decrypting all the blocks ahead of it  an expensive proposition
For this reason  yet another mode has been invented: counter mode  as illustrated in Fig
Here  the plaintext is not encrypted directly
Instead  the initialization vector plus a constant is encrypted  and the resulting ciphertext is XORed with the plaintext
By stepping the initialization vector by  for each new block  it is easy to decrypt a block anywhere in the file without first having to decrypt all of its predecessors
Although counter mode is useful  it has a weakness that is worth pointing out
Suppose that the same key  K  is used again in the future (with a different plaintext but the same IV) and an attacker acquires all the ciphertext from both runs
The keystreams are the same in both cases  exposing the cipher to a keystream reuse attack of the same kind we saw with stream ciphers
All the cryptanalyst has to do is XOR the two ciphertexts together to eliminate all the cryptographic protection and just get the XOR of the plaintexts
This weakness does not mean counter mode is a bad idea
It just means that both keys and initialization vectors should be chosen independently and at random
Even if the same key is accidentally used twice  if the IV is different each time  the plaintext is safe
NETWORK URITY
Encryption box + E IV Key P  C  + E IV+  Key P  C  + E IV+  Key P  C  + E IV+  Key P  C  Figure  -
Encryption using counter mode
Other Ciphers AES (Rijndael) and DES are the best-known symmetric-key cryptographic algorithms  and the standard industry choices  if only for liability reasons
(No one will blame you if you use AES in your product and AES is cracked  but they will certainly blame you if you use a nonstandard cipher and it is later broken
) However  it is worth mentioning that numerous other symmetric-key ciphers have been devised
Some of these are embedded inside various products
A few of the more common ones are listed in Fig
It is possible to use combinations of these ciphers  for example  AES over Twofish  so that both ciphers need to be broken to recover the data
Cipher Author Key length Comments DES IBM   bits Too weak to use now RC  Ronald Rivest  â bits Caution: some keys are weak RC  Ronald Rivest   â   bits Good  but patented AES (Rijndael) Daemen and Rijmen   â   bits Best choice Serpent Anderson  Biham  Knudsen   â   bits Very strong Triple DES IBM bits Good  but getting old Twofish Bruce Schneier   â   bits Very strong; widely used Figure  -
Some common symmetric-key cryptographic algorithms
Cryptanalysis Before leaving the subject of symmetric-key cryptography  it is worth at least mentioning four developments in cryptanalysis
The first development is differential cryptanalysis (Biham and Shamir  )
This technique can be used   SYMMETRIC-KEY ALGORITHMS to attack any block cipher
It works by beginning with a pair of plaintext blocks differing in only a small number of bits and watching carefully what happens on each internal iteration as the encryption proceeds
In many cases  some bit patterns are more common than others  which can lead to probabilistic attacks
The ond development worth noting is linear cryptanalysis (Matsui  )
It can break DES with only known plaintexts
It works by XORing certain bits in the plaintext and ciphertext together and examining the result
When done repeatedly  half the bits should be  s and half should be  s
Often  however  ciphers introduce a bias in one direction or the other  and this bias  however small  can be exploited to reduce the work factor
For the details  see Matsuiâs paper
The third development is using analysis of electrical power consumption to find ret keys
Computers typically use around  volts to represent a  bit and  volts to represent a  bit
Thus  processing a  takes more electrical energy than processing a
If a cryptographic algorithm consists of a loop in which the key bits are processed in order  an attacker who replaces the main n-GHz clock with a slow (
-Hz) clock and puts alligator clips on the CPUâs power and ground pins can precisely monitor the power consumed by each machine instruction
From this data  deducing the key is surprisingly easy
This kind of cryptanalysis can be defeated only by carefully coding the algorithm in assembly language to make sure power consumption is independent of the key and also independent of all the individual round keys
The fourth development is timing analysis
Cryptographic algorithms are full of if statements that test bits in the round keys
If the then and else parts take different amounts of time  by slowing down the clock and seeing how long various steps take  it may also be possible to deduce the round keys
Once all the round keys are known  the original key can usually be computed
Power and timing analysis can also be employed simultaneously to make the job easier
While power and timing analysis may seem exotic  in reality they are powerful techniques that can break any cipher not specifically designed to resist them  PUBLIC-KEY ALGORITHMS Historically  distributing the keys has always been the weakest link in most cryptosystems
No matter how strong a cryptosystem was  if an intruder could steal the key  the system was worthless
Cryptologists always took for granted that the encryption key and decryption key were the same (or easily derived from one another)
But the key had to be distributed to all users of the system
Thus  it seemed as if there was an inherent problem
Keys had to be protected from theft  but they also had to be distributed  so they could not be locked in a bank vault
In  two researchers at Stanford University  Diffie and Hellman (   )  proposed a radically new kind of cryptosystem  one in which the encryption and decryption keys were so different that the decryption key could not feasibly be NETWORK URITY
derived from the encryption key
In their proposal  the (keyed) encryption algorithm  E  and the (keyed) decryption algorithm  D  had to meet three requirements
These requirements can be stated simply as follows:
D(E(P)) = P It is exceedingly difficult to deduce D from E E cannot be broken by a chosen plaintext attack
The first requirement says that if we apply D to an encrypted message  E(P)  we get the original plaintext message  P  back
Without this property  the legitimate receiver could not decrypt the ciphertext
The ond requirement speaks for itself
The third requirement is needed because  as we shall see in a moment  intruders may experiment with the algorithm to their heartsâ content
Under these conditions  there is no reason that the encryption key cannot be made public
The method works like this
A person  say  Alice  who wants to receive ret messages  first devises two algorithms meeting the above requirements
The encryption algorithm and Aliceâs key are then made public  hence the name publickey cryptography
Alice might put her public key on her home page on the Web  for example
We will use the notation EA to mean the encryption algorithm parameterized by Aliceâs public key
Similarly  the (ret) decryption algorithm parameterized by Aliceâs private key is DA
Bob does the same thing  publicizing EB but keeping DB ret
Now let us see if we can solve the problem of establishing a ure channel between Alice and Bob  who have never had any previous contact
Both Aliceâs encryption key  EA  and Bobâs encryption key  EB  are assumed to be in publicly readable files
Now Alice takes her first message  P  computes EB(P)  and sends it to Bob
Bob then decrypts it by applying his ret key DB [
he computes DB(EB(P)) = P]
No one else can read the encrypted message  EB(P)  because the encryption system is assumed to be strong and because it is too difficult to derive DB from the publicly known EB
To send a reply  R  Bob transmits EA(R)
Alice and Bob can now communicate urely
A note on terminology is perhaps useful here
Public-key cryptography requires each user to have two keys: a public key  used by the entire world for encrypting messages to be sent to that user  and a private key  which the user needs for decrypting messages
We will consistently refer to these keys as the public and private keys  respectively  and distinguish them from the ret keys used for conventional symmetric-key cryptography
RSA The only catch is that we need to find algorithms that indeed satisfy all three requirements
Due to the potential advantages of public-key cryptography  many researchers are hard at work  and some algorithms have already been published
PUBLIC-KEY ALGORITHMS One good method was discovered by a group at
(Rivest et al
It is known by the initials of the three discoverers (Rivest  Shamir  Adleman): RSA
It has survived all attempts to break it for more than   years and is considered very strong
Much practical urity is based on it
For this reason  Rivest  Shamir  and Adleman were given the  ACM Turing Award
Its major disadvantage is that it requires keys of at least  bits for good urity (versus bits for symmetric-key algorithms)  which makes it quite slow
The RSA method is based on some principles from number theory
We will now summarize how to use the method; for details  consult the paper Choose two large primes  p and q (typically  bits) Compute n = p Ã q and z = (p â  ) Ã (q â  ) Choose a number relatively prime to z and call it d Find e such that e Ã d =  mod z
With these parameters computed in advance  we are ready to begin encryption
Divide the plaintext (regarded as a bit string) into blocks  so that each plaintext message  P  falls in the interval  â¤ P < n
Do that by grouping the plaintext into blocks of k bits  where k is the largest integer for which  k < n is true
To encrypt a message  P  compute C = Pe (mod n)
To decrypt C  compute P = Cd (mod n)
It can be proven that for all P in the specified range  the encryption and decryption functions are inverses
To perform the encryption  you need e and n
To perform the decryption  you need d and n
Therefore  the public key consists of the pair (e  n) and the private key consists of (d  n)
The urity of the method is based on the difficulty of factoring large numbers
If the cryptanalyst could factor the (publicly known) n  he could then find p and q  and from these z
Equipped with knowledge of z and e  d can be found using Euclidâs algorithm
Fortunately  mathematicians have been trying to factor large numbers for at least years  and the accumulated evidence suggests that it is an exceedingly difficult problem
According to Rivest and colleagues  factoring a   -digit number would require  years using brute force
In both cases  they assumed the best known algorithm and a computer with a  -Î¼ instruction time
With a million chips running in parallel  each with an instruction time of  n  it would still take  years
Even if computers continue to get faster by an order of magnitude per decade  it will be many years before factoring a   -digit number becomes feasible  at which time our descendants can simply choose p and q still larger
A trivial pedagogical example of how the RSA algorithm works is given in Fig
For this example  we have chosen p =  and q = giving n =   and z =
A suitable value for d is d =   since  and   have no common factors
With these choices  e can be found by solving the equation  e =  (mod  )  which yields e =
The ciphertext  C  corresponding to a plaintext message  P  is NETWORK URITY
given by C = P  (mod  )
The ciphertext is decrypted by the receiver by making use of the rule P = C  (mod  )
The figure shows the encryption of the plaintext ââSUZANNEââ as an example
Symbolic S U Z A N N E Symbolic S U Z A N N E Numeric Plaintext (P) Ciphertext (C) After decryption Sender's computation Receiver's computation        P       P  (mod  ) C  (mod  )     C         Figure  -
An example of the RSA algorithm
Because the primes chosen for this example are so small  P must be less than so each plaintext block can contain only a single character
The result is a monoalphabetic substitution cipher  not very impressive
If instead we had chosen p and q â¼â¼  we would have n â¼â¼   so each block could be up to  bits or eight-bit characters  versus  characters for DES and   characters for AES
It should be pointed out that using RSA as we have described is similar to using a symmetric algorithm in ECB modeâthe same input block gives the same output block
Therefore  some form of chaining is needed for data encryption
However  in practice  most RSA-based systems use public-key cryptography primarily for distributing one-time session keys for use with some symmetric-key algorithm such as AES or triple DES
RSA is too slow for actually encrypting large volumes of data but is widely used for key distribution
Other Public-Key Algorithms Although RSA is widely used  it is by no means the only public-key algorithm known
The first public-key algorithm was the knapsack algorithm (Merkle and Hellman  )
The idea here is that someone owns a large number of objects  each with a different weight
The owner encodes the message by retly selecting a subset of the objects and placing them in the knapsack
The total weight of the objects in the knapsack is made public  as is the list of all possible objects and their corresponding weights
The list of objects in the knapsack is kept ret
With certain additional restrictions  the problem of figuring out a possible list of objects with the given weight was thought to be computationally infeasible and formed the basis of the public-key algorithm
PUBLIC-KEY ALGORITHMS The algorithmâs inventor  Ralph Merkle  was quite sure that this algorithm could not be broken  so he offered a $   reward to anyone who could break it
Adi Shamir (the ââSââ in RSA) promptly broke it and collected the reward
Undeterred  Merkle strengthened the algorithm and offered a $ reward to anyone who could break the new one
Ronald Rivest (the ââRââ in RSA) promptly broke the new one and collected the reward
Merkle did not dare offer $  for the next version  so ââAââ (Leonard Adleman) was out of luck
Nevertheless  the knapsack algorithm is not considered ure and is not used in practice any more
Other public-key schemes are based on the difficulty of computing discrete logarithms
Algorithms that use this principle have been invented by El Gamal (   ) and Schnorr (   )
A few other schemes exist  such as those based on elliptic curves (Menezes and Vanstone  )  but the two major categories are those based on the difficulty of factoring large numbers and computing discrete logarithms modulo a large prime
These problems are thought to be genuinely difficult to solveâ mathematicians have been working on them for many years without any great breakthroughs  DIGITAL SIGNATURES The authenticity of many legal  financial  and other documents is determined by the presence or absence of an authorized handwritten signature
And photocopies do not count
For computerized message systems to replace the physical transport of paper-and-ink documents  a method must be found to allow documents to be signed in an unforgeable way
The problem of devising a replacement for handwritten signatures is a difficult one
Basically  what is needed is a system by which one party can send a signed message to another party in such a way that the following conditions hold:
The receiver can verify the claimed identity of the sender The sender cannot later repudiate the contents of the message The receiver cannot possibly have concocted the message himself
The first requirement is needed  for example  in financial systems
When a customerâs computer orders a bankâs computer to buy a ton of gold  the bankâs computer needs to be able to make sure that the computer giving the order really belongs to the customer whose account is to be debited
In other words  the bank has to authenticate the customer (and the customer has to authenticate the bank)
The ond requirement is needed to protect the bank against fraud
Suppose that the bank buys the ton of gold  and immediately thereafter the price of gold NETWORK URITY
drops sharply
A dishonest customer might then proceed to sue the bank  claiming that he never issued any order to buy gold
When the bank produces the message in court  the customer may deny having sent it
The property that no party to a contract can later deny having signed it is called nonrepudiation
The digital signature schemes that we will now study help provide it
The third requirement is needed to protect the customer in the event that the price of gold shoots up and the bank tries to construct a signed message in which the customer asked for one bar of gold instead of one ton
In this fraud scenario  the bank just keeps the rest of the gold for itself
Symmetric-Key Signatures One approach to digital signatures is to have a central authority that knows everything and whom everyone trusts  say  Big Brother (BB)
Each user then chooses a ret key and carries it by hand to BBâs office
Thus  only Alice and BB know Aliceâs ret key  KA  and so on
When Alice wants to send a signed plaintext message  P  to her banker  Bob  she generates KA(B  RA  t  P)  where B is Bobâs identity  RA is a random number chosen by Alice  t is a timestamp to ensure freshness  and KA(B  RA  t  P) is the message encrypted with her key  KA
Then she sends it as depicted in Fig
BB sees that the message is from Alice  decrypts it  and sends a message to Bob as shown
The message to Bob contains the plaintext of Aliceâs message and also the signed message KBB(A  t  P)
Bob now carries out Aliceâs request
A  KA (B  RA  t  P) Bob Alice BB KB (A  RA  t  P  KBB (A  t  P)) Figure  -
Digital signatures with Big Brother
What happens if Alice later denies sending the message? Step  is that everyone sues everyone (at least  in the United States)
Finally  when the case comes to court and Alice vigorously denies sending Bob the disputed message  the judge will ask Bob how he can be sure that the disputed message came from Alice and not from Trudy
Bob first points out that BB will not accept a message from Alice unless it is encrypted with KA  so there is no possibility of Trudy sending BB a false message from Alice without BB detecting it immediately
Bob then dramatically produces Exhibit A: KBB(A  t  P)
Bob says that this is a message signed by BB that proves Alice sent P to Bob
The judge then asks BB (whom everyone trusts) to decrypt Exhibit A
When BB testifies that Bob is telling the truth  the judge decides in favor of Bob
Case dismissed
DIGITAL SIGNATURES One potential problem with the signature protocol of Fig
To minimize this problem  timestamps are used throughout
Furthermore  Bob can check all recent messages to see if RA was used in any of them
If so  the message is discarded as a replay
Note that based on the timestamp  Bob will reject very old messages
To guard against instant replay attacks  Bob just checks the RA of every incoming message to see if such a message has been received from Alice in the past hour
If not  Bob can safely assume this is a new request
Public-Key Signatures A structural problem with using symmetric-key cryptography for digital signatures is that everyone has to agree to trust Big Brother
Furthermore  Big Brother gets to read all signed messages
The most logical candidates for running the Big Brother server are the government  the banks  the accountants  and the lawyers
Unfortunately  none of these inspire total confidence in all citizens
Hence  it would be nice if signing documents did not require a trusted authority
Fortunately  public-key cryptography can make an important contribution in this area
Let us assume that the public-key encryption and decryption algorithms have the property that E(D(P)) = P  in addition  of course  to the usual property that D(E(P)) = P
(RSA has this property  so the assumption is not unreasonable
) Assuming that this is the case  Alice can send a signed plaintext message  P  to Bob by transmitting EB(DA(P))
Note carefully that Alice knows her own (private) key  DA  as well as Bobâs public key  EB  so constructing this message is something Alice can do
When Bob receives the message  he transforms it using his private key  as usual  yielding DA(P)  as shown in Fig
He stores this text in a safe place and then applies EA to get the original plaintext
Bob's public key  EB Alice's private key  DA Bob's private key  DB DA(P) EB (DA(P)) DA(P) Transmission line Alice's computer Bob's computer P P Alice's public key  EA Figure  -
Digital signatures using public-key cryptography
To see how the signature property works  suppose that Alice subsequently denies having sent the message P to Bob
When the case comes up in court  Bob can produce both P and DA(P)
The judge can easily verify that Bob indeed has a valid message encrypted by DA by simply applying EA to it
Since Bob does not NETWORK URITY
know what Aliceâs private key is  the only way Bob could have acquired a message encrypted by it is if Alice did indeed send it
While in jail for perjury and fraud  Alice will have much time to devise interesting new public-key algorithms
Although using public-key cryptography for digital signatures is an elegant scheme  there are problems that are related to the environment in which they operate rather than to the basic algorithm
For one thing  Bob can prove that a message was sent by Alice only as long as DA remains ret
If Alice discloses her ret key  the argument no longer holds  because anyone could have sent the message  including Bob himself
The problem might arise  for example  if Bob is Aliceâs stockbroker
Suppose that Alice tells Bob to buy a certain stock or bond
Immediately thereafter  the price drops sharply
To repudiate her message to Bob  Alice runs to the police claiming that her home was burglarized and the PC holding her key was stolen
Depending on the laws in her state or country  she may or may not be legally liable  especially if she claims not to have discovered the break-in until getting home from work  several hours after it allegedly happened
Another problem with the signature scheme is what happens if Alice decides to change her key
Doing so is clearly legal  and it is probably a good idea to do so periodically
If a court case later arises  as described above  the judge will apply the current EA to DA(P) and discover that it does not produce P
Bob will look pretty stupid at this point
In principle  any public-key algorithm can be used for digital signatures
The de facto industry standard is the RSA algorithm
Many urity products use it
However  in  NIST proposed using a variant of the El Gamal public-key algorithm for its new Digital Signature Standard (DSS)
El Gamal gets its urity from the difficulty of computing discrete logarithms  rather than from the difficulty of factoring large numbers
As usual when the government tries to dictate cryptographic standards  there was an uproar
DSS was criticized for being
Too ret (NSA designed the protocol for using El Gamal) Too slow (  to   times slower than RSA for checking signatures) Too new (El Gamal had not yet been thoroughly analyzed) Too inure (fixed   -bit key)
In a subsequent revision  the fourth point was rendered moot when keys up to  bits were allowed
Nevertheless  the first two points remain valid
Message Digests One criticism of signature methods is that they often couple two distinct functions: authentication and recy
Often  authentication is needed but recy is not always needed
Also  getting an export license is often easier if the system in   DIGITAL SIGNATURES question provides only authentication but not recy
Below we will describe an authentication scheme that does not require encrypting the entire message
This scheme is based on the idea of a one-way hash function that takes an arbitrarily long piece of plaintext and from it computes a fixed-length bit string
This hash function  MD  often called a message digest  has four important properties:
Given P  it is easy to compute MD(P) Given MD(P)  it is effectively impossible to find P Given P  no one can find Pâ² such that MD(Pâ²) = MD(P) A change to the input of even  bit produces a very different output
To meet criterion   the hash should be at least bits long  preferably more
To meet criterion   the hash must mangle the bits very thoroughly  not unlike the symmetric-key encryption algorithms we have seen
Computing a message digest from a piece of plaintext is much faster than encrypting that plaintext with a public-key algorithm  so message digests can be used to speed up digital signature algorithms
To see how this works  consider the signature protocol of Fig
Instead  of signing P with KBB(A  t  P)  BB now computes the message digest by applying MD to P  yielding MD(P)
BB then encloses KBB(A  t  MD(P)) as the fifth item in the list encrypted with KB that is sent to Bob  instead of KBB(A  t  P)
If a dispute arises  Bob can produce both P and KBB(A  t  MD(P))
After Big Brother has decrypted it for the judge  Bob has MD(P)  which is guaranteed to be genuine  and the alleged P
However  since it is effectively impossible for Bob to find any other message that gives this hash  the judge will easily be convinced that Bob is telling the truth
Using message digests in this way saves both encryption time and message transport costs
Message digests work in public-key cryptosystems  too  as shown in Fig
Here  Alice first computes the message digest of her plaintext
She then signs the message digest and sends both the signed digest and the plaintext to Bob
If Trudy replaces P along the way  Bob will see this when he computes MD(P)
P  DA (MD (P)) Bob Alice Figure  -
Digital signatures using message digests
NETWORK URITY
SHA-  and SHA-  A variety of message digest functions have been proposed
One of the most widely used functions is SHA-  (ure Hash Algorithm  ) (NIST  )
Like all message digests  it operates by mangling bits in a sufficiently complicated way that every output bit is affected by every input bit
SHA-  was developed by NSA and blessed by NIST in FIPS   -
It processes input data in   -bit blocks  and it generates a   -bit message digest
A typical way for Alice to send a nonret but signed message to Bob is illustrated in Fig
Here  her plaintext message is fed into the SHA-  algorithm to get a   -bit SHA-  hash
Alice then signs the hash with her RSA private key and sends both the plaintext message and the signed hash to Bob
SHA-  algorithm H   -Bit SHA-  hash of M DA(H) Signed hash RSA algorithm Alice's private key  DA Sent to Bob Alice's plaintext message M (arbitrary length) Figure  -
Use of SHA-  and RSA for signing nonret messages
After receiving the message  Bob computes the SHA-  hash himself and also applies Aliceâs public key to the signed hash to get the original hash  H
If the two agree  the message is considered valid
Since there is no way for Trudy to modify the (plaintext) message while it is in transit and produce a new one that hashes to H  Bob can easily detect any changes Trudy has made to the message
For messages whose integrity is important but whose contents are not ret  the scheme of Fig
For a relatively small cost in computation  it guarantees that any modifications made to the plaintext message in transit can be detected with very high probability
Now let us briefly see how SHA-  works
It starts out by padding the message by adding a  bit to the end  followed by as many  bits as are necessary  but at least to make the length a multiple of bits
Then a  -bit number containing the message length before padding is ORed into the low-order   bits
-   the message is shown with padding on the right because English text and figures go from left to right (
the lower right is generally perceived as the end of the figure)
With computers  this orientation corresponds to big-endian machines such as the SPARC and the IBM and its successors  but SHA-  always pads the end of the message  no matter which endian machine is used
DIGITAL SIGNATURES M  H  W  M  H  W  M  H  W  H  Mn-  (a) Start of message   -Bit block  -Bit word Padding (b) (c) H  W  Figure  -
(a) A message padded out to a multiple of bits
(b) The output variables
(c) The word array
During the computation  SHA-  maintains five  -bit variables  H  through H  where the hash accumulates
These are shown in Fig
They are initialized to constants specified in the standard
Each of the blocks M  through Mn â  is now processed in turn
For the current block  the   words are first copied into the start of an auxiliary  -word array  W  as shown in Fig
Then the other   words in W are filled in using the formula Wi = S (Wi â  XOR Wi â  XOR Wi â  XOR Wi â  ) (  â¤ i â¤  ) where Sb(W) represents the left circular rotation of the  -bit word  W  by b bits
Now five scratch variables  A through E  are initialized from H  through H  respectively
The actual calculation can be expressed in pseudo-C as for (i =  ; i <  ; i++) { temp = S (A) + fi (B  C  D) + E + Wi + Ki; E = D; D = C; C = S  (B); B = A; A = temp; } where the Ki constants are defined in the standard
The mixing functions fi are defined as fi (B C D) = (B AND C) OR (NOT B AND D) (  â¤ i â¤  ) fi (B C D) = B XOR C XOR D (  â¤ i â¤  ) fi (B C D) = (B AND C) OR (B AND D) OR (C AND D) (  â¤ i â¤  ) fi (B C D) = B XOR C XOR D (  â¤ i â¤  ) When all   iterations of the loop are completed  A through E are added to H  through H  respectively
Now that the first   -bit block has been processed  the next one is started
The W array is reinitialized from the new block  but H is left as it was
When this NETWORK URITY
block is finished  the next one is started  and so on  until all the   -bit message blocks have been tossed into the soup
When the last block has been finished  the five  -bit words in the H array are output as the   -bit cryptographic hash
The complete C code for SHA-  is given in RFC
New versions of SHA-  have been developed that produce hashes of   and bits
Collectively  these versions are called SHA-
Not only are these hashes longer than SHA-  hashes  but the digest function has been changed to combat some potential weaknesses of SHA-
SHA-  is not yet widely used  but it is likely to be in the future
MD  For completeness  we will mention another digest that is popular
MD  (Rivest  ) is the fifth in a series of message digests designed by Ronald Rivest
Very briefly  the message is padded to a length of bits (modulo   )
Then the original length of the message is appended as a  -bit integer to give a total input whose length is a multiple of bits
Each round of the computation takes a   -bit block of input and mixes it thoroughly with a running   -bit buffer
For good measure  the mixing uses a table constructed from the sine function
The point of using a known function is to avoid any suspicion that the designer built in a clever back door through which only he can enter
This process continues until all the input blocks have been consumed
The contents of the   -bit buffer form the message digest
After more than a decade of solid use and study  weaknesses in MD  have led to the ability to find collisions  or different messages with the same hash (Sotirov  et al
This is the death knell for a digest function because it means that the digest cannot safely be used to represent a message
Thus  the urity community considers MD  to be broken; it should be replaced where possible and no new systems should use it as part of their design
Nevertheless  you may still see MD  used in existing systems
The Birthday Attack In the world of crypto  nothing is ever what it seems to be
One might think that it would take on the order of  m operations to subvert an m-bit message digest
In fact  m/  operations will often do using the birthday attack  an approach published by Yuval (   ) in his now-classic paper ââHow to Swindle Rabin
ââ The idea for this attack comes from a technique that math professors often use in their probability courses
The question is: how many students do you need in a class before the probability of having two people with the same birthday exceeds  / ? Most students expect the answer to be way over
In fact  probability theory says it is just
Without giving a rigorous analysis  intuitively  with     DIGITAL SIGNATURES people  we can form (  Ã  )/  = different pairs  each of which has a probability of  /   of being a hit
In this light  it is not really so surprising any more
More generally  if there is some mapping between inputs and outputs with n inputs (people  messages  etc
) and k possible outputs (birthdays  message digests  etc
)  there are n(n â  )/  input pairs
If n(n â  )/  > k  the chance of having at least one match is pretty good
Thus  approximately  a match is likely for n > âk
This result means that a  -bit message digest can probably be broken by generating about messages and looking for two with the same message digest
Let us look at a practical example
The Department of Computer Science at State University has one position for a tenured faculty member and two candidates  Tom and Dick
Tom was hired two years before Dick  so he goes up for review first
If he gets it  Dick is out of luck
Tom knows that the department chairperson  Marilyn  thinks highly of his work  so he asks her to write him a letter of recommendation to the Dean  who will decide on Tomâs case
Once sent  all letters become confidential
Marilyn tells her retary  Ellen  to write the Dean a letter  outlining what she wants in it
When it is ready  Marilyn will review it  compute and sign the  -bit digest  and send it to the Dean
Ellen can send the letter later by email
Unfortunately for Tom  Ellen is romantically involved with Dick and would like to do Tom in  so she writes the following letter with the   bracketed options: Dear Dean Smith  This [letter | message] is to give my [honest | frank] opinion of Prof
Tom Wilson  who is [a candidate | up] for tenure [now | this year]
I have [known | worked with] Prof
Wilson for [about | almost] six years
He is an [outstanding | excellent] researcher of great [talent | ability] known [worldwide | internationally] for his [brilliant | creative] insights into [many | a wide variety of] [difficult | challenging] problems
He is also a [highly | greatly] [respected | admired] [teacher | educator]
His students give his [classes | courses] [rave | spectacular] reviews
He is [our | the Departmentâs] [most popular | best-loved] [teacher | instructor]
[In addition | Additionally] Prof
Wilson is a [gifted | effective] fund raiser
His [grants | contracts] have brought a [large | substantial] amount of money into [the | our] Department
[This money has | These funds have] [enabled | permitted] us to [pursue | carry out] many [special | important] programs  [such as | for example] your State  program
Without these funds we would [be unable | not be able] to continue this program  which is so [important | essential] to both of us
I strongly urge you to grant him tenure
Unfortunately for Tom  as soon as Ellen finishes composing and typing in this letter  she also writes a ond one: NETWORK URITY
Dear Dean Smith  This [letter | message] is to give my [honest | frank] opinion of Prof
Tom Wilson  who is [a candidate | up] for tenure [now | this year]
I have [known | worked with] Tom for [about | almost] six years
He is a [poor | weak] researcher not well known in his [field | area]
His research [hardly ever | rarely] shows [insight in | understanding of] the [key | major] problems of [the | our] day
Furthermore  he is not a [respected | admired] [teacher | educator]
His students give his [classes | courses] [poor | bad ] reviews
He is [our | the Departmentâs] least popular [teacher | instructor]  known [mostly | primarily] within [the | our] Department for his [tendency | propensity] to [ridicule | embarrass] students [foolish | imprudent] enough to ask questions in his classes
[In addition | Additionally] Tom is a [poor | marginal] fund raiser
His [grants | contracts] have brought only a [meager | insignificant] amount of money into [the | our] Department
Unless new [money is | funds are] quickly located  we may have to cancel some essential programs  such as your State  program
Unfortunately  under these [conditions | circumstances] I cannot in good [conscience | faith] recommend him to you for [tenure | a permanent position]
Now Ellen programs her computer to compute the message digests of each letter overnight
Chances are  one digest of the first letter will match one digest of the ond
If not  she can add a few more options and try again tonight
Suppose that she finds a match
Call the ââgoodââ letter A and the ââbadââ one B
Ellen now emails letter A to Marilyn for approval
Letter B she keeps ret  showing it to no one
Marilyn  of course  approves it  computes her  -bit message digest  signs the digest  and emails the signed digest off to Dean Smith
Independently  Ellen emails letter B to the Dean (not letter A  as she is supposed to)
After getting the letter and signed message digest  the Dean runs the message digest algorithm on letter B  sees that it agrees with what Marilyn sent him  and fires Tom
The Dean does not realize that Ellen managed to generate two letters with the same message digest and sent her a different one than the one Marilyn saw and approved
(Optional ending: Ellen tells Dick what she did
Dick is appalled and breaks off the affair
Ellen is furious and confesses to Marilyn
Marilyn calls the Dean
Tom gets tenure after all
) With SHA-  the birthday attack is difficult because even at the ridiculous speed of  trillion digests per ond  it would take over   years to compute all digests of two letters with   variants each  and even then a match is not guaranteed
With a cloud of  chips working in parallel    years becomes  weeks  MANAGEMENT OF PUBLIC KEYS Public-key cryptography makes it possible for people who do not share a common key in advance to nevertheless communicate urely
It also makes signing messages possible without the presence of a trusted third party
Finally    MANAGEMENT OF PUBLIC KEYS signed message digests make it possible for the recipient to verify the integrity of received messages easily and urely
However  there is one problem that we have glossed over a bit too quickly: if Alice and Bob do not know each other  how do they get each otherâs public keys to start the communication process? The obvious solutionâput your public key on your Web siteâdoes not work  for the following reason
Suppose that Alice wants to look up Bobâs public key on his Web site
How does she do it? She starts by typing in Bobâs URL
Her browser then looks up the DNS address of Bobâs home page and sends it a GET request  as shown in Fig
Unfortunately  Trudy intercepts the request and replies with a fake home page  probably a copy of Bobâs home page except for the replacement of Bobâs public key with Trudyâs public key
When Alice now encrypts her first message with ET  Trudy decrypts it  reads it  re-encrypts it with Bobâs public key  and sends it to Bob  who is none the wiser that Trudy is reading his incoming messages
Worse yet  Trudy could modify the messages before reencrypting them for Bob
Clearly  some mechanism is needed to make sure that public keys can be exchanged urely EB(Message) Alice Trudy
GET Bob's home page
Fake home page with ET
ET(Message) Bob Figure  -
A way for Trudy to subvert public-key encryption
Certificates As a first attempt at distributing public keys urely  we could imagine a KDC key distribution center available online   hours a day to provide public keys on demand
One of the many problems with this solution is that it is not scalable  and the key distribution center would rapidly become a bottleneck
Also  if it ever went down  Internet urity would suddenly grind to a halt
For these reasons  people have developed a different solution  one that does not require the key distribution center to be online all the time
In fact  it does not have to be online at all
Instead  what it does is certify the public keys belonging to people  companies  and other organizations
An organization that certifies public keys is now called a CA (Certification Authority)
As an example  suppose that Bob wants to allow Alice and other people he does not know to communicate with him urely
He can go to the CA with his public key along with his passport or driverâs license and ask to be certified
The CA then issues a certificate similar to the one in Fig
hash with the CAâs private key
Bob then pays the CAâs fee and gets a CD-ROM containing the certificate and its signed hash
I hereby certify that the public key  A B CF E  FC s    FFA    A belongs to Robert John Smith  University Avenue Berkeley  CA  Birthday: July    Email: bob@  SHA-  hash of the above certificate signed with the CAâs private key Figure  -
A possible certificate and its signed hash
The fundamental job of a certificate is to bind a public key to the name of a principal (individual  company  etc
Certificates themselves are not ret or protected
Bob might  for example  decide to put his new certificate on his Web site  with a link on the main page saying: Click here for my public-key certificate
The resulting click would return both the certificate and the signature block (the signed SHA-  hash of the certificate)
Now let us run through the scenario of Fig
When Trudy intercepts Aliceâs request for Bobâs home page  what can she do? She can put her own certificate and signature block on the fake page  but when Alice reads the contents of the certificate she will immediately see that she is not talking to Bob because Bobâs name is not in it
Trudy can modify Bobâs home page on the fly  replacing Bobâs public key with her own
However  when Alice runs the SHA-  algorithm on the certificate  she will get a hash that does not agree with the one she gets when she applies the CAâs well-known public key to the signature block
Since Trudy does not have the CAâs private key  she has no way of generating a signature block that contains the hash of the modified Web page with her public key on it
In this way  Alice can be sure she has Bobâs public key and not Trudyâs or someone elseâs
And as we promised  this scheme does not require the CA to be online for verification  thus eliminating a potential bottleneck
While the standard function of a certificate is to bind a public key to a principal  a certificate can also be used to bind a public key to an attribute
For example  a certificate could say: ââThis public key belongs to someone over
ââ It could be used to prove that the owner of the private key was not a minor and thus allowed to access material not suitable for children  and so on  but without disclosing the ownerâs identity
Typically  the person holding the certificate would send it to the Web site  principal  or process that cared about age
That site  principal  or process would then generate a random number and encrypt it with the public key in the certificate
If the owner were able to decrypt it and send it back    MANAGEMENT OF PUBLIC KEYS that would be proof that the owner indeed had the attribute stated in the certificate
Alternatively  the random number could be used to generate a session key for the ensuing conversation
Another example of where a certificate might contain an attribute is in an object- oriented distributed system
Each object normally has multiple methods
The owner of the object could provide each customer with a certificate giving a bit map of which methods the customer is allowed to invoke and binding the bit map to a public key using a signed certificate
Again  if the certificate holder can prove possession of the corresponding private key  he will be allowed to perform the methods in the bit map
This approach has the property that the ownerâs identity need not be known  a property useful in situations where privacy is important
If everybody who wanted something signed went to the CA with a different kind of certificate  managing all the different formats would soon become a problem
To solve this problem  a standard for certificates has been devised and approved by ITU
The standard is called X
and is in widespread use on the Internet
It has gone through three versions since the initial standardization in
We will discuss V
has been heavily influenced by the OSI world  borrowing some of its worst features (
naming and encoding)
Surprisingly  IETF went along with X
even though in nearly every other area  from machine addresses to transport protocols to email formats  IETF generally ignored OSI and tried to do it right
The IETF version of X
is described in RFC
At its core  X
is a way to describe certificates
The primary fields in a certificate are listed in Fig
The descriptions given there should provide a general idea of what the fields do
For additional information  please consult the standard itself or RFC
For example  if Bob works in the loan department of the Money Bank  his X
address might be /C=US/O=MoneyBank/OU=Loan/CN=Bob/ where C is for country  O is for organization  OU is for organizational unit  and CN is for common name
CAs and other entities are named in a similar way
A substantial problem with X
names is that if Alice is trying to contact bob@  and is given a certificate with an X
name  it may not be obvious to her that the certificate refers to the Bob she wants
Fortunately  starting with version   DNS names are now permitted instead of X
names  so this problem may eventually vanish
Certificates are encoded using OSI ASN
(Abstract Syntax Notation  )  which is sort of like a struct in C  except with a extremely peculiar and verbose notation
More information about X
is given by Ford and Baum (   )
NETWORK URITY
Field Meaning Version Which version of X
Serial number This number plus the CAâs name uniquely identifies the certificate Signature algorithm The algorithm used to sign the certificate Issuer X
name of the CA Validity period The starting and ending times of the validity period Subject name The entity whose key is being certified Public key The subjectâs public key and the ID of the algorithm using it Issuer ID An optional ID uniquely identifying the certificateâs issuer Subject ID An optional ID uniquely identifying the certificateâs subject Extensions Many extensions have been defined Signature The certificateâs signature (signed by the CAâs private key) Figure  -
The basic fields of an X
certificate
Public Key Infrastructures Having a single CA to issue all the worldâs certificates obviously would not work
It would collapse under the load and be a central point of failure as well
A possible solution might be to have multiple CAs  all run by the same organization and all using the same private key to sign certificates
While this would solve the load and failure problems  it introduces a new problem: key leakage
If there were dozens of servers spread around the world  all holding the CAâs private key  the chance of the private key being stolen or otherwise leaking out would be greatly increased
Since the compromise of this key would ruin the worldâs electronic urity infrastructure  having a single central CA is very risky
In addition  which organization would operate the CA? It is hard to imagine any authority that would be accepted worldwide as legitimate and trustworthy
In some countries  people would insist that it be a government  while in other countries they would insist that it not be a government
For these reasons  a different way for certifying public keys has evolved
It goes under the general name of PKI (Public Key Infrastructure)
In this tion  we will summarize how it works in general  although there have been many proposals  so the details will probably evolve in time
A PKI has multiple components  including users  CAs  certificates  and directories
What the PKI does is provide a way of structuring these components and define standards for the various documents and protocols
A particularly simple form of PKI is a hierarchy of CAs  as depicted in Fig
In this example we have shown three levels  but in practice there might be fewer or more
The toplevel CA  the root  certifies ond-level CAs  which we here call RAs (Regional   MANAGEMENT OF PUBLIC KEYS Authorities) because they might cover some geographic region  such as a country or continent
This term is not standard  though; in fact  no term is really standard for the different levels of the tree
These in turn certify the real CAs  which issue the X
certificates to organizations and individuals
When the root authorizes a new RA  it generates an X
certificate stating that it has approved the RA  includes the new RAâs public key in it  signs it  and hands it to the RA
Similarly  when an RA approves a new CA  it produces and signs a certificate stating its approval and containing the CAâs public key
CA  CA  (a) (b) CA  CA  CA  RA  RA  is approved
Its public key is  AE
Root's signature RA  Root RA  is approved
Its public key is  AE
Root's signature CA  is approved
Its public key is AF  B
RA  's signature CA  is approved
Its public key is AF  B
RA  's signature Figure  -
(a) A hierarchical PKI
(b) A chain of certificates
Our PKI works like this
Suppose that Alice needs Bobâs public key in order to communicate with him  so she looks for and finds a certificate containing it  signed by CA
But Alice has never heard of CA
For all she knows  CA  might be Bobâs  -year-old daughter
She could go to CA  and say: ââProve your legitimacy
ââ CA  will respond with the certificate it got from RA   which contains CA  âs public key
Now armed with CA  âs public key  she can verify that Bobâs certificate was indeed signed by CA  and is thus legal
Unless RA  is Bobâs  -year-old son
So  the next step is for her to ask RA  to prove it is legitimate
The response to her query is a certificate signed by the root and containing RA âs public key
Now Alice is sure she has Bobâs public key
But how does Alice find the rootâs public key? Magic
It is assumed that everyone knows the rootâs public key
For example  her browser might have been shipped with the rootâs public key built in
Bob is a friendly sort of guy and does not want to cause Alice a lot of work
He knows that she is going to have to check out CA  and RA   so to save her some trouble  he collects the two needed certificates and gives her the two certificates along with his
Now she can use her own knowledge of the rootâs public key to verify the top-level certificate and the public key contained therein to verify the ond one
Alice does not need to contact anyone to do the verification
NETWORK URITY
Because the certificates are all signed  she can easily detect any attempts to tamper with their contents
A chain of certificates going back to the root like this is sometimes called a chain of trust or a certification path
The technique is widely used in practice
Of course  we still have the problem of who is going to run the root
The solution is not to have a single root  but to have many roots  each with its own RAs and CAs
In fact  modern browsers come preloaded with the public keys for over roots  sometimes referred to as trust anchors
In this way  having a single worldwide trusted authority can be avoided
But there is now the issue of how the browser vendor decides which purported trust anchors are reliable and which are sleazy
It all comes down to the user trusting the browser vendor to make wise choices and not simply approve all trust anchors willing to pay its inclusion fee
Most browsers allow users to inspect the root keys (usually in the form of certificates signed by the root) and delete any that seem shady
Directories Another issue for any PKI is where certificates (and their chains back to some known trust anchor) are stored
One possibility is to have each user store his or her own certificates
While doing this is safe (
there is no way for users to tamper with signed certificates without detection)  it is also inconvenient
One alternative that has been proposed is to use DNS as a certificate directory
Before contacting Bob  Alice probably has to look up his IP address using DNS  so why not have DNS return Bobâs entire certificate chain along with his IP address? Some people think this is the way to go  but others would prefer dedicated directory servers whose only job is managing X
certificates
Such directories could provide lookup services by using properties of the X
For example  in theory such a directory service could answer a query such as: ââGive me a list of all people named Alice who work in sales departments anywhere in the
ââ Revocation The real world is full of certificates  too  such as passports and driversâ licenses
Sometimes these certificates can be revoked  for example  driversâ licenses can be revoked for drunken driving and other driving offenses
The same problem occurs in the digital world: the grantor of a certificate may decide to revoke it because the person or organization holding it has abused it in some way
It can also be revoked if the subjectâs private key has been exposed or  worse yet  the CAâs private key has been compromised
Thus  a PKI needs to deal with the issue of revocation
The possibility of revocation complicates matters
MANAGEMENT OF PUBLIC KEYS A first step in this direction is to have each CA periodically issue a CRL (Certificate Revocation List) giving the serial numbers of all certificates that it has revoked
Since certificates contain expiry times  the CRL need only contain the serial numbers of certificates that have not yet expired
Once its expiry time has passed  a certificate is automatically invalid  so no distinction is needed between those that just timed out and those that were actually revoked
In both cases  they cannot be used any more
Unfortunately  introducing CRLs means that a user who is about to use a certificate must now acquire the CRL to see if the certificate has been revoked
If it has been  it should not be used
However  even if the certificate is not on the list  it might have been revoked just after the list was published
Thus  the only way to really be sure is to ask the CA
And on the next use of the same certificate  the CA has to be asked again  since the certificate might have been revoked a few onds ago
Another complication is that a revoked certificate could conceivably be reinstated  for example  if it was revoked for nonpayment of some fee that has since been paid
Having to deal with revocation (and possibly reinstatement) eliminates one of the best properties of certificates  namely  that they can be used without having to contact a CA
Where should CRLs be stored? A good place would be the same place the certificates themselves are stored
One strategy is for the CA to actively push out CRLs periodically and have the directories process them by simply removing the revoked certificates
If directories are not used for storing certificates  the CRLs can be cached at various places around the network
Since a CRL is itself a signed document  if it is tampered with  that tampering can be easily detected
If certificates have long lifetimes  the CRLs will be long  too
For example  if credit cards are valid for  years  the number of revocations outstanding will be much longer than if new cards are issued every  months
A standard way to deal with long CRLs is to issue a master list infrequently  but issue updates to it more often
Doing this reduces the bandwidth needed for distributing the CRLs  COMMUNICATION URITY We have now finished our study of the tools of the trade
Most of the important techniques and protocols have been covered
The rest of the  ter is about how these techniques are applied in practice to provide network urity  plus some thoughts about the social aspects of urity at the end of the  ter
In the following four tions  we will look at communication urity  that is  how to get the bits retly and without modification from source to destination and how to keep unwanted bits outside the door
These are by no means the only urity issues in networking  but they are certainly among the most important ones  making this a good place to start our study
NETWORK URITY
IP IETF has known for years that urity was lacking in the Internet
Adding it was not easy because a war broke out about where to put it
Most urity experts believe that to be really ure  encryption and integrity checks have to be end to end (
in the application layer)
That is  the source process encrypts and/or integrity protects the data and sends them to the destination process where they are decrypted and/or verified
Any tampering done in between these two processes  including within either operating system  can then be detected
The trouble with this approach is that it requires changing all the applications to make them urity aware
In this view  the next best approach is putting encryption in the transport layer or in a new layer between the application layer and the transport layer  making it still end to end but not requiring applications to be changed
The opposite view is that users do not understand urity and will not be capable of using it correctly and nobody wants to modify existing programs in any way  so the network layer should authenticate and/or encrypt packets without the users being involved
After years of pitched battles  this view won enough support that a network layer urity standard was defined
In part  the argument was that having network layer encryption does not prevent urity-aware users from doing it right and it does help urity-unaware users to some extent
The result of this war was a design called IP (IP urity)  which is described in RFCs    and  among others
Not all users want encryption (because it is computationally expensive)
Rather than make it optional  it was decided to require encryption all the time but permit the use of a null algorithm
The null algorithm is described and praised for its simplicity  ease of implementation  and great speed in RFC
The complete IP design is a framework for multiple services  algorithms  and granularities
The reason for multiple services is that not everyone wants to pay the price for having all the services all the time  so the services are available a la carte
The major services are recy  data integrity  and protection from replay attacks (where the intruder replays a conversation)
All of these are based on symmetric-key cryptography because high performance is crucial
The reason for having multiple algorithms is that an algorithm that is now thought to be ure may be broken in the future
By making IP algorithm-independent  the framework can survive even if some particular algorithm is later broken
The reason for having multiple granularities is to make it possible to protect a single TCP connection  all traffic between a pair of hosts  or all traffic between a pair of ure routers  among other possibilities
One slightly surprising aspect of IP is that even though it is in the IP layer  it is connection oriented
Actually  that is not so surprising because to have any urity  a key must be established and used for some period of timeâin essence  a kind of connection by a different name
Also  connections amortize the setup   COMMUNICATION URITY costs over many packets
A ââconnectionââ in the context of IP is called an SA (urity Association)
An SA is a simplex connection between two endpoints and has a urity identifier associated with it
If ure traffic is needed in both directions  two urity associations are required
urity identifiers are carried in packets traveling on these ure connections and are used to look up keys and other relevant information when a ure packet arrives
Technically  IP has two principal parts
The first part describes two new headers that can be added to packets to carry the urity identifier  integrity control data  and other information
The other part  ISAKMP (Internet urity Association and Key Management Protocol)  deals with establishing keys
ISAKMP is a framework
The main protocol for carrying out the work is IKE (Internet Key Exchange)
Version  of IKE as described in RFC  should be used  as the earlier version was deeply flawed  as pointed out by Perlman and Kaufman (   )
IP can be used in either of two modes
In transport mode  the IP header is inserted just after the IP header
The Protocol field in the IP header is changed to indicate that an IP header follows the normal IP header (before the TCP header)
The IP header contains urity information  primarily the SA identifier  a new sequence number  and possibly an integrity check of the payload
In tunnel mode  the entire IP packet  header and all  is encapsulated in the body of a new IP packet with a completely new IP header
Tunnel mode is useful when the tunnel ends at a location other than the final destination
In some cases  the end of the tunnel is a urity gateway machine  for example  a company firewall
This is commonly the case for a VPN (Virtual Private Network)
In this mode  the urity gateway encapsulates and decapsulates packets as they pass through it
By terminating the tunnel at this ure machine  the machines on the company LAN do not have to be aware of IP
Only the urity gateway has to know about it
Tunnel mode is also useful when a bundle of TCP connections is aggregated and handled as one encrypted stream because it prevents an intruder from seeing who is sending how many packets to whom
Sometimes just knowing how much traffic is going where is valuable information
For example  if during a military crisis  the amount of traffic flowing between the Pentagon and the White House were to drop sharply  but the amount of traffic between the Pentagon and some military installation deep in the Colorado Rocky Mountains were to increase by the same amount  an intruder might be able to deduce some useful information from these data
Studying the flow patterns of packets  even if they are encrypted  is called traffic analysis
Tunnel mode provides a way to foil it to some extent
The disadvantage of tunnel mode is that it adds an extra IP header  thus increasing packet size substantially
In contrast  transport mode does not affect packet size as much
The first new header is AH (Authentication Header)
It provides integrity checking and antireplay urity  but not recy (
no data encryption)
The NETWORK URITY
use of AH in transport mode is illustrated in Fig
In IPv  it is interposed between the IP header (including any options) and the TCP header
In IPv  it is just another extension header and is treated as such
In fact  the format is close to that of a standard IPv  extension header
The payload may have to be padded out to some particular length for the authentication algorithm  as shown
IP header AH   Bits urity parameters index Next header Payload len (Reserved) Sequence number Authentication data (HMAC) TCP header Authenticated Payload + padding Figure  -
The IP authentication header in transport mode for IPv
Let us now examine the AH header
The Next header field is used to store the value that the IP Protocol field had before it was replaced with   to indicate that an AH header follows
In most cases  the code for TCP ( ) will go here
The Payload length is the number of  -bit words in the AH header minus
The urity parameters index is the connection identifier
It is inserted by the sender to indicate a particular record in the receiverâs database
This record contains the shared key used on this connection and other information about the connection
If this protocol had been invented by ITU rather than IETF  this field would have been called Virtual circuit number
The Sequence number field is used to number all the packets sent on an SA
Every packet gets a unique number  even retransmissions
In other words  the retransmission of a packet gets a different number here than the original (even though its TCP sequence number is the same)
The purpose of this field is to detect replay attacks
These sequence numbers may not wrap around
If all are exhausted  a new SA must be established to continue communication
Finally  we come to Authentication data  which is a variable-length field that contains the payloadâs digital signature
When the SA is established  the two sides negotiate which signature algorithm they are going to use
Normally  public- key cryptography is not used here because packets must be processed extremely rapidly and all known public-key algorithms are too slow
Since IP is based on symmetric-key cryptography and the sender and receiver negotiate a shared key before setting up an SA  the shared key is used in the signature computation
One simple way is to compute the hash over the packet plus the shared key
The shared key is not transmitted  of course
A scheme like this is called an HMAC   COMMUNICATION URITY (Hashed Message Authentication Code)
It is much faster to compute than first running SHA-  and then running RSA on the result
The AH header does not allow encryption of the data  so it is mostly useful when integrity checking is needed but recy is not needed
One noteworthy feature of AH is that the integrity check covers some of the fields in the IP header  namely  those that do not change as the packet moves from router to router
The Time to live field changes on each hop  for example  so it cannot be included in the integrity check
However  the IP source address is included in the check  making it impossible for an intruder to falsify the origin of a packet
The alternative IP header is ESP (Encapsulating urity Payload)
Its use for both transport mode and tunnel mode is shown in Fig
ESP header New IP header Old IP header TCP header Authenticated (b) Payload + padding Authentication (HMAC) ESP header IP header TCP (a) header Payload + padding Authentication (HMAC) Authenticated Encrypted Encrypted Figure  -
(a) ESP in transport mode
(b) ESP in tunnel mode
The ESP header consists of two  -bit words
They are the urity parameters index and Sequence number fields that we saw in AH
A third word that generally follows them (but is technically not part of the header) is the Initialization vector used for the data encryption  unless null encryption is used  in which case it is omitted
ESP also provides for HMAC integrity checks  as does AH  but rather than being included in the header  they come after the payload  as shown in Fig
Putting the HMAC at the end has an advantage in a hardware implementation: the HMAC can be calculated as the bits are going out over the network interface and appended to the end
This is why Ethernet and other LANs have their CRCs in a trailer  rather than in a header
With AH  the packet has to be buffered and the signature computed before the packet can be sent  potentially reducing the number of packets/ that can be sent
Given that ESP can do everything AH can do and more and is more efficient to boot  the question arises: why bother having AH at all? The answer is mostly historical
Originally  AH handled only integrity and ESP handled only recy
Later  integrity was added to ESP  but the people who designed AH did not want to let it die after all that work
Their only real argument is that AH checks part of the IP header  which ESP does not  but other than that it is really a weak argument
Another weak argument is that a product supporting AH but not ESP might NETWORK URITY
have less trouble getting an export license because it cannot do encryption
AH is likely to be phased out in the future
Firewalls The ability to connect any computer  anywhere  to any other computer  anywhere  is a mixed blessing
For individuals at home  wandering around the Internet is lots of fun
For corporate urity managers  it is a nightmare
Most companies have large amounts of confidential information onlineâtrade rets  product development plans  marketing strategies  financial analyses  etc
Disclosure of this information to a competitor could have dire consequences
In addition to the danger of information leaking out  there is also a danger of information leaking in
In particular  viruses  worms  and other digital pests can breach urity  destroy valuable data  and waste large amounts of administratorsâ time trying to clean up the mess they leave
Often they are imported by careless employees who want to play some nifty new game
Consequently  mechanisms are needed to keep ââgoodââ bits in and ââbadââ bits out
One method is to use IP
This approach protects data in transit between ure sites
However  IP does nothing to keep digital pests and intruders from getting onto the company LAN
To see how to accomplish this goal  we need to look at firewalls
Firewalls are just a modern adaptation of that old medieval urity standby: digging a deep moat around your castle
This design forced everyone entering or leaving the castle to pass over a single drawbridge  where they could be inspected by the I/O police
With networks  the same trick is possible: a company can have many LANs connected in arbitrary ways  but all traffic to or from the company is forced through an electronic drawbridge (firewall)  as shown in Fig
No other route exists
Internal network DeMilitarized zone External Internet Email server Web server urity perimeter Firewall Figure  -
A firewall protecting an internal network
COMMUNICATION URITY The firewall acts as a packet filter
It inspects each and every incoming and outgoing packet
Packets meeting some criterion described in rules formulated by the network administrator are forwarded normally
Those that fail the test are uncermoniously dropped
The filtering criterion is typically given as rules or tables that list sources and destinations that are acceptable  sources and destinations that are blocked  and default rules about what to do with packets coming from or going to other machines
In the common case of a TCP/IP setting  a source or destination might consist of an IP address and a port
Ports indicate which service is desired
For example  TCP port   is for mail  and TCP port   is for HTTP
Some ports can simply be blocked
For example  a company could block incoming packets for all IP addresses combined with TCP port
It was once popular for the Finger service to look up peopleâs email addresses but is little used today
Other ports are not so easily blocked
The difficulty is that network administrators want urity but cannot cut off communication with the outside world
That arrangement would be much simpler and better for urity  but there would be no end to user complaints about it
This is where arrangements such as the DMZ (DeMilitarized Zone) shown in Fig
The DMZ is the part of the company network that lies outside of the urity perimeter
Anything goes here
By placing a machine such as a Web server in the DMZ  computers on the Internet can contact it to browse the company Web site
Now the firewall can be configured to block incoming TCP traffic to port   so that computers on the Internet cannot use this port to attack computers on the internal network
To allow the Web server to be managed  the firewall can have a rule to permit connections between internal machines and the Web server
Firewalls have become much more sophisticated over time in an arms race with attackers
Originally  firewalls applied a rule set independently for each packet  but it proved difficult to write rules that allowed useful functionality but blocked all unwanted traffic
Stateful firewalls map packets to connections and use TCP/IP header fields to keep track of connections
This allows for rules that  for example  allow an external Web server to send packets to an internal host  but only if the internal host first establishes a connection with the external Web server
Such a rule is not possible with stateless designs that must either pass or drop all packets from the external Web server
Another level of sophistication up from stateful processing is for the firewall to implement application-level gateways
This processing involves the firewall looking inside packets  beyond even the TCP header  to see what the application is doing
With this capability  it is possible to distinguish HTTP traffic used for Web browsing from HTTP traffic used for peer-to-peer file sharing
Administrators can write rules to spare the company from peer-to-peer file sharing but allow Web browsing that is vital for business
For all of these methods  outgoing traffic can be inspected as well as incoming traffic  for example  to prevent sensitive documents from being emailed outside of the company
NETWORK URITY
As the above discussion should make clear  firewalls violate the standard layering of protocols
They are network layer devices  but they peek at the transport and applications layers to do their filtering
This makes them fragile
For instance  firewalls tend to rely on standard port numbering conventions to determine what kind of traffic is carried in a packet
Standard ports are often used  but not by all computers  and not by all applications either
Some peer-to-peer applications select ports dynamically to avoid being easily spotted (and blocked)
Encryption with IP or other schemes hides higher-layer information from the firewall
Finally  a firewall cannot readily talk to the computers that communicate through it to tell them what policies are being applied and why their connection is being dropped
It must simply pretend to be a broken wire
For all these reasons  networking purists consider firewalls to be a blemish on the architecture of the Internet
However  the Internet can be a dangerous place if you are a computer
Firewalls help with that problem  so they are likely to stay
Even if the firewall is perfectly configured  plenty of urity problems still exist
For example  if a firewall is configured to allow in packets from only specific networks (
the companyâs other plants)  an intruder outside the firewall can put in false source addresses to bypass this check
If an insider wants to ship out ret documents  he can encrypt them or even photograph them and ship the photos as JPEG files  which bypasses any email filters
And we have not even discussed the fact that  although three-quarters of all attacks come from outside the firewall  the attacks that come from inside the firewall  for example  from disgruntled employees  are typically the most damaging (Verizon  )
A different problem with firewalls is that they provide a single perimeter of defense
If that defense is breached  all bets are off
For this reason  firewalls are often used in a layered defense
For example  a firewall may guard the entrance to the internal network and each computer may also run its own firewall
Readers who think that one urity checkpoint is enough clearly have not made an international flight on a scheduled airline recently
In addition  there is a whole other class of attacks that firewalls cannot deal with
The basic idea of a firewall is to prevent intruders from getting in and ret data from getting out
Unfortunately  there are people who have nothing better to do than try to bring certain sites down
They do this by sending legitimate packets at the target in great numbers until it collapses under the load
For example  to cripple a Web site  an intruder can send a TCP SYN packet to establish a connection
The site will then allocate a table slot for the connection and send a SYN + ACK packet in reply
If the intruder does not respond  the table slot will be tied up for a few onds until it times out
If the intruder sends thousands of connection requests  all the table slots will fill up and no legitimate connections will be able to get through
Attacks in which the intruderâs goal is to shut down the target rather than steal data are called DoS (Denial of Service) attacks
Usually  the request packets have false source addresses so the intruder cannot be traced easily
DoS attacks against major Web sites are common on the Internet
COMMUNICATION URITY An even worse variant is one in which the intruder has already broken into hundreds of computers elsewhere in the world  and then commands all of them to attack the same target at the same time
Not only does this approach increase the intruderâs firepower  but it also reduces his chances of detection since the packets are coming from a large number of machines belonging to unsuspecting users
Such an attack is called a DDoS (Distributed Denial of Service) attack
This attack is difficult to defend against
Even if the attacked machine can quickly recognize a bogus request  it does take some time to process and discard the request  and if enough requests per ond arrive  the CPU will spend all its time dealing with them
Virtual Private Networks Many companies have offices and plants scattered over many cities  sometimes over multiple countries
In the olden days  before public data networks  it was common for such companies to lease lines from the telephone company between some or all pairs of locations
Some companies still do this
A network built up from company computers and leased telephone lines is called a private network
Private networks work fine and are very ure
If the only lines available are the leased lines  no traffic can leak out of company locations and intruders have to physically wiretap the lines to break in  which is not easy to do
The problem with private networks is that leasing a dedicated T  line between two points costs thousands of dollars a month  and T  lines are many times more expensive
When public data networks and later the Internet appeared  many companies wanted to move their data (and possibly voice) traffic to the public network  but without giving up the urity of the private network
This demand soon led to the invention of VPNs (Virtual Private Networks)  which are overlay networks on top of public networks but with most of the properties of private networks
They are called ââvirtualââ because they are merely an illusion  just as virtual circuits are not real circuits and virtual memory is not real memory
One popular approach is to build VPNs directly over the Internet
A common design is to equip each office with a firewall and create tunnels through the Internet between all pairs of offices  as illustrated in Fig
A further advantage of using the Internet for connectivity is that the tunnels can be set up on demand to include  for example  the computer of an employee who is at home or traveling as long as the person has an Internet connection
This flexibility is much greater then is provided with leased lines  yet from the perspective of the computers on the VPN  the topology looks just like the private network case  as shown in Fig
When the system is brought up  each pair of firewalls has to negotiate the parameters of its SA  including the services  modes  algorithms  and keys
If IP is used for the tunneling  it is possible to aggregate all traffic between any NETWORK URITY
Home Internet Paris office London office Travel Home Travel London Paris (a) (b) Figure  -
(a) A virtual private network
(b) Topology as seen from the inside
two pairs of offices onto a single authenticated  encrypted SA  thus providing integrity control  recy  and even considerable immunity to traffic analysis
Many firewalls have VPN capabilities built in
Some ordinary routers can do this as well  but since firewalls are primarily in the urity business  it is natural to have the tunnels begin and end at the firewalls  providing a clear separation between the company and the Internet
Thus  firewalls  VPNs  and IP with ESP in tunnel mode are a natural combination and widely used in practice
Once the SAs have been established  traffic can begin flowing
To a router within the Internet  a packet traveling along a VPN tunnel is just an ordinary packet
The only thing unusual about it is the presence of the IP header after the IP header  but since these extra headers have no effect on the forwarding process  the routers do not care about this extra header
Another approach that is gaining popularity is to have the ISP set up the VPN
Using MPLS (as discussed in
)  paths for the VPN traffic can be set up across the ISP network between the company offices
These paths keep the VPN traffic separate from other Internet traffic and can be guaranteed a certain amount of bandwidth or other quality of service
A key advantage of a VPN is that it is completely transparent to all user software
The firewalls set up and manage the SAs
The only person who is even aware of this setup is the system administrator who has to configure and manage the urity gateways  or the ISP administrator who has to configure the MPLS paths
To everyone else  it is like having a leased-line private network again
For more about VPNs  see Lewis (   )
Wireless urity It is surprisingly easy to design a system using VPNs and firewalls that is logically completely ure but that  in practice  leaks like a sieve
This situation can occur if some of the machines are wireless and use radio communication  which passes right over the firewall in both directions
The range of
networks is   COMMUNICATION URITY often a few hundred meters  so anyone who wants to spy on a company can simply drive into the employee parking lot in the morning  leave an
By late afternoon  the hard disk will be full of valuable goodies
Theoretically  this leakage is not supposed to happen
Theoretically  people are not supposed to rob banks  either
Much of the urity problem can be traced to the manufacturers of wireless base stations (access points) trying to make their products user friendly
Usually  if the user takes the device out of the box and plugs it into the electrical power socket  it begins operating immediatelyânearly always with no urity at all  blurting rets to everyone within radio range
If it is then plugged into an Ethernet  all the Ethernet traffic suddenly appears in the parking lot as well
Wireless is a snooperâs dream come true: free data without having to do any work
It therefore goes without saying that urity is even more important for wireless systems than for wired ones
In this tion  we will look at some ways wireless networks handle urity
Some additional information is given by Nichols and Lekkas (   )
urity Part of the
standard  originally called
i  prescribes a data linklevel urity protocol for preventing a wireless node from reading or interfering with messages sent between another pair of wireless nodes
It also goes by the trade name WPA  (WiFi Protected Access  )
Plain WPA is an interim scheme that implements a subset of
It should be avoided in favor of WPA
We will describe
i shortly  but will first note that it is a replacement for WEP (Wired Equivalent Privacy)  the first generation of
urity protocols
WEP was designed by a networking standards committee  which is a completely different process than  for example  the way NIST selected the design of AES
The results were devastating
What was wrong with it? Pretty much everything from a urity perspective as it turns out
For example  WEP encrypted data for confidentiality by XORing it with the output of a stream cipher
Unfortunately  weak keying arrangements meant that the output was often reused
This led to trivial ways to defeat it
As another example  the integrity check was based on a  -bit CRC
That is an efficient code for detecting transmission errors  but it is not a cryptographically strong mechanism for defeating attackers
These and other design flaws made WEP very easy to compromise
The first practical demonstration that WEP was broken came when Adam Stubblefield was an intern at AT&T (Stubblefield et al
He was able to code up and test an attack outlined by Fluhrer et al
(   ) in one week  of which most of the time was spent convincing management to buy him a WiFi card to use in his experiments
Software to crack WEP passwords within a minute is now freely available and the use of WEP is very strongly discouraged
While it does prevent casual NETWORK URITY
access it does not provide any real form of urity
i group was put together in a hurry when it was clear that WEP was seriously broken
It produced a formal standard by June
Now we will describe
i  which does provide real urity if it is set up and used properly
There are two common scenarios in which WPA  is used
The first is a corporate setting  in which a company has a separate authentication server that has a username and password database that can be used to determine if a wireless client is allowed to access the network
In this setting  clients use standard protocols to authenticate themselves to the network
The main standards are
X  with which the access point lets the client carry on a dialogue with the authentication server and observes the result  and EAP (Extensible Authentication Protocol) (RFC )  which tells how the client and the authentication server interact
Actually  EAP is a framework and other standards define the protocol messages
However  we will not delve into the many details of this exchange because they do not much matter for an overview
The ond scenario is in a home setting in which there is no authentication server
Instead  there is a single shared password that is used by clients to access the wireless network
This setup is less complex than having an authentication server  which is why it is used at home and in small businesses  but it is less ure as well
The main difference is that with an authentication server each client gets a key for encrypting traffic that is not known by the other clients
With a single shared password  different keys are derived for each client  but all clients have the same password and can derive each othersâ keys if they want to
The keys that are used to encrypt traffic are computed as part of an authentication handshake
The handshake happens right after the client associates with a wireless network and authenticates with an authentication server  if there is one
At the start of the handshake  the client has either the shared network password or its password for the authentication server
This password is used to derive a master key
However  the master key is not used directly to encrypt packets
It is standard cryptographic practice to derive a session key for each period of usage  to change the key for different sessions  and to expose the master key to observation as little as possible
It is this session key that is computed in the handshake
The session key is computed with the four-packet handshake shown in Fig
First  the AP (access point) sends a random number for identification
Random numbers used just once in urity protocols like this one are called nonces  which is more-or-less a contraction of âânumber used once
ââ The client also picks its own nonce
It uses the nonces  its MAC address and that of the AP  and the master key to compute a session key  KS
The session key is split into portions  each of which is used for different purposes  but we have omitted this detail
Now the client has session keys  but the AP does not
So the client sends its nonce to the AP  and the AP performs the same computation to derive the same session keys
The nonces can be sent in the clear because the keys cannot be derived from them without extra  ret information
The message from the client is protected   COMMUNICATION URITY with an integrity check called a MIC (Message Integrity Check) based on the session key
The AP can check that the MIC is correct  and so the message indeed must have come from the client  after it computes the session keys
A MIC is just another name for a message authentication code  as in an HMAC
The term MIC is often used instead for networking protocols because of the potential for confusion with MAC (Medium Access Control) addresses
Client NonceAP NonceC  MICS KS (KG)  MICS    Access Point (AP) Compute session keys KS from MAC addresses  nonces  and master key Distribute group key  KG Verify client has KS Verify AP has KS Acknowledge Compute session keys KS  same as the client KS (ACK)  MICS Figure  -
i key setup handshake
In the last two messages  the AP distributes a group key  KG  to the client  and the client acknowledges the message
Receipt of these messages lets the client verify that the AP has the correct session keys  and vice versa
The group key is used for broadcast and multicast traffic on the
Because the result of the handshake is that every client has its own encryption keys  none of these keys can be used by the AP to broadcast packets to all of the wireless clients; a separate copy would need to be sent to each client using its key
Instead  a shared key is distributed so that broadcast traffic can be sent only once and received by all the clients
It must be updated as clients leave and join the network
Finally  we get to the part where the keys are actually used to provide urity
Two protocols can be used in
i to provide message confidentiality  integrity  and authentication
Like WPA  one of the protocols  called TKIP (Temporary Key Integrity Protocol)  was an interim solution
It was designed to improve urity on old and slow
cards  so that at least some urity that is better than WEP can be rolled out as a firmware upgrade
However  it  too  has now been broken so you are better off with the other  recommended protocol  CCMP
What does CCMP stand for? It is short for the somewhat spectacular name Counter mode with Cipher block chaining Message authentication code Protocol
We will just call it CCMP
You can call it anything you want
NETWORK URITY
CCMP works in a fairly straightforward way
It uses AES encryption with a   -bit key and block size
The key comes from the session key
To provide confidentiality  messages are encrypted with AES in counter mode
Recall that we discussed cipher modes in
These modes are what prevent the same message from being encrypted to the same set of bits each time
Counter mode mixes a counter into the encryption
To provide integrity  the message  including header fields  is encrypted with cipher block chaining mode and the last   -bit block is kept as the MIC
Then both the message (encrypted with counter mode) and the MIC are sent
The client and the AP can each perform this encryption  or verify this encryption when a wireless packet is received
For broadcast or multicast messages  the same procedure is used with the group key
Bluetooth urity Bluetooth has a considerably shorter range than
so it cannot easily be attacked from the parking lot  but urity is still an issue here
For example  imagine that Aliceâs computer is equipped with a wireless Bluetooth keyboard
In the absence of urity  if Trudy happened to be in the adjacent office  she could read everything Alice typed in  including all her outgoing email
She could also capture everything Aliceâs computer sent to the Bluetooth printer sitting next to it (
incoming email and confidential reports)
Fortunately  Bluetooth has an elaborate urity scheme to try to foil the worldâs Trudies
We will now summarize the main features of it
Bluetooth version
and later has four urity modes  ranging from nothing at all to full data encryption and integrity control
if urity is disabled (the default for older devices)  there is no urity
Most users have urity turned off until a serious breach has occurred; then they turn it on
In the agricultural world  this approach is known as locking the barn door after the horse has escaped
Bluetooth provides urity in multiple layers
In the physical layer  frequency hopping provides a tiny little bit of urity  but since any Bluetooth device that moves into a piconet has to be told the frequency hopping sequence  this sequence is obviously not a ret
The real urity starts when the newly arrived slave asks for a channel with the master
Before Bluetooth
two devices were assumed to share a ret key set up in advance
In some cases  both are hardwired by the manufacturer (
for a headset and mobile phone sold as a unit)
In other cases  one device (
the headset) has a hardwired key and the user has to enter that key into the other device (
the mobile phone) as a decimal number
These shared keys are called passkeys
Unfortunately  the passkeys are often hardcoded to ââ   ââ or another predictable value  and in any case are four decimal digits  allowing only choices
With simple ure pairing in Bluetooth
devices pick a code from a six-digit range  which makes the passkey much less predictable but still far from ure
COMMUNICATION URITY To establish a channel  the slave and master each check to see if the other one knows the passkey
If so  they negotiate whether that channel will be encrypted  integrity controlled  or both
Then they select a random   -bit session key  some of whose bits may be public
The point of allowing this key weakening is to comply with government restrictions in various countries designed to prevent the export or use of keys longer than the government can break
Encryption uses a stream cipher called E ; integrity control uses SAFER+
Both are traditional symmetric-key block ciphers
SAFER+ was submitted to the AES bake-off but was eliminated in the first round because it was slower than the other candidates
Bluetooth was finalized before the AES cipher was chosen; otherwise  it would most likely have used Rijndael
The actual encryption using the stream cipher is shown in Fig
Unfortunately  E  itself (like RC ) may have fatal weaknesses (Jakobsson and Wetzel  )
While it was not broken at the time of this writing  its similarities to the A /  cipher  whose spectacular failure compromises all GSM telephone traffic  are cause for concern (Biryukov et al
It sometimes amazes people (including the authors of this book)  that in the perennial cat-and-mouse game between the cryptographers and the cryptanalysts  the cryptanalysts are so often on the winning side
Another urity issue is that Bluetooth authenticates only devices  not users  so theft of a Bluetooth device may give the thief access to the userâs financial and other accounts
However  Bluetooth also implements urity in the upper layers  so even in the event of a breach of link-level urity  some urity may remain  especially for applications that require a PIN code to be entered manually from some kind of keyboard to complete the transaction  AUTHENTICATION PROTOCOLS Authentication is the technique by which a process verifies that its communication partner is who it is supposed to be and not an imposter
Verifying the identity of a remote process in the face of a malicious  active intruder is surprisingly difficult and requires complex protocols based on cryptography
In this tion  we will study some of the many authentication protocols that are used on inure computer networks
As an aside  some people confuse authorization with authentication
Authentication deals with the question of whether you are actually communicating with a specific process
Authorization is concerned with what that process is permitted to do
For example  say a client process contacts a file server and says: ââI am Scottâs process and I want to delete the file
ââ From the file serverâs point of view  two questions must be answered: NETWORK URITY
Is this actually Scottâs process (authentication)?
Is Scott allowed to delete   (authorization)? Only after both of these questions have been unambiguously answered in the affirmative can the requested action take place
The former question is really the key one
Once the file server knows to whom it is talking  checking authorization is just a matter of looking up entries in local tables or databases
For this reason  we will concentrate on authentication in this tion
The general model that essentially all authentication protocols use is this
Alice starts out by sending a message either to Bob or to a trusted KDC (Key Distribution Center)  which is expected to be honest
Several other message exchanges follow in various directions
As these messages are being sent  Trudy may intercept  modify  or replay them in order to trick Alice and Bob or just to gum up the works
Nevertheless  when the protocol has been completed  Alice is sure she is talking to Bob and Bob is sure he is talking to Alice
Furthermore  in most of the protocols  the two of them will also have established a ret session key for use in the upcoming conversation
In practice  for performance reasons  all data traffic is encrypted using symmetric-key cryptography (typically AES or triple DES)  although public-key cryptography is widely used for the authentication protocols themselves and for establishing the session key
The point of using a new  randomly chosen session key for each new connection is to minimize the amount of traffic that gets sent with the usersâ ret keys or public keys  to reduce the amount of ciphertext an intruder can obtain  and to minimize the damage done if a process crashes and its core dump falls into the wrong hands
Hopefully  the only key present then will be the session key
All the permanent keys should have been carefully zeroed out after the session was established
Authentication Based on a Shared ret Key For our first authentication protocol  we will assume that Alice and Bob already share a ret key  KAB
This shared key might have been agreed upon on the telephone or in person  but  in any event  not on the (inure) network
This protocol is based on a principle found in many authentication protocols: one party sends a random number to the other  who then transforms it in a special way and returns the result
Such protocols are called challenge-response protocols
In this and subsequent authentication protocols  the following notation will be used: A  B are the identities of Alice and Bob
Riâs are the challenges  where i identifies the challenger
Kiâs are keys  where i indicates the owner
KS is the session key
AUTHENTICATION PROTOCOLS The message sequence for our first shared-key authentication protocol is illustrated in Fig
In message   Alice sends her identity  A  to Bob in a way that Bob understands
Bob  of course  has no way of knowing whether this message came from Alice or from Trudy  so he chooses a challenge  a large random number  RB  and sends it back to ââAliceââ as message   in plaintext
Alice then encrypts the message with the key she shares with Bob and sends the ciphertext  KAB(RB)  back in message
When Bob sees this message  he immediately knows that it came from Alice because Trudy does not know KAB and thus could not have generated it
Furthermore  since RB was chosen randomly from a large space (say -bit random numbers)  it is very unlikely that Trudy would have seen RB and its response in an earlier session
It is equally unlikely that she could guess the correct response to any challenge
A Alice RB  KAB (RB) KAB (RA) Bob RA Figure  -
Two-way authentication using a challenge-response protocol
At this point  Bob is sure he is talking to Alice  but Alice is not sure of anything
For all Alice knows  Trudy might have intercepted message  and sent back RB in response
Maybe Bob died last night
To find out to whom she is talking  Alice picks a random number  RA  and sends it to Bob as plaintext  in message
When Bob responds with KAB(RA)  Alice knows she is talking to Bob
If they wish to establish a session key now  Alice can pick one  KS  and send it to Bob encrypted with KAB
The protocol of Fig
Let us see if we can be clever and eliminate some of them
One approach is illustrated in Fig
Here Alice initiates the challenge-response protocol instead of waiting for Bob to do it
Similarly  while he is responding to Aliceâs challenge  Bob sends his own
The entire protocol can be reduced to three messages instead of five
Is this new protocol an improvement over the original one? In one sense it is: it is shorter
Unfortunately  it is also wrong
Under certain circumstances  Trudy can defeat this protocol by using what is known as a reflection attack
In particular  Trudy can break it if it is possible to open multiple sessions with Bob at once
This situation would be true  for example  if Bob is a bank and is prepared to accept many simultaneous connections from teller machines at once
NETWORK URITY
Alice  RB  KAB (RA) KAB (RB) A  RA Bob Figure  -
A shortened two-way authentication protocol
Trudyâs reflection attack is shown in Fig
It starts out with Trudy claiming she is Alice and sending RT
Bob responds  as usual  with his own challenge  RB
Now Trudy is stuck
What can she do? She does not know KAB(RB)
Trudy  RB  KAB (RT) KAB (RB) A  RT RB  KAB (RB) A  RB First session ond session First session Bob Figure  -
The reflection attack
She can open a ond session with message   supplying the RB taken from message  as her challenge
Bob calmly encrypts it and sends back KAB(RB) in message
We have shaded the messages on the ond session to make them stand out
Now Trudy has the missing information  so she can complete the first session and abort the ond one
Bob is now convinced that Trudy is Alice  so when she asks for her bank account balance  he gives it to her without question
Then when she asks him to transfer it all to a ret bank account in Switzerland  he does so without a momentâs hesitation
The moral of this story is: Designing a correct authentication protocol is much harder than it looks
The following four general rules often help the designer avoid common pitfalls:   AUTHENTICATION PROTOCOLS
Have the initiator prove who she is before the responder has to
This avoids Bob giving away valuable information before Trudy has to give any evidence of who she is Have the initiator and responder use different keys for proof  even if this means having two shared keys  KAB and Kâ²AB Have the initiator and responder draw their challenges from different sets
For example  the initiator must use even numbers and the responder must use odd numbers Make the protocol resistant to attacks involving a ond parallel session in which information obtained in one session is used in a different one
If even one of these rules is violated  the protocol can frequently be broken
Here  all four rules were violated  with disastrous consequences
Now let us go take a closer look at Fig
Surely that protocol is not subject to a reflection attack? Maybe
It is quite subtle
Trudy was able to defeat our protocol by using a reflection attack because it was possible to open a ond session with Bob and trick him into answering his own questions
What would happen if Alice were a general-purpose computer that also accepted multiple sessions  rather than a person at a computer? Let us take a look what Trudy can do
To see how Trudyâs attack works  see Fig
Alice starts out by announcing her identity in message
Trudy intercepts this message and begins her own session with message   claiming to be Bob
Again we have shaded the session  messages
Alice responds to message  by saying in message  : ââYou claim to be Bob? Prove it
ââ At this point  Trudy is stuck because she cannot prove she is Bob
What does Trudy do now? She goes back to the first session  where it is her turn to send a challenge  and sends the RA she got in message
Alice kindly responds to it in message   thus supplying Trudy with the information she needs to send in message  in session
At this point  Trudy is basically home free because she has successfully responded to Aliceâs challenge in session
She can now cancel session   send over any old number for the rest of session   and she will have an authenticated session with Alice in session
But Trudy is nasty  and she really wants to rub it in
Instead  of sending any old number over to complete session   she waits until Alice sends message   Aliceâs challenge for session
Of course  Trudy does not know how to respond  so she uses the reflection attack again  sending back RA  as message
Alice conveniently encrypts RA  in message
Trudy now switches back to session  and sends Alice the number she wants in message conveniently copied from what Alice sent in message
At this point Trudy has two fully authenticated sessions with Alice
This attack has a somewhat different result than the attack on the three-message protocol that we saw in Fig
This time  Trudy has two authenticated NETWORK URITY
A Alice B  KAB (RA) Trudy RA RA  KAB (RA)  RA  KAB (RA ) RA KAB (RA ) First session First session First session First session ond session ond session ond session Figure  -
A reflection attack on the protocol of Fig
connections with Alice
In the previous example  she had one authenticated connection with Bob
Again here  if we had applied all the general authentication protocol rules discussed earlier  this attack could have been stopped
For a detailed discussion of these kinds of attacks and how to thwart them  see Bird et al
They also show how it is possible to systematically construct protocols that are provably correct
The simplest such protocol is nevertheless a bit complicated  so we will now show a different class of protocol that also works
The new authentication protocol is shown in Fig
-  (Bird et al
It uses an HMAC of the type we saw when studying IP
Alice starts out by sending Bob a nonce  RA  as message
Bob responds by selecting his own nonce  RB  and sending it back along with an HMAC
The HMAC is formed by building a data structure consisting of Aliceâs nonce  Bobâs nonce  their identities  and the shared ret key  KAB
This data structure is then hashed into the HMAC  for example  using SHA-
When Alice receives message   she now has RA (which she picked herself)  RB  which arrives as plaintext  the two identities  and the ret key  KAB  which she has known all along  so she can compute the HMAC herself
If it agrees with the HMAC in the message  she knows she is talking to Bob because Trudy does not know KAB and thus cannot figure out which HMAC to send
Alice responds to Bob with an HMAC containing just the two nonces
Can Trudy somehow subvert this protocol? No  because she cannot force either party to encrypt or hash a value of her choice  as happened in Fig
Both HMACs include values chosen by the sending party  something that Trudy cannot control
AUTHENTICATION PROTOCOLS Alice  RA Bob RB  HMAC(RA  RB  A  B  KAB) HMAC(RA  RB  KAB) Figure  -
Authentication using HMACs
Using HMACs is not the only way to use this idea
An alternative scheme that is often used instead of computing the HMAC over a series of items is to encrypt the items sequentially using cipher block chaining
Establishing a Shared Key: The Diffie-Hellman Key Exchange So far  we have assumed that Alice and Bob share a ret key
Suppose that they do not (because so far there is no universally accepted PKI for signing and distributing certificates)
How can they establish one? One way would be for Alice to call Bob and give him her key on the phone  but he would probably start out by saying: ââHow do I know you are Alice and not Trudy?ââ They could try to arrange a meeting  with each one bringing a passport  a driverâs license  and three major credit cards  but being busy people  they might not be able to find a mutually acceptable date for months
Fortunately  incredible as it may sound  there is a way for total strangers to establish a shared ret key in broad daylight  even with Trudy carefully recording every message
The protocol that allows strangers to establish a shared ret key is called the Diffie-Hellman key exchange (Diffie and Hellman  ) and works as follows
Alice and Bob have to agree on two large numbers  n and g  where n is a prime  (n â  )/  is also a prime  and certain conditions apply to g
These numbers may be public  so either one of them can just pick n and g and tell the other openly
Now Alice picks a large (say  -bit) number  x  and keeps it ret
Similarly  Bob picks a large ret number  y
Alice initiates the key exchange protocol by sending Bob a message containing (n  g  gx mod n)  as shown in Fig
Bob responds by sending Alice a message containing gy mod n
Now Alice raises the number Bob sent her to the xth power modulo n to get (gy mod n)x mod n
Bob performs a similar operation to get (gx mod n)y mod n
By the laws of modular arithmetic  both calculations yield gxy mod n
Lo and behold  as if by magic  Alice and Bob suddenly share a ret key  gxy mod n
NETWORK URITY
Alice picks x Bob picks y  gy mod n n  g  gx mod n Alice computes (gy mod n)x = gxy mod n Bob computes (gx mod n)y = gxy mod n Bob Alice mod n mod n Figure  -
The Diffie-Hellman key exchange
Trudy  of course  has seen both messages
She knows g and n from message
If she could compute x and y  she could figure out the ret key
The trouble is  given only gx mod n  she cannot find x
No practical algorithm for computing discrete logarithms modulo a very large prime number is known
To make this example more concrete  we will use the (completely unrealistic) values of n =   and g =
Alice picks x =  and Bob picks y =
Both of these are kept ret
Aliceâs message to Bob is (    ) because   mod   is
Bobâs message to Alice is (  )
Alice computes mod which is
Bob computes  mod which is
Alice and Bob have now independently determined that the ret key is now
To find the key  Trudy now has to solve the equation  x mod   = which can be done by exhaustive search for small numbers like this  but not when all the numbers are hundreds of bits long
All currently known algorithms simply take far too long  even on massively parallel  lightning fast supercomputers
Despite the elegance of the Diffie-Hellman algorithm  there is a problem: when Bob gets the triple (    )  how does he know it is from Alice and not from Trudy? There is no way he can know
Unfortunately  Trudy can exploit this fact to deceive both Alice and Bob  as illustrated in Fig
Here  while Alice and Bob are choosing x and y  respectively  Trudy picks her own random number  z
Alice sends message   intended for Bob
Trudy intercepts it and sends message  to Bob  using the correct g and n (which are public anyway) but with her own z instead of x
She also sends message  back to Alice
Later Bob sends message  to Alice  which Trudy again intercepts and keeps
Now everybody does the modular arithmetic
Alice computes the ret key as gxz mod n  and so does Trudy (for messages to Alice)
Bob computes gyz mod n and so does Trudy (for messages to Bob)
Alice thinks she is talking to Bob  so she establishes a session key (with Trudy)
So does Bob
Every message that Alice sends on the encrypted session is captured by Trudy  stored  modified if desired  and then (optionally) passed on to Bob
Similarly  in the other direction  Trudy sees everything and can modify all messages at will  while both Alice and Bob are under the illusion that they have a ure channel to one another
For this   AUTHENTICATION PROTOCOLS  Alice picks x Trudy picks z  gz mod n n  g  gx mod n Trudy  Bob picks y  gy mod n n  g  gz mod n Bob Alice Figure  -
The man-in-the-middle attack
reason  the attack is known as the man-in-the-middle attack
It is also called the bucket brigade attack  because it vaguely resembles an old-time volunteer fire department passing buckets along the line from the fire truck to the fire
Authentication Using a Key Distribution Center Setting up a shared ret with a stranger almost worked  but not quite
On the other hand  it probably was not worth doing in the first place (sour grapes attack)
To talk to n people this way  you would need n keys
For popular people  key management would become a real burden  especially if each key had to be stored on a separate plastic chip card
A different approach is to introduce a trusted key distribution center
In this model  each user has a single key shared with the KDC
Authentication and session key management now go through the KDC
The simplest known KDC authentication protocol involving two parties and a trusted KDC is depicted in Fig
A  KA (B  KS) KDC  Bob Alice KB (A  KS) Figure  -
A first attempt at an authentication protocol using a KDC
The idea behind this protocol is simple: Alice picks a session key  KS  and tells the KDC that she wants to talk to Bob using KS
This message is encrypted NETWORK URITY
with the ret key Alice shares (only) with the KDC  KA
The KDC decrypts this message  extracting Bobâs identity and the session key
It then constructs a new message containing Aliceâs identity and the session key and sends this message to Bob
This encryption is done with KB  the ret key Bob shares with the KDC
When Bob decrypts the message  he learns that Alice wants to talk to him and which key she wants to use
The authentication here happens for free
The KDC knows that message  must have come from Alice  since no one else would have been able to encrypt it with Aliceâs ret key
Similarly  Bob knows that message  must have come from the KDC  whom he trusts  since no one else knows his ret key
Unfortunately  this protocol has a serious flaw
Trudy needs some money  so she figures out some legitimate service she can perform for Alice  makes an attractive offer  and gets the job
After doing the work  Trudy then politely requests Alice to pay by bank transfer
Alice then establishes a session key with her banker  Bob
Then she sends Bob a message requesting money to be transferred to Trudyâs account
Meanwhile  Trudy is back to her old ways  snooping on the network
She copies both message  in Fig
Later  she replays both of them to Bob who thinks: ââAlice must have hired Trudy again
She clearly does good work
ââ Bob then transfers an equal amount of money from Aliceâs account to Trudyâs
Some time after the  th message pair  Bob runs out of the office to find Trudy to offer her a big loan so she can expand her obviously successful business
This problem is called the replay attack
Several solutions to the replay attack are possible
The first one is to include a timestamp in each message
Then  if anyone receives an obsolete message  it can be discarded
The trouble with this approach is that clocks are never exactly synchronized over a network  so there has to be some interval during which a timestamp is valid
Trudy can replay the message during this interval and get away with it
The ond solution is to put a nonce in each message
Each party then has to remember all previous nonces and reject any message containing a previously used nonce
But nonces have to be remembered forever  lest Trudy try replaying a  -year-old message
Also  if some machine crashes and it loses its nonce list  it is again vulnerable to a replay attack
Timestamps and nonces can be combined to limit how long nonces have to be remembered  but clearly the protocol is going to get a lot more complicated
A more sophisticated approach to mutual authentication is to use a multiway challenge-response protocol
A well-known example of such a protocol is the Needham-Schroeder authentication protocol (Needham and Schroeder  )  one variant of which is shown in Fig
The protocol begins with Alice telling the KDC that she wants to talk to Bob
This message contains a large random number  RA  as a nonce
The KDC sends back message  containing Aliceâs random number  a session key  and a ticket   AUTHENTICATION PROTOCOLS  RA  A  B  KA (RA  B  KS  KB(A  KS)) KDC  Bob Alice KB(A  KS)  KS (RA )  KS (RA  â )  RB  KS (RB â ) Figure  -
The Needham-Schroeder authentication protocol
that she can send to Bob
The point of the random number  RA  is to assure Alice that message  is fresh  and not a replay
Bobâs identity is also enclosed in case Trudy gets any funny ideas about replacing B in message  with her own identity so the KDC will encrypt the ticket at the end of message  with KT instead of KB
The ticket encrypted with KB is included inside the encrypted message to prevent Trudy from replacing it with something else on the way back to Alice
Alice now sends the ticket to Bob  along with a new random number  RA   encrypted with the session key  KS
In message   Bob sends back KS(RA  â  ) to prove to Alice that she is talking to the real Bob
Sending back KS(RA  ) would not have worked  since Trudy could just have stolen it from message
After receiving message   Alice is now convinced that she is talking to Bob and that no replays could have been used so far
After all  she just generated RA  a few millionds ago
The purpose of message  is to convince Bob that it is indeed Alice he is talking to  and no replays are being used here either
By having each party both generate a challenge and respond to one  the possibility of any kind of replay attack is eliminated
Although this protocol seems pretty solid  it does have a slight weakness
If Trudy ever manages to obtain an old session key in plaintext  she can initiate a new session with Bob by replaying the message  that corresponds to the compromised key and convince him that she is Alice (Denning and Sacco  )
This time she can plunder Aliceâs bank account without having to perform the legitimate service even once
Needham and Schroeder (   ) later published a protocol that corrects this problem
In the same issue of the same journal  Otway and Rees (   ) also published a protocol that solves the problem in a shorter way
Figure  -  shows a slightly modified Otway-Rees protocol
In the Otway-Rees protocol  Alice starts out by generating a pair of random numbers: R  which will be used as a common identifier  and RA  which Alice will use to challenge Bob
When Bob gets this message  he constructs a new message from the encrypted part of Aliceâs message and an analogous one of his own
NETWORK URITY
KA(RA  KS) KB(RB  KS) KDC  Bob Alice A  B  R  KA (A  B  R  RA) A  KA (A  B  R  RA)  B  KB (A  B  R  RB) Figure  -
The Otway-Rees authentication protocol (slightly simplified)
Both the parts encrypted with KA and KB identify Alice and Bob  contain the common identifier  and contain a challenge
The KDC checks to see if the R in both parts is the same
It might not be if Trudy has tampered with R in message  or replaced part of message
If the two Rs match  the KDC believes that the request message from Bob is valid
It then generates a session key and encrypts it twice  once for Alice and once for Bob
Each message contains the receiverâs random number  as proof that the KDC  and not Trudy  generated the message
At this point  both Alice and Bob are in possession of the same session key and can start communicating
The first time they exchange data messages  each one can see that the other one has an identical copy of KS  so the authentication is then complete
Authentication Using Kerberos An authentication protocol used in many real systems (including Windows  and later versions) is Kerberos  which is based on a variant of Needham- Schroeder
It is named for a multiheaded dog in Greek mythology that used to guard the entrance to Hades (presumably to keep undesirables out)
Kerberos was designed at
to allow workstation users to access network resources in a ure way
Its biggest difference from Needham-Schroeder is its assumption that all clocks are fairly well synchronized
The protocol has gone through several iterations
V  is the one that is widely used in industry and defined in RFC
The earlier version  V  was finally retired after serious flaws were found (Yu et al
V  improves on V  with many small changes to the protocol and some improved features  such as the fact that it no longer relies on the now-dated DES
For more information  see Neuman and Tsâo (   )
Kerberos involves three servers in addition to Alice (a client workstation):
Authentication Server (AS): Verifies users during login Ticket-Granting Server (TGS): Issues ââproof of identity tickets
Bob the server: Actually does the work Alice wants performed
AUTHENTICATION PROTOCOLS AS is similar to a KDC in that it shares a ret password with every user
The TGSâs job is to issue tickets that can convince the real servers that the bearer of a TGS ticket really is who he or she claims to be
To start a session  Alice sits down at an arbitrary public workstation and types her name
The workstation sends her name and the name of the TGS to the AS in plaintext  as shown in message  of Fig
What comes back is a session key and a ticket  KTGS(A  KS  t)  intended for the TGS
The session key is encrypted using Aliceâs ret key  so that only Alice can decrypt it
Only when message  arrives does the workstation ask for Aliceâs passwordânot before then
The password is then used to generate KA in order to decrypt message  and obtain the session key
At this point  the workstation overwrites Aliceâs password to make sure that it is only inside the workstation for a few millionds at most
If Trudy tries logging in as Alice  the password she types will be wrong and the workstation will detect this because the standard part of message  will be incorrect
Alice AS TGS Bob KAB(A  t)  KB(A  B  KAB  t) A TGS KA(TGS  KS  t)  KTGS(A  KS  t) B  KS(A  t)  KTGS(A  KS  t) KS(B  KAB  t)  KB(A  B  KAB  t) KAB (t)    Figure  -
The operation of Kerberos V
After she logs in  Alice may tell the workstation that she wants to contact Bob the file server
The workstation then sends message  to the TGS asking for a ticket to use with Bob
The key element in this request is the ticket KTGS(A  KS  t)  which is encrypted with the TGSâs ret key and used as proof that the sender really is Alice
The TGS responds in message  by creating a session key  KAB  for Alice to use with Bob
Two versions of it are sent back
The first is encrypted with only KS  so Alice can read it
The ond is another ticket  encrypted with Bobâs key  KB  so Bob can read it
NETWORK URITY
Trudy can copy message  and try to use it again  but she will be foiled by the encrypted timestamp  t  sent along with it
Trudy cannot replace the timestamp with a more recent one  because she does not know KS  the session key Alice uses to talk to the TGS
Even if Trudy replays message  quickly  all she will get is another copy of message   which she could not decrypt the first time and will not be able to decrypt the ond time either
Now Alice can send KAB to Bob via the new ticket to establish a session with him (message  )
This exchange is also timestamped
The optional response (message  ) is proof to Alice that she is actually talking to Bob  not to Trudy
After this series of exchanges  Alice can communicate with Bob under cover of KAB
If she later decides she needs to talk to another server  Carol  she just repeats message  to the TGS  only now specifying C instead of B
The TGS will promptly respond with a ticket encrypted with KC that Alice can send to Carol and that Carol will accept as proof that it came from Alice
The point of all this work is that now Alice can access servers all over the network in a ure way and her password never has to go over the network
In fact  it only had to be in her own workstation for a few millionds
However  note that each server does its own authorization
When Alice presents her ticket to Bob  this merely proves to Bob who sent it
Precisely what Alice is allowed to do is up to Bob
Since the Kerberos designers did not expect the entire world to trust a single authentication server  they made provision for having multiple realms  each with its own AS and TGS
To get a ticket for a server in a distant realm  Alice would ask her own TGS for a ticket accepted by the TGS in the distant realm
If the distant TGS has registered with the local TGS (the same way local servers do)  the local TGS will give Alice a ticket valid at the distant TGS
She can then do business over there  such as getting tickets for servers in that realm
Note  however  that for parties in two realms to do business  each one must trust the otherâs TGS
Otherwise  they cannot do business
Authentication Using Public-Key Cryptography Mutual authentication can also be done using public-key cryptography
To start with  Alice needs to get Bobâs public key
If a PKI exists with a directory server that hands out certificates for public keys  Alice can ask for Bobâs  as shown in Fig
The reply  in message   is an X
certificate containing Bobâs public key
When Alice verifies that the signature is correct  she sends Bob a message containing her identity and a nonce
When Bob receives this message  he has no idea whether it came from Alice or from Trudy  but he plays along and asks the directory server for Aliceâs public key (message  )  which he soon gets (message  )
He then sends Alice message   containing Aliceâs RA  his own nonce  RB  and a proposed session key  KS
AUTHENTICATION PROTOCOLS  EB (A  RA)  KS (RB)  EA (RA  RB  KS) Bob Alice Directory
Here is EB
Give me EA
Here is EA
Give me EB Figure  -
Mutual authentication using public-key cryptography
When Alice gets message   she decrypts it using her private key
She sees RA in it  which gives her a warm feeling inside
The message must have come from Bob  since Trudy has no way of determining RA
Furthermore  it must be fresh and not a replay  since she just sent Bob RA
Alice agrees to the session by sending back message
When Bob sees RB encrypted with the session key he just generated  he knows Alice got message  and verified RA
Bob is now a happy camper
What can Trudy do to try to subvert this protocol? She can fabricate message  and trick Bob into probing Alice  but Alice will see an RA that she did not send and will not proceed further
Trudy cannot forge message  back to Bob because she does not know RB or KS and cannot determine them without Aliceâs private key
She is out of luck  EMAIL URITY When an email message is sent between two distant sites  it will generally transit dozens of machines on the way
Any of these can read and record the message for future use
In practice  privacy is nonexistent  despite what many people think
Nevertheless  many people would like to be able to send email that can be read by the intended recipient and no one else: not their boss and not even their government
This desire has stimulated several people and groups to apply the cryptographic principles we studied earlier to email to produce ure email
In the following tions we will study a widely used ure email system  PGP  and then briefly mention one other  S/MIME
For additional information about ure email  see Kaufman et al
(   ) and Schneier (   )
NETWORK URITY
PGPâPretty Good Privacy Our first example  PGP (Pretty Good Privacy) is essentially the brainchild of one person  Phil Zimmermann (   a  b)
Zimmermann is a privacy advocate whose motto is: ââIf privacy is outlawed  only outlaws will have privacy
ââ Released in  PGP is a complete email urity package that provides privacy  authentication  digital signatures  and compression  all in an easy-to-use form
Furthermore  the complete package  including all the source code  is distributed free of charge via the Internet
Due to its quality  price (zero)  and easy availability on UNIX  Linux  Windows  and Mac OS platforms  it is widely used today
PGP encrypts data by using a block cipher called IDEA (International Data Encryption Algorithm)  which uses   -bit keys
It was devised in Switzerland at a time when DES was seen as tainted and AES had not yet been invented
Conceptually  IDEA is similar to DES and AES: it mixes up the bits in a series of rounds  but the details of the mixing functions are different from DES and AES
Key management uses RSA and data integrity uses MD  topics that we have already discussed
PGP has also been embroiled in controversy since day  (Levy  )
Because Zimmermann did nothing to stop other people from placing PGP on the Internet  where people all over the world could get it  the
Government claimed that Zimmermann had violated
laws prohibiting the export of munitions
Governmentâs investigation of Zimmermann went on for  years but was eventually dropped  probably for two reasons
First  Zimmermann did not place PGP on the Internet himself  so his lawyer claimed that he never exported anything (and then there is the little matter of whether creating a Web site constitutes export at all)
ond  the government eventually came to realize that winning a trial meant convincing a jury that a Web site containing a downloadable privacy program was covered by the arms-trafficking law prohibiting the export of war materiel such as tanks  submarines  military aircraft  and nuclear weapons
Years of negative publicity probably did not help much  either
As an aside  the export rules are bizarre  to put it mildly
The government considered putting code on a Web site to be an illegal export and harassed Zimmermann about it for  years
On the other hand  when someone published the complete PGP source code  in C  as a book (in a large font with a checksum on each page to make scanning it in easy) and then exported the book  that was fine with the government because books are not classified as munitions
The sword is mightier than the pen  at least for Uncle Sam
Another problem PGP ran into involved patent infringement
The company holding the RSA patent  RSA urity  Inc
alleged that PGPâs use of the RSA algorithm infringed on its patent  but that problem was settled with releases starting at   Furthermore  PGP uses another patented encryption algorithm  IDEA  whose use caused some problems at first
EMAIL URITY Since PGP is open source  various people and groups have modified it and produced a number of versions
Some of these were designed to get around the munitions laws  others were focused on avoiding the use of patented algorithms  and still others wanted to turn it into a closed-source commercial product
Although the munitions laws have now been slightly liberalized (otherwise  products using AES would not have been exportable from the
)  and the RSA patent expired in September  the legacy of all these problems is that several incompatible versions of PGP are in circulation  under various names
The discussion below focuses on classic PGP  which is the oldest and simplest version
Another popular version  Open PGP  is described in RFC
Yet another is the GNU Privacy Guard
PGP intentionally uses existing cryptographic algorithms rather than inventing new ones
It is largely based on algorithms that have withstood extensive peer review and were not designed or influenced by any government agency trying to weaken them
For people who distrust government  this property is a big plus
PGP supports text compression  recy  and digital signatures and also provides extensive key management facilities  but  oddly enough  not email facilities
It is like a preprocessor that takes plaintext as input and produces signed ciphertext in base  as output
This output can then be emailed  of course
Some PGP implementations call a user agent as the final step to actually send the message
To see how PGP works  let us consider the example of Fig
Here  Alice wants to send a signed plaintext message  P  to Bob in a ure way
Both Alice and Bob have private (DX) and public (EX) RSA keys
Let us assume that each one knows the otherâs public key; we will cover PGP key management shortly
Alice starts out by invoking the PGP program on her computer
PGP first hashes her message  P  using MD  and then encrypts the resulting hash using her private RSA key  DA
When Bob eventually gets the message  he can decrypt the hash with Aliceâs public key and verify that the hash is correct
Even if someone else (
Trudy) could acquire the hash at this stage and decrypt it with Aliceâs known public key  the strength of MD  guarantees that it would be computationally infeasible to produce another message with the same MD  hash
The encrypted hash and the original message are now concatenated into a single message  P  and compressed using the ZIP program  which uses the Ziv- Lempel algorithm (Ziv and Lempel  )
Call the output of this step P
Next  PGP prompts Alice for some random input
Both the content and the typing speed are used to generate a   -bit IDEA message key  KM (called a session key in the PGP literature  but this is really a misnomer since there is no session)
KM is now used to encrypt P  with IDEA in cipher feedback mode
In addition  KM is encrypted with Bobâs public key  EB
These two components are then concatenated and converted to base   as we discussed in the tion on MIME in   The resulting message contains only letters  digits  and the symbols +  /  and =  which means it can be put into an RFC body and be expected to arrive unmodified
NETWORK URITY
MD  RSA Zip IDEA Base   RSA ASCII text to P  the network P P  Original plaintext message from Alice Concatenation of P and the signed hash of P Concatenation of P  encrypted with IDEA and KM encrypted with EB Alice's private RSA key  DA P  compressed Bob's public RSA key  EB KM : One-time message key for IDEA : Concatenation KM Figure  -
PGP in operation for sending a message
When Bob gets the message  he reverses the base  encoding and decrypts the IDEA key using his private RSA key
Using this key  he decrypts the message to get P
After decompressing it  Bob separates the plaintext from the encrypted hash and decrypts the hash using Aliceâs public key
If the plaintext hash agrees with his own MD  computation  he knows that P is the correct message and that it came from Alice
It is worth noting that RSA is only used in two places here: to encrypt the   -bit MD  hash and to encrypt the   -bit IDEA key
Although RSA is slow  it has to encrypt only bits  not a large volume of data
Furthermore  all plaintext bits are exceedingly random  so a considerable amount of work will be required on Trudyâs part just to determine if a guessed key is correct
The heavyduty encryption is done by IDEA  which is orders of magnitude faster than RSA
Thus  PGP provides urity  compression  and a digital signature and does so in a much more efficient way than the scheme illustrated in Fig
PGP supports four RSA key lengths
It is up to the user to select the one that is most appropriate
The lengths are:
Casual (   bits): Can be broken easily today Commercial (   bits): Breakable by three-letter organizations Military ( bits): Not breakable by anyone on earth Alien ( bits): Not breakable by anyone on other planets  either
EMAIL URITY Since RSA is only used for two small computations  everyone should use alienstrength keys all the time
The format of a classic PGP message is shown in Fig
Numerous other formats are also in use
The message has three parts  containing the IDEA key  the signature  and the message  respectively
The key part contains not only the key  but also a key identifier  since users are permitted to have multiple public keys
ID of EB ID of EA Sig
hdr MD  hash Msg hdr File name T i m e T i m e T y p e s KM Message Encrypted by EB DA Compressed  encrypted by IDEA Base  Signature part Message key part Message part Figure  -
A PGP message
The signature part contains a header  which will not concern us here
The header is followed by a timestamp  the identifier for the senderâs public key that can be used to decrypt the signature hash  some type information that identifies the algorithms used (to allow MD  and RSA  to be used when they are invented)  and the encrypted hash itself
The message part also contains a header  the default name of the file to be used if the receiver writes the file to the disk  a message creation timestamp  and  finally  the message itself
Key management has received a large amount of attention in PGP as it is the Achillesâ heel of all urity systems
Key management works as follows
Each user maintains two data structures locally: a private key ring and a public key ring
The private key ring contains one or more personal private/public key pairs
The reason for supporting multiple pairs per user is to permit users to change their public keys periodically or when one is thought to have been compromised  without invalidating messages currently in preparation or in transit
Each pair has an identifier associated with it so that a message sender can tell the recipient which public key was used to encrypt it
Message identifiers consist of the low-order   bits of the public key
Users are themselves responsible for avoiding conflicts in their public-key identifiers
The private keys on disk are encrypted using a special (arbitrarily long) password to protect them against sneak attacks
The public key ring contains public keys of the userâs correspondents
These are needed to encrypt the message keys associated with each message
Each entry NETWORK URITY
on the public key ring contains not only the public key  but also its  -bit identifier and an indication of how strongly the user trusts the key
The problem being tackled here is the following
Suppose that public keys are maintained on bulletin boards
One way for Trudy to read Bobâs ret email is to attack the bulletin board and replace Bobâs public key with one of her choice
When Alice later fetches the key allegedly belonging to Bob  Trudy can mount a bucket brigade attack on Bob
To prevent such attacks  or at least minimize the consequences of them  Alice needs to know how much to trust the item called ââBobâs keyââ on her public key ring
If she knows that Bob personally handed her a CD-ROM containing the key  she can set the trust value to the highest value
It is this decentralized  user-controlled approach to public-key management that sets PGP apart from centralized PKI schemes
Nevertheless  people do sometimes obtain public keys by querying a trusted key server
For this reason  after X
was standardized  PGP supported these certificates as well as the traditional PGP public key ring mechanism
All current versions of PGP have X
S/MIME IETFâs venture into email urity  called S/MIME (ure/MIME)  is described in RFCs  through
It provides authentication  data integrity  recy  and nonrepudiation
It also is quite flexible  supporting a variety of cryptographic algorithms
Not surprisingly  given the name  S/MIME integrates well with MIME  allowing all kinds of messages to be protected
A variety of new MIME headers are defined  for example  for holding digital signatures
S/MIME does not have a rigid certificate hierarchy beginning at a single root  which had been one of the political problems that doomed an earlier system called PEM (Privacy Enhanced Mail)
Instead  users can have multiple trust anchors
As long as a certificate can be traced back to some trust anchor the user believes in  it is considered valid
S/MIME uses the standard algorithms and protocols we have been examining so far  so we will not discuss it any further here
For the details  please consult the RFCs  WEB URITY We have just studied two important areas where urity is needed: communications and email
You can think of these as the soup and appetizer
Now it is time for the main course: Web urity
The Web is where most of the Trudies hang out nowadays and do their dirty work
In the following tions  we will look at some of the problems and issues relating to Web urity
WEB URITY Web urity can be roughly divided into three parts
First  how are objects and resources named urely? ond  how can ure  authenticated connections be established? Third  what happens when a Web site sends a client a piece of executable code? After looking at some threats  we will examine all these issues
Threats One reads about Web site urity problems in the newspaper almost weekly
The situation is really pretty grim
Let us look at a few examples of what has already happened
First  the home pages of numerous organizations have been attacked and replaced by new home pages of the crackersâ choosing
(The popular press calls people who break into computers ââhackers ââ but many programmers reserve that term for great programmers
We prefer to call these people ââcrackers
ââ) Sites that have been cracked include those belonging to Yahoo!  the
Army  the CIA  NASA  and the New York Times
In most cases  the crackers just put up some funny text and the sites were repaired within a few hours
Now let us look at some much more serious cases
Numerous sites have been brought down by denial-of-service attacks  in which the cracker floods the site with traffic  rendering it unable to respond to legitimate queries
Often  the attack is mounted from a large number of machines that the cracker has already broken into (DDoS attacks)
These attacks are so common that they do not even make the news any more  but they can cost the attacked sites thousands of dollars in lost business
In  a Swedish cracker broke into Microsoftâs Hotmail Web site and created a mirror site that allowed anyone to type in the name of a Hotmail user and then read all of the personâs current and archived email
In another case  a  -year-old Russian cracker named Maxim broke into an e-commerce Web site and stole    credit card numbers
Then he approached the site owners and told them that if they did not pay him $    he would post all the credit card numbers to the Internet
They did not give in to his blackmail  and he indeed posted the credit card numbers  inflicting great damage on many innocent victims
In a different vein  a  -year-old California student emailed a press release to a news agency falsely stating that the Emulex Corporation was going to post a large quarterly loss and that the
was resigning immediately
Within hours  the companyâs stock dropped by  %  causing stockholders to lose over $  billion
The perpetrator made a quarter of a million dollars by selling the stock short just before sending the announcement
While this event was not a Web site break-in  it is clear that putting such an announcement on the home page of any big corporation would have a similar effect
We could (unfortunately) go on like this for many more pages
But it is now time to examine some of the technical issues related to Web urity
For more NETWORK URITY
information about urity problems of all kinds  see Anderson (   a); Stuttard and Pinto (   ); and Schneier (   )
Searching the Internet will also turn up vast numbers of specific cases
ure Naming Let us start with something very basic: Alice wants to visit Bobâs Web site
She types Bobâs URL into her browser and a few onds later  a Web page appears
But is it Bobâs? Maybe yes and maybe no
Trudy might be up to her old tricks again
For example  she might be intercepting all of Aliceâs outgoing packets and examining them
When she captures an HTTP GET request headed to Bobâs Web site  she could go to Bobâs Web site herself to get the page  modify it as she wishes  and return the fake page to Alice
Alice would be none the wiser
Worse yet  Trudy could slash the prices at Bobâs e-store to make his goods look very attractive  thereby tricking Alice into sending her credit card number to ââBobââ to buy some merchandise
One disadvantage of this classic man-in-the-middle attack is that Trudy has to be in a position to intercept Aliceâs outgoing traffic and forge her incoming traffic
In practice  she has to tap either Aliceâs phone line or Bobâs  since tapping the fiber backbone is fairly difficult
While active wiretapping is certainly possible  it is a fair amount of work  and while Trudy is clever  she is also lazy
Besides  there are easier ways to trick Alice
DNS Spoofing One way would be for Trudy to crack the DNS system or maybe just the DNS cache at Aliceâs ISP  and replace Bobâs IP address (say
) with her (Trudyâs) IP address (say
That leads to the following attack
The way it is supposed to work is illustrated in Fig
Here  Alice ( ) asks DNS for Bobâs IP address  ( ) gets it  ( ) asks Bob for his home page  and ( ) gets that  too
After Trudy has modified Bobâs DNS record to contain her own IP address instead of Bobâs  we get the situation in Fig
Here  when Alice looks up Bobâs IP address  she gets Trudyâs  so all her traffic intended for Bob goes to Trudy
Trudy can now mount a man-in-the-middle attack without having to go to the trouble of tapping any phone lines
Instead  she has to break into a DNS server and change one record  a much easier proposition
How might Trudy fool DNS? It turns out to be relatively easy
Briefly summarized  Trudy can trick the DNS server at Aliceâs ISP into sending out a query to look up Bobâs address
Unfortunately  since DNS uses UDP  the DNS server has no real way of checking who supplied the answer
Trudy can exploit this property by forging the expected reply and thus injecting a false IP address into the DNS serverâs cache
For simplicity  we will assume that Aliceâs ISP does not initially have an entry for Bobâs Web site
If it does  Trudy can wait until it times out and try later (or use other tricks)
Give me Bob's IP address
(Bob's IP address)
Bob's home page Bob's Web server (
) DNS server Alice  (a)
Give me Bob's IP address
(Trudy's IP address)
Trudy's fake of Bob's home page Trudy's Web server (
) Cracked DNS server Alice  (b)  Figure  -
(a) Normal situation
(b) An attack based on breaking into a DNS server and modifying Bobâs record
Trudy starts the attack by sending a lookup request to Aliceâs ISP asking for the IP address of
Since there is no entry for this DNS name  the cache server queries the top-level server for the com domain to get one
However  Trudy beats the com server to the punch and sends back a false reply saying: ââ  is
ââ where that IP address is hers
If her false reply gets back to Aliceâs ISP first  that one will be cached and the real reply will be rejected as an unsolicited reply to a query no longer outstanding
Tricking a DNS server into installing a false IP address is called DNS spoofing
A cache that holds an intentionally false IP address like this is called a poisoned cache
Actually  things are not quite that simple
First  Aliceâs ISP checks to see that the reply bears the correct IP source address of the top-level server
But since Trudy can put anything she wants in that IP field  she can defeat that test easily since the IP addresses of the top-level servers have to be public
ond  to allow DNS servers to tell which reply goes with which request  all requests carry a sequence number
To spoof Aliceâs ISP  Trudy has to know its current sequence number
The easiest way to learn the current sequence number is for Trudy to register a domain herself  say  trudy-the-
Let us assume its IP address is also    She also creates a DNS server for her newly hatched domain   -the-
It  too  uses Trudyâs
IP address  since Trudy has only one computer
Now she has to make Aliceâs ISP aware of her DNS server
That is easy to do
All she has to do is ask Aliceâs ISP for  -the-   which will cause Aliceâs ISP to find out who serves Trudyâs new domain by asking the top-level com server
NETWORK URITY
With  -the-  safely in the cache at Aliceâs ISP  the real attack can start
Trudy now queries Aliceâs ISP for  -the-
The ISP naturally sends Trudyâs DNS server a query asking for it
This query bears the sequence number that Trudy is looking for
Quick like a bunny  Trudy asks Aliceâs ISP to look up Bob
She immediately answers her own question by sending the ISP a forged reply  allegedly from the top-level com server  saying: ââ  is
This forged reply carries a sequence number one higher than the one she just received
While she is at it  she can also send a ond forgery with a sequence number two higher  and maybe a dozen more with increasing sequence numbers
One of them is bound to match
The rest will just be thrown out
When Aliceâs forged reply arrives  it is cached; when the real reply comes in later  it is rejected since no query is then outstanding
Now when Alice looks up    she is told to use
Trudyâs address
Trudy has mounted a successful man-in-the-middle attack from the comfort of her own living room
The various steps to this attack are illustrated in Fig
This one specific attack can be foiled by having DNS servers use random IDs in their queries rather than just counting  but it seems that every time one hole is plugged  another one turns up
In particular  the IDs are only   bits  so working through all of them is easy when it is a computer that is doing the guessing Look up  -the-  (to force it into the ISP's cache)
Look up  -the-  (to get the ISP's next sequence number)
Request for  -the-  (Carrying the ISP's next sequence number  n)
Quick like a bunny  look up   (to force the ISP to query the com server in step  )
Legitimate query for   with seq = n+
Trudy's forged answer: Bob is
Real answer (rejected  too late) Alice's ISP's cache DNS server for com Trudy  Figure  -
How Trudy spoofs Aliceâs ISP
ure DNS The real problem is that DNS was designed at a time when the Internet was a research facility for a few hundred universities  and neither Alice  nor Bob  nor Trudy was invited to the party
urity was not an issue then; making the Internet work at all was the issue
The environment has changed radically over the   WEB URITY years  so in  IETF set up a working group to make DNS fundamentally ure
This (ongoing) project is known as DNS (DNS urity); its first output was presented in RFC
Unfortunately  DNS has not been fully deployed yet  so numerous DNS servers are still vulnerable to spoofing attacks
DNS is conceptually extremely simple
It is based on public-key cryptography
Every DNS zone (in the sense of Fig
- ) has a public/private key pair
All information sent by a DNS server is signed with the originating zoneâs private key  so the receiver can verify its authenticity
DNS offers three fundamental services:
Proof of where the data originated Public key distribution Transaction and request authentication
The main service is the first one  which verifies that the data being returned has been approved by the zoneâs owner
The ond one is useful for storing and retrieving public keys urely
The third one is needed to guard against playback and spoofing attacks
Note that recy is not an offered service since all the information in DNS is considered public
Since phasing in DNS is expected to take several years  the ability for urity-aware servers to interwork with urity- ignorant servers is essential  which implies that the protocol cannot be changed
Let us now look at some of the details
DNS records are grouped into sets called RRSets (Resource Record Sets)  with all the records having the same name  class  and type being lumped together in a set
An RRSet may contain multiple A records  for example  if a DNS name resolves to a primary IP address and a ondary IP address
The RRSets are extended with several new record types (discussed below)
Each RRSet is cryptographically hashed (
using SHA- )
The hash is signed by the zoneâs private key (
using RSA)
The unit of transmission to clients is the signed RRSet
Upon receipt of a signed RRSet  the client can verify whether it was signed by the private key of the originating zone
If the signature agrees  the data are accepted
Since each RRSet contains its own signature  RRSets can be cached anywhere  even at untrustworthy servers  without endangering the urity
DNS introduces several new record types
The first of these is the KEY record
This records holds the public key of a zone  user  host  or other principal  the cryptographic algorithm used for signing  the protocol used for transmission  and a few other bits
The public key is stored naked
certificates are not used due to their bulk
The algorithm field holds a  for MD /RSA signatures (the preferred choice)  and other values for other combinations
The protocol field can indicate the use of IP or other urity protocols  if any
The ond new record type is the SIG record
It holds the signed hash according to the algorithm specified in the KEY record
The signature applies to all the records in the RRSet  including any KEY records present  but excluding NETWORK URITY
It also holds the times when the signature begins its period of validity and when it expires  as well as the signerâs name and a few other items
The DNS design is such that a zoneâs private key can be kept offline
Once or twice a day  the contents of a zoneâs database can be manually transported (
on CD-ROM) to a disconnected machine on which the private key is located
All the RRSets can be signed there and the SIG records thus produced can be conveyed back to the zoneâs primary server on CD-ROM
In this way  the private key can be stored on a CD-ROM locked in a safe except when it is inserted into the disconnected machine for signing the dayâs new RRSets
After signing is completed  all copies of the key are erased from memory and the disk and the CD-ROM are returned to the safe
This procedure reduces electronic urity to physical urity  something people understand how to deal with
This method of presigning RRSets greatly speeds up the process of answering queries since no cryptography has to be done on the fly
The trade-off is that a large amount of disk space is needed to store all the keys and signatures in the DNS databases
Some records will increase tenfold in size due to the signature
When a client process gets a signed RRSet  it must apply the originating zoneâs public key to decrypt the hash  compute the hash itself  and compare the two values
If they agree  the data are considered valid
However  this procedure begs the question of how the client gets the zoneâs public key
One way is to acquire it from a trusted server  using a ure connection (
However  in practice  it is expected that clients will be preconfigured with the public keys of all the top-level domains
If Alice now wants to visit Bobâs Web site  she can ask DNS for the RRSet of    which will contain his IP address and a KEY record containing Bobâs public key
This RRSet will be signed by the top-level com domain  so Alice can easily verify its validity
An example of what this RRSet might contain is shown in Fig
Domain name Time to live Class Type Value
IN KEY   A B  F  CE   D
IN SIG    A B  F   E C
An example RRSet for
The KEY record is Bobâs public key
The SIG record is the top-level com serverâs signed hash of the A and KEY records to verify their authenticity
Now armed with a verified copy of Bobâs public key  Alice can ask Bobâs DNS server (run by Bob) for the IP address of
This RRSet will be signed by Bobâs private key  so Alice can verify the signature on the RRSet Bob returns
If Trudy somehow manages to inject a false RRSet into any of the caches  Alice can easily detect its lack of authenticity because the SIG record contained in it will be incorrect
WEB URITY However  DNS also provides a cryptographic mechanism to bind a response to a specific query  to prevent the kind of spoof Trudy managed to pull off in Fig
This (optional) antispoofing measure adds to the response a hash of the query message signed with the respondentâs private key
Since Trudy does not know the private key of the top-level com server  she cannot forge a response to a query Aliceâs ISP sent there
She can certainly get her response back first  but it will be rejected due to its invalid signature over the hashed query
DNS also supports a few other record types
For example  the CERT record can be used for storing (
) certificates
This record has been provided because some people want to turn DNS into a PKI
Whether this will actually happen remains to be seen
We will stop our discussion of DNS here
For more details  please consult RFC
SSLâThe ure Sockets Layer ure naming is a good start  but there is much more to Web urity
The next step is ure connections
We will now look at how ure connections can be achieved
Nothing involving urity is simple and this is not either
When the Web burst into public view  it was initially used for just distributing static pages
However  before long  some companies got the idea of using it for financial transactions  such as purchasing merchandise by credit card  online banking  and electronic stock trading
These applications created a demand for ure connections
In  Netscape Communications Corp
the then-dominant browser vendor  responded by introducing a urity package called SSL (ure Sockets Layer) to meet this demand
This software and its protocol are now widely used  for example  by Firefox  Safari  and Internet Explorer  so it is worth examining in some detail
SSL builds a ure connection between two sockets  including
Parameter negotiation between client and server Authentication of the server by the client ret communication Data integrity protection
We have seen these items before  so there is no need to elaborate on them
The positioning of SSL in the usual protocol stack is illustrated in Fig
Effectively  it is a new layer interposed between the application layer and the transport layer  accepting requests from the browser and sending them down to TCP for transmission to the server
Once the ure connection has been established  SSLâs main job is handling compression and encryption
When HTTP is used over SSL  it is called HTTPS (ure HTTP)  even though it is the standard HTTP protocol
Sometimes it is available at a new port (  ) instead of port
NETWORK URITY
As an aside  SSL is not restricted to Web browsers  but that is its most common application
It can also provide mutual authentication
Application (HTTP) urity (SSL) Transport (TCP) Network (IP) Data link (PPP) Physical (modem  ADSL  cable TV) Figure  -
Layers (and protocols) for a home user browsing with SSL
The SSL protocol has gone through several versions
Below we will discuss only version   which is the most widely used version
SSL supports a variety of different options
These options include the presence or absence of compression  the cryptographic algorithms to be used  and some matters relating to export restrictions on cryptography
The last is mainly intended to make sure that serious cryptography is used only when both ends of the connection are in the United States
In other cases  keys are limited to   bits  which cryptographers regard as something of a joke
Netscape was forced to put in this restriction in order to get an export license from the
Government
SSL consists of two subprotocols  one for establishing a ure connection and one for using it
Let us start out by seeing how ure connections are established
The connection establishment subprotocol is shown in Fig
It starts out with message  when Alice sends a request to Bob to establish a connection
The request specifies the SSL version Alice has and her preferences with respect to compression and cryptographic algorithms
It also contains a nonce  RA  to be used later
Now it is Bobâs turn
In message   Bob makes a choice among the various algorithms that Alice can support and sends his own nonce  RB
Then  in message   he sends a certificate containing his public key
If this certificate is not signed by some well-known authority  he also sends a chain of certificates that can be followed back to one
All browsers  including Aliceâs  come preloaded with about public keys  so if Bob can establish a chain anchored to one of these  Alice will be able to verify Bobâs public key
At this point  Bob may send some other messages (such as a request for Aliceâs public-key certificate)
When Bob is done  he sends message  to tell Alice it is her turn
Alice responds by choosing a random   -bit premaster key and sending it to Bob encrypted with his public key (message  )
The actual session key used for encrypting data is derived from the premaster key combined with both nonces in a complex way
After message  has been received  both Alice and Bob are able to compute the session key
For this reason  Alice tells Bob to switch to the   WEB URITY SSL version  preferences  RA SSL version  choices  RB X
certificate chain Server done EB (premaster key) Change cipher Finished Change cipher Finished  Alice Bob    Figure  -
A simplified version of the SSL connection establishment subprotocol
new cipher (message  ) and also that she is finished with the establishment subprotocol (message  )
Bob then acknowledges her (messages  and  )
However  although Alice knows who Bob is  Bob does not know who Alice is (unless Alice has a public key and a corresponding certificate for it  an unlikely situation for an individual)
Therefore  Bobâs first message may well be a request for Alice to log in using a previously established login name and password
The login protocol  however  is outside the scope of SSL
Once it has been accomplished  by whatever means  data transport can begin
As mentioned above  SSL supports multiple cryptographic algorithms
The strongest one uses triple DES with three separate keys for encryption and SHA-  for message integrity
This combination is relatively slow  so it is mostly used for banking and other applications in which the highest urity is required
For ordinary e-commerce applications  RC  is used with a   -bit key for encryption and MD  is used for message authentication
RC  takes the   -bit key as a seed and expands it to a much larger number for internal use
Then it uses this internal number to generate a keystream
The keystream is XORed with the plaintext to provide a classical stream cipher  as we saw in Fig
The export versions also use RC  with   -bit keys  but   of the bits are made public to make the cipher easy to break
For actual transport  a ond subprotocol is used  as shown in Fig
Messages from the browser are first broken into units of up to   KB
If data NETWORK URITY
compression is enabled  each unit is then separately compressed
After that  a ret key derived from the two nonces and premaster key is concatenated with the compressed text and the result is hashed with the agreed-on hashing algorithm (usually MD )
This hash is appended to each fragment as the MAC
The compressed fragment plus MAC is then encrypted with the agreed-on symmetric encryption algorithm (usually by XORing it with the RC  keystream)
Finally  a fragment header is attached and the fragment is transmitted over the TCP connection
Message authentication code Header added Encryption MAC added Compression Fragmentation Part  Part  Message from browser Figure  -
Data transmission using SSL
A word of caution is in order  however
Since it has been shown that RC  has some weak keys that can be easily cryptanalyzed  the urity of SSL using RC  is on shaky ground (Fluhrer et al
Browsers that allow the user to choose the cipher suite should be configured to use triple DES with   -bit keys and SHA-  all the time  even though this combination is slower than RC  and MD
Or  better yet  users should upgrade to browsers that support the successor to SSL that we describe shortly
A problem with SSL is that the principals may not have certificates  and even if they do  they do not always verify that the keys being used match them
In  Netscape Communications Corp
turned SSL over to IETF for standardization
The result was TLS (Transport Layer urity)
It is described in RFC
TLS was built on SSL version
The changes made to SSL were relatively small  but just enough that SSL version  and TLS cannot interoperate
For example  the way the session key is derived from the premaster key and nonces was   WEB URITY changed to make the key stronger (
harder to cryptanalyze)
Because of this incompatibility  most browsers implement both protocols  with TLS falling back to SSL during negotiation if necessary
This is referred to as SSL/TLS
The first TLS implementation appeared in  with version
defined in August
It includes support for stronger cipher suites (notably AES)
SSL has remained strong in the marketplace although TLS will probably gradually replace it
Mobile Code urity Naming and connections are two areas of concern related to Web urity
But there are more
In the early days  when Web pages were just static HTML files  they did not contain executable code
Now they often contain small programs  including Java applets  ActiveX controls  and JavaScripts
Downloading and executing such mobile code is obviously a massive urity risk  so various methods have been devised to minimize it
We will now take a quick peek at some of the issues raised by mobile code and some approaches to dealing with it
Java Applet urity Java applets are small Java programs compiled to a stack-oriented machine language called JVM (Java Virtual Machine)
They can be placed on a Web page for downloading along with the page
After the page is loaded  the applets are inserted into a JVM interpreter inside the browser  as illustrated in Fig
Untrusted applet Trusted applet Web browser Sandbox Interpreter Virtual address space  xFFFFFFFF  Figure  -
Applets can be interpreted by a Web browser
The advantage of running interpreted code over compiled code is that every instruction is examined by the interpreter before being executed
This gives the interpreter the opportunity to check whether the instructionâs address is valid
In addition  system calls are also caught and interpreted
How these calls are handled is a matter of the urity policy
For example  if an applet is trusted (
it NETWORK URITY
came from the local disk)  its system calls could be carried out without question
However  if an applet is not trusted (
it came in over the Internet)  it could be encapsulated in what is called a sandbox to restrict its behavior and trap its attempts to use system resources
When an applet tries to use a system resource  its call is passed to a urity monitor for approval
The monitor examines the call in light of the local urity policy and then makes a decision to allow or reject it
In this way  it is possible to give applets access to some resources but not all
Unfortunately  the reality is that the urity model works badly and that bugs in it crop up all the time
ActiveX ActiveX controls are x  binary programs that can be embedded in Web pages
When one of them is encountered  a check is made to see if it should be executed  and it if passes the test  it is executed
It is not interpreted or sandboxed in any way  so it has as much power as any other user program and can potentially do great harm
Thus  all the urity is in the decision whether to run the ActiveX control
In retrospect  the whole idea is a gigantic urity hole
The method that Microsoft chose for making this decision is based on the idea of code signing
Each ActiveX control is accompanied by a digital signatureâa hash of the code that is signed by its creator using public-key cryptography
When an ActiveX control shows up  the browser first verifies the signature to make sure it has not been tampered with in transit
If the signature is correct  the browser then checks its internal tables to see if the programâs creator is trusted or there is a chain of trust back to a trusted creator
If the creator is trusted  the program is executed; otherwise  it is not
The Microsoft system for verifying ActiveX controls is called Authenticode
It is useful to contrast the Java and ActiveX approaches
With the Java approach  no attempt is made to determine who wrote the applet
Instead  a run-time interpreter makes sure it does not do things the machine owner has said applets may not do
In contrast  with code signing  there is no attempt to monitor the mobile codeâs run-time behavior
If it came from a trusted source and has not been modified in transit  it just runs
No attempt is made to see whether the code is malicious or not
If the original programmer intended the code to format the hard disk and then erase the flash ROM so the computer can never again be booted  and if the programmer has been certified as trusted  the code will be run and destroy the computer (unless ActiveX controls have been disabled in the browser)
Many people feel that trusting an unknown software company is scary
To demonstrate the problem  a programmer in Seattle formed a software company and got it certified as trustworthy  which is easy to do
He then wrote an ActiveX control that did a clean shutdown of the machine and distributed his ActiveX control widely
It shut down many machines  but they could just be rebooted  so no   WEB URITY harm was done
He was just trying to expose the problem to the world
The official response was to revoke the certificate for this specific ActiveX control  which ended a short episode of acute embarrassment  but the underlying problem is still there for an evil programmer to exploit (Garfinkel with Spafford  )
Since there is no way to police the thousands of software companies that might write mobile code  the technique of code signing is a disaster waiting to happen
JavaScript JavaScript does not have any formal urity model  but it does have a long history of leaky implementations
Each vendor handles urity in a different way
For example  Netscape Navigator version  used something akin to the Java model  but by version  that had been abandoned for a code-signing model
The fundamental problem is that letting foreign code run on your machine is asking for trouble
From a urity standpoint  it is like inviting a burglar into your house and then trying to watch him carefully so he cannot escape from the kitchen into the living room
If something unexpected happens and you are distracted for a moment  bad things can happen
The tension here is that mobile code allows flashy graphics and fast interaction  and many Web site designers think that this is much more important than urity  especially when it is somebody elseâs machine at risk
Browser Extensions As well as extending Web pages with code  there is a booming marketplace in browser extensions  add-ons  and plug-ins
They are computer programs that extend the functionality of Web browsers
Plug-ins often provide the capability to interpret or display a certain type of content  such as PDFs or Flash animations
Extensions and add-ons provide new browser features  such as better password management  or ways to interact with pages by  for example  marking them up or enabling easy shopping for related items
Installing an extension  add-on  or plug-in is as simple as coming across something you want when browsing and following the link to install the program
This action will cause code to be downloaded across the Internet and installed into the browser
All of these programs are written to frameworks that differ depending on the browser that is being enhanced
However  to a first approximation  they become part of the trusted computing base of the browser
That is  if the code that is installed is buggy  the entire browser can be compromised
There are two other obvious failure modes as well
The first is that the program may behave maliciously  for example  by gathering personal information and sending it to a remote server
For all the browser knows  the user installed the extension for precisely this purpose
The ond problem is that plug-ins give the browser the ability to interpret new types of content
Often this content is a full NETWORK URITY
blown programming language itself
PDF and Flash are good examples
When users view pages with PDF and Flash content  the plug-ins in their browser are executing the PDF and Flash code
That code had better be safe; often there are vulnerabilities that it can exploit
For all of these reasons  add-ons and plug-ins should only be installed as needed and only from trusted vendors
Viruses Viruses are another form of mobile code
Only  unlike the examples above  viruses are not invited in at all
The difference between a virus and ordinary mobile code is that viruses are written to reproduce themselves
When a virus arrives  either via a Web page  an email attachment  or some other way  it usually starts out by infecting executable programs on the disk
When one of these programs is run  control is transferred to the virus  which usually tries to spread itself to other machines  for example  by emailing copies of itself to everyone in the victimâs email address book
Some viruses infect the boot tor of the hard disk  so when the machine is booted  the virus gets to run
Viruses have become a huge problem on the Internet and have caused billions of dollarsâ worth of damage
There is no obvious solution
Perhaps a whole new generation of operating systems based on ure microkernels and tight compartmentalization of users  processes  and resources might help  SOCIAL ISSUES The Internet and its urity technology is an area where social issues  public policy  and technology meet head on  often with huge consequences
Below we will just briefly examine three areas: privacy  freedom of speech  and copyright
Needless to say  we can only scratch the surface
For additional reading  see Anderson (   a)  Garfinkel with Spafford (   )  and Schneier (   )
The Internet is also full of material
Just type words such as ââprivacy ââ ââcensorship ââ and ââcopyrightââ into any search engine
Also  see this bookâs Web site for some links
It is at http:// /tanenbaum
Privacy Do people have a right to privacy? Good question
The Fourth Amendment to the
Constitution prohibits the government from searching peopleâs houses  papers  and effects without good reason  and goes on to restrict the circumstances under which search warrants shall be issued
Thus  privacy has been on the public agenda for over years  at least in the
What has changed in the past decade is both the ease with which governments can spy on their citizens and the ease with which the citizens can prevent such   SOCIAL ISSUES spying
In the  th century  for the government to search a citizenâs papers  it had to send out a policeman on a horse to go to the citizenâs farm demanding to see certain documents
It was a cumbersome procedure
Nowadays  telephone companies and Internet providers readily provide wiretaps when presented with search warrants
It makes life much easier for the policeman and there is no danger of falling off a horse
Cryptography changes all that
Anybody who goes to the trouble of downloading and installing PGP and who uses a well-guarded alien-strength key can be fairly sure that nobody in the known universe can read his email  search warrant or no search warrant
Governments well understand this and do not like it
Real privacy means it is much harder for them to spy on criminals of all stripes  but it is also much harder to spy on journalists and political opponents
Consequently  some governments restrict or forbid the use or export of cryptography
In France  for example  prior to  all cryptography was banned unless the government was given the keys
France was not alone
In April  the
Government announced its intention to make a hardware cryptoprocessor  the clipper chip  the standard for all networked communication
It was said that this would guarantee citizensâ privacy
It also mentioned that the chip provided the government with the ability to decrypt all traffic via a scheme called key escrow  which allowed the government access to all the keys
However  the government promised only to snoop when it had a valid search warrant
Needless to say  a huge furor ensued  with privacy advocates denouncing the whole plan and law enforcement officials praising it
Eventually  the government backed down and dropped the idea
A large amount of information about electronic privacy is available at the Electronic Frontier Foundationâs Web site
Anonymous Remailers PGP  SSL  and other technologies make it possible for two parties to establish ure  authenticated communication  free from third-party surveillance and interference
However  sometimes privacy is best served by not having authentication  in fact  by making communication anonymous
The anonymity may be desired for point-to-point messages  newsgroups  or both
Let us consider some examples
First  political dissidents living under authoritarian regimes often wish to communicate anonymously to escape being jailed or killed
ond  wrongdoing in many corporate  educational  governmental  and other organizations has often been exposed by whistleblowers  who frequently prefer to remain anonymous to avoid retribution
Third  people with unpopular social  political  or religious views may wish to communicate with each other via email or newsgroups without exposing themselves
Fourth  people may wish to discuss alcoholism  mental illness  sexual harassment  child abuse  or being a NETWORK URITY
member of a peruted minority in a newsgroup without having to go public
Numerous other examples exist  of course
Let us consider a specific example
In the s  some critics of a nontraditional religious group posted their views to a USENET newsgroup via an anonymous remailer
This server allowed users to create pseudonyms and send email to the server  which then remailed or re-posted them using the pseudonyms  so no one could tell where the messages really came from
Some postings revealed what the religious group claimed were trade rets and copyrighted documents
The religious group responded by telling local authorities that its trade rets had been disclosed and its copyright infringed  both of which were crimes where the server was located
A court case followed and the server operator was compelled to turn over the mapping information that revealed the true identities of the persons who had made the postings
(Incidentally  this was not the first time that a religious group was unhappy when someone leaked its trade rets: William Tyndale was burned at the stake in  for translating the Bible into English)
A substantial segment of the Internet community was completely outraged by this breach of confidentiality
The conclusion that everyone drew is that an anonymous remailer that stores a mapping between real email addresses and pseudonyms (now called a type  remailer) is not worth much
This case stimulated various people into designing anonymous remailers that could withstand subpoena attacks
These new remailers  often called cypherpunk remailers  work as follows
The user produces an email message  complete with RFC headers (except From:  of course)  encrypts it with the remailerâs public key  and sends it to the remailer
There the outer RFC headers are stripped off  the content is decrypted and the message is remailed
The remailer has no accounts and maintains no logs  so even if the server is later confiscated  it retains no trace of messages that have passed through it
Many users who wish anonymity chain their requests through multiple anonymous remailers  as shown in Fig
Here  Alice wants to send Bob a really  really  really anonymous Valentineâs Day card  so she uses three remailers
She composes the message  M  and puts a header on it containing Bobâs email address
Then she encrypts the whole thing with remailer  âs public key  E  (indicated by horizontal hatching)
To this she prepends a header with remailer  âs email address in plaintext
This is the message shown between remailers  and  in the figure
Then she encrypts this message with remailer  âs public key  E  (indicated by vertical hatching) and prepends a plaintext header containing remailer  âs email address
This message is shown between  and  in Fig
Finally  she encrypts the entire message with remailer  âs public key  E  and prepends a plaintext header with remailer  âs email address
This is the message shown to the right of Alice in the figure and this is the message she actually transmits
SOCIAL ISSUES Alice  Bob To  To  Anonymous remailer Encrypted with E  Encrypted with E  Encrypted with E  To Bob To  M To Bob M To  To Bob M To  To  To Bob M Figure  -
How Alice uses three remailers to send Bob a message
When the message hits remailer   the outer header is stripped off
The body is decrypted and then emailed to remailer
Similar steps occur at the other two remailers
Although it is extremely difficult for anyone to trace the final message back to Alice  many remailers take additional safety precautions
For example  they may hold messages for a random time  add or remove junk at the end of a message  and reorder messages  all to make it harder for anyone to tell which message output by a remailer corresponds to which input  in order to thwart traffic analysis
For a description of this kind of remailer  see Mazie`res and Kaashoek (   )
Anonymity is not restricted to email
Services also exist that allow anonymous Web surfing using the same form of layered path in which one node only knows the next node in the chain
This method is called onion routing because each node peels off another layer of the onion to determine where to forward the packet next
The user configures his browser to use the anonymizer service as a proxy
Tor is a well-known example of such a system (Dingledine et al
Henceforth  all HTTP requests go through the anonymizer network  which requests the page and sends it back
The Web site sees an exit node of the anonymizer network as the source of the request  not the user
As long as the anonymizer network refrains from keeping a log  after the fact no one can determine who requested which page
Freedom of Speech Privacy relates to individuals wanting to restrict what other people can see about them
A ond key social issue is freedom of speech  and its opposite  censorship  which is about governments wanting to restrict what individuals can read and publish
With the Web containing millions and millions of pages  it has become a censorâs paradise
Depending on the nature and ideology of the regime  banned material may include Web sites containing any of the following: NETWORK URITY
Material inappropriate for children or teenagers Hate aimed at various ethnic  religious  sexual or other groups Information about democracy and democratic values Accounts of historical events contradicting the governmentâs version Manuals for picking locks  building weapons  encrypting messages  etc
The usual response is to ban the ââbadââ sites
Sometimes the results are unexpected
For example  some public libraries have installed Web filters on their computers to make them child friendly by blocking pornography sites
The filters veto sites on their blacklists but also check pages for dirty words before displaying them
In one case in Loudoun County  Virginia  the filter blocked a patronâs search for information on breast cancer because the filter saw the word ââbreast
ââ The library patron sued Loudoun County
However  in Livermore  California  a parent sued the public library for not installing a filter after her  -year-old son was caught viewing pornography there
Whatâs a library to do? It has escaped many people that the World Wide Web is a worldwide Web
It covers the whole world
Not all countries agree on what should be allowed on the Web
For example  in November  a French court ordered Yahoo!  a California Corporation  to block French users from viewing auctions of Nazi memorabilia on Yahoo!âs Web site because owning such material violates French law
Yahoo! appealed to a
court  which sided with it  but the issue of whose laws apply where is far from settled
Just imagine
What would happen if some court in Utah instructed France to block Web sites dealing with wine because they do not comply with Utahâs much stricter laws about alcohol? Suppose that China demanded that all Web sites dealing with democracy be banned as not in the interest of the State
Do Iranian laws on religion apply to more liberal Sweden? Can Saudi Arabia block Web sites dealing with womenâs rights? The whole issue is a veritable Pandoraâs box
A relevant comment from John Gilmore is: ââThe net interprets censorship as damage and routes around it
ââ For a concrete implementation  consider the eternity service (Anderson  )
Its goal is to make sure published information cannot be depublished or rewritten  as was common in the Soviet Union during Josef Stalinâs reign
To use the eternity service  the user specifies how long the material is to be preserved  pays a fee proportional to its duration and size  and uploads it
Thereafter  no one can remove or edit it  not even the uploader
How could such a service be implemented? The simplest model is to use a peer-to-peer system in which stored documents would be placed on dozens of participating servers  each of which gets a fraction of the fee  and thus an incentive to join the system
The servers should be spread over many legal jurisdictions for maximum resilience
Lists of   randomly selected servers would be stored   SOCIAL ISSUES urely in multiple places  so that if some were compromised  others would still exist
An authority bent on destroying the document could never be sure it had found all copies
The system could also be made self-repairing in the sense that if it became known that some copies had been destroyed  the remaining sites would attempt to find new repositories to replace them
The eternity service was the first proposal for a censorship-resistant system
Since then  others have been proposed and  in some cases  implemented
Various new features have been added  such as encryption  anonymity  and fault tolerance
Often the files to be stored are broken up into multiple fragments  with each fragment stored on many servers
Some of these systems are Freenet (Clarke et al
)  PASIS (Wylie et al
)  and Publius (Waldman et al
Other work is reported by Serjantov (   )
Increasingly  many countries are trying to regulate the export of intangibles  which often include Web sites  software  scientific papers  email  telephone helpdesks  and more
which has a centuries-long tradition of freedom of speech  is now seriously considering highly restrictive laws  that would  for example  define technical discussions between a British professor and his foreign
student  both located at the University of Cambridge  as regulated export needing a government license (Anderson  )
Needless to say  many people consider such a policy to be outrageous
Steganography In countries where censorship abounds  dissidents often try to use technology to evade it
Cryptography allows ret messages to be sent (although possibly not lawfully)  but if the government thinks that Alice is a Bad Person  the mere fact that she is communicating with Bob may get him put in this category  too  as repressive governments understand the concept of transitive closure  even if they are short on mathematicians
Anonymous remailers can help  but if they are banned domestically and messages to foreign ones require a government export license  they cannot help much
But the Web can
People who want to communicate retly often try to hide the fact that any communication at all is taking place
The science of hiding messages is called steganography  from the Greek words for ââcovered writing
ââ In fact  the ancient Greeks used it themselves
Herodotus wrote of a general who shaved the head of a messenger  tattooed a message on his scalp  and let the hair grow back before sending him off
Modern techniques are conceptually the same  only they have a higher bandwidth  lower latency  and do not require the services of a barber
As a case in point  consider Fig
This photograph  taken by one of the authors (AST) in Kenya  contains three zebras contemplating an acacia tree
It contains the complete  unabridged text of five of NETWORK URITY
Shakespeareâs plays embedded in it: Hamlet  King Lear  Macbeth  The Merchant of Venice  and Julius Caesar
Together  these plays total over KB of text
(a) (b) Figure  -
(a) Three zebras and a tree
(b) Three zebras  a tree  and the complete text of five plays by William Shakespeare
How does this steganographic channel work? The original color image is  Ã pixels
Each pixel consists of three  -bit numbers  one each for the red  green  and blue intensity of that pixel
The pixelâs color is formed by the linear superposition of the three colors
The steganographic encoding method uses the low-order bit of each RGB color value as a covert channel
Thus  each pixel has room for  bits of ret information   in the red value   in the green value  and  in the blue value
With an image of this size  up to  Ã Ã  bits or    bytes of ret information can be stored in it
The full text of the five plays and a short notice add up to    bytes
This text was first compressed to about KB using a standard compression algorithm
The compressed output was then encrypted using IDEA and inserted into the low-order bits of each color value
As can be seen (or actually  cannot be seen)  the existence of the information is completely invisible
It is equally invisible in the large  full-color version of the photo
The eye cannot easily distinguish  -bit color from  -bit color
Viewing the two images in black and white with low resolution does not do justice to how powerful the technique is
To get a better feel for how steganography works  we have prepared a demonstration  including the full-color highresolution image of Fig
The demonstration  including tools for inserting and extracting text into images  can be found at the bookâs Web site
To use steganography for undetected communication  dissidents could create a Web site bursting with politically correct pictures  such as photographs of the Great Leader  local sports  movie  and television stars  etc
Of course  the pictures would be riddled with steganographic messages
If the messages were first   SOCIAL ISSUES compressed and then encrypted  even someone who suspected their presence would have immense difficulty in distinguishing the messages from white noise
Of course  the images should be fresh scans; copying a picture from the Internet and changing some of the bits is a dead giveaway
Images are by no means the only carrier for steganographic messages
Audio files also work fine
Hidden information can be carried in a voice-over-IP call by manipulating the packet delays  distorting the audio  or even in the header fields of packets (Lubacz et al
Even the layout and ordering of tags in an HTML file can carry information
Although we have examined steganography in the context of free speech  it has numerous other uses
One common use is for the owners of images to encode ret messages in them stating their ownership rights
If such an image is stolen and placed on a Web site  the lawful owner can reveal the steganographic message in court to prove whose image it is
This technique is called watermarking
It is discussed in Piva et al
For more on steganography  see Wayner (   )
Copyright Privacy and censorship are just two areas where technology meets public policy
A third one is the copyright law
Copyright is granting to the creators of IP (Intellectual Property)  including writers  poets  artists  composers  musicians  photographers  cinematographers  choreographers  and others  the exclusive right to exploit their IP for some period of time  typically the life of the author plus   years or   years in the case of corporate ownership
After the copyright of a work expires  it passes into the public domain and anyone can use or sell it as they wish
The Gutenberg Project ( /pg)  for example  has placed thousands of public-domain works (
by Shakespeare  Twain  and Dickens) on the Web
Congress extended copyright in the
by another   years at the request of Hollywood  which claimed that without an extension nobody would create anything any more
By way of contrast  patents last for only   years and people still invent things
Copyright came to the forefront when Napster  a music-swapping service  had   million members
Although Napster did not actually copy any music  the courts held that its holding a central database of who had which song was contributory infringement  that is  it was helping other people infringe
While nobody seriously claims copyright is a bad idea (although many claim that the term is far too long  favoring big corporations over the public)  the next generation of music sharing is already raising major ethical issues
For example  consider a peer-to-peer network in which people share legal files (public-domain music  home videos  religious tracts that are not trade rets  etc
) and perhaps a few that are copyrighted
Assume that everyone is online all the time via ADSL or cable
Each machine has an index of what is on the hard NETWORK URITY
disk  plus a list of other members
Someone looking for a specific item can pick a random member and see if he has it
If not  he can check out all the members in that personâs list  and all the members in their lists  and so on
Computers are very good at this kind of work
Having found the item  the requester just copies it
If the work is copyrighted  chances are the requester is infringing (although for international transfers  the question of whose law applies matters because in some countries uploading is illegal but downloading is not)
But what about the supplier? Is it a crime to keep music you have paid for and legally downloaded on your hard disk where others might find it? If you have an unlocked cabin in the country and an IP thief sneaks in carrying a notebook computer and scanner  scans a copyrighted book to the notebookâs hard disk  and sneaks out  are you guilty of the crime of failing to protect someone elseâs copyright? But there is more trouble brewing on the copyright front
There is a huge battle going on now between Hollywood and the computer industry
The former wants stringent protection of all intellectual property but the latter does not want to be Hollywoodâs policeman
In October  Congress passed the DMCA (Digital Millennium Copyright Act)  which makes it a crime to circumvent any protection mechanism present in a copyrighted work or to tell others how to circumvent it
Similar legislation has been enacted in the European Union
While virtually no one thinks that pirates in the Far East should be allowed to duplicate copyrighted works  many people think that the DMCA completely shifts the balance between the copyright ownerâs interest and the public interest
A case in point: in September  a music industry consortium charged with building an unbreakable system for selling music online sponsored a contest inviting people to try to break the system (which is precisely the right thing to do with any new urity system)
A team of urity researchers from several universities  led by Prof
Edward Felten of Princeton  took up the challenge and broke the system
They then wrote a paper about their findings and submitted it to a USENIX urity conference  where it underwent peer review and was accepted
Before the paper was to be presented  Felten received a letter from the Recording Industry Association of America that threatened to sue the authors under the DMCA if they published the paper
Their response was to file a lawsuit asking a federal court to rule on whether publishing scientific papers on urity research was still legal
Fearing a definitive court ruling against it  the industry withdrew its threat and the court dismissed Feltenâs suit
No doubt the industry was motivated by the weakness of its case: it had invited people to try to break its system and then threatened to sue some of them for accepting its own challenge
With the threat withdrawn  the paper was published (Craver et al
A new confrontation is virtually certain
Meanwhile  pirated music and movies have fueled the massive growth of peer-to-peer networks
This has not pleased the copyright holders  who have used the DMCA to take action
There are now automated systems that search peer-topeer networks and then fire off warnings to network operators and users who are   SOCIAL ISSUES suspected of infringing copyright
In the United States  these warnings are known as DMCA takedown notices
This search is an armsâ race because it is hard to reliably catch copyright infringers
Even your printer might be mistaken for a culprit (Piatek et al
A related issue is the extent of the fair use doctrine  which has been established by court rulings in various countries
This doctrine says that purchasers of a copyrighted work have certain limited rights to copy the work  including the right to quote parts of it for scientific purposes  use it as teaching material in schools or colleges  and in some cases make backup copies for personal use in case the original medium fails
The tests for what constitutes fair use include ( ) whether the use is commercial  ( ) what percentage of the whole is being copied  and ( ) the effect of the copying on sales of the work
Since the DMCA and similar laws within the European Union prohibit circumvention of copy protection schemes  these laws also prohibit legal fair use
In effect  the DMCA takes away historical rights from users to give content sellers more power
A major showdown is inevitable
Another development in the works that dwarfs even the DMCA in its shifting of the balance between copyright owners and users is trusted computing as advocated by industry bodies such as the TCG (Trusted Computing Group)  led by companies like Intel and Microsoft
The idea is to provide support for carefully monitoring user behavior in various ways (
playing pirated music) at a level below the operating system in order to prohibit unwanted behavior
This is accomplished with a small chip  called a TPM (Trusted Platform Module)  which it is difficult to tamper with
Most PCs sold nowadays come equipped with a TPM
The system allows software written by content owners to manipulate PCs in ways that users cannot change
This raises the question of who is trusted in trusted computing
Certainly  it is not the user
Needless to say  the social consequences of this scheme are immense
It is nice that the industry is finally paying attention to urity  but it is lamentable that the driver is enforcing copyright law rather than dealing with viruses  crackers  intruders  and other urity issues that most people are concerned about
In short  the lawmakers and lawyers will be busy balancing the economic interests of copyright owners with the public interest for years to come
Cyberspace is no different from meatspace: it constantly pits one group against another  resulting in power struggles  litigation  and (hopefully) eventually some kind of resolution  at least until some new disruptive technology comes along  SUMMARY Cryptography is a tool that can be used to keep information confidential and to ensure its integrity and authenticity
All modern cryptographic systems are based on Kerckhoffâs principle of having a publicly known algorithm and a ret NETWORK URITY
Many cryptographic algorithms use complex transformations involving substitutions and permutations to transform the plaintext into the ciphertext
However  if quantum cryptography can be made practical  the use of one-time pads may provide truly unbreakable cryptosystems
Cryptographic algorithms can be divided into symmetric-key algorithms and public-key algorithms
Symmetric-key algorithms mangle the bits in a series of rounds parameterized by the key to turn the plaintext into the ciphertext
AES (Rijndael) and triple DES are the most popular symmetric-key algorithms at present
These algorithms can be used in electronic code book mode  cipher block chaining mode  stream cipher mode  counter mode  and others
Public-key algorithms have the property that different keys are used for encryption and decryption and that the decryption key cannot be derived from the encryption key
These properties make it possible to publish the public key
The main public-key algorithm is RSA  which derives its strength from the fact that it is very difficult to factor large numbers
Legal  commercial  and other documents need to be signed
Accordingly  various schemes have been devised for digital signatures  using both symmetric-key and public-key algorithms
Commonly  messages to be signed are hashed using algorithms such as SHA-  and then the hashes are signed rather than the original messages
Public-key management can be done using certificates  which are documents that bind a principal to a public key
Certificates are signed by a trusted authority or by someone (recursively) approved by a trusted authority
The root of the chain has to be obtained in advance  but browsers generally have many root certificates built into them
These cryptographic tools can be used to ure network traffic
IP operates in the network layer  encrypting packet flows from host to host
Firewalls can screen traffic going into or out of an organization  often based on the protocol and port used
Virtual private networks can simulate an old leased-line network to provide certain desirable urity properties
Finally  wireless networks need good urity lest everyone read all the messages  and protocols like
i provide it
When two parties establish a session  they have to authenticate each other and  if need be  establish a shared session key
Various authentication protocols exist  including some that use a trusted third party  Diffie-Hellman  Kerberos  and public-key cryptography
Email urity can be achieved by a combination of the techniques we have studied in this  ter
PGP  for example  compresses messages  then encrypts them with a ret key and sends the ret key encrypted with the receiverâs public key
In addition  it also hashes the message and sends the signed hash to verify message integrity
Web urity is also an important topic  starting with ure naming
DNS provides a way to prevent DNS spoofing
Most e-commerce Web sites use   SUMMARY SSL/TLS to establish ure  authenticated sessions between the client and server
Various techniques are used to deal with mobile code  especially sandboxing and code signing
The Internet raises many issues in which technology interacts strongly with public policy
Some of the areas include privacy  freedom of speech  and copyright
Break the following monoalphabetic substitution cipher
The plaintext  consisting of letters only  is an excerpt from a poem by Lewis Carroll
mvyy bek mnyx n yvjjyr snijrh invq n muvjvdt je n idnvy jurhri n fehfevir pyeir oruvdq ki ndq uri jhrnqvdt ed zb jnvy Irr uem rntrhyb jur yeoijrhi ndq jur jkhjyri nyy nqlndpr Jurb nhr mnvjvdt ed jur iuvdtyr mvyy bek pezr ndq wevd jur qndpr mvyy bek  medj bek  mvyy bek  medj bek  mvyy bek wevd jur qndpr mvyy bek  medj bek  mvyy bek  medj bek  medj bek wevd jur qndpr
An affine cipher is a version of a monoalphabetic substitution cipher  in which the letters of an alphabet of size m are first map to the integers in the range  to m-
Subsequently  the integer representing each plaintext letter is transformed to an integer representing the corresponding cipher text letter
The encryption function for a single letter is E(x) = (ax + b) mod m  where m is the size of the alphabet and a and b are the key of the cipher  and are co-prime
Trudy finds out that Bob generated a ciphertext using an affine cipher
She gets a copy of the ciphertext  and finds out that the most frequent letter of the ciphertext is âRâ  and the ond most frequent letter of the ciphertext is âKâ
Show how Trudy can break the code and retrieve the plaintext Break the following columnar transposition cipher
The plaintext is taken from a popular computer textbook  so ââcomputerââ is a probable word
The plaintext consists entirely of letters (no spaces)
The ciphertext is broken up into blocks of five characters for readability
aauan cvlre rurnn dltme aeepb ytust iceat npmey iicgo gorch srsoc nntii imiha oofpa gsivt tpsit lbolr otoex
Alice used a transposition cipher to encrypt her messages to Bob
For added urity  she encrypted the transposition cipher key using a substitution cipher  and kept the encrypted cipher in her computer
Trudy managed to get hold of the encrypted transposition cipher key
Can Trudy decipher Aliceâs messages to Bob? Why or why not?
Find a  -bit one-time pad that generates the text ââHello Worldââ from the ciphertext of Fig
Your operator also has such a library at his disposal
You have agreed NETWORK URITY
to use Lord of the Rings as a one-time pad
Explain how you could use these assets to generate an infinitely long one-time pad Quantum cryptography requires having a photon gun that can  on demand  fire a single photon carrying  bit
In this problem  calculate how many photons a bit carries on a   -Gbps fiber link
Assume that the length of a photon is equal to its wavelength  which for purposes of this problem  is  micron
The speed of light in fiber is   cm/n If Trudy captures and regenerates photons when quantum cryptography is in use  she will get some of them wrong and cause errors to appear in Bobâs one-time pad
What fraction of Bobâs one-time pad bits will be in error  on average?
A fundamental cryptographic principle states that all messages must have redundancy
But we also know that redundancy helps an intruder tell if a guessed key is correct
Consider two forms of redundancy
First  the initial n bits of the plaintext contain a known pattern
ond  the final n bits of the message contain a hash over the message
From a urity point of view  are these two equivalent? Discuss your answer In Fig
Although this arrangement is esthetically pleasing  is it any more ure than first having all the P-boxes and then all the S-boxes? Discuss your answer Design an attack on DES based on the knowledge that the plaintext consists exclusively of uppercase ASCII letters  plus space  comma  period  semicolon  carriage return  and line feed
Nothing is known about the plaintext parity bits In the text  we computed that a cipher-breaking machine with a million processors that could analyze a key in  nanoond would take  years to break the   -bit version of AES
Let us compute how long it will take for this time to get down to  year  still along time  of course
To achieve this goal  we need computers to be  times faster
If Mooreâs Law (computing power doubles every   months) continues to hold  how many years will it take before a parallel computer can get the cipherbreaking time down to a year?
AES supports a   -bit key
How many keys does AES-   have? See if you can find some number in physics  chemistry  or astronomy of about the same size
Use the Internet to help search for big numbers
Draw a conclusion from your research Suppose that a message has been encrypted using DES in counter mode
One bit of ciphertext in block Ci is accidentally transformed from a  to a  during transmission
How much plaintext will be garbled as a result?
Now consider ciphertext block chaining again
Instead of a single  bit being transformed into a  bit  an extra  bit is inserted into the ciphertext stream after block Ci
How much plaintext will be garbled as a result?
Compare cipher block chaining with cipher feedback mode in terms of the number of encryption operations needed to transmit a large file
Which one is more efficient and by how much?
Using the RSA public key cryptosystem  with a =   b =
(a) If p =  and q = list five legal values for d  PROBLEMS (b) If p =   q = and d = find e
(c) Using p =   q = and d =   find e and encrypt ââhelloââ Alice and Bob use RSA public key encryption in order to communicate between them
Trudy finds out that Alice and Bob shared one of the primes used to determine the number n of their public key pairs
In other words  Trudy found out that na = pa Ã q and nb = pb Ã q
How can Trudy use this information to break Aliceâs code?
Consider the use of counter mode  as shown in Fig
-   but with IV =
Does the use of  threaten the urity of the cipher in general?
If Trudy replaces P  Bob can detect it
But what happens if Trudy replaces both P and the signature?
Digital signatures have a potential weakness due to lazy users
In e-commerce transactions  a contract might be drawn up and the user asked to sign its SHA-  hash
If the user does not actually verify that the contract and hash correspond  the user may inadvertently sign a different contract
Suppose that the Mafia try to exploit this weakness to make some money
They set up a pay Web site (
pornography  gambling  etc
) and ask new customers for a credit card number
Then they send over a contract saying that the customer wishes to use their service and pay by credit card and ask the customer to sign it  knowing that most of them will just sign without verifying that the contract and hash agree
Show how the Mafia can buy diamonds from a legitimate Internet jeweler and charge them to unsuspecting customers A math class has   students
Assuming that all of the students were born in the first half of the yearâbetween January  st and June  thâ what is the probability that at least two students have the same birthday? Assume that nobody was born on leap day  so there are possible birthdays After Ellen confessed to Marilyn about tricking her in the matter of Tomâs tenure  Marilyn resolved to avoid this problem by dictating the contents of future messages into a dictating machine and having her new retary just type them in
Marilyn then planned to examine the messages on her terminal after they had been typed in to make sure they contained her exact words
Can the new retary still use the birthday attack to falsify a message  and if so  how? Hint: She can Consider the failed attempt of Alice to get Bobâs public key in Fig
Suppose that Bob and Alice already share a ret key  but Alice still wants Bobâs public key
Is there now a way to get it urely? If so  how?
Alice wants to communicate with Bob  using public-key cryptography
She establishes a connection to someone she hopes is Bob
She asks him for his public key and he sends it to her in plaintext along with an X
certificate signed by the root CA
Alice already has the public key of the root CA
What steps does Alice carry out to verify that she is talking to Bob? Assume that Bob does not care who he is talking to (
Bob is some kind of public service) Suppose that a system uses PKI based on a tree-structured hierarchy of CAs
Alice wants to communicate with Bob  and receives a certificate from Bob signed by a CA X after establishing a communication channel with Bob
Suppose Alice has never heard of X
What steps does Alice take to verify that she is talking to Bob? NETWORK URITY   Can IP using AH be used in transport mode if one of the machines is behind a NAT box? Explain your answer Alice wants to send a message to Bob using SHA-  hashes
She consults with you regarding the appropriate signature algorithm to be used
What would you suggest?
Give one reason why a firewall might be configured to inspect incoming traffic
Give one reason why it might be configured to inspect outgoing traffic
Do you think the inspections are likely to be successful?
Suppose an organization uses VPN to urely connect its sites over the Internet
Jim  a user in the organization  uses the VPN to communicate with his boss  Mary
Describe one type of communication between Jim and Mary which would not require use of encryption or other urity mechanism  and another type of communication which would require encryption or other urity mechanisms
Explain your answer Change one message in the protocol of Fig
Explain why your change works The Diffie-Hellman key exchange is being used to establish a ret key between Alice and Bob
Alice sends Bob (  )
Bob responds with (  )
Aliceâs ret number  x  is and Bobâs ret number  y  is
Show how Alice and Bob compute the ret key Two users can establish a shared ret key using the Diffie-Hellman algorithm  even if they have never met  share no rets  and have no certificates (a) Explain how this algorithm is susceptible to a man-in-the-middle attack
(b) How would this susceptibility change if n or g were ret?
In the protocol of Fig
In the Needham-Schroeder protocol  Alice generates two challenges  RA and RA
This seems like overkill
Would one not have done the job?
Suppose an organization uses Kerberos for authentication
In terms of urity and service availability  what is the effect if AS or TGS goes down?
Alice is using the public-key authentication protocol of Fig
However  when sending message   Alice forgot to encrypt RB
Trudy now knows the value of RB
Do Alice and Bob need to repeat the authentication procedure with new parameters in order to ensure ure communication? Explain your answer In the public-key authentication protocol of Fig
Is this encryption necessary  or would it have been adequate to send it back in plaintext? Explain your answer Point-of-sale terminals that use magnetic-stripe cards and PIN codes have a fatal flaw: a malicious merchant can modify his card reader to log all the information on the card and the PIN code in order to post additional (fake) transactions in the future
Next generation terminals will use cards with a complete CPU  keyboard  and tiny display on the card
Devise a protocol for this system that malicious merchants cannot break  PROBLEMS
Is it possible to multicast a PGP message? What restrictions would apply?
Assuming that everyone on the Internet used PGP  could a PGP message be sent to an arbitrary Internet address and be decoded correctly by all concerned? Discuss your answer The attack shown in Fig
The step is not needed for the spoof to work  but including it might reduce potential suspicion after the fact
What is the missing step?
The SSL data transport protocol involves two nonces as well as a premaster key
What value  if any  does using the nonces have?
Consider an image of  Ã pixels
You want to encrypt a file sized
What fraction of the file can you encrypt in this image? What fraction would you be able to encrypt if you compressed the file to a quarter of its original size? Show your calculations The image of Fig
Would it be possible to hide music among the zebras instead of text? If so  how would it work and how much could you hide in this picture? If not  why not?
You are given a text file of size   MB  which is to be encrypted using steganography in the low-order bits of each color in an image file
What size image would be required in order to encrypt the entire file? What size would be needed if the file were first compressed to a third of its original size? Give your answer in pixels  and show your calculations
Assume that the images have an aspect ratio of  :  for example  Ã  pixels Alice was a heavy user of a type  anonymous remailer
She would post many messages to her favorite newsgroup     and everyone would know they all came from Alice because they all bore the same pseudonym
Assuming that the remailer worked correctly  Trudy could not impersonate Alice
After type  remailers were all shut down  Alice switched to a cypherpunk remailer and started a new thread in her newsgroup
Devise a way for her to prevent Trudy from posting new messages to the newsgroup  impersonating Alice Search the Internet for an interesting case involving privacy and write a one-page report on it Search the Internet for some court case involving copyright versus fair use and write a  -page report summarizing your findings Write a program that encrypts its input by XORing it with a keystream
Find or write as good a random number generator as you can to generate the keystream
The program should act as a filter  taking plaintext on standard input and producing ciphertext on standard output (and vice versa)
The program should take one parameter  the key that seeds the random number generator Write a procedure that computes the SHA-  hash of a block of data
The procedure should have two parameters: a pointer to the input buffer and a pointer to a  -byte output buffer
To see the exact specification of SHA-  search the Internet for FIPS   -  which is the full specification
NETWORK URITY   Write a function that accepts a stream of ASCII characters and encrypts this input using a substitution cipher with the Cipher Block Chaining mode
The block size should be  bytes
The program should take plaintext from the standard input and print the ciphertext on the standard output
For this problem  you are allowed to select any reasonable system to determine that the end of the input is reached  and/or when padding should be applied to complete the block
You may select any output format  as long as it is unambiguous
The program should receive two parameters:
A pointer to the initializing vector; and
A number  k  representing the substitution cipher shift  such that each ASCII character would be encrypted by the kth character ahead of it in the alphabet
For example  if x =   then A is encoded by D  B is encoded by E etc
Make reasonable assumptions with respect to reaching the last character in the ASCII set
Make sure to document clearly in your code any assumptions you make about the input and encryption algorithm The purpose of this problem is to give you a better understanding as to the mechanisms of RSA
Write a function that receives as its parameters primes p and q  calculates public and private RSA keys using these parameters  and outputs n  z  d and e as printouts to the standard output
The function should also accept a stream of ASCII characters and encrypt this input using the calculated RSA keys
The program should take plaintext from the standard input and print the ciphertext to the standard output
The encryption should be carried out character-wise  that is  take each character in the input and encrypt it independently of other characters in the input
For this problem  you are allowed to select any reasonable system to determine that the end of the input is reached
You may select any output format  as long as it is unambiguous
Make sure to document clearly in your code any assumptions you make about the input and encryption algorithm
Chapter Computer Networks and the Internet Todays Internet is arguably the largest engineered system ever created by mankind with hundreds of millions of connected computers communication links and switches with billions of users who connect via laptops tablets and smartphones and with an array of new Internetconnected things including game consoles surveillance systems watches eye glasses thermostats body scales and cars
Given that the Internet is so large and has so many diverse components and uses is there any hope of understanding how it works Are there guiding principles and structure that can provide a foundation for understanding such an amazingly large and complex system And if so is it possible that it actually could be both interesting and fun to learn about computer networks Fortunately the answer to all of these questions is a resounding YES Indeed its our aim in this book to provide you with a modern introduction to the dynamic field of computer networking giving you the principles and practical insights youll need to understand not only todays networks but tomorrows as well
This first chapter presents a broad overview of computer networking and the Internet
Our goal here is to paint a broad picture and set the context for the rest of this book to see the forest through the trees
Well cover a lot of ground in this introductory chapter and discuss a lot of the pieces of a computer network without losing sight of the big picture
Well structure our overview of computer networks in this chapter as follows
After introducing some basic terminology and concepts well first examine the basic hardware and software components that make up a network
Well begin at the networks edge and look at the end systems and network applications running in the network
Well then explore the core of a computer network examining the links and the switches that transport data as well as the access networks and physical media that connect end systems to the network core
Well learn that the Internet is a network of networks and well learn how these networks connect with each other
After having completed this overview of the edge and core of a computer network well take the broader and more abstract view in the second half of this chapter
Well examine delay loss and throughput of data in a computer network and provide simple quantitative models for endtoend throughput and delay models that take into account transmission propagation and queuing delays
Well then introduce some of the key architectural principles in computer networking namely protocol layering and service models
Well also learn that computer networks are vulnerable to many different types of attacks well survey some of these attacks and consider how computer networks can be made more secure
Finally well close this chapter with a brief history of computer networking
What Is the Internet In this book well use the public Internet a specific computer network as our principal vehicle for discussing computer networks and their protocols
But what is the Internet There are a couple of ways to answer this question
First we can describe the nuts and bolts of the Internet that is the basic hardware and software components that make up the Internet
Second we can describe the Internet in terms of a networking infrastructure that provides services to distributed applications
Lets begin with the nutsandbolts description using Figure 
to illustrate our discussion
A NutsandBolts Description The Internet is a computer network that interconnects billions of computing devices throughout the world
Not too long ago these computing devices were primarily traditional desktop PCs Linux workstations and socalled servers that store and transmit information such as Web pages and email messages
Increasingly however nontraditional Internet things such as laptops smartphones tablets TVs gaming consoles thermostats home security systems home appliances watches eye glasses cars traffic control systems and more are being connected to the Internet
Indeed the term computer network is beginning to sound a bit dated given the many nontraditional devices that are being hooked up to the Internet
In Internet jargon all of these devices are called hosts or end systems
By some estimates in there were about billion devices connected to the Internet and the number will reach billion by Gartner 
It is estimated that in there were over 
billion Internet users worldwide approximately of the world population ITU 
Some pieces of the Internet End systems are connected together by a network of communication links and packet switches
Well see in Section 
that there are many types of communication links which are made up of different types of physical media including coaxial cable copper wire optical fiber and radio spectrum
Different links can transmit data at different rates with the transmission rate of a link measured in bitssecond
When one end system has data to send to another end system the sending end system segments the data and adds header bytes to each segment
The resulting packages of information known as packets in the jargon of computer networks are then sent through the network to the destination end system where they are reassembled into the original data
A packet switch takes a packet arriving on one of its incoming communication links and forwards that packet on one of its outgoing communication links
Packet switches come in many shapes and flavors but the two most prominent types in todays Internet are routers and linklayer switches
Both types of switches forward packets toward their ultimate destinations
Linklayer switches are typically used in access networks while routers are typically used in the network core
The sequence of communication links and packet switches traversed by a packet from the sending end system to the receiving end system is known as a route or path through the network
Cisco predicts annual global IP traffic will pass the zettabyte bytes threshold by the end of and will reach zettabytes per year by Cisco VNI 
Packetswitched networks which transport packets are in many ways similar to transportation networks of highways roads and intersections which transport vehicles
Consider for example a factory that needs to move a large amount of cargo to some destination warehouse located thousands of kilometers away
At the factory the cargo is segmented and loaded into a fleet of trucks
Each of the trucks then independently travels through the network of highways roads and intersections to the destination warehouse
At the destination warehouse the cargo is unloaded and grouped with the rest of the cargo arriving from the same shipment
Thus in many ways packets are analogous to trucks communication links are analogous to highways and roads packet switches are analogous to intersections and end systems are analogous to buildings
Just as a truck takes a path through the transportation network a packet takes a path through a computer network
End systems access the Internet through Internet Service Providers ISPs including residential ISPs such as local cable or telephone companies corporate ISPs university ISPs ISPs that provide WiFi access in airports hotels coffee shops and other public places and cellular data ISPs providing mobile access to our smartphones and other devices
Each ISP is in itself a network of packet switches and communication links
ISPs provide a variety of types of network access to the end systems including residential broadband access such as cable modem or DSL highspeed local area network access and mobile wireless access
ISPs also provide Internet access to content providers connecting Web sites and video servers directly to the Internet
The Internet is all about connecting end systems to each other so the ISPs that provide access to end systems must also be interconnected
These lowertier ISPs are interconnected through national and international uppertier ISPs such as Level Communications ATT Sprint and NTT
An uppertier ISP consists of highspeed routers interconnected with highspeed fiberoptic links
Each ISP network whether uppertier or lowertier is managed independently runs the IP protocol see below and conforms to certain naming and address conventions
Well examine ISPs and their interconnection more closely in Section 
End systems packet switches and other pieces of the Internet run protocols that control the sending and receiving of information within the Internet
The Transmission Control Protocol TCP and the Internet Protocol IP are two of the most important protocols in the Internet
The IP protocol specifies the format of the packets that are sent and received among routers and end systems
The Internets principal protocols are collectively known as TCPIP
Well begin looking into protocols in this introductory chapter
But thats just a startmuch of this book is concerned with computer network protocols Given the importance of protocols to the Internet its important that everyone agree on what each and every protocol does so that people can create systems and products that interoperate
This is where standards come into play
Internet standards are developed by the Internet Engineering Task Force IETF IETF 
The IETF standards documents are called requests for comments RFCs
RFCs started out as general requests for comments hence the name to resolve network and protocol design problems that faced the precursor to the Internet Allman 
RFCs tend to be quite technical and detailed
They define protocols such as TCP IP HTTP for the Web and SMTP for email
There are currently more than RFCs
Other bodies also specify standards for network components most notably for network links
The IEEE LANMAN Standards Committee IEEE for example specifies the Ethernet and wireless WiFi standards
A Services Description Our discussion above has identified many of the pieces that make up the Internet
But we can also describe the Internet from an entirely different anglenamely as an infrastructure that provides services to applications
In addition to traditional applications such as email and Web surfing Internet applications include mobile smartphone and tablet applications including Internet messaging mapping with realtime roadtraffic information music streaming from the cloud movie and television streaming online social networks video conferencing multiperson games and locationbased recommendation systems
The applications are said to be distributed applications since they involve multiple end systems that exchange data with each other
Importantly Internet applications run on end systems they do not run in the packet switches in the network core
Although packet switches facilitate the exchange of data among end systems they are not concerned with the application that is the source or sink of data
Lets explore a little more what we mean by an infrastructure that provides services to applications
To this end suppose you have an exciting new idea for a distributed Internet application one that may greatly benefit humanity or one that may simply make you rich and famous
How might you go about transforming this idea into an actual Internet application Because applications run on end systems you are going to need to write programs that run on the end systems
You might for example write your programs in Java C or Python
Now because you are developing a distributed Internet application the programs running on the different end systems will need to send data to each other
And here we get to a central issueone that leads to the alternative way of describing the Internet as a platform for applications
How does one program running on one end system instruct the Internet to deliver data to another program running on another end system End systems attached to the Internet provide a socket interface that specifies how a program running on one end system asks the Internet infrastructure to deliver data to a specific destination program running on another end system
This Internet socket interface is a set of rules that the sending program must follow so that the Internet can deliver the data to the destination program
Well discuss the Internet socket interface in detail in Chapter 
For now lets draw upon a simple analogy one that we will frequently use in this book
Suppose Alice wants to send a letter to Bob using the postal service
Alice of course cant just write the letter the data and drop the letter out her window
Instead the postal service requires that Alice put the letter in an envelope write Bobs full name address and zip code in the center of the envelope seal the envelope put a stamp in the upperrighthand corner of the envelope and finally drop the envelope into an official postal service mailbox
Thus the postal service has its own postal service interface or set of rules that Alice must follow to have the postal service deliver her letter to Bob
In a similar manner the Internet has a socket interface that the program sending data must follow to have the Internet deliver the data to the program that will receive the data
The postal service of course provides more than one service to its customers
It provides express delivery reception confirmation ordinary use and many more services
In a similar manner the Internet provides multiple services to its applications
When you develop an Internet application you too must choose one of the Internets services for your application
Well describe the Internets services in Chapter 
We have just given two descriptions of the Internet one in terms of its hardware and software components the other in terms of an infrastructure for providing services to distributed applications
But perhaps you are still confused as to what the Internet is
What are packet switching and TCPIP What are routers What kinds of communication links are present in the Internet What is a distributed application How can a thermostat or body scale be attached to the Internet If you feel a bit overwhelmed by all of this now dont worrythe purpose of this book is to introduce you to both the nuts and bolts of the Internet and the principles that govern how and why it works
Well explain these important terms and questions in the following sections and chapters
What Is a Protocol Now that weve got a bit of a feel for what the Internet is lets consider another important buzzword in computer networking protocol
What is a protocol What does a protocol do A Human Analogy It is probably easiest to understand the notion of a computer network protocol by first considering some human analogies since we humans execute protocols all of the time
Consider what you do when you want to ask someone for the time of day
A typical exchange is shown in Figure 
Human protocol or good manners at least dictates that one first offer a greeting the first Hi in Figure 
to initiate communication with someone else
The typical response to a Hi is a returned Hi message
Implicitly one then takes a cordial Hi response as an indication that one can proceed and ask for the time of day
A different response to the initial Hi such as Dont bother me or I dont speak English or some unprintable reply might Figure 
A human protocol and a computer network protocol indicate an unwillingness or inability to communicate
In this case the human protocol would be not to ask for the time of day
Sometimes one gets no response at all to a question in which case one typically gives up asking that person for the time
Note that in our human protocol there are specific messages we send and specific actions we take in response to the received reply messages or other events such as no reply within some given amount of time
Clearly transmitted and received messages and actions taken when these messages are sent or received or other events occur play a central role in a human protocol
If people run different protocols for example if one person has manners but the other does not or if one understands the concept of time and the other does not the protocols do not interoperate and no useful work can be accomplished
The same is true in networkingit takes two or more communicating entities running the same protocol in order to accomplish a task
Lets consider a second human analogy
Suppose youre in a college class a computer networking class for example
The teacher is droning on about protocols and youre confused
The teacher stops to ask Are there any questions a message that is transmitted to and received by all students who are not sleeping
You raise your hand transmitting an implicit message to the teacher
Your teacher acknowledges you with a smile saying Yes 
a transmitted message encouraging you to ask your questionteachers love to be asked questions and you then ask your question that is transmit your message to your teacher
Your teacher hears your question receives your question message and answers transmits a reply to you
Once again we see that the transmission and receipt of messages and a set of conventional actions taken when these messages are sent and received are at the heart of this questionandanswer protocol
Network Protocols A network protocol is similar to a human protocol except that the entities exchanging messages and taking actions are hardware or software components of some device for example computer smartphone tablet router or other networkcapable device
All activity in the Internet that involves two or more communicating remote entities is governed by a protocol
For example hardwareimplemented protocols in two physically connected computers control the flow of bits on the wire between the two network interface cards congestioncontrol protocols in end systems control the rate at which packets are transmitted between sender and receiver protocols in routers determine a packets path from source to destination
Protocols are running everywhere in the Internet and consequently much of this book is about computer network protocols
As an example of a computer network protocol with which you are probably familiar consider what happens when you make a request to a Web server that is when you type the URL of a Web page into your Web browser
The scenario is illustrated in the right half of Figure 
First your computer will send a connection request message to the Web server and wait for a reply
The Web server will eventually receive your connection request message and return a connection reply message
Knowing that it is now OK to request the Web document your computer then sends the name of the Web page it wants to fetch from that Web server in a GET message
Finally the Web server returns the Web page file to your computer
Given the human and networking examples above the exchange of messages and the actions taken when these messages are sent and received are the key defining elements of a protocol A protocol defines the format and the order of messages exchanged between two or more communicating entities as well as the actions taken on the transmission andor receipt of a message or other event
The Internet and computer networks in general make extensive use of protocols
Different protocols are used to accomplish different communication tasks
As you read through this book you will learn that some protocols are simple and straightforward while others are complex and intellectually deep
Mastering the field of computer networking is equivalent to understanding the what why and how of networking protocols
The Network Edge In the previous section we presented a highlevel overview of the Internet and networking protocols
We are now going to delve a bit more deeply into the components of a computer network and the Internet in particular
We begin in this section at the edge of a network and look at the components with which we are most familiarnamely the computers smartphones and other devices that we use on a daily basis
In the next section well move from the network edge to the network core and examine switching and routing in computer networks
Recall from the previous section that in computer networking jargon the computers and other devices connected to the Internet are often referred to as end systems
They are referred to as end systems because they sit at the edge of the Internet as shown in Figure 
The Internets end systems include desktop computers e.g
desktop PCs Macs and Linux boxes servers e.g
Web and email servers and mobile devices e.g
laptops smartphones and tablets
Furthermore an increasing number of nontraditional things are being attached to the Internet as end systems see the Case History feature
End systems are also referred to as hosts because they host that is run application programs such as a Web browser program a Web server program an email client program or an email server program
Throughout this book we will use the Figure 
Endsystem interaction CASE HISTORY THE INTERNET OF THINGS Can you imagine a world in which just about everything is wirelessly connected to the Internet A world in which most people cars bicycles eye glasses watches toys hospital equipment home sensors classrooms video surveillance systems atmospheric sensors storeshelf products and pets are connected This world of the Internet of Things IoT may actually be just around the corner
By some estimates as of there are already billion things connected to the Internet and the number could reach billion by Gartner 
These things include our smartphones which already follow us around in our homes offices and cars reporting our geo locations and usage data to our ISPs and Internet applications
But in addition to our smartphones a widevariety of nontraditional things are already available as products
For example there are Internetconnected wearables including watches from Apple and many others and eye glasses
Internetconnected glasses can for example upload everything we see to the cloud allowing us to share our visual experiences with people around the world in real time
There are Internetconnected things already available for the smart home including Internetconnected thermostats that can be controlled remotely from our smartphones and Internetconnected body scales enabling us to graphically review the progress of our diets from our smartphones
There are Internetconnected toys including dolls that recognize and interpret a childs speech and respond appropriately
The IoT offers potentially revolutionary benefits to users
But at the same time there are also huge security and privacy risks
For example attackers via the Internet might be able to hack into IoT devices or into the servers collecting data from IoT devices
For example an attacker could hijack an Internetconnected doll and talk directly with a child or an attacker could hack into a database that stores personal health and activity information collected from wearable devices
These security and privacy concerns could undermine the consumer confidence necessary for the technologies to meet their full potential and may result in less widespread adoption FTC 
terms hosts and end systems interchangeably that is host end system
Hosts are sometimes further divided into two categories clients and servers
Informally clients tend to be desktop and mobile PCs smartphones and so on whereas servers tend to be more powerful machines that store and distribute Web pages stream video relay email and so on
Today most of the servers from which we receive search results email Web pages and videos reside in large data centers
For example Google has data centers including about large centers each with more than servers
Access Networks Having considered the applications and end systems at the edge of the network lets next consider the access networkthe network that physically connects an end system to the first router also known as the edge router on a path from the end system to any other distant end system
shows several types of access Figure 
Access networks networks with thick shaded lines and the settings home enterprise and widearea mobile wireless in which they are used
Home Access DSL Cable FTTH DialUp and Satellite In developed countries as of more than percent of the households have Internet access with Korea Netherlands Finland and Sweden leading the way with more than percent of households having Internet access almost all via a highspeed broadband connection ITU 
Given this widespread use of home access networks lets begin our overview of access networks by considering how homes connect to the Internet
Today the two most prevalent types of broadband residential access are digital subscriber line DSL and cable
A residence typically obtains DSL Internet access from the same local telephone company telco that provides its wired local phone access
Thus when DSL is used a customers telco is also its ISP
As shown in Figure 
each customers DSL modem uses the existing telephone line twisted pair copper wire which well discuss in Section 
to exchange data with a digital subscriber line access multiplexer DSLAM located in the telcos local central office CO
The homes DSL modem takes digital data and translates it to highfrequency tones for transmission over telephone wires to the CO the analog signals from many such houses are translated back into digital format at the DSLAM
The residential telephone line carries both data and traditional telephone signals simultaneously which are encoded at different frequencies A highspeed downstream channel in the kHz to MHz band A mediumspeed upstream channel in the kHz to kHz band An ordinary twoway telephone channel in the to kHz band This approach makes the single DSL link appear as if there were three separate links so that a telephone call and an Internet connection can share the DSL link at the same time
DSL Internet access Well describe this technique of frequencydivision multiplexing in Section 
On the customer side a splitter separates the data and telephone signals arriving to the home and forwards the data signal to the DSL modem
On the telco side in the CO the DSLAM separates the data and phone signals and sends the data into the Internet
Hundreds or even thousands of households connect to a single DSLAM Dischinger 
The DSL standards define multiple transmission rates including Mbps downstream and 
Mbps upstream ITU and Mbps downstream and Mbps upstream ITU 
Because the downstream and upstream rates are different the access is said to be asymmetric
The actual downstream and upstream transmission rates achieved may be less than the rates noted above as the DSL provider may purposefully limit a residential rate when tiered service different rates available at different prices are offered
The maximum rate is also limited by the distance between the home and the CO the gauge of the twistedpair line and the degree of electrical interference
Engineers have expressly designed DSL for short distances between the home and the CO generally if the residence is not located within to miles of the CO the residence must resort to an alternative form of Internet access
While DSL makes use of the telcos existing local telephone infrastructure cable Internet access makes use of the cable television companys existing cable television infrastructure
A residence obtains cable Internet access from the same company that provides its cable television
As illustrated in Figure 
fiber optics connect the cable head end to neighborhoodlevel junctions from which traditional coaxial cable is then used to reach individual houses and apartments
Each neighborhood junction typically supports to homes
Because both fiber and coaxial cable are employed in this system it is often referred to as hybrid fiber coax HFC
A hybrid fibercoaxial access network Cable internet access requires special modems called cable modems
As with a DSL modem the cable modem is typically an external device and connects to the home PC through an Ethernet port
We will discuss Ethernet in great detail in Chapter 
At the cable head end the cable modem termination system CMTS serves a similar function as the DSL networks DSLAMturning the analog signal sent from the cable modems in many downstream homes back into digital format
Cable modems divide the HFC network into two channels a downstream and an upstream channel
As with DSL access is typically asymmetric with the downstream channel typically allocated a higher transmission rate than the upstream channel
The DOCSIS 
standard defines downstream rates up to 
Mbps and upstream rates of up to 
As in the case of DSL networks the maximum achievable rate may not be realized due to lower contracted data rates or media impairments
One important characteristic of cable Internet access is that it is a shared broadcast medium
In particular every packet sent by the head end travels downstream on every link to every home and every packet sent by a home travels on the upstream channel to the head end
For this reason if several users are simultaneously downloading a video file on the downstream channel the actual rate at which each user receives its video file will be significantly lower than the aggregate cable downstream rate
On the other hand if there are only a few active users and they are all Web surfing then each of the users may actually receive Web pages at the full cable downstream rate because the users will rarely request a Web page at exactly the same time
Because the upstream channel is also shared a distributed multiple access protocol is needed to coordinate transmissions and avoid collisions
Well discuss this collision issue in some detail in Chapter 
Although DSL and cable networks currently represent more than percent of residential broadband access in the United States an upandcoming technology that provides even higher speeds is fiber to the home FTTH FTTH Council 
As the name suggests the FTTH concept is simpleprovide an optical fiber path from the CO directly to the home
Many countries todayincluding the UAE South Korea Hong Kong Japan Singapore Taiwan Lithuania and Swedennow have household penetration rates exceeding FTTH Council 
There are several competing technologies for optical distribution from the CO to the homes
The simplest optical distribution network is called direct fiber with one fiber leaving the CO for each home
More commonly each fiber leaving the central office is actually shared by many homes it is not until the fiber gets relatively close to the homes that it is split into individual customerspecific fibers
There are two competing opticaldistribution network architectures that perform this splitting active optical networks AONs and passive optical networks PONs
AON is essentially switched Ethernet which is discussed in Chapter 
Here we briefly discuss PON which is used in Verizons FIOS service
shows FTTH using the PON distribution architecture
Each home has an optical network terminator ONT which is connected by dedicated optical fiber to a neighborhood splitter
The splitter combines a number of homes typically less Figure 
FTTH Internet access than onto a single shared optical fiber which connects to an optical line terminator OLT in the telcos CO
The OLT providing conversion between optical and electrical signals connects to the Internet via a telco router
In the home users connect a home router typically a wireless router to the ONT and access the Internet via this home router
In the PON architecture all packets sent from OLT to the splitter are replicated at the splitter similar to a cable head end
FTTH can potentially provide Internet access rates in the gigabits per second range
However most FTTH ISPs provide different rate offerings with the higher rates naturally costing more money
The average downstream speed of US FTTH customers was approximately Mbps in compared with Mbps for cable access networks and less than Mbps for DSL FTTH Council b
Two other access network technologies are also used to provide Internet access to the home
In locations where DSL cable and FTTH are not available e.g
in some rural settings a satellite link can be used to connect a residence to the Internet at speeds of more than Mbps StarBand and HughesNet are two such satellite access providers
Dialup access over traditional phone lines is based on the same model as DSLa home modem connects over a phone line to a modem in the ISP
Compared with DSL and other broadband access networks dialup access is excruciatingly slow at kbps
Access in the Enterprise and the Home Ethernet and WiFi On corporate and university campuses and increasingly in home settings a local area network LAN is used to connect an end system to the edge router
Although there are many types of LAN technologies Ethernet is by far the most prevalent access technology in corporate university and home networks
As shown in Figure 
Ethernet users use twistedpair copper wire to connect to an Ethernet switch a technology discussed in detail in Chapter 
The Ethernet switch or a network of such Figure 
Ethernet Internet access interconnected switches is then in turn connected into the larger Internet
With Ethernet access users typically have Mbps or Gbps access to the Ethernet switch whereas servers may have Gbps or even Gbps access
Increasingly however people are accessing the Internet wirelessly from laptops smartphones tablets and other things see earlier sidebar on Internet of Things
In a wireless LAN setting wireless users transmitreceive packets tofrom an access point that is connected into the enterprises network most likely using wired Ethernet which in turn is connected to the wired Internet
A wireless LAN user must typically be within a few tens of meters of the access point
Wireless LAN access based on IEEE 
technology more colloquially known as WiFi is now just about everywhereuniversities business offices cafes airports homes and even in airplanes
In many cities one can stand on a street corner and be within range of ten or twenty base stations for a browseable global map of 
base stations that have been discovered and logged on a Web site by people who take great enjoyment in doing such things see wigle.net 
As discussed in detail in Chapter 
today provides a shared transmission rate of up to more than Mbps
Even though Ethernet and WiFi access networks were initially deployed in enterprise corporate university settings they have recently become relatively common components of home networks
Many homes combine broadband residential access that is cable modems or DSL with these inexpensive wireless LAN technologies to create powerful home networks Edwards 
shows a typical home network
This home network consists of a roaming laptop as well as a wired PC a base station the wireless access point which communicates with the wireless PC and other wireless devices in the home a cable modem providing broadband access to the Internet and a router which interconnects the base station and the stationary PC with the cable modem
This network allows household members to have broadband access to the Internet with one member roaming from the kitchen to the backyard to the bedrooms
A typical home network WideArea Wireless Access G and LTE Increasingly devices such as iPhones and Android devices are being used to message share photos in social networks watch movies and stream music while on the run
These devices employ the same wireless infrastructure used for cellular telephony to sendreceive packets through a base station that is operated by the cellular network provider
Unlike WiFi a user need only be within a few tens of kilometers as opposed to a few tens of meters of the base station
Telecommunications companies have made enormous investments in socalled thirdgeneration G wireless which provides packetswitched widearea wireless Internet access at speeds in excess of Mbps
But even higherspeed widearea access technologiesa fourthgeneration G of widearea wireless networksare already being deployed
LTE for LongTerm Evolutiona candidate for Bad Acronym of the Year Award has its roots in G technology and can achieve rates in excess of Mbps
LTE downstream rates of many tens of Mbps have been reported in commercial deployments
Well cover the basic principles of wireless networks and mobility as well as WiFi G and LTE technologies and more in Chapter 
Physical Media In the previous subsection we gave an overview of some of the most important network access technologies in the Internet
As we described these technologies we also indicated the physical media used
For example we said that HFC uses a combination of fiber cable and coaxial cable
We said that DSL and Ethernet use copper wire
And we said that mobile access networks use the radio spectrum
In this subsection we provide a brief overview of these and other transmission media that are commonly used in the Internet
In order to define what is meant by a physical medium let us reflect on the brief life of a bit
Consider a bit traveling from one end system through a series of links and routers to another end system
This poor bit gets kicked around and transmitted many many times The source end system first transmits the bit and shortly thereafter the first router in the series receives the bit the first router then transmits the bit and shortly thereafter the second router receives the bit and so on
Thus our bit when traveling from source to destination passes through a series of transmitterreceiver pairs
For each transmitter receiver pair the bit is sent by propagating electromagnetic waves or optical pulses across a physical medium
The physical medium can take many shapes and forms and does not have to be of the same type for each transmitterreceiver pair along the path
Examples of physical media include twistedpair copper wire coaxial cable multimode fiberoptic cable terrestrial radio spectrum and satellite radio spectrum
Physical media fall into two categories guided media and unguided media
With guided media the waves are guided along a solid medium such as a fiberoptic cable a twistedpair copper wire or a coaxial cable
With unguided media the waves propagate in the atmosphere and in outer space such as in a wireless LAN or a digital satellite channel
But before we get into the characteristics of the various media types let us say a few words about their costs
The actual cost of the physical link copper wire fiberoptic cable and so on is often relatively minor compared with other networking costs
In particular the labor cost associated with the installation of the physical link can be orders of magnitude higher than the cost of the material
For this reason many builders install twisted pair optical fiber and coaxial cable in every room in a building
Even if only one medium is initially used there is a good chance that another medium could be used in the near future and so money is saved by not having to lay additional wires in the future
TwistedPair Copper Wire The least expensive and most commonly used guided transmission medium is twistedpair copper wire
For over a hundred years it has been used by telephone networks
In fact more than percent of the wired connections from the telephone handset to the local telephone switch use twistedpair copper wire
Most of us have seen twisted pair in our homes or those of our parents or grandparents and work environments
Twisted pair consists of two insulated copper wires each about mm thick arranged in a regular spiral pattern
The wires are twisted together to reduce the electrical interference from similar pairs close by
Typically a number of pairs are bundled together in a cable by wrapping the pairs in a protective shield
A wire pair constitutes a single communication link
Unshielded twisted pair UTP is commonly used for computer networks within a building that is for LANs
Data rates for LANs using twisted pair today range from Mbps to Gbps
The data rates that can be achieved depend on the thickness of the wire and the distance between transmitter and receiver
When fiberoptic technology emerged in the s many people disparaged twisted pair because of its relatively low bit rates
Some people even felt that fiberoptic technology would completely replace twisted pair
But twisted pair did not give up so easily
Modern twistedpair technology such as category a cable can achieve data rates of Gbps for distances up to a hundred meters
In the end twisted pair has emerged as the dominant solution for highspeed LAN networking
As discussed earlier twisted pair is also commonly used for residential Internet access
We saw that dialup modem technology enables access at rates of up to kbps over twisted pair
We also saw that DSL digital subscriber line technology has enabled residential users to access the Internet at tens of Mbps over twisted pair when users live close to the ISPs central office
Coaxial Cable Like twisted pair coaxial cable consists of two copper conductors but the two conductors are concentric rather than parallel
With this construction and special insulation and shielding coaxial cable can achieve high data transmission rates
Coaxial cable is quite common in cable television systems
As we saw earlier cable television systems have recently been coupled with cable modems to provide residential users with Internet access at rates of tens of Mbps
In cable television and cable Internet access the transmitter shifts the digital signal to a specific frequency band and the resulting analog signal is sent from the transmitter to one or more receivers
Coaxial cable can be used as a guided shared medium
Specifically a number of end systems can be connected directly to the cable with each of the end systems receiving whatever is sent by the other end systems
Fiber Optics An optical fiber is a thin flexible medium that conducts pulses of light with each pulse representing a bit
A single optical fiber can support tremendous bit rates up to tens or even hundreds of gigabits per second
They are immune to electromagnetic interference have very low signal attenuation up to kilometers and are very hard to tap
These characteristics have made fiber optics the preferred long haul guided transmission media particularly for overseas links
Many of the longdistance telephone networks in the United States and elsewhere now use fiber optics exclusively
Fiber optics is also prevalent in the backbone of the Internet
However the high cost of optical devicessuch as transmitters receivers and switcheshas hindered their deployment for shorthaul transport such as in a LAN or into the home in a residential access network
The Optical Carrier OC standard link speeds range from 
Mbps to 
Gbps these specifications are often referred to as OCn where the link speed equals n 
Standards in use today include OC OC OC OC OC OC OC OC
Mukherjee Ramaswami provide coverage of various aspects of optical networking
Terrestrial Radio Channels Radio channels carry signals in the electromagnetic spectrum
They are an attractive medium because they require no physical wire to be installed can penetrate walls provide connectivity to a mobile user and can potentially carry a signal for long distances
The characteristics of a radio channel depend significantly on the propagation environment and the distance over which a signal is to be carried
Environmental considerations determine path loss and shadow fading which decrease the signal strength as the signal travels over a distance and aroundthrough obstructing objects multipath fading due to signal reflection off of interfering objects and interference due to other transmissions and electromagnetic signals
Terrestrial radio channels can be broadly classified into three groups those that operate over very short distance e.g
with one or two meters those that operate in local areas typically spanning from ten to a few hundred meters and those that operate in the wide area spanning tens of kilometers
Personal devices such as wireless headsets keyboards and medical devices operate over short distances the wireless LAN technologies described in Section 
use localarea radio channels the cellular access technologies use widearea radio channels
Well discuss radio channels in detail in Chapter 
Satellite Radio Channels A communication satellite links two or more Earthbased microwave transmitter receivers known as ground stations
The satellite receives transmissions on one frequency band regenerates the signal using a repeater discussed below and transmits the signal on another frequency
Two types of satellites are used in communications geostationary satellites and lowearth orbiting LEO satellites Wiki Satellite 
Geostationary satellites permanently remain above the same spot on Earth
This stationary presence is achieved by placing the satellite in orbit at kilometers above Earths surface
This huge distance from ground station through satellite back to ground station introduces a substantial signal propagation delay of milliseconds
Nevertheless satellite links which can operate at speeds of hundreds of Mbps are often used in areas without access to DSL or cablebased Internet access
LEO satellites are placed much closer to Earth and do not remain permanently above one spot on Earth
They rotate around Earth just as the Moon does and may communicate with each other as well as with ground stations
To provide continuous coverage to an area many satellites need to be placed in orbit
There are currently many lowaltitude communication systems in development
LEO satellite technology may be used for Internet access sometime in the future
The Network Core Having examined the Internets edge let us now delve more deeply inside the network corethe mesh of packet switches and links that interconnects the Internets end systems
highlights the network core with thick shaded lines
The network core 
Packet Switching In a network application end systems exchange messages with each other
Messages can contain anything the application designer wants
Messages may perform a control function for example the Hi messages in our handshaking example in Figure 
or can contain data such as an email message a JPEG image or an MP audio file
To send a message from a source end system to a destination end system the source breaks long messages into smaller chunks of data known as packets
Between source and destination each packet travels through communication links and packet switches for which there are two predominant types routers and linklayer switches
Packets are transmitted over each communication link at a rate equal to the full transmission rate of the link
So if a source end system or a packet switch is sending a packet of L bits over a link with transmission rate R bitssec then the time to transmit the packet is L R seconds
StoreandForward Transmission Most packet switches use storeandforward transmission at the inputs to the links
Storeandforward transmission means that the packet switch must receive the entire packet before it can begin to transmit the first bit of the packet onto the outbound link
To explore storeandforward transmission in more detail consider a simple network consisting of two end systems connected by a single router as shown in Figure 
A router will typically have many incident links since its job is to switch an incoming packet onto an outgoing link in this simple example the router has the rather simple task of transferring a packet from one input link to the only other attached link
In this example the source has three packets each consisting of L bits to send to the destination
At the snapshot of time shown in Figure 
the source has transmitted some of packet and the front of packet has already arrived at the router
Because the router employs storeandforwarding at this instant of time the router cannot transmit the bits it has received instead it must first buffer i.e
store the packets bits
Only after the router has received all of the packets bits can it begin to transmit i.e
forward the packet onto the outbound link
To gain some insight into storeandforward transmission lets now calculate the amount of time that elapses from when the source begins to send the packet until the destination has received the entire packet
Here we will ignore propagation delaythe time it takes for the bits to travel across the wire at near the speed of lightwhich will be discussed in Section 
The source begins to transmit at time at time LR seconds the source has transmitted the entire packet and the entire packet has been received and stored at the router since there is no propagation delay
At time LR seconds since the router has just received the entire packet it can begin to transmit the packet onto the outbound link towards the destination at time LR the router has transmitted the entire packet and the entire packet has been received by the destination
Thus the total delay is LR
If the Figure 
Storeandforward packet switching switch instead forwarded bits as soon as they arrive without first receiving the entire packet then the total delay would be LR since bits are not held up at the router
But as we will discuss in Section 
routers need to receive store and process the entire packet before forwarding
Now lets calculate the amount of time that elapses from when the source begins to send the first packet until the destination has received all three packets
As before at time LR the router begins to forward the first packet
But also at time LR the source will begin to send the second packet since it has just finished sending the entire first packet
Thus at time LR the destination has received the first packet and the router has received the second packet
Similarly at time LR the destination has received the first two packets and the router has received the third packet
Finally at time LR the destination has received all three packets Lets now consider the general case of sending one packet from source to destination over a path consisting of N links each of rate R thus there are N routers between source and destination
Applying the same logic as above we see that the endtoend delay is dendtoendNLR 
You may now want to try to determine what the delay would be for P packets sent over a series of N links
Queuing Delays and Packet Loss Each packet switch has multiple links attached to it
For each attached link the packet switch has an output buffer also called an output queue which stores packets that the router is about to send into that link
The output buffers play a key role in packet switching
If an arriving packet needs to be transmitted onto a link but finds the link busy with the transmission of another packet the arriving packet must wait in the output buffer
Thus in addition to the storeandforward delays packets suffer output buffer queuing delays
These delays are variable and depend on the level of congestion in the network
Since the amount of buffer space is finite an Figure 
Packet switching arriving packet may find that the buffer is completely full with other packets waiting for transmission
In this case packet loss will occureither the arriving packet or one of the alreadyqueued packets will be dropped
illustrates a simple packetswitched network
As in Figure 
packets are represented by threedimensional slabs
The width of a slab represents the number of bits in the packet
In this figure all packets have the same width and hence the same length
Suppose Hosts A and B are sending packets to Host E
Hosts A and B first send their packets along Mbps Ethernet links to the first router
The router then directs these packets to the Mbps link
If during a short interval of time the arrival rate of packets to the router when converted to bits per second exceeds Mbps congestion will occur at the router as packets queue in the links output buffer before being transmitted onto the link
For example if Host A and B each send a burst of five packets backtoback at the same time then most of these packets will spend some time waiting in the queue
The situation is in fact entirely analogous to many commonday situationsfor example when we wait in line for a bank teller or wait in front of a tollbooth
Well examine this queuing delay in more detail in Section 
Forwarding Tables and Routing Protocols Earlier we said that a router takes a packet arriving on one of its attached communication links and forwards that packet onto another one of its attached communication links
But how does the router determine which link it should forward the packet onto Packet forwarding is actually done in different ways in different types of computer networks
Here we briefly describe how it is done in the Internet
In the Internet every end system has an address called an IP address
When a source end system wants to send a packet to a destination end system the source includes the destinations IP address in the packets header
As with postal addresses this address has a hierarchical structure
When a packet arrives at a router in the network the router examines a portion of the packets destination address and forwards the packet to an adjacent router
More specifically each router has a forwarding table that maps destination addresses or portions of the destination addresses to that routers outbound links
When a packet arrives at a router the router examines the address and searches its forwarding table using this destination address to find the appropriate outbound link
The router then directs the packet to this outbound link
The endtoend routing process is analogous to a car driver who does not use maps but instead prefers to ask for directions
For example suppose Joe is driving from Philadelphia to Lakeside Drive in Orlando Florida
Joe first drives to his neighborhood gas station and asks how to get to Lakeside Drive in Orlando Florida
The gas station attendant extracts the Florida portion of the address and tells Joe that he needs to get onto the interstate highway I South which has an entrance just next to the gas station
He also tells Joe that once he enters Florida he should ask someone else there
Joe then takes I South until he gets to Jacksonville Florida at which point he asks another gas station attendant for directions
The attendant extracts the Orlando portion of the address and tells Joe that he should continue on I to Daytona Beach and then ask someone else
In Daytona Beach another gas station attendant also extracts the Orlando portion of the address and tells Joe that he should take I directly to Orlando
Joe takes I and gets off at the Orlando exit
Joe goes to another gas station attendant and this time the attendant extracts the Lakeside Drive portion of the address and tells Joe the road he must follow to get to Lakeside Drive
Once Joe reaches Lakeside Drive he asks a kid on a bicycle how to get to his destination
The kid extracts the portion of the address and points to the house
Joe finally reaches his ultimate destination
In the above analogy the gas station attendants and kids on bicycles are analogous to routers
We just learned that a router uses a packets destination address to index a forwarding table and determine the appropriate outbound link
But this statement begs yet another question How do forwarding tables get set Are they configured by hand in each and every router or does the Internet use a more automated procedure This issue will be studied in depth in Chapter 
But to whet your appetite here well note now that the Internet has a number of special routing protocols that are used to automatically set the forwarding tables
A routing protocol may for example determine the shortest path from each router to each destination and use the shortest path results to configure the forwarding tables in the routers
How would you actually like to see the endtoend route that packets take in the Internet We now invite you to get your hands dirty by interacting with the Traceroute program
Simply visit the site www.traceroute.org choose a source in a particular country and trace the route from that source to your computer
For a discussion of Traceroute see Section 
Circuit Switching There are two fundamental approaches to moving data through a network of links and switches circuit switching and packet switching
Having covered packetswitched networks in the previous subsection we now turn our attention to circuitswitched networks
In circuitswitched networks the resources needed along a path buffers link transmission rate to provide for communication between the end systems are reserved for the duration of the communication session between the end systems
In packetswitched networks these resources are not reserved a sessions messages use the resources on demand and as a consequence may have to wait that is queue for access to a communication link
As a simple analogy consider two restaurants one that requires reservations and another that neither requires reservations nor accepts them
For the restaurant that requires reservations we have to go through the hassle of calling before we leave home
But when we arrive at the restaurant we can in principle immediately be seated and order our meal
For the restaurant that does not require reservations we dont need to bother to reserve a table
But when we arrive at the restaurant we may have to wait for a table before we can be seated
Traditional telephone networks are examples of circuitswitched networks
Consider what happens when one person wants to send information voice or facsimile to another over a telephone network
Before the sender can send the information the network must establish a connection between the sender and the receiver
This is a bona fide connection for which the switches on the path between the sender and receiver maintain connection state for that connection
In the jargon of telephony this connection is called a circuit
When the network establishes the circuit it also reserves a constant transmission rate in the networks links representing a fraction of each links transmission capacity for the duration of the connection
Since a given transmission rate has been reserved for this senderto receiver connection the sender can transfer the data to the receiver at the guaranteed constant rate
illustrates a circuitswitched network
In this network the four circuit switches are interconnected by four links
Each of these links has four circuits so that each link can support four simultaneous connections
The hosts for example PCs and workstations are each directly connected to one of the switches
When two hosts want to communicate the network establishes a dedicated end toend connection between the two hosts
Thus in order for Host A to communicate with Host B the network must first reserve one circuit on each of two links
In this example the dedicated endtoend connection uses the second circuit in the first link and the fourth circuit in the second link
Because each link has four circuits for each link used by the endtoend connection the connection gets one fourth of the links total transmission capacity for the duration of the connection
Thus for example if each link between adjacent switches has a transmission rate of Mbps then each endtoend circuitswitch connection gets kbps of dedicated transmission rate
A simple circuitswitched network consisting of four switches and four links In contrast consider what happens when one host wants to send a packet to another host over a packetswitched network such as the Internet
As with circuit switching the packet is transmitted over a series of communication links
But different from circuit switching the packet is sent into the network without reserving any link resources whatsoever
If one of the links is congested because other packets need to be transmitted over the link at the same time then the packet will have to wait in a buffer at the sending side of the transmission link and suffer a delay
The Internet makes its best effort to deliver packets in a timely manner but it does not make any guarantees
Multiplexing in CircuitSwitched Networks A circuit in a link is implemented with either frequencydivision multiplexing FDM or timedivision multiplexing TDM
With FDM the frequency spectrum of a link is divided up among the connections established across the link
Specifically the link dedicates a frequency band to each connection for the duration of the connection
In telephone networks this frequency band typically has a width of kHz that is hertz or cycles per second
The width of the band is called not surprisingly the bandwidth
FM radio stations also use FDM to share the frequency spectrum between MHz and MHz with each station being allocated a specific frequency band
For a TDM link time is divided into frames of fixed duration and each frame is divided into a fixed number of time slots
When the network establishes a connection across a link the network dedicates one time slot in every frame to this connection
These slots are dedicated for the sole use of that connection with one time slot available for use in every frame to transmit the connections data
With FDM each circuit continuously gets a fraction of the bandwidth
With TDM each circuit gets all of the bandwidth periodically during brief intervals of time that is during slots Figure 
illustrates FDM and TDM for a specific network link supporting up to four circuits
For FDM the frequency domain is segmented into four bands each of bandwidth kHz
For TDM the time domain is segmented into frames with four time slots in each frame each circuit is assigned the same dedicated slot in the revolving TDM frames
For TDM the transmission rate of a circuit is equal to the frame rate multiplied by the number of bits in a slot
For example if the link transmits frames per second and each slot consists of bits then the transmission rate of each circuit is kbps
Proponents of packet switching have always argued that circuit switching is wasteful because the dedicated circuits are idle during silent periods
For example when one person in a telephone call stops talking the idle network resources frequency bands or time slots in the links along the connections route cannot be used by other ongoing connections
As another example of how these resources can be underutilized consider a radiologist who uses a circuitswitched network to remotely access a series of xrays
The radiologist sets up a connection requests an image contemplates the image and then requests a new image
Network resources are allocated to the connection but are not used i.e
are wasted during the radiologists contemplation periods
Proponents of packet switching also enjoy pointing out that establishing endtoend circuits and reserving endtoend transmission capacity is complicated and requires complex signaling software to coordinate the operation of the switches along the endtoend path
Before we finish our discussion of circuit switching lets work through a numerical example that should shed further insight on the topic
Let us consider how long it takes to send a file of bits from Host A to Host B over a circuitswitched network
Suppose that all links in the network use TDM with slots and have a bit rate of 
Also suppose that it takes msec to establish an endtoend circuit before Host A can begin to transmit the file
How long does it take to send the file Each circuit has a transmission rate of 
Mbps kbps so it takes bits kbps seconds to transmit the file
To this seconds we add the circuit establishment time giving 
seconds to send the file
Note that the transmission time is independent of the number of links The transmission time would be seconds if the endtoend circuit passed through one link or a hundred links
The actual endtoend delay also includes a propagation delay see Section 
Packet Switching Versus Circuit Switching Having described circuit switching and packet switching let us compare the two
Critics of packet switching have often argued that packet switching is not suitable for realtime services for example telephone calls and video conference calls because of its variable and unpredictable endtoend delays due primarily to variable and unpredictable queuing delays
Proponents of packet switching argue that it offers better sharing of transmission capacity than circuit switching and it is simpler more efficient and less costly to implement than circuit switching
An interesting discussion of packet switching versus circuit switching is MolineroFernandez 
Generally speaking people who do not like to hassle with restaurant reservations prefer packet switching to circuit switching
Why is packet switching more efficient Lets look at a simple example
Suppose users share a Mbps link
Also suppose that each user alternates between periods of activity when a user generates data at a constant rate of kbps and periods of inactivity when a user generates no data
Suppose further that a user is active only percent of the time and is idly drinking coffee during the remaining percent of the time
With circuit switching kbps must be reserved for each user at all times
For example with circuitswitched TDM if a onesecond frame is divided into time slots of ms each then each user would be allocated one time slot per frame
Thus the circuitswitched link can support only Mbps kbps simultaneous users
With packet switching the probability that a specific user is active is 
that is percent
If there are users the probability that there are or more simultaneously active users is approximately 
Homework Problem P outlines how this probability is obtained
When there are or fewer simultaneously active users which happens with probability 
the aggregate arrival rate of data is less than or equal to Mbps the output rate of the link
Thus when there are or fewer active users users packets flow through the link essentially without delay as is the case with circuit switching
When there are more than simultaneously active users then the aggregate arrival rate of packets exceeds the output capacity of the link and the output queue will begin to grow
It continues to grow until the aggregate input rate falls back below Mbps at which point the queue will begin to diminish in length
Because the probability of having more than simultaneously active users is minuscule in this example packet switching provides essentially the same performance as circuit switching but does so while allowing for more than three times the number of users
Lets now consider a second simple example
Suppose there are users and that one user suddenly generates one thousand bit packets while other users remain quiescent and do not generate packets
Under TDM circuit switching with slots per frame and each slot consisting of bits the active user can only use its one time slot per frame to transmit data while the remaining nine time slots in each frame remain idle
It will be seconds before all of the active users one million bits of data has been transmitted
In the case of packet switching the active user can continuously send its packets at the full link rate of Mbps since there are no other users generating packets that need to be multiplexed with the active users packets
In this case all of the active users data will be transmitted within second
The above examples illustrate two ways in which the performance of packet switching can be superior to that of circuit switching
They also highlight the crucial difference between the two forms of sharing a links transmission rate among multiple data streams
Circuit switching preallocates use of the transmission link regardless of demand with allocated but unneeded link time going unused
Packet switching on the other hand allocates link use on demand
Link transmission capacity will be shared on a packetbypacket basis only among those users who have packets that need to be transmitted over the link
Although packet switching and circuit switching are both prevalent in todays telecommunication networks the trend has certainly been in the direction of packet switching
Even many of todays circuit switched telephone networks are slowly migrating toward packet switching
In particular telephone networks often use packet switching for the expensive overseas portion of a telephone call
A Network of Networks We saw earlier that end systems PCs smartphones Web servers mail servers and so on connect into the Internet via an access ISP
The access ISP can provide either wired or wireless connectivity using an array of access technologies including DSL cable FTTH WiFi and cellular
Note that the access ISP does not have to be a telco or a cable company instead it can be for example a university providing Internet access to students staff and faculty or a company providing access for its employees
But connecting end users and content providers into an access ISP is only a small piece of solving the puzzle of connecting the billions of end systems that make up the Internet
To complete this puzzle the access ISPs themselves must be interconnected
This is done by creating a network of networksunderstanding this phrase is the key to understanding the Internet
Over the years the network of networks that forms the Internet has evolved into a very complex structure
Much of this evolution is driven by economics and national policy rather than by performance considerations
In order to understand todays Internet network structure lets incrementally build a series of network structures with each new structure being a better approximation of the complex Internet that we have today
Recall that the overarching goal is to interconnect the access ISPs so that all end systems can send packets to each other
One naive approach would be to have each access ISP directly connect with every other access ISP
Such a mesh design is of course much too costly for the access ISPs as it would require each access ISP to have a separate communication link to each of the hundreds of thousands of other access ISPs all over the world
Our first network structure Network Structure interconnects all of the access ISPs with a single global transit ISP
Our imaginary global transit ISP is a network of routers and communication links that not only spans the globe but also has at least one router near each of the hundreds of thousands of access ISPs
Of course it would be very costly for the global ISP to build such an extensive network
To be profitable it would naturally charge each of the access ISPs for connectivity with the pricing reflecting but not necessarily directly proportional to the amount of traffic an access ISP exchanges with the global ISP
Since the access ISP pays the global transit ISP the access ISP is said to be a customer and the global transit ISP is said to be a provider
Now if some company builds and operates a global transit ISP that is profitable then it is natural for other companies to build their own global transit ISPs and compete with the original global transit ISP
This leads to Network Structure which consists of the hundreds of thousands of access ISPs and multiple global transit ISPs
The access ISPs certainly prefer Network Structure over Network Structure since they can now choose among the competing global transit providers as a function of their pricing and services
Note however that the global transit ISPs themselves must interconnect Otherwise access ISPs connected to one of the global transit providers would not be able to communicate with access ISPs connected to the other global transit providers
Network Structure just described is a twotier hierarchy with global transit providers residing at the top tier and access ISPs at the bottom tier
This assumes that global transit ISPs are not only capable of getting close to each and every access ISP but also find it economically desirable to do so
In reality although some ISPs do have impressive global coverage and do directly connect with many access ISPs no ISP has presence in each and every city in the world
Instead in any given region there may be a regional ISP to which the access ISPs in the region connect
Each regional ISP then connects to tier ISPs
Tier ISPs are similar to our imaginary global transit ISP but tier ISPs which actually do exist do not have a presence in every city in the world
There are approximately a dozen tier ISPs including Level Communications ATT Sprint and NTT
Interestingly no group officially sanctions tier status as the saying goesif you have to ask if youre a member of a group youre probably not
Returning to this network of networks not only are there multiple competing tier ISPs there may be multiple competing regional ISPs in a region
In such a hierarchy each access ISP pays the regional ISP to which it connects and each regional ISP pays the tier ISP to which it connects
An access ISP can also connect directly to a tier ISP in which case it pays the tier ISP
Thus there is customer provider relationship at each level of the hierarchy
Note that the tier ISPs do not pay anyone as they are at the top of the hierarchy
To further complicate matters in some regions there may be a larger regional ISP possibly spanning an entire country to which the smaller regional ISPs in that region connect the larger regional ISP then connects to a tier ISP
For example in China there are access ISPs in each city which connect to provincial ISPs which in turn connect to national ISPs which finally connect to tier ISPs Tian 
We refer to this multitier hierarchy which is still only a crude approximation of todays Internet as Network Structure 
To build a network that more closely resembles todays Internet we must add points of presence PoPs multihoming peering and Internet exchange points IXPs to the hierarchical Network Structure 
PoPs exist in all levels of the hierarchy except for the bottom access ISP level
A PoP is simply a group of one or more routers at the same location in the providers network where customer ISPs can connect into the provider ISP
For a customer network to connect to a providers PoP it can lease a highspeed link from a thirdparty telecommunications provider to directly connect one of its routers to a router at the PoP
Any ISP except for tier ISPs may choose to multihome that is to connect to two or more provider ISPs
So for example an access ISP may multihome with two regional ISPs or it may multihome with two regional ISPs and also with a tier ISP
Similarly a regional ISP may multihome with multiple tier ISPs
When an ISP multihomes it can continue to send and receive packets into the Internet even if one of its providers has a failure
As we just learned customer ISPs pay their provider ISPs to obtain global Internet interconnectivity
The amount that a customer ISP pays a provider ISP reflects the amount of traffic it exchanges with the provider
To reduce these costs a pair of nearby ISPs at the same level of the hierarchy can peer that is they can directly connect their networks together so that all the traffic between them passes over the direct connection rather than through upstream intermediaries
When two ISPs peer it is typically settlementfree that is neither ISP pays the other
As noted earlier tier ISPs also peer with one another settlementfree
For a readable discussion of peering and customerprovider relationships see Van der Berg 
Along these same lines a thirdparty company can create an Internet Exchange Point IXP which is a meeting point where multiple ISPs can peer together
An IXP is typically in a standalone building with its own switches Ager 
There are over IXPs in the Internet today IXP List 
We refer to this ecosystemconsisting of access ISPs regional ISPs tier ISPs PoPs multihoming peering and IXPsas Network Structure 
We now finally arrive at Network Structure which describes todays Internet
Network Structure illustrated in Figure 
builds on top of Network Structure by adding contentprovider networks
Google is currently one of the leading examples of such a contentprovider network
As of this writing it is estimated that Google has data centers distributed across North America Europe Asia South America and Australia
Some of these data centers house over one hundred thousand servers while other data centers are smaller housing only hundreds of servers
The Google data centers are all interconnected via Googles private TCPIP network which spans the entire globe but is nevertheless separate from the public Internet
Importantly the Google private network only carries traffic tofrom Google servers
As shown in Figure 
the Google private network attempts to bypass the upper tiers of the Internet by peering settlement free with lowertier ISPs either by directly connecting with them or by connecting with them at IXPs Labovitz 
However because many access ISPs can still only be reached by transiting through tier networks the Google network also connects to tier ISPs and pays those ISPs for the traffic it exchanges with them
By creating its own network a content provider not only reduces its payments to uppertier ISPs but also has greater control of how its services are ultimately delivered to end users
Googles network infrastructure is described in greater detail in Section 
In summary todays Interneta network of networksis complex consisting of a dozen or so tier ISPs and hundreds of thousands of lowertier ISPs
The ISPs are diverse in their coverage with some spanning multiple continents and oceans and others limited to narrow geographic regions
The lower tier ISPs connect to the highertier ISPs and the highertier ISPs interconnect with one another
Users and content providers are customers of lowertier ISPs and lowertier ISPs are customers of highertier ISPs
In recent years major content providers have also created their own networks and connect directly into lowertier ISPs where possible
Interconnection of ISPs 
Delay Loss and Throughput in PacketSwitched Networks Back in Section 
we said that the Internet can be viewed as an infrastructure that provides services to distributed applications running on end systems
Ideally we would like Internet services to be able to move as much data as we want between any two end systems instantaneously without any loss of data
Alas this is a lofty goal one that is unachievable in reality
Instead computer networks necessarily constrain throughput the amount of data per second that can be transferred between end systems introduce delays between end systems and can actually lose packets
On one hand it is unfortunate that the physical laws of reality introduce delay and loss as well as constrain throughput
On the other hand because computer networks have these problems there are many fascinating issues surrounding how to deal with the problemsmore than enough issues to fill a course on computer networking and to motivate thousands of PhD theses In this section well begin to examine and quantify delay loss and throughput in computer networks
Overview of Delay in PacketSwitched Networks Recall that a packet starts in a host the source passes through a series of routers and ends its journey in another host the destination
As a packet travels from one node host or router to the subsequent node host or router along this path the packet suffers from several types of delays at each node along the path
The most important of these delays are the nodal processing delay queuing delay transmission delay and propagation delay together these delays accumulate to give a total nodal delay
The performance of many Internet applicationssuch as search Web browsing email maps instant messaging and voiceoverIPare greatly affected by network delays
In order to acquire a deep understanding of packet switching and computer networks we must understand the nature and importance of these delays
Types of Delay Lets explore these delays in the context of Figure 
As part of its endtoend route between source and destination a packet is sent from the upstream node through router A to router B
Our goal is to characterize the nodal delay at router A
Note that router A has an outbound link leading to router B
This link is preceded by a queue also known as a buffer
When the packet arrives at router A from the upstream node router A examines the packets header to determine the appropriate outbound link for the packet and then directs the packet to this link
In this example the outbound link for the packet is the one that leads to router B
A packet can be transmitted on a link only if there is no other packet currently being transmitted on the link and if there are no other packets preceding it in the queue if the link is Figure 
The nodal delay at router A currently busy or if there are other packets already queued for the link the newly arriving packet will then join the queue
Processing Delay The time required to examine the packets header and determine where to direct the packet is part of the processing delay
The processing delay can also include other factors such as the time needed to check for bitlevel errors in the packet that occurred in transmitting the packets bits from the upstream node to router A
Processing delays in highspeed routers are typically on the order of microseconds or less
After this nodal processing the router directs the packet to the queue that precedes the link to router B
In Chapter well study the details of how a router operates
Queuing Delay At the queue the packet experiences a queuing delay as it waits to be transmitted onto the link
The length of the queuing delay of a specific packet will depend on the number of earlierarriving packets that are queued and waiting for transmission onto the link
If the queue is empty and no other packet is currently being transmitted then our packets queuing delay will be zero
On the other hand if the traffic is heavy and many other packets are also waiting to be transmitted the queuing delay will be long
We will see shortly that the number of packets that an arriving packet might expect to find is a function of the intensity and nature of the traffic arriving at the queue
Queuing delays can be on the order of microseconds to milliseconds in practice
Transmission Delay Assuming that packets are transmitted in a firstcomefirstserved manner as is common in packet switched networks our packet can be transmitted only after all the packets that have arrived before it have been transmitted
Denote the length of the packet by L bits and denote the transmission rate of the link from router A to router B by R bitssec
For example for a Mbps Ethernet link the rate is R Mbps for a Mbps Ethernet link the rate is R Mbps
The transmission delay is LR
This is the amount of time required to push that is transmit all of the packets bits into the link
Transmission delays are typically on the order of microseconds to milliseconds in practice
Propagation Delay Once a bit is pushed into the link it needs to propagate to router B
The time required to propagate from the beginning of the link to router B is the propagation delay
The bit propagates at the propagation speed of the link
The propagation speed depends on the physical medium of the link that is fiber optics twistedpair copper wire and so on and is in the range of meterssec to meterssec which is equal to or a little less than the speed of light
The propagation delay is the distance between two routers divided by the propagation speed
That is the propagation delay is ds where d is the distance between router A and router B and s is the propagation speed of the link
Once the last bit of the packet propagates to node B it and all the preceding bits of the packet are stored in router B
The whole process then continues with router B now performing the forwarding
In widearea networks propagation delays are on the order of milliseconds
Comparing Transmission and Propagation Delay Exploring propagation delay and transmission delay Newcomers to the field of computer networking sometimes have difficulty understanding the difference between transmission delay and propagation delay
The difference is subtle but important
The transmission delay is the amount of time required for the router to push out the packet it is a function of the packets length and the transmission rate of the link but has nothing to do with the distance between the two routers
The propagation delay on the other hand is the time it takes a bit to propagate from one router to the next it is a function of the distance between the two routers but has nothing to do with the packets length or the transmission rate of the link
An analogy might clarify the notions of transmission and propagation delay
Consider a highway that has a tollbooth every kilometers as shown in Figure 
You can think of the highway segments between tollbooths as links and the tollbooths as routers
Suppose that cars travel that is propagate on the highway at a rate of kmhour that is when a car leaves a tollbooth it instantaneously accelerates to kmhour and maintains that speed between tollbooths
Suppose next that cars traveling together as a caravan follow each other in a fixed order
You can think of each car as a bit and the caravan as a packet
Also suppose that each Figure 
Caravan analogy tollbooth services that is transmits a car at a rate of one car per seconds and that it is late at night so that the caravans cars are the only cars on the highway
Finally suppose that whenever the first car of the caravan arrives at a tollbooth it waits at the entrance until the other nine cars have arrived and lined up behind it
Thus the entire caravan must be stored at the tollbooth before it can begin to be forwarded
The time required for the tollbooth to push the entire caravan onto the highway is cars carsminute minutes
This time is analogous to the transmission delay in a router
The time required for a car to travel from the exit of one tollbooth to the next tollbooth is km kmhour hour
This time is analogous to propagation delay
Therefore the time from when the caravan is stored in front of a tollbooth until the caravan is stored in front of the next tollbooth is the sum of transmission delay and propagation delayin this example minutes
Lets explore this analogy a bit more
What would happen if the tollbooth service time for a caravan were greater than the time for a car to travel between tollbooths For example suppose now that the cars travel at the rate of kmhour and the tollbooth services cars at the rate of one car per minute
Then the traveling delay between two tollbooths is minutes and the time to serve a caravan is minutes
In this case the first few cars in the caravan will arrive at the second tollbooth before the last cars in the caravan leave the first tollbooth
This situation also arises in packetswitched networksthe first bits in a packet can arrive at a router while many of the remaining bits in the packet are still waiting to be transmitted by the preceding router
If a picture speaks a thousand words then an animation must speak a million words
The Web site for this textbook provides an interactive Java applet that nicely illustrates and contrasts transmission delay and propagation delay
The reader is highly encouraged to visit that applet
Smith also provides a very readable discussion of propagation queueing and transmission delays
If we let dproc dqueue dtrans and dprop denote the processing queuing transmission and propagation delays then the total nodal delay is given by dnodaldprocdqueuedtransdprop The contribution of these delay components can vary significantly
For example dprop can be negligible for example a couple of microseconds for a link connecting two routers on the same university campus however dprop is hundreds of milliseconds for two routers interconnected by a geostationary satellite link and can be the dominant term in dnodal
Similarly dtrans can range from negligible to significant
Its contribution is typically negligible for transmission rates of Mbps and higher for example for LANs however it can be hundreds of milliseconds for large Internet packets sent over lowspeed dialup modem links
The processing delay dproc is often negligible however it strongly influences a routers maximum throughput which is the maximum rate at which a router can forward packets
Queuing Delay and Packet Loss The most complicated and interesting component of nodal delay is the queuing delay dqueue
In fact queuing delay is so important and interesting in computer networking that thousands of papers and numerous books have been written about it Bertsekas Daigle Kleinrock Kleinrock Ross 
We give only a highlevel intuitive discussion of queuing delay here the more curious reader may want to browse through some of the books or even eventually write a PhD thesis on the subject
Unlike the other three delays namely dproc dtrans and dprop the queuing delay can vary from packet to packet
For example if packets arrive at an empty queue at the same time the first packet transmitted will suffer no queuing delay while the last packet transmitted will suffer a relatively large queuing delay while it waits for the other nine packets to be transmitted
Therefore when characterizing queuing delay one typically uses statistical measures such as average queuing delay variance of queuing delay and the probability that the queuing delay exceeds some specified value
When is the queuing delay large and when is it insignificant The answer to this question depends on the rate at which traffic arrives at the queue the transmission rate of the link and the nature of the arriving traffic that is whether the traffic arrives periodically or arrives in bursts
To gain some insight here let a denote the average rate at which packets arrive at the queue a is in units of packetssec
Recall that R is the transmission rate that is it is the rate in bitssec at which bits are pushed out of the queue
Also suppose for simplicity that all packets consist of L bits
Then the average rate at which bits arrive at the queue is La bitssec
Finally assume that the queue is very big so that it can hold essentially an infinite number of bits
The ratio LaR called the traffic intensity often plays an important role in estimating the extent of the queuing delay
If LaR then the average rate at which bits arrive at the queue exceeds the rate at which the bits can be transmitted from the queue
In this unfortunate situation the queue will tend to increase without bound and the queuing delay will approach infinity Therefore one of the golden rules in traffic engineering is Design your system so that the traffic intensity is no greater than 
Now consider the case LaR 
Here the nature of the arriving traffic impacts the queuing delay
For example if packets arrive periodicallythat is one packet arrives every LR secondsthen every packet will arrive at an empty queue and there will be no queuing delay
On the other hand if packets arrive in bursts but periodically there can be a significant average queuing delay
For example suppose N packets arrive simultaneously every LRN seconds
Then the first packet transmitted has no queuing delay the second packet transmitted has a queuing delay of LR seconds and more generally the nth packet transmitted has a queuing delay of nLR seconds
We leave it as an exercise for you to calculate the average queuing delay in this example
The two examples of periodic arrivals described above are a bit academic
Typically the arrival process to a queue is random that is the arrivals do not follow any pattern and the packets are spaced apart by random amounts of time
In this more realistic case the quantity LaR is not usually sufficient to fully characterize the queuing delay statistics
Nonetheless it is useful in gaining an intuitive understanding of the extent of the queuing delay
In particular if the traffic intensity is close to zero then packet arrivals are few and far between and it is unlikely that an arriving packet will find another packet in the queue
Hence the average queuing delay will be close to zero
On the other hand when the traffic intensity is close to there will be intervals of time when the arrival rate exceeds the transmission capacity due to variations in packet arrival rate and a queue will form during these periods of time when the arrival rate is less than the transmission capacity the length of the queue will shrink
Nonetheless as the traffic intensity approaches the average queue length gets larger and larger
The qualitative dependence of average queuing delay on the traffic intensity is shown in Figure 
One important aspect of Figure 
is the fact that as the traffic intensity approaches the average queuing delay increases rapidly
A small percentage increase in the intensity will result in a much larger percentagewise increase in delay
Perhaps you have experienced this phenomenon on the highway
If you regularly drive on a road that is typically congested the fact that the road is typically Figure 
Dependence of average queuing delay on traffic intensity congested means that its traffic intensity is close to 
If some event causes an even slightly largerthan usual amount of traffic the delays you experience can be huge
To really get a good feel for what queuing delays are about you are encouraged once again to visit the textbook Web site which provides an interactive Java applet for a queue
If you set the packet arrival rate high enough so that the traffic intensity exceeds you will see the queue slowly build up over time
Packet Loss In our discussions above we have assumed that the queue is capable of holding an infinite number of packets
In reality a queue preceding a link has finite capacity although the queuing capacity greatly depends on the router design and cost
Because the queue capacity is finite packet delays do not really approach infinity as the traffic intensity approaches 
Instead a packet can arrive to find a full queue
With no place to store such a packet a router will drop that packet that is the packet will be lost
This overflow at a queue can again be seen in the Java applet for a queue when the traffic intensity is greater than 
From an endsystem viewpoint a packet loss will look like a packet having been transmitted into the network core but never emerging from the network at the destination
The fraction of lost packets increases as the traffic intensity increases
Therefore performance at a node is often measured not only in terms of delay but also in terms of the probability of packet loss
As well discuss in the subsequent chapters a lost packet may be retransmitted on an endtoend basis in order to ensure that all data are eventually transferred from source to destination
EndtoEnd Delay Our discussion up to this point has focused on the nodal delay that is the delay at a single router
Lets now consider the total delay from source to destination
To get a handle on this concept suppose there are N routers between the source host and the destination host
Lets also suppose for the moment that the network is uncongested so that queuing delays are negligible the processing delay at each router and at the source host is dproc the transmission rate out of each router and out of the source host is R bitssec and the propagation on each link is dprop
The nodal delays accumulate and give an endto end delay dendendNdprocdtransdprop 
where once again dtransLR where L is the packet size
Note that Equation 
is a generalization of Equation 
which did not take into account processing and propagation delays
We leave it to you to generalize Equation 
to the case of heterogeneous delays at the nodes and to the presence of an average queuing delay at each node
Traceroute Using Traceroute to discover network paths and measure network delay To get a handson feel for endtoend delay in a computer network we can make use of the Traceroute program
Traceroute is a simple program that can run in any Internet host
When the user specifies a destination hostname the program in the source host sends multiple special packets toward that destination
As these packets work their way toward the destination they pass through a series of routers
When a router receives one of these special packets it sends back to the source a short message that contains the name and address of the router
More specifically suppose there are N routers between the source and the destination
Then the source will send N special packets into the network with each packet addressed to the ultimate destination
These N special packets are marked through N with the first packet marked and the last packet marked N
When the nth router receives the nth packet marked n the router does not forward the packet toward its destination but instead sends a message back to the source
When the destination host receives the Nth packet it too returns a message back to the source
The source records the time that elapses between when it sends a packet and when it receives the corresponding return message it also records the name and address of the router or the destination host that returns the message
In this manner the source can reconstruct the route taken by packets flowing from source to destination and the source can determine the roundtrip delays to all the intervening routers
Traceroute actually repeats the experiment just described three times so the source actually sends N packets to the destination
RFC describes Traceroute in detail
Here is an example of the output of the Traceroute program where the route was being traced from the source host gaia.cs.umass.edu at the University of Massachusetts to the host cis.poly.edu at Polytechnic University in Brooklyn
The output has six columns the first column is the n value described above that is the number of the router along the route the second column is the name of the router the third column is the address of the router of the form xxx.xxx.xxx.xxx the last three columns are the roundtrip delays for three experiments
If the source receives fewer than three messages from any given router due to packet loss in the network Traceroute places an asterisk just after the router number and reports fewer than three roundtrip times for that router
ms borderrtgi.gw.umass.edu 
ms acrge.Boston.cw.net 
ms agrloopback.NewYork.cw.net 
ms acrloopback.NewYork.cw.net 
ms pos.core.NewYork.Level.net 
ms gige.hsipaccess.NewYork.Level.net 
ms p.polyu.bbnplanet.net 
ms cis.poly.edu 
ms In the trace above there are nine routers between the source and the destination
Most of these routers have a name and all of them have addresses
For example the name of Router is borderrtgi .gw.umass.edu and its address is 
Looking at the data provided for this same router we see that in the first of the three trials the roundtrip delay between the source and the router was 
The roundtrip delays for the subsequent two trials were 
These roundtrip delays include all of the delays just discussed including transmission delays propagation delays router processing delays and queuing delays
Because the queuing delay is varying with time the roundtrip delay of packet n sent to a router n can sometimes be longer than the roundtrip delay of packet n sent to router n
Indeed we observe this phenomenon in the above example the delays to Router are larger than the delays to Router Want to try out Traceroute for yourself We highly recommended that you visit http www.traceroute.org which provides a Web interface to an extensive list of sources for route tracing
You choose a source and supply the hostname for any destination
The Traceroute program then does all the work
There are a number of free software programs that provide a graphical interface to Traceroute one of our favorites is PingPlotter PingPlotter 
End System Application and Other Delays In addition to processing transmission and propagation delays there can be additional significant delays in the end systems
For example an end system wanting to transmit a packet into a shared medium e.g
as in a WiFi or cable modem scenario may purposefully delay its transmission as part of its protocol for sharing the medium with other end systems well consider such protocols in detail in Chapter 
Another important delay is media packetization delay which is present in VoiceoverIP VoIP applications
In VoIP the sending side must first fill a packet with encoded digitized speech before passing the packet to the Internet
This time to fill a packetcalled the packetization delaycan be significant and can impact the userperceived quality of a VoIP call
This issue will be further explored in a homework problem at the end of this chapter
Throughput in Computer Networks In addition to delay and packet loss another critical performance measure in computer networks is end toend throughput
To define throughput consider transferring a large file from Host A to Host B across a computer network
This transfer might be for example a large video clip from one peer to another in a PP file sharing system
The instantaneous throughput at any instant of time is the rate in bitssec at which Host B is receiving the file
Many applications including many PP file sharing systems display the instantaneous throughput during downloads in the user interfaceperhaps you have observed this before If the file consists of F bits and the transfer takes T seconds for Host B to receive all F bits then the average throughput of the file transfer is FT bitssec
For some applications such as Internet telephony it is desirable to have a low delay and an instantaneous throughput consistently above some threshold for example over kbps for some Internet telephony applications and over kbps for some realtime video applications
For other applications including those involving file transfers delay is not critical but it is desirable to have the highest possible throughput
To gain further insight into the important concept of throughput lets consider a few examples
Figure .a shows two end systems a server and a client connected by two communication links and a router
Consider the throughput for a file transfer from the server to the client
Let Rs denote the rate of the link between the server and the router and Rc denote the rate of the link between the router and the client
Suppose that the only bits being sent in the entire network are those from the server to the client
We now ask in this ideal scenario what is the servertoclient throughput To answer this question we may think of bits as fluid and communication links as pipes
Clearly the server cannot pump bits through its link at a rate faster than Rs bps and the router cannot forward bits at a rate faster than Rc bps
If RsRc then the bits pumped by the server will flow right through the router and arrive at the client at a rate of Rs bps giving a throughput of Rs bps
If on the other hand RcRs then the router will not be able to forward bits as quickly as it receives them
In this case bits will only leave the router at rate Rc giving an endtoend throughput of Rc
Note also that if bits continue to arrive at the router at rate Rs and continue to leave the router at Rc the backlog of bits at the router waiting Figure 
Throughput for a file transfer from server to client for transmission to the client will grow and growa most undesirable situation Thus for this simple twolink network the throughput is minRc Rs that is it is the transmission rate of the bottleneck link
Having determined the throughput we can now approximate the time it takes to transfer a large file of F bits from server to client as FminRs Rc
For a specific example suppose you are downloading an MP file of F million bits the server has a transmission rate of Rs Mbps and you have an access link of Rc Mbps
The time needed to transfer the file is then seconds
Of course these expressions for throughput and transfer time are only approximations as they do not account for storeandforward and processing delays as well as protocol issues
Figure .b now shows a network with N links between the server and the client with the transmission rates of the N links being RR RN
Applying the same analysis as for the twolink network we find that the throughput for a file transfer from server to client is minRR RN which is once again the transmission rate of the bottleneck link along the path between server and client
Now consider another example motivated by todays Internet
Figure .a shows two end systems a server and a client connected to a computer network
Consider the throughput for a file transfer from the server to the client
The server is connected to the network with an access link of rate Rs and the client is connected to the network with an access link of rate Rc
Now suppose that all the links in the core of the communication network have very high transmission rates much higher than Rs and Rc
Indeed today the core of the Internet is overprovisioned with high speed links that experience little congestion
Also suppose that the only bits being sent in the entire network are those from the server to the client
Because the core of the computer network is like a wide pipe in this example the rate at which bits can flow from source to destination is again the minimum of Rs and Rc that is throughput minRs Rc
Therefore the constraining factor for throughput in todays Internet is typically the access network
For a final example consider Figure .b in which there are servers and clients connected to the core of the computer network
In this example there are simultaneous downloads taking place involving clientserver pairs
Suppose that these downloads are the only traffic in the network at the current time
As shown in the figure there is a link in the core that is traversed by all downloads
Denote R for the transmission rate of this link R
Lets suppose that all server access links have the same rate Rs all client access links have the same rate Rc and the transmission rates of all the links in the coreexcept the one common link of rate Rare much larger than Rs Rc and R
Now we ask what are the throughputs of the downloads Clearly if the rate of the common link R is largesay a hundred times larger than both Rs and Rcthen the throughput for each download will once again be minRs Rc
But what if the rate of the common link is of the same order as Rs and Rc What will the throughput be in this case Lets take a look at a specific example
Suppose Rs Mbps Rc Mbps R Mbps and the Figure 
Endtoend throughput a Client downloads a file from server b clients downloading with servers common link divides its transmission rate equally among the downloads
Then the bottleneck for each download is no longer in the access network but is now instead the shared link in the core which only provides each download with kbps of throughput
Thus the endtoend throughput for each download is now reduced to kbps
The examples in Figure 
and Figure .a show that throughput depends on the transmission rates of the links over which the data flows
We saw that when there is no other intervening traffic the throughput can simply be approximated as the minimum transmission rate along the path between source and destination
The example in Figure .b shows that more generally the throughput depends not only on the transmission rates of the links along the path but also on the intervening traffic
In particular a link with a high transmission rate may nonetheless be the bottleneck link for a file transfer if many other data flows are also passing through that link
We will examine throughput in computer networks more closely in the homework problems and in the subsequent chapters
Protocol Layers and Their Service Models From our discussion thus far it is apparent that the Internet is an extremely complicated system
We have seen that there are many pieces to the Internet numerous applications and protocols various types of end systems packet switches and various types of linklevel media
Given this enormous complexity is there any hope of organizing a network architecture or at least our discussion of network architecture Fortunately the answer to both questions is yes
Layered Architecture Before attempting to organize our thoughts on Internet architecture lets look for a human analogy
Actually we deal with complex systems all the time in our everyday life
Imagine if someone asked you to describe for example the airline system
How would you find the structure to describe this complex system that has ticketing agents baggage checkers gate personnel pilots airplanes air traffic control and a worldwide system for routing airplanes One way to describe this system might be to describe the series of actions you take or others take for you when you fly on an airline
You purchase your ticket check your bags go to the gate and eventually get loaded onto the plane
The plane takes off and is routed to its destination
After your plane lands you deplane at the gate and claim your bags
If the trip was bad you complain about the flight to the ticket agent getting nothing for your effort
This scenario is shown in Figure 
Taking an airplane trip actions Figure 
Horizontal layering of airline functionality Already we can see some analogies here with computer networking You are being shipped from source to destination by the airline a packet is shipped from source host to destination host in the Internet
But this is not quite the analogy we are after
We are looking for some structure in Figure 
Looking at Figure 
we note that there is a ticketing function at each end there is also a baggage function for alreadyticketed passengers and a gate function for alreadyticketed and alreadybaggage checked passengers
For passengers who have made it through the gate that is passengers who are already ticketed baggagechecked and through the gate there is a takeoff and landing function and while in flight there is an airplanerouting function
This suggests that we can look at the functionality in Figure 
in a horizontal manner as shown in Figure 
has divided the airline functionality into layers providing a framework in which we can discuss airline travel
Note that each layer combined with the layers below it implements some functionality some service
At the ticketing layer and below airlinecountertoairlinecounter transfer of a person is accomplished
At the baggage layer and below baggagechecktobaggageclaim transfer of a person and bags is accomplished
Note that the baggage layer provides this service only to an alreadyticketed person
At the gate layer departuregatetoarrivalgate transfer of a person and bags is accomplished
At the takeofflanding layer runwaytorunway transfer of people and their bags is accomplished
Each layer provides its service by performing certain actions within that layer for example at the gate layer loading and unloading people from an airplane and by using the services of the layer directly below it for example in the gate layer using the runwaytorunway passenger transfer service of the takeofflanding layer
A layered architecture allows us to discuss a welldefined specific part of a large and complex system
This simplification itself is of considerable value by providing modularity making it much easier to change the implementation of the service provided by the layer
As long as the layer provides the same service to the layer above it and uses the same services from the layer below it the remainder of the system remains unchanged when a layers implementation is changed
Note that changing the implementation of a service is very different from changing the service itself For example if the gate functions were changed for instance to have people board and disembark by height the remainder of the airline system would remain unchanged since the gate layer still provides the same function loading and unloading people it simply implements that function in a different manner after the change
For large and complex systems that are constantly being updated the ability to change the implementation of a service without affecting other components of the system is another important advantage of layering
Protocol Layering But enough about airlines
Lets now turn our attention to network protocols
To provide structure to the design of network protocols network designers organize protocolsand the network hardware and software that implement the protocolsin layers
Each protocol belongs to one of the layers just as each function in the airline architecture in Figure 
belonged to a layer
We are again interested in the services that a layer offers to the layer abovethe socalled service model of a layer
Just as in the case of our airline example each layer provides its service by performing certain actions within that layer and by using the services of the layer directly below it
For example the services provided by layer n may include reliable delivery of messages from one edge of the network to the other
This might be implemented by using an unreliable edgetoedge message delivery service of layer n and adding layer n functionality to detect and retransmit lost messages
A protocol layer can be implemented in software in hardware or in a combination of the two
Applicationlayer protocolssuch as HTTP and SMTPare almost always implemented in software in the end systems so are transportlayer protocols
Because the physical layer and data link layers are responsible for handling communication over a specific link they are typically implemented in a network interface card for example Ethernet or WiFi interface cards associated with a given link
The network layer is often a mixed implementation of hardware and software
Also note that just as the functions in the layered airline architecture were distributed among the various airports and flight control centers that make up the system so too is a layer n protocol distributed among the end systems packet switches and other components that make up the network
That is theres often a piece of a layer n protocol in each of these network components
Protocol layering has conceptual and structural advantages RFC 
As we have seen layering provides a structured way to discuss system components
Modularity makes it easier to update system components
We mention however that some researchers and networking engineers are vehemently opposed to layering Wakeman 
One potential drawback of layering is that one layer may duplicate lowerlayer functionality
For example many protocol stacks provide error recovery Figure 
The Internet protocol stack a and OSI reference model b on both a perlink basis and an endtoend basis
A second potential drawback is that functionality at one layer may need information for example a timestamp value that is present only in another layer this violates the goal of separation of layers
When taken together the protocols of the various layers are called the protocol stack
The Internet protocol stack consists of five layers the physical link network transport and application layers as shown in Figure .a
If you examine the Table of Contents you will see that we have roughly organized this book using the layers of the Internet protocol stack
We take a topdown approach first covering the application layer and then proceeding downward
Application Layer The application layer is where network applications and their applicationlayer protocols reside
The Internets application layer includes many protocols such as the HTTP protocol which provides for Web document request and transfer SMTP which provides for the transfer of email messages and FTP which provides for the transfer of files between two end systems
Well see that certain network functions such as the translation of humanfriendly names for Internet end systems like www.ietf.org to a bit network address are also done with the help of a specific applicationlayer protocol namely the domain name system DNS
Well see in Chapter that it is very easy to create and deploy our own new applicationlayer protocols
An applicationlayer protocol is distributed over multiple end systems with the application in one end system using the protocol to exchange packets of information with the application in another end system
Well refer to this packet of information at the application layer as a message
Transport Layer The Internets transport layer transports applicationlayer messages between application endpoints
In the Internet there are two transport protocols TCP and UDP either of which can transport application layer messages
TCP provides a connectionoriented service to its applications
This service includes guaranteed delivery of applicationlayer messages to the destination and flow control that is senderreceiver speed matching
TCP also breaks long messages into shorter segments and provides a congestioncontrol mechanism so that a source throttles its transmission rate when the network is congested
The UDP protocol provides a connectionless service to its applications
This is a nofrills service that provides no reliability no flow control and no congestion control
In this book well refer to a transportlayer packet as a segment
Network Layer The Internets network layer is responsible for moving networklayer packets known as datagrams from one host to another
The Internet transportlayer protocol TCP or UDP in a source host passes a transportlayer segment and a destination address to the network layer just as you would give the postal service a letter with a destination address
The network layer then provides the service of delivering the segment to the transport layer in the destination host
The Internets network layer includes the celebrated IP protocol which defines the fields in the datagram as well as how the end systems and routers act on these fields
There is only one IP protocol and all Internet components that have a network layer must run the IP protocol
The Internets network layer also contains routing protocols that determine the routes that datagrams take between sources and destinations
The Internet has many routing protocols
As we saw in Section 
the Internet is a network of networks and within a network the network administrator can run any routing protocol desired
Although the network layer contains both the IP protocol and numerous routing protocols it is often simply referred to as the IP layer reflecting the fact that IP is the glue that binds the Internet together
Link Layer The Internets network layer routes a datagram through a series of routers between the source and destination
To move a packet from one node host or router to the next node in the route the network layer relies on the services of the link layer
In particular at each node the network layer passes the datagram down to the link layer which delivers the datagram to the next node along the route
At this next node the link layer passes the datagram up to the network layer
The services provided by the link layer depend on the specific linklayer protocol that is employed over the link
For example some linklayer protocols provide reliable delivery from transmitting node over one link to receiving node
Note that this reliable delivery service is different from the reliable delivery service of TCP which provides reliable delivery from one end system to another
Examples of linklayer protocols include Ethernet WiFi and the cable access networks DOCSIS protocol
As datagrams typically need to traverse several links to travel from source to destination a datagram may be handled by different linklayer protocols at different links along its route
For example a datagram may be handled by Ethernet on one link and by PPP on the next link
The network layer will receive a different service from each of the different linklayer protocols
In this book well refer to the linklayer packets as frames
Physical Layer While the job of the link layer is to move entire frames from one network element to an adjacent network element the job of the physical layer is to move the individual bits within the frame from one node to the next
The protocols in this layer are again link dependent and further depend on the actual transmission medium of the link for example twistedpair copper wire singlemode fiber optics
For example Ethernet has many physicallayer protocols one for twistedpair copper wire another for coaxial cable another for fiber and so on
In each case a bit is moved across the link in a different way
The OSI Model Having discussed the Internet protocol stack in detail we should mention that it is not the only protocol stack around
In particular back in the late s the International Organization for Standardization ISO proposed that computer networks be organized around seven layers called the Open Systems Interconnection OSI model ISO 
The OSI model took shape when the protocols that were to become the Internet protocols were in their infancy and were but one of many different protocol suites under development in fact the inventors of the original OSI model probably did not have the Internet in mind when creating it
Nevertheless beginning in the late s many training and university courses picked up on the ISO mandate and organized courses around the sevenlayer model
Because of its early impact on networking education the sevenlayer model continues to linger on in some networking textbooks and training courses
The seven layers of the OSI reference model shown in Figure .b are application layer presentation layer session layer transport layer network layer data link layer and physical layer
The functionality of five of these layers is roughly the same as their similarly named Internet counterparts
Thus lets consider the two additional layers present in the OSI reference modelthe presentation layer and the session layer
The role of the presentation layer is to provide services that allow communicating applications to interpret the meaning of data exchanged
These services include data compression and data encryption which are selfexplanatory as well as data description which frees the applications from having to worry about the internal format in which data are representedstoredformats that may differ from one computer to another
The session layer provides for delimiting and synchronization of data exchange including the means to build a checkpointing and recovery scheme
The fact that the Internet lacks two layers found in the OSI reference model poses a couple of interesting questions Are the services provided by these layers unimportant What if an application needs one of these services The Internets answer to both of these questions is the sameits up to the application developer
Its up to the application developer to decide if a service is important and if the service is important its up to the application developer to build that functionality into the application
Encapsulation Figure 
shows the physical path that data takes down a sending end systems protocol stack up and down the protocol stacks of an intervening linklayer switch Figure 
Hosts routers and linklayer switches each contains a different set of layers reflecting their differences in functionality and router and then up the protocol stack at the receiving end system
As we discuss later in this book routers and linklayer switches are both packet switches
Similar to end systems routers and linklayer switches organize their networking hardware and software into layers
But routers and linklayer switches do not implement all of the layers in the protocol stack they typically implement only the bottom layers
As shown in Figure 
linklayer switches implement layers and routers implement layers through 
This means for example that Internet routers are capable of implementing the IP protocol a layer protocol while linklayer switches are not
Well see later that while linklayer switches do not recognize IP addresses they are capable of recognizing layer addresses such as Ethernet addresses
Note that hosts implement all five layers this is consistent with the view that the Internet architecture puts much of its complexity at the edges of the network
also illustrates the important concept of encapsulation
At the sending host an applicationlayer message M in Figure 
is passed to the transport layer
In the simplest case the transport layer takes the message and appends additional information socalled transportlayer header information Ht in Figure 
that will be used by the receiverside transport layer
The applicationlayer message and the transportlayer header information together constitute the transport layer segment
The transportlayer segment thus encapsulates the applicationlayer message
The added information might include information allowing the receiverside transport layer to deliver the message up to the appropriate application and errordetection bits that allow the receiver to determine whether bits in the message have been changed in route
The transport layer then passes the segment to the network layer which adds networklayer header information Hn in Figure 
such as source and destination end system addresses creating a networklayer datagram
The datagram is then passed to the link layer which of course will add its own linklayer header information and create a linklayer frame
Thus we see that at each layer a packet has two types of fields header fields and a payload field
The payload is typically a packet from the layer above
A useful analogy here is the sending of an interoffice memo from one corporate branch office to another via the public postal service
Suppose Alice who is in one branch office wants to send a memo to Bob who is in another branch office
The memo is analogous to the applicationlayer message
Alice puts the memo in an interoffice envelope with Bobs name and department written on the front of the envelope
The interoffice envelope is analogous to a transportlayer segmentit contains header information Bobs name and department number and it encapsulates the applicationlayer message the memo
When the sending branchoffice mailroom receives the interoffice envelope it puts the interoffice envelope inside yet another envelope which is suitable for sending through the public postal service
The sending mailroom also writes the postal address of the sending and receiving branch offices on the postal envelope
Here the postal envelope is analogous to the datagramit encapsulates the transport layer segment the interoffice envelope which encapsulates the original message the memo
The postal service delivers the postal envelope to the receiving branchoffice mailroom
There the process of deencapsulation is begun
The mailroom extracts the interoffice memo and forwards it to Bob
Finally Bob opens the envelope and removes the memo
The process of encapsulation can be more complex than that described above
For example a large message may be divided into multiple transportlayer segments which might themselves each be divided into multiple networklayer datagrams
At the receiving end such a segment must then be reconstructed from its constituent datagrams
Networks Under Attack The Internet has become mission critical for many institutions today including large and small companies universities and government agencies
Many individuals also rely on the Internet for many of their professional social and personal activities
Billions of things including wearables and home devices are currently being connected to the Internet
But behind all this utility and excitement there is a dark side a side where bad guys attempt to wreak havoc in our daily lives by damaging our Internet connected computers violating our privacy and rendering inoperable the Internet services on which we depend
The field of network security is about how the bad guys can attack computer networks and about how we soontobe experts in computer networking can defend networks against those attacks or better yet design new architectures that are immune to such attacks in the first place
Given the frequency and variety of existing attacks as well as the threat of new and more destructive future attacks network security has become a central topic in the field of computer networking
One of the features of this textbook is that it brings network security issues to the forefront
Since we dont yet have expertise in computer networking and Internet protocols well begin here by surveying some of todays more prevalent securityrelated problems
This will whet our appetite for more substantial discussions in the upcoming chapters
So we begin here by simply asking what can go wrong How are computer networks vulnerable What are some of the more prevalent types of attacks today The Bad Guys Can Put Malware into Your Host Via the Internet We attach devices to the Internet because we want to receivesend data fromto the Internet
This includes all kinds of good stuff including Instagram posts Internet search results streaming music video conference calls streaming movies and so on
But unfortunately along with all that good stuff comes malicious stuffcollectively known as malwarethat can also enter and infect our devices
Once malware infects our device it can do all kinds of devious things including deleting our files and installing spyware that collects our private information such as social security numbers passwords and keystrokes and then sends this over the Internet of course back to the bad guys
Our compromised host may also be enrolled in a network of thousands of similarly compromised devices collectively known as a botnet which the bad guys control and leverage for spam email distribution or distributed denialofservice attacks soon to be discussed against targeted hosts
Much of the malware out there today is selfreplicating once it infects one host from that host it seeks entry into other hosts over the Internet and from the newly infected hosts it seeks entry into yet more hosts
In this manner selfreplicating malware can spread exponentially fast
Malware can spread in the form of a virus or a worm
Viruses are malware that require some form of user interaction to infect the users device
The classic example is an email attachment containing malicious executable code
If a user receives and opens such an attachment the user inadvertently runs the malware on the device
Typically such email viruses are selfreplicating once executed the virus may send an identical message with an identical malicious attachment to for example every recipient in the users address book
Worms are malware that can enter a device without any explicit user interaction
For example a user may be running a vulnerable network application to which an attacker can send malware
In some cases without any user intervention the application may accept the malware from the Internet and run it creating a worm
The worm in the newly infected device then scans the Internet searching for other hosts running the same vulnerable network application
When it finds other vulnerable hosts it sends a copy of itself to those hosts
Today malware is pervasive and costly to defend against
As you work through this textbook we encourage you to think about the following question What can computer network designers do to defend Internetattached devices from malware attacks The Bad Guys Can Attack Servers and Network Infrastructure Another broad class of security threats are known as denialofservice DoS attacks
As the name suggests a DoS attack renders a network host or other piece of infrastructure unusable by legitimate users
Web servers email servers DNS servers discussed in Chapter and institutional networks can all be subject to DoS attacks
Internet DoS attacks are extremely common with thousands of DoS attacks occurring every year Moore 
The site Digital Attack Map allows use to visualize the top daily DoS attacks worldwide DAM 
Most Internet DoS attacks fall into one of three categories Vulnerability attack
This involves sending a few wellcrafted messages to a vulnerable application or operating system running on a targeted host
If the right sequence of packets is sent to a vulnerable application or operating system the service can stop or worse the host can crash
The attacker sends a deluge of packets to the targeted hostso many packets that the targets access link becomes clogged preventing legitimate packets from reaching the server
The attacker establishes a large number of halfopen or fully open TCP connections TCP connections are discussed in Chapter at the target host
The host can become so bogged down with these bogus connections that it stops accepting legitimate connections
Lets now explore the bandwidthflooding attack in more detail
Recalling our delay and loss analysis discussion in Section 
its evident that if the server has an access rate of R bps then the attacker will need to send traffic at a rate of approximately R bps to cause damage
If R is very large a single attack source may not be able to generate enough traffic to harm the server
Furthermore if all the traffic emanates from a single source an upstream router may be able to detect the attack and block all traffic from that source before the traffic gets near the server
In a distributed DoS DDoS attack illustrated in Figure 
the attacker controls multiple sources and has each source blast traffic at the target
With this approach the aggregate traffic rate across all the controlled sources needs to be approximately R to cripple the service
DDoS attacks leveraging botnets with thousands of comprised hosts are a common occurrence today DAM 
DDos attacks are much harder to detect and defend against than a DoS attack from a single host
We encourage you to consider the following question as you work your way through this book What can computer network designers do to defend against DoS attacks We will see that different defenses are needed for the three types of DoS attacks
A distributed denialofservice attack The Bad Guys Can Sniff Packets Many users today access the Internet via wireless devices such as WiFiconnected laptops or handheld devices with cellular Internet connections covered in Chapter 
While ubiquitous Internet access is extremely convenient and enables marvelous new applications for mobile users it also creates a major security vulnerabilityby placing a passive receiver in the vicinity of the wireless transmitter that receiver can obtain a copy of every packet that is transmitted These packets can contain all kinds of sensitive information including passwords social security numbers trade secrets and private personal messages
A passive receiver that records a copy of every packet that flies by is called a packet sniffer
Sniffers can be deployed in wired environments as well
In wired broadcast environments as in many Ethernet LANs a packet sniffer can obtain copies of broadcast packets sent over the LAN
As described in Section 
cable access technologies also broadcast packets and are thus vulnerable to sniffing
Furthermore a bad guy who gains access to an institutions access router or access link to the Internet may be able to plant a sniffer that makes a copy of every packet going tofrom the organization
Sniffed packets can then be analyzed offline for sensitive information
Packetsniffing software is freely available at various Web sites and as commercial products
Professors teaching a networking course have been known to assign lab exercises that involve writing a packet sniffing and applicationlayer data reconstruction program
Indeed the Wireshark Wireshark labs associated with this text see the introductory Wireshark lab at the end of this chapter use exactly such a packet sniffer Because packet sniffers are passivethat is they do not inject packets into the channelthey are difficult to detect
So when we send packets into a wireless channel we must accept the possibility that some bad guy may be recording copies of our packets
As you may have guessed some of the best defenses against packet sniffing involve cryptography
We will examine cryptography as it applies to network security in Chapter 
The Bad Guys Can Masquerade as Someone You Trust It is surprisingly easy you will have the knowledge to do so shortly as you proceed through this text to create a packet with an arbitrary source address packet content and destination address and then transmit this handcrafted packet into the Internet which will dutifully forward the packet to its destination
Imagine the unsuspecting receiver say an Internet router who receives such a packet takes the false source address as being truthful and then performs some command embedded in the packets contents say modifies its forwarding table
The ability to inject packets into the Internet with a false source address is known as IP spoofing and is but one of many ways in which one user can masquerade as another user
To solve this problem we will need endpoint authentication that is a mechanism that will allow us to determine with certainty if a message originates from where we think it does
Once again we encourage you to think about how this can be done for network applications and protocols as you progress through the chapters of this book
We will explore mechanisms for endpoint authentication in Chapter 
In closing this section its worth considering how the Internet got to be such an insecure place in the first place
The answer in essence is that the Internet was originally designed to be that way based on the model of a group of mutually trusting users attached to a transparent network Blumenthal a model in which by definition there is no need for security
Many aspects of the original Internet architecture deeply reflect this notion of mutual trust
For example the ability for one user to send a packet to any other user is the default rather than a requestedgranted capability and user identity is taken at declared face value rather than being authenticated by default
But todays Internet certainly does not involve mutually trusting users
Nonetheless todays users still need to communicate when they dont necessarily trust each other may wish to communicate anonymously may communicate indirectly through third parties e.g
Web caches which well study in Chapter or mobilityassisting agents which well study in Chapter and may distrust the hardware software and even the air through which they communicate
We now have many securityrelated challenges before us as we progress through this book We should seek defenses against sniffing end point masquerading maninthemiddle attacks DDoS attacks malware and more
We should keep in mind that communication among mutually trusted users is the exception rather than the rule
Welcome to the world of modern computer networking 
History of Computer Networking and the Internet Sections 
presented an overview of the technology of computer networking and the Internet
You should know enough now to impress your family and friends However if you really want to be a big hit at the next cocktail party you should sprinkle your discourse with tidbits about the fascinating history of the Internet Segaller 
The Development of Packet Switching The field of computer networking and todays Internet trace their beginnings back to the early s when the telephone network was the worlds dominant communication network
Recall from Section 
that the telephone network uses circuit switching to transmit information from a sender to a receiveran appropriate choice given that voice is transmitted at a constant rate between sender and receiver
Given the increasing importance of computers in the early s and the advent of timeshared computers it was perhaps natural to consider how to hook computers together so that they could be shared among geographically distributed users
The traffic generated by such users was likely to be burstyintervals of activity such as the sending of a command to a remote computer followed by periods of inactivity while waiting for a reply or while contemplating the received response
Three research groups around the world each unaware of the others work Leiner began inventing packet switching as an efficient and robust alternative to circuit switching
The first published work on packetswitching techniques was that of Leonard Kleinrock Kleinrock Kleinrock then a graduate student at MIT
Using queuing theory Kleinrocks work elegantly demonstrated the effectiveness of the packetswitching approach for bursty traffic sources
In Paul Baran Baran at the Rand Institute had begun investigating the use of packet switching for secure voice over military networks and at the National Physical Laboratory in England Donald Davies and Roger Scantlebury were also developing their ideas on packet switching
The work at MIT Rand and the NPL laid the foundations for todays Internet
But the Internet also has a long history of a letsbuilditanddemonstrateit attitude that also dates back to the s
Licklider DEC and Lawrence Roberts both colleagues of Kleinrocks at MIT went on to lead the computer science program at the Advanced Research Projects Agency ARPA in the United States
Roberts published an overall plan for the ARPAnet Roberts the first packetswitched computer network and a direct ancestor of todays public Internet
On Labor Day in the first packet switch was installed at UCLA under Kleinrocks supervision and three additional packet switches were installed shortly thereafter at the Stanford Research Institute SRI UC Santa Barbara and the University of Utah Figure 
The fledgling precursor to the Internet was four nodes large by the end of 
Kleinrock recalls the very first use of the network to perform a remote login from UCLA to SRI crashing the system Kleinrock 
By ARPAnet had grown to approximately nodes and was given its first public demonstration by Robert Kahn
The first hosttohost protocol between ARPAnet end systems known as the network control protocol NCP was completed RFC 
With an endtoend protocol available applications could now be written
Ray Tomlinson wrote the first email program in 
Proprietary Networks and Internetworking The initial ARPAnet was a single closed network
In order to communicate with an ARPAnet host one had to be actually attached to another ARPAnet IMP
In the early to mids additional standalone packetswitching networks besides ARPAnet came into being ALOHANet a microwave network linking universities on the Hawaiian islands Abramson as well as DARPAs packetsatellite RFC Figure 
An early packet switch and packetradio networks Kahn Telenet a BBN commercial packetswitching network based on ARPAnet technology Cyclades a French packetswitching network pioneered by Louis Pouzin Think Timesharing networks such as Tymnet and the GE Information Services network among others in the late s and early s Schwartz IBMs SNA which paralleled the ARPAnet work Schwartz 
The number of networks was growing
With perfect hindsight we can see that the time was ripe for developing an encompassing architecture for connecting networks together
Pioneering work on interconnecting networks under the sponsorship of the Defense Advanced Research Projects Agency DARPA in essence creating a network of networks was done by Vinton Cerf and Robert Kahn Cerf the term internetting was coined to describe this work
These architectural principles were embodied in TCP
The early versions of TCP however were quite different from todays TCP
The early versions of TCP combined a reliable insequence delivery of data via endsystem retransmission still part of todays TCP with forwarding functions which today are performed by IP
Early experimentation with TCP combined with the recognition of the importance of an unreliable nonflowcontrolled endtoend transport service for applications such as packetized voice led to the separation of IP out of TCP and the development of the UDP protocol
The three key Internet protocols that we see todayTCP UDP and IPwere conceptually in place by the end of the s
In addition to the DARPA Internetrelated research many other important networking activities were underway
In Hawaii Norman Abramson was developing ALOHAnet a packetbased radio network that allowed multiple remote sites on the Hawaiian Islands to communicate with each other
The ALOHA protocol Abramson was the first multipleaccess protocol allowing geographically distributed users to share a single broadcast communication medium a radio frequency
Metcalfe and Boggs built on Abramsons multipleaccess protocol work when they developed the Ethernet protocol Metcalfe for wirebased shared broadcast networks
Interestingly Metcalfe and Boggs Ethernet protocol was motivated by the need to connect multiple PCs printers and shared disks Perkins 
Twenty five years ago well before the PC revolution and the explosion of networks Metcalfe and Boggs were laying the foundation for todays PC LANs
A Proliferation of Networks By the end of the s approximately two hundred hosts were connected to the ARPAnet
By the end of the s the number of hosts connected to the public Internet a confederation of networks looking much like todays Internet would reach a hundred thousand
The s would be a time of tremendous growth
Much of that growth resulted from several distinct efforts to create computer networks linking universities together
BITNET provided email and file transfers among several universities in the Northeast
CSNET computer science network was formed to link university researchers who did not have access to ARPAnet
In NSFNET was created to provide access to NSFsponsored supercomputing centers
Starting with an initial backbone speed of kbps NSFNETs backbone would be running at 
Mbps by the end of the decade and would serve as a primary backbone linking regional networks
In the ARPAnet community many of the final pieces of todays Internet architecture were falling into place
January saw the official deployment of TCPIP as the new standard host protocol for ARPAnet replacing the NCP protocol
The transition RFC from NCP to TCPIP was a flag day eventall hosts were required to transfer over to TCPIP as of that day
In the late s important extensions were made to TCP to implement hostbased congestion control Jacobson 
The DNS used to map between a humanreadable Internet name for example gaia.cs.umass.edu and its bit IP address was also developed RFC 
Paralleling this development of the ARPAnet which was for the most part a US effort in the early s the French launched the Minitel project an ambitious plan to bring data networking into everyones home
Sponsored by the French government the Minitel system consisted of a public packetswitched network based on the X
protocol suite Minitel servers and inexpensive terminals with builtin lowspeed modems
The Minitel became a huge success in when the French government gave away a free Minitel terminal to each French household that wanted one
Minitel sites included free sitessuch as a telephone directory siteas well as private sites which collected a usagebased fee from each user
At its peak in the mid s it offered more than services ranging from home banking to specialized research databases
The Minitel was in a large proportion of French homes years before most Americans had ever heard of the Internet
The Internet Explosion The s The s were ushered in with a number of events that symbolized the continued evolution and the soontoarrive commercialization of the Internet
ARPAnet the progenitor of the Internet ceased to exist
In NSFNET lifted its restrictions on the use of NSFNET for commercial purposes
NSFNET itself would be decommissioned in with Internet backbone traffic being carried by commercial Internet Service Providers
The main event of the s was to be the emergence of the World Wide Web application which brought the Internet into the homes and businesses of millions of people worldwide
The Web served as a platform for enabling and deploying hundreds of new applications that we take for granted today including search e.g
Google and Bing Internet commerce e.g
Amazon and eBay and social networks e.g
The Web was invented at CERN by Tim BernersLee between and BernersLee based on ideas originating in earlier work on hypertext from the s by Vannevar Bush Bush and since the s by Ted Nelson Xanadu 
BernersLee and his associates developed initial versions of HTML HTTP a Web server and a browserthe four key components of the Web
Around the end of there were about two hundred Web servers in operation this collection of servers being just a harbinger of what was about to come
At about this time several researchers were developing Web browsers with GUI interfaces including Marc Andreessen who along with Jim Clark formed Mosaic Communications which later became Netscape Communications Corporation Cusumano Quittner 
By university students were using Netscape browsers to surf the Web on a daily basis
At about this time companiesbig and smallbegan to operate Web servers and transact commerce over the Web
In Microsoft started to make browsers which started the browser war between Netscape and Microsoft which Microsoft won a few years later Cusumano 
The second half of the s was a period of tremendous growth and innovation for the Internet with major corporations and thousands of startups creating Internet products and services
By the end of the millennium the Internet was supporting hundreds of popular applications including four killer applications Email including attachments and Webaccessible email The Web including Web browsing and Internet commerce Instant messaging with contact lists Peertopeer file sharing of MPs pioneered by Napster Interestingly the first two killer applications came from the research community whereas the last two were created by a few young entrepreneurs
The period from to was a rollercoaster ride for the Internet in the financial markets
Before they were even profitable hundreds of Internet startups made initial public offerings and started to be traded in a stock market
Many companies were valued in the billions of dollars without having any significant revenue streams
The Internet stocks collapsed in and many startups shut down
Nevertheless a number of companies emerged as big winners in the Internet space including Microsoft Cisco Yahoo eBay Google and Amazon
The New Millennium Innovation in computer networking continues at a rapid pace
Advances are being made on all fronts including deployments of faster routers and higher transmission speeds in both access networks and in network backbones
But the following developments merit special attention Since the beginning of the millennium we have been seeing aggressive deployment of broadband Internet access to homesnot only cable modems and DSL but also fiber to the home as discussed in Section 
This highspeed Internet access has set the stage for a wealth of video applications including the distribution of usergenerated video for example YouTube ondemand streaming of movies and television shows e.g
Netflix and multiperson video conference e.g
Skype Facetime and Google Hangouts
The increasing ubiquity of highspeed Mbps and higher public WiFi networks and medium speed tens of Mbps Internet access via G cellular telephony networks is not only making it possible to remain constantly connected while on the move but also enabling new locationspecific applications such as Yelp Tinder Yik Yak and Waz
The number of wireless devices connecting to the Internet surpassed the number of wired devices in 
This highspeed wireless access has set the stage for the rapid emergence of handheld computers iPhones Androids iPads and so on which enjoy constant and untethered access to the Internet
Online social networkssuch as Facebook Instagram Twitter and WeChat hugely popular in Chinahave created massive people networks on top of the Internet
Many of these social networks are extensively used for messaging as well as photo sharing
Many Internet users today live primarily within one or more social networks
Through their APIs the online social networks create platforms for new networked applications and distributed games
As discussed in Section 
online service providers such as Google and Microsoft have deployed their own extensive private networks which not only connect together their globally distributed data centers but are used to bypass the Internet as much as possible by peering directly with lowertier ISPs
As a result Google provides search results and email access almost instantaneously as if their data centers were running within ones own computer
Many Internet commerce companies are now running their applications in the cloudsuch as in Amazons EC in Googles Application Engine or in Microsofts Azure
Many companies and universities have also migrated their Internet applications e.g
email and Web hosting to the cloud
Cloud companies not only provide applications scalable computing and storage environments but also provide the applications implicit access to their highperformance private networks
Summary In this chapter weve covered a tremendous amount of material Weve looked at the various pieces of hardware and software that make up the Internet in particular and computer networks in general
We started at the edge of the network looking at end systems and applications and at the transport service provided to the applications running on the end systems
We also looked at the linklayer technologies and physical media typically found in the access network
We then dove deeper inside the network into the network core identifying packet switching and circuit switching as the two basic approaches for transporting data through a telecommunication network and we examined the strengths and weaknesses of each approach
We also examined the structure of the global Internet learning that the Internet is a network of networks
We saw that the Internets hierarchical structure consisting of higher and lowertier ISPs has allowed it to scale to include thousands of networks
In the second part of this introductory chapter we examined several topics central to the field of computer networking
We first examined the causes of delay throughput and packet loss in a packet switched network
We developed simple quantitative models for transmission propagation and queuing delays as well as for throughput well make extensive use of these delay models in the homework problems throughout this book
Next we examined protocol layering and service models key architectural principles in networking that we will also refer back to throughout this book
We also surveyed some of the more prevalent security attacks in the Internet day
We finished our introduction to networking with a brief history of computer networking
The first chapter in itself constitutes a mini course in computer networking
So we have indeed covered a tremendous amount of ground in this first chapter If youre a bit overwhelmed dont worry
In the following chapters well revisit all of these ideas covering them in much more detail thats a promise not a threat
At this point we hope you leave this chapter with a stilldeveloping intuition for the pieces that make up a network a stilldeveloping command of the vocabulary of networking dont be shy about referring back to this chapter and an evergrowing desire to learn more about networking
Thats the task ahead of us for the rest of this book
RoadMapping This Book Before starting any trip you should always glance at a road map in order to become familiar with the major roads and junctures that lie ahead
For the trip we are about to embark on the ultimate destination is a deep understanding of the how what and why of computer networks
Our road map is the sequence of chapters of this book 
Computer Networks and the Internet 
Application Layer 
Transport Layer 
Network Layer Data Plane 
Network Layer Control Plane 
The Link Layer and LANs 
Wireless and Mobile Networks 
Security in Computer Networks 
Multimedia Networking Chapters through are the five core chapters of this book
You should notice that these chapters are organized around the top four layers of the fivelayer Internet protocol
Further note that our journey will begin at the top of the Internet protocol stack namely the application layer and will work its way downward
The rationale behind this topdown journey is that once we understand the applications we can understand the network services needed to support these applications
We can then in turn examine the various ways in which such services might be implemented by a network architecture
Covering applications early thus provides motivation for the remainder of the text
The second half of the bookChapters through zooms in on three enormously important and somewhat independent topics in modern computer networking
In Chapter we examine wireless and mobile networks including wireless LANs including WiFi and Bluetooth Cellular telephony networks including GSM G and G and mobility in both IP and GSM networks
Chapter which addresses security in computer networks first looks at the underpinnings of encryption and network security and then we examine how the basic theory is being applied in a broad range of Internet contexts
The last chapter which addresses multimedia networking examines audio and video applications such as Internet phone video conferencing and streaming of stored media
We also look at how a packet switched network can be designed to provide consistent quality of service to audio and video applications
Homework Problems and Questions Chapter Review Questions SECTION 
What is the difference between a host and an end system List several different types of end systems
Is a Web server an end system R
The word protocol is often used to describe diplomatic relations
How does Wikipedia describe diplomatic protocol R
Why are standards important for protocols SECTION 
List six access technologies
Classify each one as home access enterprise access or wide area wireless access
Is HFC transmission rate dedicated or shared among users Are collisions possible in a downstream HFC channel Why or why not R
List the available residential access technologies in your city
For each type of access provide the advertised downstream rate upstream rate and monthly price
What is the transmission rate of Ethernet LANs R
What are some of the physical media that Ethernet can run over R
Dialup modems HFC DSL and FTTH are all used for residential access
For each of these access technologies provide a range of transmission rates and comment on whether the transmission rate is shared or dedicated
Describe the most popular wireless Internet access technologies today
Compare and contrast them
Suppose there is exactly one packet switch between a sending host and a receiving host
The transmission rates between the sending host and the switch and between the switch and the receiving host are R and R respectively
Assuming that the switch uses storeandforward packet switching what is the total endtoend delay to send a packet of length L Ignore queuing propagation delay and processing delay
What advantage does a circuitswitched network have over a packetswitched network What advantages does TDM have over FDM in a circuitswitched network R
Suppose users share a Mbps link
Also suppose each user transmits continuously at Mbps when transmitting but each user transmits only percent of the time
See the discussion of statistical multiplexing in Section 
When circuit switching is used how many users can be supported b
For the remainder of this problem suppose packet switching is used
Why will there be essentially no queuing delay before the link if two or fewer users transmit at the same time Why will there be a queuing delay if three users transmit at the same time c
Find the probability that a given user is transmitting
Suppose now there are three users
Find the probability that at any given time all three users are transmitting simultaneously
Find the fraction of time during which the queue grows
Why will two ISPs at the same level of the hierarchy often peer with each other How does an IXP earn money R
Some content providers have created their own networks
Describe Googles network
What motivates content providers to create these networks SECTION 
Consider sending a packet from a source host to a destination host over a fixed route
List the delay components in the endtoend delay
Which of these delays are constant and which are variable R
Visit the Transmission Versus Propagation Delay applet at the companion Web site
Among the rates propagation delay and packet sizes available find a combination for which the sender finishes transmitting before the first bit of the packet reaches the receiver
Find another combination for which the first bit of the packet reaches the receiver before the sender finishes transmitting
How long does it take a packet of length bytes to propagate over a link of distance km propagation speed 
ms and transmission rate Mbps More generally how long does it take a packet of length L to propagate over a link of distance d propagation speed s and transmission rate R bps Does this delay depend on packet length Does this delay depend on transmission rate R
Suppose Host A wants to send a large file to Host B
The path from Host A to Host B has three links of rates R kbps R Mbps and R Mbps
Assuming no other traffic in the network what is the throughput for the file transfer b
Suppose the file is million bytes
Dividing the file size by the throughput roughly how long will it take to transfer the file to Host B c
Repeat a and b but now with R reduced to kbps
Suppose end system A wants to send a large file to end system B
At a very high level describe how end system A creates packets from the file
When one of these packets arrives to a router what information in the packet does the router use to determine the link onto which the packet is forwarded Why is packet switching in the Internet analogous to driving from one city to another and asking directions along the way R
Visit the Queuing and Loss applet at the companion Web site
What is the maximum emission rate and the minimum transmission rate With those rates what is the traffic intensity Run the applet with these rates and determine how long it takes for packet loss to occur
Then repeat the experiment a second time and determine again how long it takes for packet loss to occur
Are the values different Why or why not SECTION 
List five tasks that a layer can perform
Is it possible that one or more of these tasks could be performed by two or more layers R
What are the five layers in the Internet protocol stack What are the principal responsibilities of each of these layers R
What is an applicationlayer message A transportlayer segment A networklayer datagram A linklayer frame R
Which layers in the Internet protocol stack does a router process Which layers does a linklayer switch process Which layers does a host process SECTION 
What is the difference between a virus and a worm R
Describe how a botnet can be created and how it can be used for a DDoS attack
Suppose Alice and Bob are sending packets to each other over a computer network
Suppose Trudy positions herself in the network so that she can capture all the packets sent by Alice and send whatever she wants to Bob she can also capture all the packets sent by Bob and send whatever she wants to Alice
List some of the malicious things Trudy can do from this position
Design and describe an applicationlevel protocol to be used between an automatic teller machine and a banks centralized computer
Your protocol should allow a users card and password to be verified the account balance which is maintained at the centralized computer to be queried and an account withdrawal to be made that is money disbursed to the user
Your protocol entities should be able to handle the alltoocommon case in which there is not enough money in the account to cover the withdrawal
Specify your protocol by listing the messages exchanged and the action taken by the automatic teller machine or the banks centralized computer on transmission and receipt of messages
Sketch the operation of your protocol for the case of a simple withdrawal with no errors using a diagram similar to that in Figure 
Explicitly state the assumptions made by your protocol about the underlying endto end transport service
gives a formula for the endtoend delay of sending one packet of length L over N links of transmission rate R
Generalize this formula for sending P such packets backto back over the N links
Consider an application that transmits data at a steady rate for example the sender generates an Nbit unit of data every k time units where k is small and fixed
Also when such an application starts it will continue running for a relatively long period of time
Answer the following questions briefly justifying your answer a
Would a packetswitched network or a circuitswitched network be more appropriate for this application Why b
Suppose that a packetswitched network is used and the only traffic in this network comes from such applications as described above
Furthermore assume that the sum of the application data rates is less than the capacities of each and every link
Is some form of congestion control needed Why P
Consider the circuitswitched network in Figure 
Recall that there are circuits on each link
Label the four switches A B C and D going in the clockwise direction
What is the maximum number of simultaneous connections that can be in progress at any one time in this network b
Suppose that all connections are between switches A and C
What is the maximum number of simultaneous connections that can be in progress c
Suppose we want to make four connections between switches A and C and another four connections between switches B and D
Can we route these calls through the four links to accommodate all eight connections P
Review the carcaravan analogy in Section 
Assume a propagation speed of kmhour
Suppose the caravan travels km beginning in front of one tollbooth passing through a second tollbooth and finishing just after a third tollbooth
What is the endtoend delay b
Repeat a now assuming that there are eight cars in the caravan instead of ten
This elementary problem begins to explore propagation delay and transmission delay two central concepts in data networking
Consider two hosts A and B connected by a single link of rate R bps
Suppose that the two hosts are separated by m meters and suppose the propagation speed along the link is s meterssec
Host A is to send a packet of size L bits to Host B
Exploring propagation delay and transmission delay a
Express the propagation delay dprop in terms of m and s
Determine the transmission time of the packet dtrans in terms of L and R
Ignoring processing and queuing delays obtain an expression for the endtoend delay
Suppose Host A begins to transmit the packet at time t
At time t dtrans where is the last bit of the packet e
Suppose dprop is greater than dtrans
At time tdtrans where is the first bit of the packet f
Suppose dprop is less than dtrans
At time tdtrans where is the first bit of the packet g
L bits and R kbps
Find the distance m so that dprop equals dtrans
In this problem we consider sending realtime voice from Host A to Host B over a packet switched network VoIP
Host A converts analog voice to a digital kbps bit stream on the fly
Host A then groups the bits into byte packets
There is one link between Hosts A and B its transmission rate is Mbps and its propagation delay is msec
As soon as Host A gathers a packet it sends it to Host B
As soon as Host B receives an entire packet it converts the packets bits to an analog signal
How much time elapses from the time a bit is created from the original analog signal at Host A until the bit is decoded as part of the analog signal at Host B P
Suppose users share a Mbps link
Also suppose each user requires kbps when transmitting but each user transmits only percent of the time
See the discussion of packet switching versus circuit switching in Section 
When circuit switching is used how many users can be supported b
For the remainder of this problem suppose packet switching is used
Find the probability that a given user is transmitting
Suppose there are users
Find the probability that at any given time exactly n users are transmitting simultaneously
Hint Use the binomial distribution
Find the probability that there are or more users transmitting simultaneously
Consider the discussion in Section 
of packet switching versus circuit switching in which an example is provided with a Mbps link
Users are generating data at a rate of kbps when busy but are busy generating data only with probability p
Suppose that the Mbps link is replaced by a Gbps link
What is N the maximum number of users that can be supported simultaneously under circuit switching b
Now consider packet switching and a user population of M users
Give a formula in terms of p M N for the probability that more than N users are sending data
Consider a packet of length L that begins at end system A and travels over three links to a destination end system
These three links are connected by two packet switches
Let di si and Ri denote the length propagation speed and the transmission rate of link i for i
The packet switch delays each packet by dproc
Assuming no queuing delays in terms of di si Ri i and L what is the total endtoend delay for the packet Suppose now the packet is bytes the propagation speed on all three links is .ms the transmission rates of all three links are Mbps the packet switch processing delay is msec the length of the first link is km the length of the second link is km and the length of the last link is km
For these values what is the endtoend delay P
In the above problem suppose RRRR and dproc
Further suppose the packet switch does not storeandforward packets but instead immediately transmits each bit it receives before waiting for the entire packet to arrive
What is the endtoend delay P
A packet switch receives a packet and determines the outbound link to which the packet should be forwarded
When the packet arrives one other packet is halfway done being transmitted on this outbound link and four other packets are waiting to be transmitted
Packets are transmitted in order of arrival
Suppose all packets are bytes and the link rate is Mbps
What is the queuing delay for the packet More generally what is the queuing delay when all packets have length L the transmission rate is R x bits of the currentlybeingtransmitted packet have been transmitted and n packets are already in the queue P
Suppose N packets arrive simultaneously to a link at which no packets are currently being transmitted or queued
Each packet is of length L and the link has transmission rate R
What is the average queuing delay for the N packets b
Now suppose that N such packets arrive to the link every LNR seconds
What is the average queuing delay of a packet P
Consider the queuing delay in a router buffer
Let I denote traffic intensity that is ILaR
Suppose that the queuing delay takes the form ILRI for I
Provide a formula for the total delay that is the queuing delay plus the transmission delay
Plot the total delay as a function of L R
Let a denote the rate of packets arriving at a link in packetssec and let Âµ denote the links transmission rate in packetssec
Based on the formula for the total delay i.e
the queuing delay plus the transmission delay derived in the previous problem derive a formula for the total delay in terms of a and Âµ
Consider a router buffer preceding an outbound link
In this problem you will use Littles formula a famous formula from queuing theory
Let N denote the average number of packets in the buffer plus the packet being transmitted
Let a denote the rate of packets arriving at the link
Let d denote the average total delay i.e
the queuing delay plus the transmission delay experienced by a packet
Littles formula is Nad
Suppose that on average the buffer contains packets and the average packet queuing delay is msec
The links transmission rate is packetssec
Using Littles formula what is the average packet arrival rate assuming there is no packet loss P
Generalize Equation 
in Section 
for heterogeneous processing rates transmission rates and propagation delays
Repeat a but now also suppose that there is an average queuing delay of dqueue at each node
Perform a Traceroute between source and destination on the same continent at three different hours of the day
Using Traceroute to discover network paths and measure network delay a
Find the average and standard deviation of the roundtrip delays at each of the three hours
Find the number of routers in the path at each of the three hours
Did the paths change during any of the hours c
Try to identify the number of ISP networks that the Traceroute packets pass through from source to destination
Routers with similar names andor similar IP addresses should be considered as part of the same ISP
In your experiments do the largest delays occur at the peering interfaces between adjacent ISPs d
Repeat the above for a source and destination on different continents
Compare the intracontinent and intercontinent results
Visit the site www.traceroute.org and perform traceroutes from two different cities in France to the same destination host in the United States
How many links are the same in the two traceroutes Is the transatlantic link the same b
Repeat a but this time choose one city in France and another city in Germany
Pick a city in the United States and perform traceroutes to two hosts each in a different city in China
How many links are common in the two traceroutes Do the two traceroutes diverge before reaching China P
Consider the throughput example corresponding to Figure .b 
Now suppose that there are M clientserver pairs rather than 
Denote Rs Rc and R for the rates of the server links client links and network link
Assume all other links have abundant capacity and that there is no other traffic in the network besides the traffic generated by the M clientserver pairs
Derive a general expression for throughput in terms of Rs Rc R and M
Consider Figure .b 
Now suppose that there are M paths between the server and the client
No two paths share any link
Path kkM consists of N links with transmission rates RkRkRNk
If the server can only use one path to send data to the client what is the maximum throughput that the server can achieve If the server can use all M paths to send data what is the maximum throughput that the server can achieve P
Consider Figure .b 
Suppose that each link between the server and the client has a packet loss probability p and the packet loss probabilities for these links are independent
What is the probability that a packet sent by the server is successfully received by the receiver If a packet is lost in the path from the server to the client then the server will retransmit the packet
On average how many times will the server retransmit the packet in order for the client to successfully receive the packet P
Consider Figure .a 
Assume that we know the bottleneck link along the path from the server to the client is the first link with rate Rs bitssec
Suppose we send a pair of packets back to back from the server to the client and there is no other traffic on this path
Assume each packet of size L bits and both links have the same propagation delay dprop
What is the packet interarrival time at the destination That is how much time elapses from when the last bit of the first packet arrives until the last bit of the second packet arrives b
Now assume that the second link is the bottleneck link i.e
Is it possible that the second packet queues at the input queue of the second link Explain
Now suppose that the server sends the second packet T seconds after sending the first packet
How large must T be to ensure no queuing before the second link Explain
Suppose you would like to urgently deliver terabytes data from Boston to Los Angeles
You have available a Mbps dedicated link for data transfer
Would you prefer to transmit the data via this link or instead use FedEx overnight delivery Explain
Suppose two hosts A and B are separated by kilometers and are connected by a direct link of R Mbps
Suppose the propagation speed over the link is 
Calculate the bandwidthdelay product Rdprop
Consider sending a file of bits from Host A to Host B
Suppose the file is sent continuously as one large message
What is the maximum number of bits that will be in the link at any given time c
Provide an interpretation of the bandwidthdelay product
What is the width in meters of a bit in the link Is it longer than a football field e
Derive a general expression for the width of a bit in terms of the propagation speed s the transmission rate R and the length of the link m
Referring to problem P suppose we can modify R
For what value of R is the width of a bit as long as the length of the link P
Consider problem P but now with a link of R Gbps
Calculate the bandwidthdelay product Rdprop
Consider sending a file of bits from Host A to Host B
Suppose the file is sent continuously as one big message
What is the maximum number of bits that will be in the link at any given time c
What is the width in meters of a bit in the link P
Refer again to problem P
How long does it take to send the file assuming it is sent continuously b
Suppose now the file is broken up into packets with each packet containing bits
Suppose that each packet is acknowledged by the receiver and the transmission time of an acknowledgment packet is negligible
Finally assume that the sender cannot send a packet until the preceding one is acknowledged
How long does it take to send the file c
Compare the results from a and b
Suppose there is a Mbps microwave link between a geostationary satellite and its base station on Earth
Every minute the satellite takes a digital photo and sends it to the base station
Assume a propagation speed of 
What is the propagation delay of the link b
What is the bandwidthdelay product Rdprop c
Let x denote the size of the photo
What is the minimum value of x for the microwave link to be continuously transmitting P
Consider the airline travel analogy in our discussion of layering in Section 
and the addition of headers to protocol data units as they flow down the protocol stack
Is there an equivalent notion of header information that is added to passengers and baggage as they move down the airline protocol stack P
In modern packetswitched networks including the Internet the source host segments long applicationlayer messages for example an image or a music file into smaller packets and sends the packets into the network
The receiver then reassembles the packets back into the original message
We refer to this process as message segmentation
illustrates the endtoend transport of a message with and without message segmentation
Consider a message that is bits long that is to be sent from source to destination in Figure 
Suppose each link in the figure is Mbps
Ignore propagation queuing and processing delays
Consider sending the message from source to destination without message segmentation
How long does it take to move the message from the source host to the first packet switch Keeping in mind that each switch uses storeandforward packet switching what is the total time to move the message from source host to destination host b
Now suppose that the message is segmented into packets with each packet being bits long
How long does it take to move the first packet from source host to the first switch When the first packet is being sent from the first switch to the second switch the second packet is being sent from the source host to the first switch
At what time will the second packet be fully received at the first switch c
How long does it take to move the file from source host to destination host when message segmentation is used Compare this result with your answer in part a and comment
Endtoend message transport a without message segmentation b with message segmentation d
In addition to reducing delay what are reasons to use message segmentation e
Discuss the drawbacks of message segmentation
Experiment with the Message Segmentation applet at the books Web site
Do the delays in the applet correspond to the delays in the previous problem How do link propagation delays affect the overall endtoend delay for packet switching with message segmentation and for message switching P
Consider sending a large file of F bits from Host A to Host B
There are three links and two switches between A and B and the links are uncongested that is no queuing delays
Host A segments the file into segments of S bits each and adds bits of header to each segment forming packets of L S bits
Each link has a transmission rate of R bps
Find the value of S that minimizes the delay of moving the file from Host A to Host B
Disregard propagation delay
Skype offers a service that allows you to make a phone call from a PC to an ordinary phone
This means that the voice call must pass through both the Internet and through a telephone network
Discuss how this might be done
Wireshark Lab Tell me and I forget
Show me and I remember
Involve me and I understand
Chinese proverb Ones understanding of network protocols can often be greatly deepened by seeing them in action and by playing around with themobserving the sequence of messages exchanged between two protocol entities delving into the details of protocol operation causing protocols to perform certain actions and observing these actions and their consequences
This can be done in simulated scenarios or in a real network environment such as the Internet
The Java applets at the textbook Web site take the first approach
In the Wireshark labs well take the latter approach
Youll run network applications in various scenarios using a computer on your desk at home or in a lab
Youll observe the network protocols in your computer interacting and exchanging messages with protocol entities executing elsewhere in the Internet
Thus you and your computer will be an integral part of these live labs
Youll observeand youll learnby doing
The basic tool for observing the messages exchanged between executing protocol entities is called a packet sniffer
As the name suggests a packet sniffer passively copies sniffs messages being sent from and received by your computer it also displays the contents of the various protocol fields of these captured messages
A screenshot of the Wireshark packet sniffer is shown in Figure 
Wireshark is a free packet sniffer that runs on Windows LinuxUnix and Mac computers
A Wireshark screenshot Wireshark screenshot reprinted by permission of the Wireshark Foundation
Throughout the textbook you will find Wireshark labs that allow you to explore a number of the protocols studied in the chapter
In this first Wireshark lab youll obtain and install a copy of Wireshark access a Web site and capture and examine the protocol messages being exchanged between your Web browser and the Web server
You can find full details about this first Wireshark lab including instructions about how to obtain and install Wireshark at the Web site httpwww.pearsonhighered.comcs resources
AN INTERVIEW WITH Leonard Kleinrock Leonard Kleinrock is a professor of computer science at the University of California Los Angeles
In his computer at UCLA became the first node of the Internet
His creation of packetswitching principles in became the technology behind the Internet
He received his B.E.E
from the City College of New York CCNY and his masters and PhD in electrical engineering from MIT
What made you decide to specialize in networkingInternet technology As a PhD student at MIT in I looked around and found that most of my classmates were doing research in the area of information theory and coding theory
At MIT there was the great researcher Claude Shannon who had launched these fields and had solved most of the important problems already
The research problems that were left were hard and of lesser consequence
So I decided to launch out in a new area that no one else had yet conceived of
Remember that at MIT I was surrounded by lots of computers and it was clear to me that soon these machines would need to communicate with each other
At the time there was no effective way for them to do so so I decided to develop the technology that would permit efficient and reliable data networks to be created
What was your first job in the computer industry What did it entail I went to the evening session at CCNY from to for my bachelors degree in electrical engineering
During the day I worked first as a technician and then as an engineer at a small industrial electronics firm called Photobell
While there I introduced digital technology to their product line
Essentially we were using photoelectric devices to detect the presence of certain items boxes people etc
and the use of a circuit known then as a bistable multivibrator was just the kind of technology we needed to bring digital processing into this field of detection
These circuits happen to be the building blocks for computers and have come to be known as flipflops or switches in todays vernacular
What was going through your mind when you sent the first hosttohost message from UCLA to the Stanford Research Institute Frankly we had no idea of the importance of that event
We had not prepared a special message of historic significance as did so many inventors of the past Samuel Morse with What hath God wrought
or Alexander Graham Bell with Watson come here I want you
or Neal Amstrong with Thats one small step for a man one giant leap for mankind
Those guys were smart They understood media and public relations
All we wanted to do was to login to the SRI computer
So we typed the L which was correctly received we typed the o which was received and then we typed the g which caused the SRI host computer to crash So it turned out that our message was the shortest and perhaps the most prophetic message ever namely Lo as in Lo and behold Earlier that year I was quoted in a UCLA press release saying that once the network was up and running it would be possible to gain access to computer utilities from our homes and offices as easily as we gain access to electricity and telephone connectivity
So my vision at that time was that the Internet would be ubiquitous always on always available anyone with any device could connect from any location and it would be invisible
However I never anticipated that my yearold mother would use the Internetand indeed she did What is your vision for the future of networking The easy part of the vision is to predict the infrastructure itself
I anticipate that we see considerable deployment of nomadic computing mobile devices and smart spaces
Indeed the availability of lightweight inexpensive highperformance portable computing and communication devices plus the ubiquity of the Internet has enabled us to become nomads
Nomadic computing refers to the technology that enables end users who travel from place to place to gain access to Internet services in a transparent fashion no matter where they travel and no matter what device they carry or gain access to
The harder part of the vision is to predict the applications and services which have consistently surprised us in dramatic ways email search technologies the World Wide Web blogs social networks user generation and sharing of music photos and videos etc
We are on the verge of a new class of surprising and innovative mobile applications delivered to our handheld devices
The next step will enable us to move out from the netherworld of cyberspace to the physical world of smart spaces
Our environments desks walls vehicles watches belts and so on will come alive with technology through actuators sensors logic processing storage cameras microphones speakers displays and communication
This embedded technology will allow our environment to provide the IP services we want
When I walk into a room the room will know I entered
I will be able to communicate with my environment naturally as in spoken English my requests will generate replies that present Web pages to me from wall displays through my eyeglasses as speech holograms and so forth
Looking a bit further out I see a networking future that includes the following additional key components
I see intelligent software agents deployed across the network whose function it is to mine data act on that data observe trends and carry out tasks dynamically and adaptively
I see considerably more network traffic generated not so much by humans but by these embedded devices and these intelligent software agents
I see large collections of self organizing systems controlling this vast fast network
I see huge amounts of information flashing across this network instantaneously with this information undergoing enormous processing and filtering
The Internet will essentially be a pervasive global nervous system
I see all these things and more as we move headlong through the twentyfirst century
What people have inspired you professionally By far it was Claude Shannon from MIT a brilliant researcher who had the ability to relate his mathematical ideas to the physical world in highly intuitive ways
He was on my PhD thesis committee
Do you have any advice for students entering the networkingInternet field The Internet and all that it enables is a vast new frontier full of amazing challenges
There is room for great innovation
Dont be constrained by todays technology
Reach out and imagine what could be and then make it happen
Chapter Application Layer Network applications are the raisons dÃªtre of a computer networkif we couldnt conceive of any useful applications there wouldnt be any need for networking infrastructure and protocols to support them
Since the Internets inception numerous useful and entertaining applications have indeed been created
These applications have been the driving force behind the Internets success motivating people in homes schools governments and businesses to make the Internet an integral part of their daily activities
Internet applications include the classic textbased applications that became popular in the s and s text email remote access to computers file transfers and newsgroups
They include the killer application of the mids the World Wide Web encompassing Web surfing search and electronic commerce
They include instant messaging and PP file sharing the two killer applications introduced at the end of the millennium
In the new millennium new and highly compelling applications continue to emerge including voice over IP and video conferencing such as Skype Facetime and Google Hangouts user generated video such as YouTube and movies on demand such as Netflix multiplayer online games such as Second Life and World of Warcraft
During this same period we have seen the emergence of a new generation of social networking applicationssuch as Facebook Instagram Twitter and WeChatwhich have created engaging human networks on top of the Internets network or routers and communication links
And most recently along with the arrival of the smartphone there has been a profusion of location based mobile apps including popular checkin dating and roadtraffic forecasting apps such as Yelp Tinder Waz and Yik Yak
Clearly there has been no slowing down of new and exciting Internet applications
Perhaps some of the readers of this text will create the next generation of killer Internet applications In this chapter we study the conceptual and implementation aspects of network applications
We begin by defining key applicationlayer concepts including network services required by applications clients and servers processes and transportlayer interfaces
We examine several network applications in detail including the Web email DNS peertopeer PP file distribution and video streaming
Chapter will further examine multimedia applications including streaming video and VoIP
We then cover network application development over both TCP and UDP
In particular we study the socket interface and walk through some simple clientserver applications in Python
We also provide several fun and interesting socket programming assignments at the end of the chapter
The application layer is a particularly good place to start our study of protocols
Its familiar ground
Were acquainted with many of the applications that rely on the protocols well study
It will give us a good feel for what protocols are all about and will introduce us to many of the same issues that well see again when we study transport network and link layer protocols
Principles of Network Applications Suppose you have an idea for a new network application
Perhaps this application will be a great service to humanity or will please your professor or will bring you great wealth or will simply be fun to develop
Whatever the motivation may be lets now examine how you transform the idea into a realworld network application
At the core of network application development is writing programs that run on different end systems and communicate with each other over the network
For example in the Web application there are two distinct programs that communicate with each other the browser program running in the users host desktop laptop tablet smartphone and so on and the Web server program running in the Web server host
As another example in a PP filesharing system there is a program in each host that participates in the filesharing community
In this case the programs in the various hosts may be similar or identical
Thus when developing your new application you need to write software that will run on multiple end systems
This software could be written for example in C Java or Python
Importantly you do not need to write software that runs on networkcore devices such as routers or linklayer switches
Even if you wanted to write application software for these networkcore devices you wouldnt be able to do so
As we learned in Chapter and as shown earlier in Figure 
networkcore devices do not function at the application layer but instead function at lower layersspecifically at the network layer and below
This basic designnamely confining application software to the end systemsas shown in Figure 
has facilitated the rapid development and deployment of a vast array of network applications
Communication for a network application takes place between end systems at the application layer 
Network Application Architectures Before diving into software coding you should have a broad architectural plan for your application
Keep in mind that an applications architecture is distinctly different from the network architecture e.g
the fivelayer Internet architecture discussed in Chapter 
From the application developers perspective the network architecture is fixed and provides a specific set of services to applications
The application architecture on the other hand is designed by the application developer and dictates how the application is structured over the various end systems
In choosing the application architecture an application developer will likely draw on one of the two predominant architectural paradigms used in modern network applications the clientserver architecture or the peertopeer PP architecture
In a clientserver architecture there is an alwayson host called the server which services requests from many other hosts called clients
A classic example is the Web application for which an alwayson Web server services requests from browsers running on client hosts
When a Web server receives a request for an object from a client host it responds by sending the requested object to the client host
Note that with the clientserver architecture clients do not directly communicate with each other for example in the Web application two browsers do not directly communicate
Another characteristic of the clientserver architecture is that the server has a fixed wellknown address called an IP address which well discuss soon
Because the server has a fixed wellknown address and because the server is always on a client can always contact the server by sending a packet to the servers IP address
Some of the betterknown applications with a clientserver architecture include the Web FTP Telnet and email
The clientserver architecture is shown in Figure .a
Often in a clientserver application a singleserver host is incapable of keeping up with all the requests from clients
For example a popular socialnetworking site can quickly become overwhelmed if it has only one server handling all of its requests
For this reason a data center housing a large number of hosts is often used to create a powerful virtual server
The most popular Internet servicessuch as search engines e.g
Google Bing Baidu Internet commerce e.g
Amazon eBay Alibaba Web based email e.g
Gmail and Yahoo Mail social networking e.g
Facebook Instagram Twitter and WeChatemploy one or more data centers
As discussed in Section 
Google has to data centers distributed around the world which collectively handle search YouTube Gmail and other services
A data center can have hundreds of thousands of servers which must be powered and maintained
Additionally the service providers must pay recurring interconnection and bandwidth costs for sending data from their data centers
In a PP architecture there is minimal or no reliance on dedicated servers in data centers
Instead the application exploits direct communication between pairs of intermittently connected hosts called peers
The peers are not owned by the service provider but are instead desktops and laptops controlled by users with most of the Figure 
a Clientserver architecture b PP architecture peers residing in homes universities and offices
Because the peers communicate without passing through a dedicated server the architecture is called peertopeer
Many of todays most popular and trafficintensive applications are based on PP architectures
These applications include file sharing e.g
BitTorrent peerassisted download acceleration e.g
Xunlei and Internet telephony and video conference e.g
The PP architecture is illustrated in Figure .b
We mention that some applications have hybrid architectures combining both clientserver and PP elements
For example for many instant messaging applications servers are used to track the IP addresses of users but userto user messages are sent directly between user hosts without passing through intermediate servers
One of the most compelling features of PP architectures is their selfscalability
For example in a PP filesharing application although each peer generates workload by requesting files each peer also adds service capacity to the system by distributing files to other peers
PP architectures are also cost effective since they normally dont require significant server infrastructure and server bandwidth in contrast with clientsserver designs with datacenters
However PP applications face challenges of security performance and reliability due to their highly decentralized structure
Processes Communicating Before building your network application you also need a basic understanding of how the programs running in multiple end systems communicate with each other
In the jargon of operating systems it is not actually programs but processes that communicate
A process can be thought of as a program that is running within an end system
When processes are running on the same end system they can communicate with each other with interprocess communication using rules that are governed by the end systems operating system
But in this book we are not particularly interested in how processes in the same host communicate but instead in how processes running on different hosts with potentially different operating systems communicate
Processes on two different end systems communicate with each other by exchanging messages across the computer network
A sending process creates and sends messages into the network a receiving process receives these messages and possibly responds by sending messages back
illustrates that processes communicating with each other reside in the application layer of the fivelayer protocol stack
Client and Server Processes A network application consists of pairs of processes that send messages to each other over a network
For example in the Web application a client browser process exchanges messages with a Web server process
In a PP filesharing system a file is transferred from a process in one peer to a process in another peer
For each pair of communicating processes we typically label one of the two processes as the client and the other process as the server
With the Web a browser is a client process and a Web server is a server process
With PP file sharing the peer that is downloading the file is labeled as the client and the peer that is uploading the file is labeled as the server
You may have observed that in some applications such as in PP file sharing a process can be both a client and a server
Indeed a process in a PP filesharing system can both upload and download files
Nevertheless in the context of any given communication session between a pair of processes we can still label one process as the client and the other process as the server
We define the client and server processes as follows In the context of a communication session between a pair of processes the process that initiates the communication that is initially contacts the other process at the beginning of the session is labeled as the client
The process that waits to be contacted to begin the session is the server
In the Web a browser process initializes contact with a Web server process hence the browser process is the client and the Web server process is the server
In PP file sharing when Peer A asks Peer B to send a specific file Peer A is the client and Peer B is the server in the context of this specific communication session
When theres no confusion well sometimes also use the terminology client side and server side of an application
At the end of this chapter well step through simple code for both the client and server sides of network applications
The Interface Between the Process and the Computer Network As noted above most applications consist of pairs of communicating processes with the two processes in each pair sending messages to each other
Any message sent from one process to another must go through the underlying network
A process sends messages into and receives messages from the network through a software interface called a socket
Lets consider an analogy to help us understand processes and sockets
A process is analogous to a house and its socket is analogous to its door
When a process wants to send a message to another process on another host it shoves the message out its door socket
This sending process assumes that there is a transportation infrastructure on the other side of its door that will transport the message to the door of the destination process
Once the message arrives at the destination host the message passes through the receiving processs door socket and the receiving process then acts on the message
illustrates socket communication between two processes that communicate over the Internet
assumes that the underlying transport protocol used by the processes is the Internets TCP protocol
As shown in this figure a socket is the interface between the application layer and the transport layer within a host
It is also referred to as the Application Programming Interface API between the application and the network since the socket is the programming interface with which network applications are built
The application developer has control of everything on the application layer side of the socket but has little control of the transportlayer side of the socket
The only control that the application developer has on the transportlayer side is the choice of transport protocol and perhaps the ability to fix a few transportlayer parameters such as maximum buffer and maximum segment sizes to be covered in Chapter 
Once the application developer chooses a transport protocol if a choice is available the application is built using the transportlayer services provided by that protocol
Well explore sockets in some detail in Section 
Addressing Processes In order to send postal mail to a particular destination the destination needs to have an address
Similarly in order for a process running on one host to send packets to a process running on another host the receiving process needs to have an address
Application processes sockets and underlying transport protocol To identify the receiving process two pieces of information need to be specified the address of the host and an identifier that specifies the receiving process in the destination host
In the Internet the host is identified by its IP address
Well discuss IP addresses in great detail in Chapter 
For now all we need to know is that an IP address is a bit quantity that we can think of as uniquely identifying the host
In addition to knowing the address of the host to which a message is destined the sending process must also identify the receiving process more specifically the receiving socket running in the host
This information is needed because in general a host could be running many network applications
A destination port number serves this purpose
Popular applications have been assigned specific port numbers
For example a Web server is identified by port number 
A mail server process using the SMTP protocol is identified by port number 
A list of wellknown port numbers for all Internet standard protocols can be found at www.iana.org
Well examine port numbers in detail in Chapter 
Transport Services Available to Applications Recall that a socket is the interface between the application process and the transportlayer protocol
The application at the sending side pushes messages through the socket
At the other side of the socket the transportlayer protocol has the responsibility of getting the messages to the socket of the receiving process
Many networks including the Internet provide more than one transportlayer protocol
When you develop an application you must choose one of the available transportlayer protocols
How do you make this choice Most likely you would study the services provided by the available transportlayer protocols and then pick the protocol with the services that best match your applications needs
The situation is similar to choosing either train or airplane transport for travel between two cities
You have to choose one or the other and each transportation mode offers different services
For example the train offers downtown pickup and dropoff whereas the plane offers shorter travel time
What are the services that a transportlayer protocol can offer to applications invoking it We can broadly classify the possible services along four dimensions reliable data transfer throughput timing and security
Reliable Data Transfer As discussed in Chapter packets can get lost within a computer network
For example a packet can overflow a buffer in a router or can be discarded by a host or router after having some of its bits corrupted
For many applicationssuch as electronic mail file transfer remote host access Web document transfers and financial applicationsdata loss can have devastating consequences in the latter case for either the bank or the customer
Thus to support these applications something has to be done to guarantee that the data sent by one end of the application is delivered correctly and completely to the other end of the application
If a protocol provides such a guaranteed data delivery service it is said to provide reliable data transfer
One important service that a transportlayer protocol can potentially provide to an application is processtoprocess reliable data transfer
When a transport protocol provides this service the sending process can just pass its data into the socket and know with complete confidence that the data will arrive without errors at the receiving process
When a transportlayer protocol doesnt provide reliable data transfer some of the data sent by the sending process may never arrive at the receiving process
This may be acceptable for losstolerant applications most notably multimedia applications such as conversational audiovideo that can tolerate some amount of data loss
In these multimedia applications lost data might result in a small glitch in the audiovideonot a crucial impairment
Throughput In Chapter we introduced the concept of available throughput which in the context of a communication session between two processes along a network path is the rate at which the sending process can deliver bits to the receiving process
Because other sessions will be sharing the bandwidth along the network path and because these other sessions will be coming and going the available throughput can fluctuate with time
These observations lead to another natural service that a transport layer protocol could provide namely guaranteed available throughput at some specified rate
With such a service the application could request a guaranteed throughput of r bitssec and the transport protocol would then ensure that the available throughput is always at least r bitssec
Such a guaranteed throughput service would appeal to many applications
For example if an Internet telephony application encodes voice at kbps it needs to send data into the network and have data delivered to the receiving application at this rate
If the transport protocol cannot provide this throughput the application would need to encode at a lower rate and receive enough throughput to sustain this lower coding rate or may have to give up since receiving say half of the needed throughput is of little or no use to this Internet telephony application
Applications that have throughput requirements are said to be bandwidthsensitive applications
Many current multimedia applications are bandwidth sensitive although some multimedia applications may use adaptive coding techniques to encode digitized voice or video at a rate that matches the currently available throughput
While bandwidthsensitive applications have specific throughput requirements elastic applications can make use of as much or as little throughput as happens to be available
Electronic mail file transfer and Web transfers are all elastic applications
Of course the more throughput the better
Theresan adage that says that one cannot be too rich too thin or have too much throughput Timing A transportlayer protocol can also provide timing guarantees
As with throughput guarantees timing guarantees can come in many shapes and forms
An example guarantee might be that every bit that the sender pumps into the socket arrives at the receivers socket no more than msec later
Such a service would be appealing to interactive realtime applications such as Internet telephony virtual environments teleconferencing and multiplayer games all of which require tight timing constraints on data delivery in order to be effective
See Chapter Gauthier Ramjee 
Long delays in Internet telephony for example tend to result in unnatural pauses in the conversation in a multiplayer game or virtual interactive environment a long delay between taking an action and seeing the response from the environment for example from another player at the end of an endtoend connection makes the application feel less realistic
For nonrealtime applications lower delay is always preferable to higher delay but no tight constraint is placed on the endtoend delays
Security Finally a transport protocol can provide an application with one or more security services
For example in the sending host a transport protocol can encrypt all data transmitted by the sending process and in the receiving host the transportlayer protocol can decrypt the data before delivering the data to the receiving process
Such a service would provide confidentiality between the two processes even if the data is somehow observed between sending and receiving processes
A transport protocol can also provide other security services in addition to confidentiality including data integrity and endpoint authentication topics that well cover in detail in Chapter 
Transport Services Provided by the Internet Up until this point we have been considering transport services that a computer network could provide in general
Lets now get more specific and examine the type of transport services provided by the Internet
The Internet and more generally TCPIP networks makes two transport protocols available to applications UDP and TCP
When you as an application developer create a new network application for the Internet one of the first decisions you have to make is whether to use UDP or TCP
Each of these protocols offers a different set of services to the invoking applications
shows the service requirements for some selected applications
TCP Services The TCP service model includes a connectionoriented service and a reliable data transfer service
When an application invokes TCP as its transport protocol the application receives both of these services from TCP
TCP has the client and server exchange transportlayer control information with each other before the applicationlevel messages begin to flow
This socalled handshaking procedure alerts the client and server allowing them to prepare for an onslaught of packets
After the handshaking phase a TCP connection is said to exist between the sockets Figure 
Requirements of selected network applications of the two processes
The connection is a fullduplex connection in that the two processes can send messages to each other over the connection at the same time
When the application finishes sending messages it must tear down the connection
In Chapter well discuss connectionoriented service in detail and examine how it is implemented
Reliable data transfer service
The communicating processes can rely on TCP to deliver all data sent without error and in the proper order
When one side of the application passes a stream of bytes into a socket it can count on TCP to deliver the same stream of bytes to the receiving socket with no missing or duplicate bytes
TCP also includes a congestioncontrol mechanism a service for the general welfare of the Internet rather than for the direct benefit of the communicating processes
The TCP congestioncontrol mechanism throttles a sending process client or server when the network is congested between sender and receiver
As we will see FOCUS ON SECURITY SECURING TCP Neither TCP nor UDP provides any encryptionthe data that the sending process passes into its socket is the same data that travels over the network to the destination process
So for example if the sending process sends a password in cleartext i.e
unencrypted into its socket the cleartext password will travel over all the links between sender and receiver potentially getting sniffed and discovered at any of the intervening links
Because privacy and other security issues have become critical for many applications the Internet community has developed an enhancement for TCP called Secure Sockets Layer SSL
TCPenhancedwithSSL not only does everything that traditional TCP does but also provides critical processtoprocess security services including encryption data integrity and endpoint authentication
We emphasize that SSL is not a third Internet transport protocol on the same level as TCP and UDP but instead is an enhancement of TCP with the enhancements being implemented in the application layer
In particular if an application wants to use the services of SSL it needs to include SSL code existing highly optimized libraries and classes in both the client and server sides of the application
SSL has its own socket API that is similar to the traditional TCP socket API
When an application uses SSL the sending process passes cleartext data to the SSL socket SSL in the sending host then encrypts the data and passes the encrypted data to the TCP socket
The encrypted data travels over the Internet to the TCP socket in the receiving process
The receiving socket passes the encrypted data to SSL which decrypts the data
Finally SSL passes the cleartext data through its SSL socket to the receiving process
Well cover SSL in some detail in Chapter 
in Chapter TCP congestion control also attempts to limit each TCP connection to its fair share of network bandwidth
UDP Services UDP is a nofrills lightweight transport protocol providing minimal services
UDP is connectionless so there is no handshaking before the two processes start to communicate
UDP provides an unreliable data transfer servicethat is when a process sends a message into a UDP socket UDP provides no guarantee that the message will ever reach the receiving process
Furthermore messages that do arrive at the receiving process may arrive out of order
UDP does not include a congestioncontrol mechanism so the sending side of UDP can pump data into the layer below the network layer at any rate it pleases
Note however that the actual endtoend throughput may be less than this rate due to the limited transmission capacity of intervening links or due to congestion
Services Not Provided by Internet Transport Protocols We have organized transport protocol services along four dimensions reliable data transfer throughput timing and security
Which of these services are provided by TCP and UDP We have already noted that TCP provides reliable endtoend data transfer
And we also know that TCP can be easily enhanced at the application layer with SSL to provide security services
But in our brief description of TCP and UDP conspicuously missing was any mention of throughput or timing guarantees services not provided by todays Internet transport protocols
Does this mean that timesensitive applications such as Internet telephony cannot run in todays Internet The answer is clearly nothe Internet has been hosting timesensitive applications for many years
These applications often work fairly well because they have been designed to cope to the greatest extent possible with this lack of guarantee
Well investigate several of these design tricks in Chapter 
Nevertheless clever design has its limitations when delay is excessive or the endtoend throughput is limited
In summary todays Internet can often provide satisfactory service to timesensitive applications but it cannot provide any timing or throughput guarantees
indicates the transport protocols used by some popular Internet applications
We see that e mail remote terminal access the Web and file transfer all use TCP
These applications have chosen TCP primarily because TCP provides reliable data transfer guaranteeing that all data will eventually get to its destination
Because Internet telephony applications such as Skype can often tolerate some loss but require a minimal rate to be effective developers of Internet telephony applications usually prefer to run their applications over UDP thereby circumventing TCPs congestion control mechanism and packet overheads
But because many firewalls are configured to block most types of UDP traffic Internet telephony applications often are designed to use TCP as a backup if UDP communication fails
Popular Internet applications their applicationlayer protocols and their underlying transport protocols 
ApplicationLayer Protocols We have just learned that network processes communicate with each other by sending messages into sockets
But how are these messages structured What are the meanings of the various fields in the messages When do the processes send the messages These questions bring us into the realm of applicationlayer protocols
An applicationlayer protocol defines how an applications processes running on different end systems pass messages to each other
In particular an applicationlayer protocol defines The types of messages exchanged for example request messages and response messages The syntax of the various message types such as the fields in the message and how the fields are delineated The semantics of the fields that is the meaning of the information in the fields Rules for determining when and how a process sends messages and responds to messages Some applicationlayer protocols are specified in RFCs and are therefore in the public domain
For example the Webs applicationlayer protocol HTTP the HyperText Transfer Protocol RFC is available as an RFC
If a browser developer follows the rules of the HTTP RFC the browser will be able to retrieve Web pages from any Web server that has also followed the rules of the HTTP RFC
Many other applicationlayer protocols are proprietary and intentionally not available in the public domain
For example Skype uses proprietary applicationlayer protocols
It is important to distinguish between network applications and applicationlayer protocols
An applicationlayer protocol is only one piece of a network application albeit a very important piece of the application from our point of view
Lets look at a couple of examples
The Web is a clientserver application that allows users to obtain documents from Web servers on demand
The Web application consists of many components including a standard for document formats that is HTML Web browsers for example Firefox and Microsoft Internet Explorer Web servers for example Apache and Microsoft servers and an applicationlayer protocol
The Webs applicationlayer protocol HTTP defines the format and sequence of messages exchanged between browser and Web server
Thus HTTP is only one piece albeit an important piece of the Web application
As another example an Internet email application also has many components including mail servers that house user mailboxes mail clients such as Microsoft Outlook that allow users to read and create messages a standard for defining the structure of an email message and applicationlayer protocols that define how messages are passed between servers how messages are passed between servers and mail clients and how the contents of message headers are to be interpreted
The principal applicationlayer protocol for electronic mail is SMTP Simple Mail Transfer Protocol RFC 
Thus emails principal applicationlayer protocol SMTP is only one piece albeit an important piece of the email application
Network Applications Covered in This Book New public domain and proprietary Internet applications are being developed every day
Rather than covering a large number of Internet applications in an encyclopedic manner we have chosen to focus on a small number of applications that are both pervasive and important
In this chapter we discuss five important applications the Web electronic mail directory service video streaming and PP applications
We first discuss the Web not only because it is an enormously popular application but also because its applicationlayer protocol HTTP is straightforward and easy to understand
We then discuss electronic mail the Internets first killer application
Email is more complex than the Web in the sense that it makes use of not one but several applicationlayer protocols
After email we cover DNS which provides a directory service for the Internet
Most users do not interact with DNS directly instead users invoke DNS indirectly through other applications including the Web file transfer and electronic mail
DNS illustrates nicely how a piece of core network functionality networkname to network address translation can be implemented at the application layer in the Internet
We then discuss PP file sharing applications and complete our application study by discussing video streaming on demand including distributing stored video over content distribution networks
In Chapter well cover multimedia applications in more depth including voice over IP and video conferencing
The Web and HTTP Until the early s the Internet was used primarily by researchers academics and university students to log in to remote hosts to transfer files from local hosts to remote hosts and vice versa to receive and send news and to receive and send electronic mail
Although these applications were and continue to be extremely useful the Internet was essentially unknown outside of the academic and research communities
Then in the early s a major new application arrived on the scenethe World Wide Web BernersLee 
The Web was the first Internet application that caught the general publics eye
It dramatically changed and continues to change how people interact inside and outside their work environments
It elevated the Internet from just one of many data networks to essentially the one and only data network
Perhaps what appeals the most to users is that the Web operates on demand
Users receive what they want when they want it
This is unlike traditional broadcast radio and television which force users to tune in when the content provider makes the content available
In addition to being available on demand the Web has many other wonderful features that people love and cherish
It is enormously easy for any individual to make information available over the Webeveryone can become a publisher at extremely low cost
Hyperlinks and search engines help us navigate through an ocean of information
Photos and videos stimulate our senses
Forms JavaScript Java applets and many other devices enable us to interact with pages and sites
And the Web and its protocols serve as a platform for YouTube Webbased email such as Gmail and most mobile Internet applications including Instagram and Google Maps
Overview of HTTP The HyperText Transfer Protocol HTTP the Webs applicationlayer protocol is at the heart of the Web
It is defined in RFC and RFC 
HTTP is implemented in two programs a client program and a server program
The client program and server program executing on different end systems talk to each other by exchanging HTTP messages
HTTP defines the structure of these messages and how the client and server exchange the messages
Before explaining HTTP in detail we should review some Web terminology
A Web page also called a document consists of objects
An object is simply a filesuch as an HTML file a JPEG image a Java applet or a video clipthat is addressable by a single URL
Most Web pages consist of a base HTML file and several referenced objects
For example if a Web page contains HTML text and five JPEG images then the Web page has six objects the base HTML file plus the five images
The base HTML file references the other objects in the page with the objects URLs
Each URL has two components the hostname of the server that houses the object and the objects path name
For example the URL httpwww.someSchool.edusomeDepartmentpicture.gif has www.someSchool.edu for a hostname and someDepartmentpicture.gif for a path name
Because Web browsers such as Internet Explorer and Firefox implement the client side of HTTP in the context of the Web we will use the words browser and client interchangeably
Web servers which implement the server side of HTTP house Web objects each addressable by a URL
Popular Web servers include Apache and Microsoft Internet Information Server
HTTP defines how Web clients request Web pages from Web servers and how servers transfer Web pages to clients
We discuss the interaction between client and server in detail later but the general idea is illustrated in Figure 
When a user requests a Web page for example clicks on a hyperlink the browser sends HTTP request messages for the objects in the page to the server
The server receives the requests and responds with HTTP response messages that contain the objects
HTTP uses TCP as its underlying transport protocol rather than running on top of UDP
The HTTP client first initiates a TCP connection with the server
Once the connection is established the browser and the server processes access TCP through their socket interfaces
As described in Section 
on the client side the socket interface is the door between the client process and the TCP connection on the server side it is the door between the server process and the TCP connection
The client sends HTTP request messages into its socket interface and receives HTTP response messages from its socket interface
Similarly the HTTP server receives request messages Figure 
HTTP requestresponse behavior from its socket interface and sends response messages into its socket interface
Once the client sends a message into its socket interface the message is out of the clients hands and is in the hands of TCP
Recall from Section 
that TCP provides a reliable data transfer service to HTTP
This implies that each HTTP request message sent by a client process eventually arrives intact at the server similarly each HTTP response message sent by the server process eventually arrives intact at the client
Here we see one of the great advantages of a layered architectureHTTP need not worry about lost data or the details of how TCP recovers from loss or reordering of data within the network
That is the job of TCP and the protocols in the lower layers of the protocol stack
It is important to note that the server sends requested files to clients without storing any state information about the client
If a particular client asks for the same object twice in a period of a few seconds the server does not respond by saying that it just served the object to the client instead the server resends the object as it has completely forgotten what it did earlier
Because an HTTP server maintains no information about the clients HTTP is said to be a stateless protocol
We also remark that the Web uses the clientserver application architecture as described in Section 
A Web server is always on with a fixed IP address and it services requests from potentially millions of different browsers
NonPersistent and Persistent Connections In many Internet applications the client and server communicate for an extended period of time with the client making a series of requests and the server responding to each of the requests
Depending on the application and on how the application is being used the series of requests may be made backtoback periodically at regular intervals or intermittently
When this clientserver interaction is taking place over TCP the application developer needs to make an important decisionshould each requestresponse pair be sent over a separate TCP connection or should all of the requests and their corresponding responses be sent over the same TCP connection In the former approach the application is said to use nonpersistent connections and in the latter approach persistent connections
To gain a deep understanding of this design issue lets examine the advantages and disadvantages of persistent connections in the context of a specific application namely HTTP which can use both nonpersistent connections and persistent connections
Although HTTP uses persistent connections in its default mode HTTP clients and servers can be configured to use nonpersistent connections instead
HTTP with NonPersistent Connections Lets walk through the steps of transferring a Web page from server to client for the case of non persistent connections
Lets suppose the page consists of a base HTML file and JPEG images and that all of these objects reside on the same server
Further suppose the URL for the base HTML file is httpwww.someSchool.edusomeDepartmenthome.index Here is what happens 
The HTTP client process initiates a TCP connection to the server www.someSchool.edu on port number which is the default port number for HTTP
Associated with the TCP connection there will be a socket at the client and a socket at the server
The HTTP client sends an HTTP request message to the server via its socket
The request message includes the path name someDepartmenthome .index 
We will discuss HTTP messages in some detail below
The HTTP server process receives the request message via its socket retrieves the object someDepartmenthome.index from its storage RAM or disk encapsulates the object in an HTTP response message and sends the response message to the client via its socket
The HTTP server process tells TCP to close the TCP connection
But TCP doesnt actually terminate the connection until it knows for sure that the client has received the response message intact
The HTTP client receives the response message
The TCP connection terminates
The message indicates that the encapsulated object is an HTML file
The client extracts the file from the response message examines the HTML file and finds references to the JPEG objects
The first four steps are then repeated for each of the referenced JPEG objects
As the browser receives the Web page it displays the page to the user
Two different browsers may interpret that is display to the user a Web page in somewhat different ways
HTTP has nothing to do with how a Web page is interpreted by a client
The HTTP specifications RFC and RFC define only the communication protocol between the client HTTP program and the server HTTP program
The steps above illustrate the use of nonpersistent connections where each TCP connection is closed after the server sends the objectthe connection does not persist for other objects
Note that each TCP connection transports exactly one request message and one response message
Thus in this example when a user requests the Web page TCP connections are generated
In the steps described above we were intentionally vague about whether the client obtains the JPEGs over serial TCP connections or whether some of the JPEGs are obtained over parallel TCP connections
Indeed users can configure modern browsers to control the degree of parallelism
In their default modes most browsers open to parallel TCP connections and each of these connections handles one requestresponse transaction
If the user prefers the maximum number of parallel connections can be set to one in which case the connections are established serially
As well see in the next chapter the use of parallel connections shortens the response time
Before continuing lets do a backoftheenvelope calculation to estimate the amount of time that elapses from when a client requests the base HTML file until the entire file is received by the client
To this end we define the roundtrip time RTT which is the time it takes for a small packet to travel from client to server and then back to the client
The RTT includes packetpropagation delays packet queuing delays in intermediate routers and switches and packetprocessing delays
These delays were discussed in Section 
Now consider what happens when a user clicks on a hyperlink
As shown in Figure 
this causes the browser to initiate a TCP connection between the browser and the Web server this involves a threeway handshakethe client sends a small TCP segment to the server the server acknowledges and responds with a small TCP segment and finally the client acknowledges back to the server
The first two parts of the threeway handshake take one RTT
After completing the first two parts of the handshake the client sends the HTTP request message combined with the third part of the threeway handshake the acknowledgment into the TCP connection
Once the request message arrives at Figure 
Backoftheenvelope calculation for the time needed to request and receive an HTML file the server the server sends the HTML file into the TCP connection
This HTTP requestresponse eats up another RTT
Thus roughly the total response time is two RTTs plus the transmission time at the server of the HTML file
HTTP with Persistent Connections Nonpersistent connections have some shortcomings
First a brandnew connection must be established and maintained for each requested object
For each of these connections TCP buffers must be allocated and TCP variables must be kept in both the client and server
This can place a significant burden on the Web server which may be serving requests from hundreds of different clients simultaneously
Second as we just described each object suffers a delivery delay of two RTTsone RTT to establish the TCP connection and one RTT to request and receive an object
With HTTP 
persistent connections the server leaves the TCP connection open after sending a response
Subsequent requests and responses between the same client and server can be sent over the same connection
In particular an entire Web page in the example above the base HTML file and the images can be sent over a single persistent TCP connection
Moreover multiple Web pages residing on the same server can be sent from the server to the same client over a single persistent TCP connection
These requests for objects can be made backtoback without waiting for replies to pending requests pipelining
Typically the HTTP server closes a connection when it isnt used for a certain time a configurable timeout interval
When the server receives the backtoback requests it sends the objects backtoback
The default mode of HTTP uses persistent connections with pipelining
Most recently HTTP RFC builds on HTTP 
by allowing multiple requests and replies to be interleaved in the same connection and a mechanism for prioritizing HTTP message requests and replies within this connection
Well quantitatively compare the performance of nonpersistent and persistent connections in the homework problems of Chapters and 
You are also encouraged to see Heidemann Nielsen RFC 
HTTP Message Format The HTTP specifications RFC RFC RFC include the definitions of the HTTP message formats
There are two types of HTTP messages request messages and response messages both of which are discussed below
HTTP Request Message Below we provide a typical HTTP request message GET somedirpage.html HTTP
Host www.someschool.edu Connection close Useragent Mozilla
Acceptlanguage fr We can learn a lot by taking a close look at this simple request message
First of all we see that the message is written in ordinary ASCII text so that your ordinary computerliterate human being can read it
Second we see that the message consists of five lines each followed by a carriage return and a line feed
The last line is followed by an additional carriage return and line feed
Although this particular request message has five lines a request message can have many more lines or as few as one line
The first line of an HTTP request message is called the request line the subsequent lines are called the header lines
The request line has three fields the method field the URL field and the HTTP version field
The method field can take on several different values including GET POST HEAD PUT and DELETE 
The great majority of HTTP request messages use the GET method
The GET method is used when the browser requests an object with the requested object identified in the URL field
In this example the browser is requesting the object somedirpage.html 
The version is self explanatory in this example the browser implements version HTTP
Now lets look at the header lines in the example
The header line Host www.someschool.edu specifies the host on which the object resides
You might think that this header line is unnecessary as there is already a TCP connection in place to the host
But as well see in Section 
the information provided by the host header line is required by Web proxy caches
By including the Connection close header line the browser is telling the server that it doesnt want to bother with persistent connections it wants the server to close the connection after sending the requested object
The User agent header line specifies the user agent that is the browser type that is making the request to the server
Here the user agent is Mozilla
a Firefox browser
This header line is useful because the server can actually send different versions of the same object to different types of user agents
Each of the versions is addressed by the same URL
Finally the Acceptlanguage header indicates that the user prefers to receive a French version of the object if such an object exists on the server otherwise the server should send its default version
The Acceptlanguage header is just one of many content negotiation headers available in HTTP
Having looked at an example lets now look at the general format of a request message as shown in Figure 
We see that the general format closely follows our earlier example
You may have noticed however that after the header lines and the additional carriage return and line feed there is an entity body
The entity body is empty with the GET method but is used with the POST method
An HTTP client often uses the POST method when the user fills out a formfor example when a user provides search words to a search engine
With a POST message the user is still requesting a Web page from the server but the specific contents of the Web page Figure 
General format of an HTTP request message depend on what the user entered into the form fields
If the value of the method field is POST then the entity body contains what the user entered into the form fields
We would be remiss if we didnt mention that a request generated with a form does not necessarily use the POST method
Instead HTML forms often use the GET method and include the inputted data in the form fields in the requested URL
For example if a form uses the GET method has two fields and the inputs to the two fields are monkeys and bananas then the URL will have the structure www.somesite.comanimalsearchmonkeysbananas 
In your daytoday Web surfing you have probably noticed extended URLs of this sort
The HEAD method is similar to the GET method
When a server receives a request with the HEAD method it responds with an HTTP message but it leaves out the requested object
Application developers often use the HEAD method for debugging
The PUT method is often used in conjunction with Web publishing tools
It allows a user to upload an object to a specific path directory on a specific Web server
The PUT method is also used by applications that need to upload objects to Web servers
The DELETE method allows a user or an application to delete an object on a Web server
HTTP Response Message Below we provide a typical HTTP response message
This response message could be the response to the example request message just discussed
OK Connection close Date Tue Aug GMT Server Apache
CentOS LastModified Tue Aug GMT ContentLength ContentType texthtml data data data data data 
Lets take a careful look at this response message
It has three sections an initial status line six header lines and then the entity body
The entity body is the meat of the messageit contains the requested object itself represented by data data data data data 
The status line has three fields the protocol version field a status code and a corresponding status message
In this example the status line indicates that the server is using HTTP
and that everything is OK that is the server has found and is sending the requested object
Now lets look at the header lines
The server uses the Connection close header line to tell the client that it is going to close the TCP connection after sending the message
The Date header line indicates the time and date when the HTTP response was created and sent by the server
Note that this is not the time when the object was created or last modified it is the time when the server retrieves the object from its file system inserts the object into the response message and sends the response message
The Server header line indicates that the message was generated by an Apache Web server it is analogous to the Useragent header line in the HTTP request message
The Last Modified header line indicates the time and date when the object was created or last modified
The LastModified header which we will soon cover in more detail is critical for object caching both in the local client and in network cache servers also known as proxy servers
The ContentLength header line indicates the number of bytes in the object being sent
The ContentType header line indicates that the object in the entity body is HTML text
The object type is officially indicated by the ContentType header and not by the file extension
Having looked at an example lets now examine the general format of a response message which is shown in Figure 
This general format of the response message matches the previous example of a response message
Lets say a few additional words about status codes and their phrases
The status code and associated phrase indicate the result of the request
Some common status codes and associated phrases include OK Request succeeded and the information is returned in the response
Moved Permanently Requested object has been permanently moved the new URL is specified in Location header of the response message
The client software will automatically retrieve the new URL
Bad Request This is a generic error code indicating that the request could not be understood by the server
General format of an HTTP response message Not Found The requested document does not exist on this server
HTTP Version Not Supported The requested HTTP protocol version is not supported by the server
How would you like to see a real HTTP response message This is highly recommended and very easy to do First Telnet into your favorite Web server
Then type in a oneline request message for some object that is housed on the server
For example if you have access to a command prompt type Using Wireshark to investigate the HTTP protocol telnet gaia.cs.umass.edu GET kurose_rossinteractiveindex.php HTTP
Host gaia.cs.umass.edu Press the carriage return twice after typing the last line
This opens a TCP connection to port of the host gaia.cs.umass.edu and then sends the HTTP request message
You should see a response message that includes the base HTML file for the interactive homework problems for this textbook
If youd rather just see the HTTP message lines and not receive the object itself replace GET with HEAD 
In this section we discussed a number of header lines that can be used within HTTP request and response messages
The HTTP specification defines many many more header lines that can be inserted by browsers Web servers and network cache servers
We have covered only a small number of the totality of header lines
Well cover a few more below and another small number when we discuss network Web caching in Section 
A highly readable and comprehensive discussion of the HTTP protocol including its headers and status codes is given in Krishnamurthy 
How does a browser decide which header lines to include in a request message How does a Web server decide which header lines to include in a response message A browser will generate header lines as a function of the browser type and version for example an HTTP
browser will not generate any 
header lines the user configuration of the browser for example preferred language and whether the browser currently has a cached but possibly outofdate version of the object
Web servers behave similarly There are different products versions and configurations all of which influence which header lines are included in response messages
UserServer Interaction Cookies We mentioned above that an HTTP server is stateless
This simplifies server design and has permitted engineers to develop highperformance Web servers that can handle thousands of simultaneous TCP connections
However it is often desirable for a Web site to identify users either because the server wishes to restrict user access or because it wants to serve content as a function of the user identity
For these purposes HTTP uses cookies
Cookies defined in RFC allow sites to keep track of users
Most major commercial Web sites use cookies today
As shown in Figure 
cookie technology has four components a cookie header line in the HTTP response message a cookie header line in the HTTP request message a cookie file kept on the users end system and managed by the users browser and a backend database at the Web site
Using Figure 
lets walk through an example of how cookies work
Suppose Susan who always accesses the Web using Internet Explorer from her home PC contacts Amazon.com for the first time
Let us suppose that in the past she has already visited the eBay site
When the request comes into the Amazon Web server the server creates a unique identification number and creates an entry in its back end database that is indexed by the identification number
The Amazon Web server then responds to Susans browser including in the HTTP response a Setcookie header which contains the identification number
For example the header line might be Setcookie When Susans browser receives the HTTP response message it sees the Setcookie header
The browser then appends a line to the special cookie file that it manages
This line includes the hostname of the server and the identification number in the Setcookie header
Note that the cookie file already has an entry for eBay since Susan has visited that site in the past
As Susan continues to browse the Amazon site each time she requests a Web page her browser consults her cookie file extracts her identification number for this site and puts a cookie header line that Figure 
Keeping user state with cookies includes the identification number in the HTTP request
Specifically each of her HTTP requests to the Amazon server includes the header line Cookie In this manner the Amazon server is able to track Susans activity at the Amazon site
Although the Amazon Web site does not necessarily know Susans name it knows exactly which pages user visited in which order and at what times Amazon uses cookies to provide its shopping cart service Amazon can maintain a list of all of Susans intended purchases so that she can pay for them collectively at the end of the session
If Susan returns to Amazons site say one week later her browser will continue to put the header line Cookie in the request messages
Amazon also recommends products to Susan based on Web pages she has visited at Amazon in the past
If Susan also registers herself with Amazon providing full name email address postal address and credit card informationAmazon can then include this information in its database thereby associating Susans name with her identification number and all of the pages she has visited at the site in the past
This is how Amazon and other ecommerce sites provide oneclick shoppingwhen Susan chooses to purchase an item during a subsequent visit she doesnt need to reenter her name credit card number or address
From this discussion we see that cookies can be used to identify a user
The first time a user visits a site the user can provide a user identification possibly his or her name
During the subsequent sessions the browser passes a cookie header to the server thereby identifying the user to the server
Cookies can thus be used to create a user session layer on top of stateless HTTP
For example when a user logs in to a Webbased email application such as Hotmail the browser sends cookie information to the server permitting the server to identify the user throughout the users session with the application
Although cookies often simplify the Internet shopping experience for the user they are controversial because they can also be considered as an invasion of privacy
As we just saw using a combination of cookies and usersupplied account information a Web site can learn a lot about a user and potentially sell this information to a third party
Cookie Central Cookie Central includes extensive information on the cookie controversy
Web Caching A Web cachealso called a proxy serveris a network entity that satisfies HTTP requests on the behalf of an origin Web server
The Web cache has its own disk storage and keeps copies of recently requested objects in this storage
As shown in Figure 
a users browser can be configured so that all of the users HTTP requests are first directed to the Web cache
Once a browser is configured each browser request for an object is first directed to the Web cache
As an example suppose a browser is requesting the object httpwww.someschool.educampus.gif 
Here is what happens 
The browser establishes a TCP connection to the Web cache and sends an HTTP request for the object to the Web cache
The Web cache checks to see if it has a copy of the object stored locally
If it does the Web cache returns the object within an HTTP response message to the client browser
Clients requesting objects through a Web cache 
If the Web cache does not have the object the Web cache opens a TCP connection to the origin server that is to www.someschool.edu 
The Web cache then sends an HTTP request for the object into the cachetoserver TCP connection
After receiving this request the origin server sends the object within an HTTP response to the Web cache
When the Web cache receives the object it stores a copy in its local storage and sends a copy within an HTTP response message to the client browser over the existing TCP connection between the client browser and the Web cache
Note that a cache is both a server and a client at the same time
When it receives requests from and sends responses to a browser it is a server
When it sends requests to and receives responses from an origin server it is a client
Typically a Web cache is purchased and installed by an ISP
For example a university might install a cache on its campus network and configure all of the campus browsers to point to the cache
Or a major residential ISP such as Comcast might install one or more caches in its network and preconfigure its shipped browsers to point to the installed caches
Web caching has seen deployment in the Internet for two reasons
First a Web cache can substantially reduce the response time for a client request particularly if the bottleneck bandwidth between the client and the origin server is much less than the bottleneck bandwidth between the client and the cache
If there is a highspeed connection between the client and the cache as there often is and if the cache has the requested object then the cache will be able to deliver the object rapidly to the client
Second as we will soon illustrate with an example Web caches can substantially reduce traffic on an institutions access link to the Internet
By reducing traffic the institution for example a company or a university does not have to upgrade bandwidth as quickly thereby reducing costs
Furthermore Web caches can substantially reduce Web traffic in the Internet as a whole thereby improving performance for all applications
To gain a deeper understanding of the benefits of caches lets consider an example in the context of Figure 
This figure shows two networksthe institutional network and the rest of the public Internet
The institutional network is a highspeed LAN
A router in the institutional network and a router in the Internet are connected by a Mbps link
The origin servers are attached to the Internet but are located all over the globe
Suppose that the average object size is Mbits and that the average request rate from the institutions browsers to the origin servers is requests per second
Suppose that the HTTP request messages are negligibly small and thus create no traffic in the networks or in the access link from institutional router to Internet router
Also suppose that the amount of time it takes from when the router on the Internet side of the access link in Figure 
forwards an HTTP request within an IP datagram until it receives the response typically within many IP datagrams is two seconds on average
Informally we refer to this last delay as the Internet delay
Bottleneck between an institutional network and the Internet The total response timethat is the time from the browsers request of an object until its receipt of the objectis the sum of the LAN delay the access delay that is the delay between the two routers and the Internet delay
Lets now do a very crude calculation to estimate this delay
The traffic intensity on the LAN see Section 
is requestssec Mbitsrequest Mbps
whereas the traffic intensity on the access link from the Internet router to institution router is requestssec Mbitsrequest Mbps A traffic intensity of 
on a LAN typically results in at most tens of milliseconds of delay hence we can neglect the LAN delay
However as discussed in Section 
as the traffic intensity approaches as is the case of the access link in Figure 
the delay on a link becomes very large and grows without bound
Thus the average response time to satisfy requests is going to be on the order of minutes if not more which is unacceptable for the institutions users
Clearly something must be done
One possible solution is to increase the access rate from Mbps to say Mbps
This will lower the traffic intensity on the access link to 
which translates to negligible delays between the two routers
In this case the total response time will roughly be two seconds that is the Internet delay
But this solution also means that the institution must upgrade its access link from Mbps to Mbps a costly proposition
Now consider the alternative solution of not upgrading the access link but instead installing a Web cache in the institutional network
This solution is illustrated in Figure 
Hit ratesthe fraction of requests that are satisfied by a cache typically range from 
For illustrative purposes lets suppose that the cache provides a hit rate of 
for this institution
Because the clients and the cache are connected to the same highspeed LAN percent of the requests will be satisfied almost immediately say within milliseconds by the cache
Nevertheless the remaining percent of the requests still need to be satisfied by the origin servers
But with only percent of the requested objects passing through the access link the traffic intensity on the access link is reduced from 
Typically a traffic intensity less than 
corresponds to a small delay say tens of milliseconds on a Mbps link
This delay is negligible compared with the twosecond Internet delay
Given these considerations average delay therefore is 
seconds which is just slightly greater than 
Thus this second solution provides an even lower response time than the first solution and it doesnt require the institution Figure 
Adding a cache to the institutional network to upgrade its link to the Internet
The institution does of course have to purchase and install a Web cache
But this cost is lowmany caches use publicdomain software that runs on inexpensive PCs
Through the use of Content Distribution Networks CDNs Web caches are increasingly playing an important role in the Internet
A CDN company installs many geographically distributed caches throughout the Internet thereby localizing much of the traffic
There are shared CDNs such as Akamai and Limelight and dedicated CDNs such as Google and Netflix
We will discuss CDNs in more detail in Section 
The Conditional GET Although caching can reduce userperceived response times it introduces a new problemthe copy of an object residing in the cache may be stale
In other words the object housed in the Web server may have been modified since the copy was cached at the client
Fortunately HTTP has a mechanism that allows a cache to verify that its objects are up to date
This mechanism is called the conditional GET
An HTTP request message is a socalled conditional GET message if the request message uses the GET method and the request message includes an IfModifiedSince header line
To illustrate how the conditional GET operates lets walk through an example
First on the behalf of a requesting browser a proxy cache sends a request message to a Web server GET fruitkiwi.gif HTTP
Host www.exotiquecuisine.com Second the Web server sends a response message with the requested object to the cache HTTP
OK Date Sat Oct Server Apache
Unix LastModified Wed Sep ContentType imagegif data data data data data 
The cache forwards the object to the requesting browser but also caches the object locally
Importantly the cache also stores the lastmodified date along with the object
Third one week later another browser requests the same object via the cache and the object is still in the cache
Since this object may have been modified at the Web server in the past week the cache performs an uptodate check by issuing a conditional GET
Specifically the cache sends GET fruitkiwi.gif HTTP
Host www.exotiquecuisine.com Ifmodifiedsince Wed Sep Note that the value of the Ifmodifiedsince header line is exactly equal to the value of the LastModified header line that was sent by the server one week ago
This conditional GET is telling the server to send the object only if the object has been modified since the specified date
Suppose the object has not been modified since Sep 
Then fourth the Web server sends a response message to the cache HTTP
Not Modified Date Sat Oct Server Apache
Unix empty entity body We see that in response to the conditional GET the Web server still sends a response message but does not include the requested object in the response message
Including the requested object would only waste bandwidth and increase userperceived response time particularly if the object is large
Note that this last response message has Not Modified in the status line which tells the cache that it can go ahead and forward its the proxy caches cached copy of the object to the requesting browser
This ends our discussion of HTTP the first Internet protocol an applicationlayer protocol that weve studied in detail
Weve seen the format of HTTP messages and the actions taken by the Web client and server as these messages are sent and received
Weve also studied a bit of the Webs application infrastructure including caches cookies and backend databases all of which are tied in some way to the HTTP protocol
Electronic Mail in the Internet Electronic mail has been around since the beginning of the Internet
It was the most popular application when the Internet was in its infancy Segaller and has become more elaborate and powerful over the years
It remains one of the Internets most important and utilized applications
As with ordinary postal mail email is an asynchronous communication mediumpeople send and read messages when it is convenient for them without having to coordinate with other peoples schedules
In contrast with postal mail electronic mail is fast easy to distribute and inexpensive
Modern email has many powerful features including messages with attachments hyperlinks HTMLformatted text and embedded photos
In this section we examine the applicationlayer protocols that are at the heart of Internet email
But before we jump into an indepth discussion of these protocols lets take a highlevel view of the Internet mail system and its key components
presents a highlevel view of the Internet mail system
We see from this diagram that it has three major components user agents mail servers and the Simple Mail Transfer Protocol SMTP
We now describe each of these components in the context of a sender Alice sending an email message to a recipient Bob
User agents allow users to read reply to forward save and compose messages
Microsoft Outlook and Apple Mail are examples of user agents for email
When Alice is finished composing her message her user agent sends the message to her mail server where the message is placed in the mail servers outgoing message queue
When Bob wants to read a message his user agent retrieves the message from his mailbox in his mail server
Mail servers form the core of the email infrastructure
Each recipient such as Bob has a mailbox located in one of the mail servers
Bobs mailbox manages and Figure 
A highlevel view of the Internet email system maintains the messages that have been sent to him
A typical message starts its journey in the senders user agent travels to the senders mail server and travels to the recipients mail server where it is deposited in the recipients mailbox
When Bob wants to access the messages in his mailbox the mail server containing his mailbox authenticates Bob with usernames and passwords
Alices mail server must also deal with failures in Bobs mail server
If Alices server cannot deliver mail to Bobs server Alices server holds the message in a message queue and attempts to transfer the message later
Reattempts are often done every minutes or so if there is no success after several days the server removes the message and notifies the sender Alice with an email message
SMTP is the principal applicationlayer protocol for Internet electronic mail
It uses the reliable data transfer service of TCP to transfer mail from the senders mail server to the recipients mail server
As with most applicationlayer protocols SMTP has two sides a client side which executes on the senders mail server and a server side which executes on the recipients mail server
Both the client and server sides of SMTP run on every mail server
When a mail server sends mail to other mail servers it acts as an SMTP client
When a mail server receives mail from other mail servers it acts as an SMTP server
SMTP SMTP defined in RFC is at the heart of Internet electronic mail
As mentioned above SMTP transfers messages from senders mail servers to the recipients mail servers
SMTP is much older than HTTP
The original SMTP RFC dates back to and SMTP was around long before that
Although SMTP has numerous wonderful qualities as evidenced by its ubiquity in the Internet it is nevertheless a legacy technology that possesses certain archaic characteristics
For example it restricts the body not just the headers of all mail messages to simple bit ASCII
This restriction made sense in the early s when transmission capacity was scarce and no one was emailing large attachments or large image audio or video files
But today in the multimedia era the bit ASCII restriction is a bit of a pain it requires binary multimedia data to be encoded to ASCII before being sent over SMTP and it requires the corresponding ASCII message to be decoded back to binary after SMTP transport
Recall from Section 
that HTTP does not require multimedia data to be ASCII encoded before transfer
To illustrate the basic operation of SMTP lets walk through a common scenario
Suppose Alice wants to send Bob a simple ASCII message
Alice invokes her user agent for email provides Bobs email address for example bobsomeschool.edu composes a message and instructs the user agent to send the message
Alices user agent sends the message to her mail server where it is placed in a message queue
The client side of SMTP running on Alices mail server sees the message in the message queue
It opens a TCP connection to an SMTP server running on Bobs mail server
After some initial SMTP handshaking the SMTP client sends Alices message into the TCP connection
At Bobs mail server the server side of SMTP receives the message
Bobs mail server then places the message in Bobs mailbox
Bob invokes his user agent to read the message at his convenience
The scenario is summarized in Figure 
It is important to observe that SMTP does not normally use intermediate mail servers for sending mail even when the two mail servers are located at opposite ends of the world
If Alices server is in Hong Kong and Bobs server is in St
Louis the TCP Figure 
Alice sends a message to Bob connection is a direct connection between the Hong Kong and St
In particular if Bobs mail server is down the message remains in Alices mail server and waits for a new attemptthe message does not get placed in some intermediate mail server
Lets now take a closer look at how SMTP transfers a message from a sending mail server to a receiving mail server
We will see that the SMTP protocol has many similarities with protocols that are used for facetoface human interaction
First the client SMTP running on the sending mail server host has TCP establish a connection to port at the server SMTP running on the receiving mail server host
If the server is down the client tries again later
Once this connection is established the server and client perform some applicationlayer handshakingjust as humans often introduce themselves before transferring information from one to another SMTP clients and servers introduce themselves before transferring information
During this SMTP handshaking phase the SMTP client indicates the e mail address of the sender the person who generated the message and the email address of the recipient
Once the SMTP client and server have introduced themselves to each other the client sends the message
SMTP can count on the reliable data transfer service of TCP to get the message to the server without errors
The client then repeats this process over the same TCP connection if it has other messages to send to the server otherwise it instructs TCP to close the connection
Lets next take a look at an example transcript of messages exchanged between an SMTP client C and an SMTP server S
The hostname of the client is crepes.fr and the hostname of the server is hamburger.edu 
The ASCII text lines prefaced with C are exactly the lines the client sends into its TCP socket and the ASCII text lines prefaced with S are exactly the lines the server sends into its TCP socket
The following transcript begins as soon as the TCP connection is established
S hamburger.edu C HELO crepes.fr S Hello crepes.fr pleased to meet you C MAIL FROM alicecrepes.fr S alicecrepes.fr 
Sender ok C RCPT TO bobhamburger.edu S bobhamburger.edu 
Recipient ok C DATA S Enter mail end with 
on a line by itself C Do you like ketchup C How about pickles C 
S Message accepted for delivery C QUIT S hamburger.edu closing connection In the example above the client sends a message Do you like ketchup How about pickles from mail server crepes.fr to mail server hamburger.edu 
As part of the dialogue the client issued five commands HELO an abbreviation for HELLO MAIL FROM RCPT TO DATA and QUIT 
These commands are selfexplanatory
The client also sends a line consisting of a single period which indicates the end of the message to the server
In ASCII jargon each message ends with CRLF.CRLF where CR and LF stand for carriage return and line feed respectively
The server issues replies to each command with each reply having a reply code and some optional English language explanation
We mention here that SMTP uses persistent connections If the sending mail server has several messages to send to the same receiving mail server it can send all of the messages over the same TCP connection
For each message the client begins the process with a new MAIL FROM crepes.fr designates the end of message with an isolated period and issues QUIT only after all messages have been sent
It is highly recommended that you use Telnet to carry out a direct dialogue with an SMTP server
To do this issue telnet serverName where serverName is the name of a local mail server
When you do this you are simply establishing a TCP connection between your local host and the mail server
After typing this line you should immediately receive the reply from the server
Then issue the SMTP commands HELO MAIL FROM RCPT TO DATA CRLF.CRLF and QUIT at the appropriate times
It is also highly recommended that you do Programming Assignment at the end of this chapter
In that assignment youll build a simple user agent that implements the client side of SMTP
It will allow you to send an e mail message to an arbitrary recipient via a local mail server
Comparison with HTTP Lets now briefly compare SMTP with HTTP
Both protocols are used to transfer files from one host to another HTTP transfers files also called objects from a Web server to a Web client typically a browser SMTP transfers files that is email messages from one mail server to another mail server
When transferring the files both persistent HTTP and SMTP use persistent connections
Thus the two protocols have common characteristics
However there are important differences
First HTTP is mainly a pull protocolsomeone loads information on a Web server and users use HTTP to pull the information from the server at their convenience
In particular the TCP connection is initiated by the machine that wants to receive the file
On the other hand SMTP is primarily a push protocolthe sending mail server pushes the file to the receiving mail server
In particular the TCP connection is initiated by the machine that wants to send the file
A second difference which we alluded to earlier is that SMTP requires each message including the body of each message to be in bit ASCII format
If the message contains characters that are not bit ASCII for example French characters with accents or contains binary data such as an image file then the message has to be encoded into bit ASCII
HTTP data does not impose this restriction
A third important difference concerns how a document consisting of text and images along with possibly other media types is handled
As we learned in Section 
HTTP encapsulates each object in its own HTTP response message
SMTP places all of the messages objects into one message
Mail Message Formats When Alice writes an ordinary snailmail letter to Bob she may include all kinds of peripheral header information at the top of the letter such as Bobs address her own return address and the date
Similarly when an email message is sent from one person to another a header containing peripheral information precedes the body of the message itself
This peripheral information is contained in a series of header lines which are defined in RFC 
The header lines and the body of the message are separated by a blank line that is by CRLF 
RFC specifies the exact format for mail header lines as well as their semantic interpretations
As with HTTP each header line contains readable text consisting of a keyword followed by a colon followed by a value
Some of the keywords are required and others are optional
Every header must have a From header line and a To header line a header may include a Subject header line as well as other optional header lines
It is important to note that these header lines are different from the SMTP commands we studied in Section 
even though they contain some common words such as from and to
The commands in that section were part of the SMTP handshaking protocol the header lines examined in this section are part of the mail message itself
A typical message header looks like this From alicecrepes.fr To bobhamburger.edu Subject Searching for the meaning of life
After the message header a blank line follows then the message body in ASCII follows
You should use Telnet to send a message to a mail server that contains some header lines including the Subject header line
To do this issue telnet serverName as discussed in Section 
Mail Access Protocols Once SMTP delivers the message from Alices mail server to Bobs mail server the message is placed in Bobs mailbox
Throughout this discussion we have tacitly assumed that Bob reads his mail by logging onto the server host and then executing a mail reader that runs on that host
Up until the early s this was the standard way of doing things
But today mail access uses a clientserver architecturethe typical user reads email with a client that executes on the users end system for example on an office PC a laptop or a smartphone
By executing a mail client on a local PC users enjoy a rich set of features including the ability to view multimedia messages and attachments
Given that Bob the recipient executes his user agent on his local PC it is natural to consider placing a mail server on his local PC as well
With this approach Alices mail server would dialogue directly with Bobs PC
There is a problem with this approach however
Recall that a mail server manages mailboxes and runs the client and server sides of SMTP
If Bobs mail server were to reside on his local PC then Bobs PC would have to remain always on and connected to the Internet in order to receive new mail which can arrive at any time
This is impractical for many Internet users
Instead a typical user runs a user agent on the local PC but accesses its mailbox stored on an alwayson shared mail server
This mail server is shared with other users and is typically maintained by the users ISP for example university or company
Now lets consider the path an email message takes when it is sent from Alice to Bob
We just learned that at some point along the path the email message needs to be deposited in Bobs mail server
This could be done simply by having Alices user agent send the message directly to Bobs mail server
And this could be done with SMTPindeed SMTP has been designed for pushing email from one host to another
However typically the senders user agent does not dialogue directly with the recipients mail server
Instead as shown in Figure 
Alices user agent uses SMTP to push the email message into her mail server then Alices mail server uses SMTP as an SMTP client to relay the email message to Bobs mail server
Why the twostep procedure Primarily because without relaying through Alices mail server Alices user agent doesnt have any recourse to an unreachable destination Figure 
Email protocols and their communicating entities mail server
By having Alice first deposit the email in her own mail server Alices mail server can repeatedly try to send the message to Bobs mail server say every minutes until Bobs mail server becomes operational
And if Alices mail server is down then she has the recourse of complaining to her system administrator The SMTP RFC defines how the SMTP commands can be used to relay a message across multiple SMTP servers
But there is still one missing piece to the puzzle How does a recipient like Bob running a user agent on his local PC obtain his messages which are sitting in a mail server within Bobs ISP Note that Bobs user agent cant use SMTP to obtain the messages because obtaining the messages is a pull operation whereas SMTP is a push protocol
The puzzle is completed by introducing a special mail access protocol that transfers messages from Bobs mail server to his local PC
There are currently a number of popular mail access protocols including Post Office ProtocolVersion POP Internet Mail Access Protocol IMAP and HTTP
provides a summary of the protocols that are used for Internet mail SMTP is used to transfer mail from the senders mail server to the recipients mail server SMTP is also used to transfer mail from the senders user agent to the senders mail server
A mail access protocol such as POP is used to transfer mail from the recipients mail server to the recipients user agent
POP POP is an extremely simple mail access protocol
It is defined in RFC which is short and quite readable
Because the protocol is so simple its functionality is rather limited
POP begins when the user agent the client opens a TCP connection to the mail server the server on port 
With the TCP connection established POP progresses through three phases authorization transaction and update
During the first phase authorization the user agent sends a username and a password in the clear to authenticate the user
During the second phase transaction the user agent retrieves messages also during this phase the user agent can mark messages for deletion remove deletion marks and obtain mail statistics
The third phase update occurs after the client has issued the quit command ending the POP session at this time the mail server deletes the messages that were marked for deletion
In a POP transaction the user agent issues commands and the server responds to each command with a reply
There are two possible responses OK sometimes followed by servertoclient data used by the server to indicate that the previous command was fine and ERR used by the server to indicate that something was wrong with the previous command
The authorization phase has two principal commands user username and pass password 
To illustrate these two commands we suggest that you Telnet directly into a POP server using port and issue these commands
Suppose that mailServer is the name of your mail server
You will see something like telnet mailServer OK POP server ready user bob OK pass hungry OK user successfully logged on If you misspell a command the POP server will reply with an ERR message
Now lets take a look at the transaction phase
A user agent using POP can often be configured by the user to download and delete or to download and keep
The sequence of commands issued by a POP user agent depends on which of these two modes the user agent is operating in
In the download anddelete mode the user agent will issue the list retr and dele commands
As an example suppose the user has two messages in his or her mailbox
In the dialogue below C standing for client is the user agent and S standing for server is the mail server
The transaction will look something like C list S S S 
C dele C quit S OK POP server signing off The user agent first asks the mail server to list the size of each of the stored messages
The user agent then retrieves and deletes each message from the server
Note that after the authorization phase the user agent employed only four commands list retr dele and quit 
The syntax for these commands is defined in RFC 
After processing the quit command the POP server enters the update phase and removes messages and from the mailbox
A problem with this downloadanddelete mode is that the recipient Bob may be nomadic and may want to access his mail messages from multiple machines for example his office PC his home PC and his portable computer
The downloadanddelete mode partitions Bobs mail messages over these three machines in particular if Bob first reads a message on his office PC he will not be able to reread the message from his portable at home later in the evening
In the downloadandkeep mode the user agent leaves the messages on the mail server after downloading them
In this case Bob can reread messages from different machines he can access a message from work and access it again later in the week from home
During a POP session between a user agent and the mail server the POP server maintains some state information in particular it keeps track of which user messages have been marked deleted
However the POP server does not carry state information across POP sessions
This lack of state information across sessions greatly simplifies the implementation of a POP server
IMAP With POP access once Bob has downloaded his messages to the local machine he can create mail folders and move the downloaded messages into the folders
Bob can then delete messages move messages across folders and search for messages by sender name or subject
But this paradigm namely folders and messages in the local machineposes a problem for the nomadic user who would prefer to maintain a folder hierarchy on a remote server that can be accessed from any computer
This is not possible with POPthe POP protocol does not provide any means for a user to create remote folders and assign messages to folders
To solve this and other problems the IMAP protocol defined in RFC was invented
Like POP IMAP is a mail access protocol
It has many more features than POP but it is also significantly more complex
And thus the client and server side implementations are significantly more complex
An IMAP server will associate each message with a folder when a message first arrives at the server it is associated with the recipients INBOX folder
The recipient can then move the message into a new usercreated folder read the message delete the message and so on
The IMAP protocol provides commands to allow users to create folders and move messages from one folder to another
IMAP also provides commands that allow users to search remote folders for messages matching specific criteria
Note that unlike POP an IMAP server maintains user state information across IMAP sessionsfor example the names of the folders and which messages are associated with which folders
Another important feature of IMAP is that it has commands that permit a user agent to obtain components of messages
For example a user agent can obtain just the message header of a message or just one part of a multipart MIME message
This feature is useful when there is a lowbandwidth connection for example a slowspeed modem link between the user agent and its mail server
With a lowbandwidth connection the user may not want to download all of the messages in its mailbox particularly avoiding long messages that might contain for example an audio or video clip
WebBased EMail More and more users today are sending and accessing their email through their Web browsers
Hotmail introduced Webbased access in the mid s
Now Webbased email is also provided by Google Yahoo as well as just about every major university and corporation
With this service the user agent is an ordinary Web browser and the user communicates with its remote mailbox via HTTP
When a recipient such as Bob wants to access a message in his mailbox the email message is sent from Bobs mail server to Bobs browser using the HTTP protocol rather than the POP or IMAP protocol
When a sender such as Alice wants to send an email message the email message is sent from her browser to her mail server over HTTP rather than over SMTP
Alices mail server however still sends messages to and receives messages from other mail servers using SMTP
DNSThe Internets Directory Service We human beings can be identified in many ways
For example we can be identified by the names that appear on our birth certificates
We can be identified by our social security numbers
We can be identified by our drivers license numbers
Although each of these identifiers can be used to identify people within a given context one identifier may be more appropriate than another
For example the computers at the IRS the infamous taxcollecting agency in the United States prefer to use fixedlength social security numbers rather than birth certificate names
On the other hand ordinary people prefer the more mnemonic birth certificate names rather than social security numbers
Indeed can you imagine saying Hi
My name is 
Please meet my husband 
Just as humans can be identified in many ways so too can Internet hosts
One identifier for a host is its hostname
Hostnamessuch as www.facebook.com www.google.com gaia.cs.umass.edu are mnemonic and are therefore appreciated by humans
However hostnames provide little if any information about the location within the Internet of the host
A hostname such as www.eurecom.fr which ends with the country code .fr tells us that the host is probably in France but doesnt say much more
Furthermore because hostnames can consist of variablelength alphanumeric characters they would be difficult to process by routers
For these reasons hosts are also identified by socalled IP addresses
We discuss IP addresses in some detail in Chapter but it is useful to say a few brief words about them now
An IP address consists of four bytes and has a rigid hierarchical structure
An IP address looks like 
where each period separates one of the bytes expressed in decimal notation from to 
An IP address is hierarchical because as we scan the address from left to right we obtain more and more specific information about where the host is located in the Internet that is within which network in the network of networks
Similarly when we scan a postal address from bottom to top we obtain more and more specific information about where the addressee is located
Services Provided by DNS We have just seen that there are two ways to identify a hostby a hostname and by an IP address
People prefer the more mnemonic hostname identifier while routers prefer fixedlength hierarchically structured IP addresses
In order to reconcile these preferences we need a directory service that translates hostnames to IP addresses
This is the main task of the Internets domain name system DNS
The DNS is a distributed database implemented in a hierarchy of DNS servers and an applicationlayer protocol that allows hosts to query the distributed database
The DNS servers are often UNIX machines running the Berkeley Internet Name Domain BIND software BIND 
The DNS protocol runs over UDP and uses port 
DNS is commonly employed by other applicationlayer protocolsincluding HTTP and SMTP to translate usersupplied hostnames to IP addresses
As an example consider what happens when a browser that is an HTTP client running on some users host requests the URL www.someschool.eduindex.html 
In order for the users host to be able to send an HTTP request message to the Web server www.someschool.edu the users host must first obtain the IP address of www.someschool.edu 
This is done as follows
The same user machine runs the client side of the DNS application
The browser extracts the hostname www.someschool.edu from the URL and passes the hostname to the client side of the DNS application
The DNS client sends a query containing the hostname to a DNS server
The DNS client eventually receives a reply which includes the IP address for the hostname
Once the browser receives the IP address from DNS it can initiate a TCP connection to the HTTP server process located at port at that IP address
We see from this example that DNS adds an additional delaysometimes substantialto the Internet applications that use it
Fortunately as we discuss below the desired IP address is often cached in a nearby DNS server which helps to reduce DNS network traffic as well as the average DNS delay
DNS provides a few other important services in addition to translating hostnames to IP addresses Host aliasing
A host with a complicated hostname can have one or more alias names
For example a hostname such as relay.westcoast.enterprise.com could have say two aliases such as enterprise.com and www.enterprise.com 
In this case the hostname relay.westcoast.enterprise.com is said to be a canonical hostname
Alias hostnames when present are typically more mnemonic than canonical hostnames
DNS can be invoked by an application to obtain the canonical hostname for a supplied alias hostname as well as the IP address of the host
Mail server aliasing
For obvious reasons it is highly desirable that email addresses be mnemonic
For example if Bob has an account with Yahoo Mail Bobs email address might be as simple as bobyahoo.mail 
However the hostname of the Yahoo mail server is more complicated and much less mnemonic than simply yahoo.com for example the canonical hostname might be something like relay.westcoast.yahoo.com 
DNS can be invoked by a mail application to obtain the canonical hostname for a supplied alias hostname as well as the IP address of the host
In fact the MX record see below permits a companys mail server and Web server to have identical aliased hostnames for example a companys Web server and mail server can both be called enterprise.com 
DNS is also used to perform load distribution among replicated servers such as replicated Web servers
Busy sites such as cnn.com are replicated over multiple servers with each server running on a different end system and each having a different IP address
For replicated Web servers a set of IP addresses is thus associated with one canonical hostname
The DNS database contains this set of IP addresses
When clients make a DNS query for a name mapped to a set of addresses the server responds with the entire set of IP addresses but rotates the ordering of the addresses within each reply
Because a client typically sends its HTTP request message to the IP address that is listed first in the set DNS rotation distributes the traffic among the replicated servers
DNS rotation is also used for email so that multiple mail servers can have the same alias name
Also content distribution companies such as Akamai have used DNS in more sophisticated ways Dilley to provide Web content distribution see Section 
The DNS is specified in RFC and RFC and updated in several additional RFCs
It is a complex system and we only touch upon key aspects of its PRINCIPLES IN PRACTICE DNS CRITICAL NETWORK FUNCTIONS VIA THE CLIENTSERVER PARADIGM Like HTTP FTP and SMTP the DNS protocol is an applicationlayer protocol since it runs between communicating end systems using the clientserver paradigm and relies on an underlying endtoend transport protocol to transfer DNS messages between communicating end systems
In another sense however the role of the DNS is quite different from Web file transfer and email applications
Unlike these applications the DNS is not an application with which a user directly interacts
Instead the DNS provides a core Internet functionnamely translating hostnames to their underlying IP addresses for user applications and other software in the Internet
We noted in Section 
that much of the complexity in the Internet architecture is located at the edges of the network
The DNS which implements the critical nameto address translation process using clients and servers located at the edge of the network is yet another example of that design philosophy
The interested reader is referred to these RFCs and the book by Albitz and Liu Albitz see also the retrospective paper Mockapetris which provides a nice description of the what and why of DNS and Mockapetris 
Overview of How DNS Works We now present a highlevel overview of how DNS works
Our discussion will focus on the hostnameto IPaddress translation service
Suppose that some application such as a Web browser or a mail reader running in a users host needs to translate a hostname to an IP address
The application will invoke the client side of DNS specifying the hostname that needs to be translated
On many UNIXbased machines gethostbyname is the function call that an application calls in order to perform the translation
DNS in the users host then takes over sending a query message into the network
All DNS query and reply messages are sent within UDP datagrams to port 
After a delay ranging from milliseconds to seconds DNS in the users host receives a DNS reply message that provides the desired mapping
This mapping is then passed to the invoking application
Thus from the perspective of the invoking application in the users host DNS is a black box providing a simple straightforward translation service
But in fact the black box that implements the service is complex consisting of a large number of DNS servers distributed around the globe as well as an applicationlayer protocol that specifies how the DNS servers and querying hosts communicate
A simple design for DNS would have one DNS server that contains all the mappings
In this centralized design clients simply direct all queries to the single DNS server and the DNS server responds directly to the querying clients
Although the simplicity of this design is attractive it is inappropriate for todays Internet with its vast and growing number of hosts
The problems with a centralized design include A single point of failure
If the DNS server crashes so does the entire Internet Traffic volume
A single DNS server would have to handle all DNS queries for all the HTTP requests and email messages generated from hundreds of millions of hosts
Distant centralized database
A single DNS server cannot be close to all the querying clients
If we put the single DNS server in New York City then all queries from Australia must travel to the other side of the globe perhaps over slow and congested links
This can lead to significant delays
The single DNS server would have to keep records for all Internet hosts
Not only would this centralized database be huge but it would have to be updated frequently to account for every new host
In summary a centralized database in a single DNS server simply doesnt scale
Consequently the DNS is distributed by design
In fact the DNS is a wonderful example of how a distributed database can be implemented in the Internet
A Distributed Hierarchical Database In order to deal with the issue of scale the DNS uses a large number of servers organized in a hierarchical fashion and distributed around the world
No single DNS server has all of the mappings for all of the hosts in the Internet
Instead the mappings are distributed across the DNS servers
To a first approximation there are three classes of DNS serversroot DNS servers toplevel domain TLD DNS servers and authoritative DNS serversorganized in a hierarchy as shown in Figure 
To understand how these three classes of servers interact suppose a DNS client wants to determine the IP address for the hostname www.amazon.com 
To a first Figure 
Portion of the hierarchy of DNS servers approximation the following events will take place
The client first contacts one of the root servers which returns IP addresses for TLD servers for the toplevel domain com 
The client then contacts one of these TLD servers which returns the IP address of an authoritative server for amazon.com 
Finally the client contacts one of the authoritative servers for amazon.com which returns the IP address for the hostname www.amazon.com 
Well soon examine this DNS lookup process in more detail
But lets first take a closer look at these three classes of DNS servers Root DNS servers
There are over root name servers scattered all over the world
shows the countries that have root names servers with countries having more than ten darkly shaded
These root name servers are managed by different organizations
The full list of root name servers along with the organizations that manage them and their IP addresses can be found at Root Servers 
Root name servers provide the IP addresses of the TLD servers
Toplevel domain TLD servers
For each of the toplevel domains toplevel domains such as com org net edu and gov and all of the country toplevel domains such as uk fr ca and jp there is TLD server or server cluster
The company Verisign Global Registry Services maintains the TLD servers for the com toplevel domain and the company Educause maintains the TLD servers for the edu toplevel domain
The network infrastructure supporting a TLD can be large and complex see Osterweil for a nice overview of the Verisign network
See TLD list for a list of all toplevel domains
TLD servers provide the IP addresses for authoritative DNS servers
DNS root servers in Authoritative DNS servers
Every organization with publicly accessible hosts such as Web servers and mail servers on the Internet must provide publicly accessible DNS records that map the names of those hosts to IP addresses
An organizations authoritative DNS server houses these DNS records
An organization can choose to implement its own authoritative DNS server to hold these records alternatively the organization can pay to have these records stored in an authoritative DNS server of some service provider
Most universities and large companies implement and maintain their own primary and secondary backup authoritative DNS server
The root TLD and authoritative DNS servers all belong to the hierarchy of DNS servers as shown in Figure 
There is another important type of DNS server called the local DNS server
A local DNS server does not strictly belong to the hierarchy of servers but is nevertheless central to the DNS architecture
Each ISPsuch as a residential ISP or an institutional ISPhas a local DNS server also called a default name server
When a host connects to an ISP the ISP provides the host with the IP addresses of one or more of its local DNS servers typically through DHCP which is discussed in Chapter 
You can easily determine the IP address of your local DNS server by accessing network status windows in Windows or UNIX
A hosts local DNS server is typically close to the host
For an institutional ISP the local DNS server may be on the same LAN as the host for a residential ISP it is typically separated from the host by no more than a few routers
When a host makes a DNS query the query is sent to the local DNS server which acts a proxy forwarding the query into the DNS server hierarchy as well discuss in more detail below
Lets take a look at a simple example
Suppose the host cse.nyu.edu desires the IP address of gaia.cs.umass.edu 
Also suppose that NYUs ocal DNS server for cse.nyu.edu is called dns.nyu.edu and that an authoritative DNS server for gaia.cs.umass.edu is called dns.umass.edu 
As shown in Figure 
the host cse.nyu.edu first sends a DNS query message to its local DNS server dns.nyu.edu 
The query message contains the hostname to be translated namely gaia.cs.umass.edu 
The local DNS server forwards the query message to a root DNS server
The root DNS server takes note of the edu suffix and returns to the local DNS server a list of IP addresses for TLD servers responsible for edu 
The local DNS server then resends the query message to one of these TLD servers
The TLD server takes note of the umass.edu suffix and responds with the IP address of the authoritative DNS server for the University of Massachusetts namely dns.umass.edu 
Finally the local DNS server resends the query message directly to dns.umass.edu which responds with the IP address of gaia.cs.umass.edu 
Note that in this example in order to obtain the mapping for one hostname eight DNS messages were sent four query messages and four reply messages Well soon see how DNS caching reduces this query traffic
Our previous example assumed that the TLD server knows the authoritative DNS server for the hostname
In general this not always true
Instead the TLD server Figure 
Interaction of the various DNS servers may know only of an intermediate DNS server which in turn knows the authoritative DNS server for the hostname
For example suppose again that the University of Massachusetts has a DNS server for the university called dns.umass.edu 
Also suppose that each of the departments at the University of Massachusetts has its own DNS server and that each departmental DNS server is authoritative for all hosts in the department
In this case when the intermediate DNS server dns.umass.edu receives a query for a host with a hostname ending with cs.umass.edu it returns to dns.nyu.edu the IP address of dns.cs.umass.edu which is authoritative for all hostnames ending with cs.umass.edu 
The local DNS server dns.nyu.edu then sends the query to the authoritative DNS server which returns the desired mapping to the local DNS server which in turn returns the mapping to the requesting host
In this case a total of DNS messages are sent The example shown in Figure 
makes use of both recursive queries and iterative queries
The query sent from cse.nyu.edu to dns.nyu.edu is a recursive query since the query asks dns.nyu.edu to obtain the mapping on its behalf
But the subsequent three queries are iterative since all of the replies are directly returned to dns.nyu.edu 
In theory any DNS query can be iterative or recursive
For example Figure 
shows a DNS query chain for which all of the queries are recursive
In practice the queries typically follow the pattern in Figure 
The query from the requesting host to the local DNS server is recursive and the remaining queries are iterative
DNS Caching Our discussion thus far has ignored DNS caching a critically important feature of the DNS system
In truth DNS extensively exploits DNS caching in order to improve the delay performance and to reduce the number of DNS messages Figure 
Recursive queries in DNS ricocheting around the Internet
The idea behind DNS caching is very simple
In a query chain when a DNS server receives a DNS reply containing for example a mapping from a hostname to an IP address it can cache the mapping in its local memory
For example in Figure 
each time the local DNS server dns.nyu.edu receives a reply from some DNS server it can cache any of the information contained in the reply
If a hostnameIP address pair is cached in a DNS server and another query arrives to the DNS server for the same hostname the DNS server can provide the desired IP address even if it is not authoritative for the hostname
Because hosts and mappings between hostnames and IP addresses are by no means permanent DNS servers discard cached information after a period of time often set to two days
As an example suppose that a host apricot.nyu.edu queries dns.nyu.edu for the IP address for the hostname cnn.com 
Furthermore suppose that a few hours later another NYU host say kiwi.nyu.edu also queries dns.nyu.edu with the same hostname
Because of caching the local DNS server will be able to immediately return the IP address of cnn.com to this second requesting host without having to query any other DNS servers
A local DNS server can also cache the IP addresses of TLD servers thereby allowing the local DNS server to bypass the root DNS servers in a query chain
In fact because of caching root servers are bypassed for all but a very small fraction of DNS queries
DNS Records and Messages The DNS servers that together implement the DNS distributed database store resource records RRs including RRs that provide hostnametoIP address mappings
Each DNS reply message carries one or more resource records
In this and the following subsection we provide a brief overview of DNS resource records and messages more details can be found in Albitz or in the DNS RFCs RFC RFC 
A resource record is a fourtuple that contains the following fields Name Value Type TTL TTL is the time to live of the resource record it determines when a resource should be removed from a cache
In the example records given below we ignore the TTL field
The meaning of Name and Value depend on Type If TypeA then Name is a hostname and Value is the IP address for the hostname
Thus a Type A record provides the standard hostnametoIP address mapping
As an example relay.bar.foo.com 
A is a Type A record
If TypeNS then Name is a domain such as foo.com and Value is the hostname of an authoritative DNS server that knows how to obtain the IP addresses for hosts in the domain
This record is used to route DNS queries further along in the query chain
As an example foo.com dns.foo.com NS is a Type NS record
If TypeCNAME then Value is a canonical hostname for the alias hostname Name 
This record can provide querying hosts the canonical name for a hostname
As an example foo.com relay.bar.foo.com CNAME is a CNAME record
If TypeMX then Value is the canonical name of a mail server that has an alias hostname Name 
As an example foo.com mail.bar.foo.com MX is an MX record
MX records allow the hostnames of mail servers to have simple aliases
Note that by using the MX record a company can have the same aliased name for its mail server and for one of its other servers such as its Web server
To obtain the canonical name for the mail server a DNS client would query for an MX record to obtain the canonical name for the other server the DNS client would query for the CNAME record
If a DNS server is authoritative for a particular hostname then the DNS server will contain a Type A record for the hostname
Even if the DNS server is not authoritative it may contain a Type A record in its cache
If a server is not authoritative for a hostname then the server will contain a Type NS record for the domain that includes the hostname it will also contain a Type A record that provides the IP address of the DNS server in the Value field of the NS record
As an example suppose an edu TLD server is not authoritative for the host gaia.cs.umass.edu 
Then this server will contain a record for a domain that includes the host gaia.cs.umass.edu for example umass.edu dns.umass.edu NS 
The edu TLD server would also contain a Type A record which maps the DNS server dns.umass.edu to an IP address for example dns.umass.edu 
DNS Messages Earlier in this section we referred to DNS query and reply messages
These are the only two kinds of DNS messages
Furthermore both query and reply messages have the same format as shown in Figure ..The semantics of the various fields in a DNS message are as follows The first bytes is the header section which has a number of fields
The first field is a bit number that identifies the query
This identifier is copied into the reply message to a query allowing the client to match received replies with sent queries
There are a number of flags in the flag field
A bit queryreply flag indicates whether the message is a query or a reply 
A bit authoritative flag is Figure 
DNS message format set in a reply message when a DNS server is an authoritative server for a queried name
A bit recursiondesired flag is set when a client host or DNS server desires that the DNS server perform recursion when it doesnt have the record
A bit recursionavailable field is set in a reply if the DNS server supports recursion
In the header there are also four numberof fields
These fields indicate the number of occurrences of the four types of data sections that follow the header
The question section contains information about the query that is being made
This section includes a name field that contains the name that is being queried and a type field that indicates the type of question being asked about the namefor example a host address associated with a name Type A or the mail server for a name Type MX
In a reply from a DNS server the answer section contains the resource records for the name that was originally queried
Recall that in each resource record there is the Type for example A NS CNAME and MX the Value and the TTL 
A reply can return multiple RRs in the answer since a hostname can have multiple IP addresses for example for replicated Web servers as discussed earlier in this section
The authority section contains records of other authoritative servers
The additional section contains other helpful records
For example the answer field in a reply to an MX query contains a resource record providing the canonical hostname of a mail server
The additional section contains a Type A record providing the IP address for the canonical hostname of the mail server
How would you like to send a DNS query message directly from the host youre working on to some DNS server This can easily be done with the nslookup program which is available from most Windows and UNIX platforms
For example from a Windows host open the Command Prompt and invoke the nslookup program by simply typing nslookup
After invoking nslookup you can send a DNS query to any DNS server root TLD or authoritative
After receiving the reply message from the DNS server nslookup will display the records included in the reply in a humanreadable format
As an alternative to running nslookup from your own host you can visit one of many Web sites that allow you to remotely employ nslookup
Just type nslookup into a search engine and youll be brought to one of these sites
The DNS Wireshark lab at the end of this chapter will allow you to explore the DNS in much more detail
Inserting Records into the DNS Database The discussion above focused on how records are retrieved from the DNS database
You might be wondering how records get into the database in the first place
Lets look at how this is done in the context of a specific example
Suppose you have just created an exciting new startup company called Network Utopia
The first thing youll surely want to do is register the domain name networkutopia.com at a registrar
A registrar is a commercial entity that verifies the uniqueness of the domain name enters the domain name into the DNS database as discussed below and collects a small fee from you for its services
Prior to a single registrar Network Solutions had a monopoly on domain name registration for com net and org domains
But now there are many registrars competing for customers and the Internet Corporation for Assigned Names and Numbers ICANN accredits the various registrars
A complete list of accredited registrars is available at http www.internic.net 
When you register the domain name networkutopia.com with some registrar you also need to provide the registrar with the names and IP addresses of your primary and secondary authoritative DNS servers
Suppose the names and IP addresses are dns.networkutopia.com dns.networkutopia.com 
For each of these two authoritative DNS servers the registrar would then make sure that a Type NS and a Type A record are entered into the TLD com servers
Specifically for the primary authoritative server for networkutopia.com the registrar would insert the following two resource records into the DNS system networkutopia.com dns.networkutopia.com NS dns.networkutopia.com 
A Youll also have to make sure that the Type A resource record for your Web server www.networkutopia.com and the Type MX resource record for your mail server mail.networkutopia.com are entered into your authoritative DNS FOCUS ON SECURITY DNS VULNERABILITIES We have seen that DNS is a critical component of the Internet infrastructure with many important servicesincluding the Web and emailsimply incapable of functioning without it
We therefore naturally ask how can DNS be attacked Is DNS a sitting duck waiting to be knocked out of service while taking most Internet applications down with it The first type of attack that comes to mind is a DDoS bandwidthflooding attack see Section 
against DNS servers
For example an attacker could attempt to send to each DNS root server a deluge of packets so many that the majority of legitimate DNS queries never get answered
Such a largescale DDoS attack against DNS root servers actually took place on October 
In this attack the attackers leveraged a botnet to send truck loads of ICMP ping messages to each of the DNS root IP addresses
ICMP messages are discussed in Section 
For now it suffices to know that ICMP packets are special types of IP datagrams
Fortunately this largescale attack caused minimal damage having little or no impact on users Internet experience
The attackers did succeed at directing a deluge of packets at the root servers
But many of the DNS root servers were protected by packet filters configured to always block all ICMP ping messages directed at the root servers
These protected servers were thus spared and functioned as normal
Furthermore most local DNS servers cache the IP addresses of topleveldomain servers allowing the query process to often bypass the DNS root servers
A potentially more effective DDoS attack against DNS would be send a deluge of DNS queries to topleveldomain servers for example to all the topleveldomain servers that handle the .com domain
It would be harder to filter DNS queries directed to DNS servers and topleveldomain servers are not as easily bypassed as are root servers
But the severity of such an attack would be partially mitigated by caching in local DNS servers
DNS could potentially be attacked in other ways
In a maninthemiddle attack the attacker intercepts queries from hosts and returns bogus replies
In the DNS poisoning attack the attacker sends bogus replies to a DNS server tricking the server into accepting bogus records into its cache
Either of these attacks could be used for example to redirect an unsuspecting Web user to the attackers Web site
These attacks however are difficult to implement as they require intercepting packets or throttling servers Skoudis 
In summary DNS has demonstrated itself to be surprisingly robust against attacks
To date there hasnt been an attack that has successfully impeded the DNS service
Until recently the contents of each DNS server were configured statically for example from a configuration file created by a system manager
More recently an UPDATE option has been added to the DNS protocol to allow data to be dynamically added or deleted from the database via DNS messages
RFC and RFC specify DNS dynamic updates
Once all of these steps are completed people will be able to visit your Web site and send email to the employees at your company
Lets conclude our discussion of DNS by verifying that this statement is true
This verification also helps to solidify what we have learned about DNS
Suppose Alice in Australia wants to view the Web page www.networkutopia.com 
As discussed earlier her host will first send a DNS query to her local DNS server
The local DNS server will then contact a TLD com server
The local DNS server will also have to contact a root DNS server if the address of a TLD com server is not cached
This TLD server contains the Type NS and Type A resource records listed above because the registrar had these resource records inserted into all of the TLD com servers
The TLD com server sends a reply to Alices local DNS server with the reply containing the two resource records
The local DNS server then sends a DNS query to 
asking for the Type A record corresponding to www.networkutopia.com 
This record provides the IP address of the desired Web server say 
which the local DNS server passes back to Alices host
Alices browser can now initiate a TCP connection to the host 
and send an HTTP request over the connection
Whew Theres a lot more going on than what meets the eye when one surfs the Web 
PeertoPeer File Distribution The applications described in this chapter thus farincluding the Web email and DNSall employ clientserver architectures with significant reliance on alwayson infrastructure servers
Recall from Section 
that with a PP architecture there is minimal or no reliance on alwayson infrastructure servers
Instead pairs of intermittently connected hosts called peers communicate directly with each other
The peers are not owned by a service provider but are instead desktops and laptops controlled by users
In this section we consider a very natural PP application namely distributing a large file from a single server to a large number of hosts called peers
The file might be a new version of the Linux operating system a software patch for an existing operating system or application an MP music file or an MPEG video file
In clientserver file distribution the server must send a copy of the file to each of the peersplacing an enormous burden on the server and consuming a large amount of server bandwidth
In PP file distribution each peer can redistribute any portion of the file it has received to any other peers thereby assisting the server in the distribution process
As of the most popular PP file distribution protocol is BitTorrent
Originally developed by Bram Cohen there are now many different independent BitTorrent clients conforming to the BitTorrent protocol just as there are a number of Web browser clients that conform to the HTTP protocol
In this subsection we first examine the self scalability of PP architectures in the context of file distribution
We then describe BitTorrent in some detail highlighting its most important characteristics and features
Scalability of PP Architectures To compare clientserver architectures with peertopeer architectures and illustrate the inherent self scalability of PP we now consider a simple quantitative model for distributing a file to a fixed set of peers for both architecture types
As shown in Figure 
the server and the peers are connected to the Internet with access links
Denote the upload rate of the servers access link by us the upload rate of the ith peers access link by ui and the download rate of the ith peers access link by di
Also denote the size of the file to be distributed in bits by F and the number of peers that want to obtain a copy of the file by N
The distribution time is the time it takes to get Figure 
An illustrative file distribution problem a copy of the file to all N peers
In our analysis of the distribution time below for both clientserver and PP architectures we make the simplifying and generally accurate Akella assumption that the Internet core has abundant bandwidth implying that all of the bottlenecks are in access networks
We also suppose that the server and clients are not participating in any other network applications so that all of their upload and download access bandwidth can be fully devoted to distributing this file
Lets first determine the distribution time for the clientserver architecture which we denote by Dcs
In the clientserver architecture none of the peers aids in distributing the file
We make the following observations The server must transmit one copy of the file to each of the N peers
Thus the server must transmit NF bits
Since the servers upload rate is us the time to distribute the file must be at least NFus
Let dmin denote the download rate of the peer with the lowest download rate that is dminminddp
The peer with the lowest download rate cannot obtain all F bits of the file in less than Fdmin seconds
Thus the minimum distribution time is at least Fdmin
Putting these two observations together we obtain DcsmaxNFusFdmin
This provides a lower bound on the minimum distribution time for the clientserver architecture
In the homework problems you will be asked to show that the server can schedule its transmissions so that the lower bound is actually achieved
So lets take this lower bound provided above as the actual distribution time that is DcsmaxNFusFdmin 
We see from Equation 
that for N large enough the clientserver distribution time is given by NFus
Thus the distribution time increases linearly with the number of peers N
So for example if the number of peers from one week to the next increases a thousandfold from a thousand to a million the time required to distribute the file to all peers increases by 
Lets now go through a similar analysis for the PP architecture where each peer can assist the server in distributing the file
In particular when a peer receives some file data it can use its own upload capacity to redistribute the data to other peers
Calculating the distribution time for the PP architecture is somewhat more complicated than for the clientserver architecture since the distribution time depends on how each peer distributes portions of the file to the other peers
Nevertheless a simple expression for the minimal distribution time can be obtained Kumar 
To this end we first make the following observations At the beginning of the distribution only the server has the file
To get this file into the community of peers the server must send each bit of the file at least once into its access link
Thus the minimum distribution time is at least Fus
Unlike the clientserver scheme a bit sent once by the server may not have to be sent by the server again as the peers may redistribute the bit among themselves
As with the clientserver architecture the peer with the lowest download rate cannot obtain all F bits of the file in less than Fdmin seconds
Thus the minimum distribution time is at least Fdmin
Finally observe that the total upload capacity of the system as a whole is equal to the upload rate of the server plus the upload rates of each of the individual peers that is utotalusuuN
The system must deliver upload F bits to each of the N peers thus delivering a total of NF bits
This cannot be done at a rate faster than utotal
Thus the minimum distribution time is also at least NFusuuN
Putting these three observations together we obtain the minimum distribution time for PP denoted by DPP
provides a lower bound for the minimum distribution time for the PP architecture
It turns out that if we imagine that each peer can redistribute a bit as soon as it receives the bit then there is a redistribution scheme that actually achieves this lower bound Kumar 
We will prove a special case of this result in the homework
In reality where chunks of the file are redistributed rather than individual bits Equation 
serves as a good approximation of the actual minimum distribution time
Thus lets take the lower bound provided by Equation 
as the actual minimum distribution time that is DPPmaxFusFdminNFusiNui 
compares the minimum distribution time for the clientserver and PP architectures assuming that all peers have the same upload rate u
Thus a peer can transmit the entire file in one hour the server transmission rate is times the peer upload rate Figure 
Distribution time for PP and clientserver architectures and for simplicity the peer download rates are set large enough so as not to have an effect
However for the PP architecture the minimal distribution time is not only always less than the distribution time of the clientserver architecture it is also less than one hour for any number of peers N
Thus applications with the PP architecture can be selfscaling
This scalability is a direct consequence of peers being redistributors as well as consumers of bits
BitTorrent BitTorrent is a popular PP protocol for file distribution Chao 
In BitTorrent lingo the collection of all peers participating in the distribution of a particular file is called a torrent
Peers in a torrent download equalsize chunks of the file from one another with a typical chunk size of KBytes
When a peer first joins a torrent it has no chunks
Over time it accumulates more and more chunks
While it downloads chunks it also uploads chunks to other peers
Once a peer has acquired the entire file it may selfishly leave the torrent or altruistically remain in the torrent and continue to upload chunks to other peers
Also any peer may leave the torrent at any time with only a subset of chunks and later rejoin the torrent
Lets now take a closer look at how BitTorrent operates
Since BitTorrent is a rather complicated protocol and system well only describe its most important mechanisms sweeping some of the details under the rug this will allow us to see the forest through the trees
Each torrent has an infrastructure node called a tracker
File distribution with BitTorrent When a peer joins a torrent it registers itself with the tracker and periodically informs the tracker that it is still in the torrent
In this manner the tracker keeps track of the peers that are participating in the torrent
A given torrent may have fewer than ten or more than a thousand peers participating at any instant of time
As shown in Figure 
when a new peer Alice joins the torrent the tracker randomly selects a subset of peers for concreteness say from the set of participating peers and sends the IP addresses of these peers to Alice
Possessing this list of peers Alice attempts to establish concurrent TCP connections with all the peers on this list
Lets call all the peers with which Alice succeeds in establishing a TCP connection neighboring peers
In Figure 
Alice is shown to have only three neighboring peers
Normally she would have many more
As time evolves some of these peers may leave and other peers outside the initial may attempt to establish TCP connections with Alice
So a peers neighboring peers will fluctuate over time
At any given time each peer will have a subset of chunks from the file with different peers having different subsets
Periodically Alice will ask each of her neighboring peers over the TCP connections for the list of the chunks they have
If Alice has L different neighbors she will obtain L lists of chunks
With this knowledge Alice will issue requests again over the TCP connections for chunks she currently does not have
So at any given instant of time Alice will have a subset of chunks and will know which chunks her neighbors have
With this information Alice will have two important decisions to make
First which chunks should she request first from her neighbors And second to which of her neighbors should she send requested chunks In deciding which chunks to request Alice uses a technique called rarest first
The idea is to determine from among the chunks she does not have the chunks that are the rarest among her neighbors that is the chunks that have the fewest repeated copies among her neighbors and then request those rarest chunks first
In this manner the rarest chunks get more quickly redistributed aiming to roughly equalize the numbers of copies of each chunk in the torrent
To determine which requests she responds to BitTorrent uses a clever trading algorithm
The basic idea is that Alice gives priority to the neighbors that are currently supplying her data at the highest rate
Specifically for each of her neighbors Alice continually measures the rate at which she receives bits and determines the four peers that are feeding her bits at the highest rate
She then reciprocates by sending chunks to these same four peers
Every seconds she recalculates the rates and possibly modifies the set of four peers
In BitTorrent lingo these four peers are said to be unchoked
Importantly every seconds she also picks one additional neighbor at random and sends it chunks
Lets call the randomly chosen peer Bob
In BitTorrent lingo Bob is said to be optimistically unchoked
Because Alice is sending data to Bob she may become one of Bobs top four uploaders in which case Bob would start to send data to Alice
If the rate at which Bob sends data to Alice is high enough Bob could then in turn become one of Alices top four uploaders
In other words every seconds Alice will randomly choose a new trading partner and initiate trading with that partner
If the two peers are satisfied with the trading they will put each other in their top four lists and continue trading with each other until one of the peers finds a better partner
The effect is that peers capable of uploading at compatible rates tend to find each other
The random neighbor selection also allows new peers to get chunks so that they can have something to trade
All other neighboring peers besides these five peers four top peers and one probing peer are choked that is they do not receive any chunks from Alice
BitTorrent has a number of interesting mechanisms that are not discussed here including pieces mini chunks pipelining random first selection endgame mode and antisnubbing Cohen 
The incentive mechanism for trading just described is often referred to as titfortat Cohen 
It has been shown that this incentive scheme can be circumvented Liogkas Locher Piatek 
Nevertheless the BitTorrent ecosystem is wildly successful with millions of simultaneous peers actively sharing files in hundreds of thousands of torrents
If BitTorrent had been designed without titfor tat or a variant but otherwise exactly the same BitTorrent would likely not even exist now as the majority of the users would have been freeriders Saroiu 
We close our discussion on PP by briefly mentioning another application of PP namely Distributed Hast Table DHT
A distributed hash table is a simple database with the database records being distributed over the peers in a PP system
DHTs have been widely implemented e.g
in BitTorrent and have been the subject of extensive research
An overview is provided in a Video Note in the companion website
Walking though distributed hash tables 
Video Streaming and Content Distribution Networks Streaming prerecorded video now accounts for the majority of the traffic in residential ISPs in North America
In particular the Netflix and YouTube services alone consumed a whopping and respectively of residential ISP traffic in Sandvine 
In this section we will provide an overview of how popular video streaming services are implemented in todays Internet
We will see they are implemented using applicationlevel protocols and servers that function in some ways like a cache
In Chapter devoted to multimedia networking we will further examine Internet video as well as other Internet multimedia services
Internet Video In streaming stored video applications the underlying medium is prerecorded video such as a movie a television show a prerecorded sporting event or a prerecorded usergenerated video such as those commonly seen on YouTube
These prerecorded videos are placed on servers and users send requests to the servers to view the videos on demand
Many Internet companies today provide streaming video including Netflix YouTube Google Amazon and Youku
But before launching into a discussion of video streaming we should first get a quick feel for the video medium itself
A video is a sequence of images typically being displayed at a constant rate for example at or images per second
An uncompressed digitally encoded image consists of an array of pixels with each pixel encoded into a number of bits to represent luminance and color
An important characteristic of video is that it can be compressed thereby trading off video quality with bit rate
Todays offtheshelf compression algorithms can compress a video to essentially any bit rate desired
Of course the higher the bit rate the better the image quality and the better the overall user viewing experience
From a networking perspective perhaps the most salient characteristic of video is its high bit rate
Compressed Internet video typically ranges from kbps for lowquality video to over Mbps for streaming highdefinition movies K streaming envisions a bitrate of more than Mbps
This can translate to huge amount of traffic and storage particularly for highend video
For example a single Mbps video with a duration of minutes will consume gigabyte of storage and traffic
By far the most important performance measure for streaming video is average endtoend throughput
In order to provide continuous playout the network must provide an average throughput to the streaming application that is at least as large as the bit rate of the compressed video
We can also use compression to create multiple versions of the same video each at a different quality level
For example we can use compression to create say three versions of the same video at rates of kbps Mbps and Mbps
Users can then decide which version they want to watch as a function of their current available bandwidth
Users with highspeed Internet connections might choose the Mbps version users watching the video over G with a smartphone might choose the kbps version
HTTP Streaming and DASH In HTTP streaming the video is simply stored at an HTTP server as an ordinary file with a specific URL
When a user wants to see the video the client establishes a TCP connection with the server and issues an HTTP GET request for that URL
The server then sends the video file within an HTTP response message as quickly as the underlying network protocols and traffic conditions will allow
On the client side the bytes are collected in a client application buffer
Once the number of bytes in this buffer exceeds a predetermined threshold the client application begins playbackspecifically the streaming video application periodically grabs video frames from the client application buffer decompresses the frames and displays them on the users screen
Thus the video streaming application is displaying video as it is receiving and buffering frames corresponding to latter parts of the video
Although HTTP streaming as described in the previous paragraph has been extensively deployed in practice for example by YouTube since its inception it has a major shortcoming All clients receive the same encoding of the video despite the large variations in the amount of bandwidth available to a client both across different clients and also over time for the same client
This has led to the development of a new type of HTTPbased streaming often referred to as Dynamic Adaptive Streaming over HTTP DASH
In DASH the video is encoded into several different versions with each version having a different bit rate and correspondingly a different quality level
The client dynamically requests chunks of video segments of a few seconds in length
When the amount of available bandwidth is high the client naturally selects chunks from a highrate version and when the available bandwidth is low it naturally selects from a lowrate version
The client selects different chunks one at a time with HTTP GET request messages Akhshabi 
DASH allows clients with different Internet access rates to stream in video at different encoding rates
Clients with lowspeed G connections can receive a low bitrate and lowquality version and clients with fiber connections can receive a highquality version
DASH also allows a client to adapt to the available bandwidth if the available endtoend bandwidth changes during the session
This feature is particularly important for mobile users who typically see their bandwidth availability fluctuate as they move with respect to the base stations
With DASH each video version is stored in the HTTP server each with a different URL
The HTTP server also has a manifest file which provides a URL for each version along with its bit rate
The client first requests the manifest file and learns about the various versions
The client then selects one chunk at a time by specifying a URL and a byte range in an HTTP GET request message for each chunk
While downloading chunks the client also measures the received bandwidth and runs a rate determination algorithm to select the chunk to request next
Naturally if the client has a lot of video buffered and if the measured receive bandwidth is high it will choose a chunk from a highbitrate version
And naturally if the client has little video buffered and the measured received bandwidth is low it will choose a chunk from a lowbitrate version
DASH therefore allows the client to freely switch among different quality levels
Content Distribution Networks Today many Internet video companies are distributing ondemand multiMbps streams to millions of users on a daily basis
YouTube for example with a library of hundreds of millions of videos distributes hundreds of millions of video streams to users around the world every day
Streaming all this traffic to locations all over the world while providing continuous playout and high interactivity is clearly a challenging task
For an Internet video company perhaps the most straightforward approach to providing streaming video service is to build a single massive data center store all of its videos in the data center and stream the videos directly from the data center to clients worldwide
But there are three major problems with this approach
First if the client is far from the data center servertoclient packets will cross many communication links and likely pass through many ISPs with some of the ISPs possibly located on different continents
If one of these links provides a throughput that is less than the video consumption rate the endtoend throughput will also be below the consumption rate resulting in annoying freezing delays for the user
Recall from Chapter that the endtoend throughput of a stream is governed by the throughput at the bottleneck link
The likelihood of this happening increases as the number of links in the endtoend path increases
A second drawback is that a popular video will likely be sent many times over the same communication links
Not only does this waste network bandwidth but the Internet video company itself will be paying its provider ISP connected to the data center for sending the same bytes into the Internet over and over again
A third problem with this solution is that a single data center represents a single point of failureif the data center or its links to the Internet goes down it would not be able to distribute any video streams
In order to meet the challenge of distributing massive amounts of video data to users distributed around the world almost all major videostreaming companies make use of Content Distribution Networks CDNs
A CDN manages servers in multiple geographically distributed locations stores copies of the videos and other types of Web content including documents images and audio in its servers and attempts to direct each user request to a CDN location that will provide the best user experience
The CDN may be a private CDN that is owned by the content provider itself for example Googles CDN distributes YouTube videos and other types of content
The CDN may alternatively be a thirdparty CDN that distributes content on behalf of multiple content providers Akamai Limelight and Level all operate thirdparty CDNs
A very readable overview of modern CDNs is Leighton Nygren 
CDNs typically adopt one of two different server placement philosophies Huang Enter Deep
One philosophy pioneered by Akamai is to enter deep into the access networks of Internet Service Providers by deploying server clusters in access ISPs all over the world
Access networks are described in Section 
Akamai takes this approach with clusters in approximately locations
The goal is to get close to end users thereby improving userperceived delay and throughput by decreasing the number of links and routers between the end user and the CDN server from which it receives content
Because of this highly distributed design the task of maintaining and managing the clusters becomes challenging
A second design philosophy taken by Limelight and many other CDN companies is to bring the ISPs home by building large clusters at a smaller number for example tens of sites
Instead of getting inside the access ISPs these CDNs typically place their clusters in Internet Exchange Points IXPs see Section 
Compared with the enterdeep design philosophy the bringhome design typically results in lower maintenance and management overhead possibly at the expense of higher delay and lower throughput to end users
Once its clusters are in place the CDN replicates content across its clusters
The CDN may not want to place a copy of every video in each cluster since some videos are rarely viewed or are only popular in some countries
In fact many CDNs do not push videos to their clusters but instead use a simple pull strategy If a client requests a video from a cluster that is not storing the video then the cluster retrieves the video from a central repository or from another cluster and stores a copy locally while streaming the video to the client at the same time
Similar Web caching see Section 
when a clusters storage becomes full it removes videos that are not frequently requested
CDN Operation Having identified the two major approaches toward deploying a CDN lets now dive down into the nuts and bolts of how a CDN operates
When a browser in a users CASE STUDY GOOGLES NETWORK INFRASTRUCTURE To support its vast array of cloud servicesincluding search Gmail calendar YouTube video maps documents and social networksGoogle has deployed an extensive private network and CDN infrastructure
Googles CDN infrastructure has three tiers of server clusters Fourteen mega data centers with eight in North America four in Europe and two in Asia Google Locations with each data center having on the order of servers
These mega data centers are responsible for serving dynamic and often personalized content including search results and Gmail messages
An estimated clusters in IXPs scattered throughout the world with each cluster consisting on the order of servers Adhikari a
These clusters are responsible for serving static content including YouTube videos Adhikari a
Many hundreds of enterdeep clusters located within an access ISP
Here a cluster typically consists of tens of servers within a single rack
These enterdeep servers perform TCP splitting see Section 
and serve static content Chen including the static portions of Web pages that embody search results
All of these data centers and cluster locations are networked together with Googles own private network
When a user makes a search query often the query is first sent over the local ISP to a nearby enterdeep cache from where the static content is retrieved while providing the static content to the client the nearby cache also forwards the query over Googles private network to one of the mega data centers from where the personalized search results are retrieved
For a YouTube video the video itself may come from one of the bringhome caches whereas portions of the Web page surrounding the video may come from the nearby enterdeep cache and the advertisements surrounding the video come from the data centers
In summary except for the local ISPs the Google cloud services are largely provided by a network infrastructure that is independent of the public Internet
host is instructed to retrieve a specific video identified by a URL the CDN must intercept the request so that it can determine a suitable CDN server cluster for that client at that time and redirect the clients request to a server in that cluster
Well shortly discuss how a CDN can determine a suitable cluster
But first lets examine the mechanics behind intercepting and redirecting a request
Most CDNs take advantage of DNS to intercept and redirect requests an interesting discussion of such a use of the DNS is Vixie 
Lets consider a simple example to illustrate how the DNS is typically involved
Suppose a content provider NetCinema employs the thirdparty CDN company KingCDN to distribute its videos to its customers
On the NetCinema Web pages each of its videos is assigned a URL that includes the string video and a unique identifier for the video itself for example Transformers might be assigned httpvideo.netcinema.comYBV
Six steps then occur as shown in Figure 
The user visits the Web page at NetCinema
When the user clicks on the link httpvideo.netcinema.comYBV the users host sends a DNS query for video.netcinema.com
The users Local DNS Server LDNS relays the DNS query to an authoritative DNS server for NetCinema which observes the string video in the hostname video.netcinema.com
To hand over the DNS query to KingCDN instead of returning an IP address the NetCinema authoritative DNS server returns to the LDNS a hostname in the KingCDNs domain for example a.kingcdn.com
From this point on the DNS query enters into KingCDNs private DNS infrastructure
The users LDNS then sends a second query now for a.kingcdn.com and KingCDNs DNS system eventually returns the IP addresses of a KingCDN content server to the LDNS
It is thus here within the KingCDNs DNS system that the CDN server from which the client will receive its content is specified
DNS redirects a users request to a CDN server 
The LDNS forwards the IP address of the contentserving CDN node to the users host
Once the client receives the IP address for a KingCDN content server it establishes a direct TCP connection with the server at that IP address and issues an HTTP GET request for the video
If DASH is used the server will first send to the client a manifest file with a list of URLs one for each version of the video and the client will dynamically select chunks from the different versions
Cluster Selection Strategies At the core of any CDN deployment is a cluster selection strategy that is a mechanism for dynamically directing clients to a server cluster or a data center within the CDN
As we just saw the CDN learns the IP address of the clients LDNS server via the clients DNS lookup
After learning this IP address the CDN needs to select an appropriate cluster based on this IP address
CDNs generally employ proprietary cluster selection strategies
We now briefly survey a few approaches each of which has its own advantages and disadvantages
One simple strategy is to assign the client to the cluster that is geographically closest
Using commercial geolocation databases such as Quova Quova and MaxMind MaxMind each LDNS IP address is mapped to a geographic location
When a DNS request is received from a particular LDNS the CDN chooses the geographically closest cluster that is the cluster that is the fewest kilometers from the LDNS as the bird flies
Such a solution can work reasonably well for a large fraction of the clients Agarwal 
However for some clients the solution may perform poorly since the geographically closest cluster may not be the closest cluster in terms of the length or number of hops of the network path
Furthermore a problem inherent with all DNSbased approaches is that some endusers are configured to use remotely located LDNSs Shaikh Mao in which case the LDNS location may be far from the clients location
Moreover this simple strategy ignores the variation in delay and available bandwidth over time of Internet paths always assigning the same cluster to a particular client
In order to determine the best cluster for a client based on the current traffic conditions CDNs can instead perform periodic realtime measurements of delay and loss performance between their clusters and clients
For instance a CDN can have each of its clusters periodically send probes for example ping messages or DNS queries to all of the LDNSs around the world
One drawback of this approach is that many LDNSs are configured to not respond to such probes
Case Studies Netflix YouTube and Kankan We conclude our discussion of streaming stored video by taking a look at three highly successful large scale deployments Netflix YouTube and Kankan
Well see that each of these systems take a very different approach yet employ many of the underlying principles discussed in this section
Netflix Generating of the downstream traffic in residential ISPs in North America in Netflix has become the leading service provider for online movies and TV series in the United States Sandvine 
As we discuss below Netflix video distribution has two major components the Amazon cloud and its own private CDN infrastructure
Netflix has a Web site that handles numerous functions including user registration and login billing movie catalogue for browsing and searching and a movie recommendation system
As shown in Figure 
this Web site and its associated backend databases run entirely on Amazon servers in the Amazon cloud
Additionally the Amazon cloud handles the following critical functions Content ingestion
Before Netflix can distribute a movie to its customers it must first ingest and process the movie
Netflix receives studio master versions of movies and uploads them to hosts in the Amazon cloud
The machines in the Amazon cloud create many different formats for each movie suitable for a diverse array of client video players running on desktop computers smartphones and game consoles connected to televisions
A different version is created for each of these formats and at multiple bit rates allowing for adaptive streaming over HTTP using DASH
Uploading versions to its CDN
Once all of the versions of a movie have been created the hosts in the Amazon cloud upload the versions to its CDN
Netflix video streaming platform When Netflix first rolled out its video streaming service in it employed three thirdparty CDN companies to distribute its video content
Netflix has since created its own private CDN from which it now streams all of its videos
Netflix still uses Akamai to distribute its Web pages however
To create its own CDN Netflix has installed server racks both in IXPs and within residential ISPs themselves
Netflix currently has server racks in over IXP locations see Netflix Open Connect for a current list of IXPs housing Netflix racks
There are also hundreds of ISP locations housing Netflix racks also see Netflix Open Connect where Netflix provides to potential ISP partners instructions about installing a free Netflix rack for their networks
Each server in the rack has several Gbps Ethernet ports and over terabytes of storage
The number of servers in a rack varies IXP installations often have tens of servers and contain the entire Netflix streaming video library including multiple versions of the videos to support DASH local IXPs may only have one server and contain only the most popular videos
Netflix does not use pullcaching Section 
to populate its CDN servers in the IXPs and ISPs
Instead Netflix distributes by pushing the videos to its CDN servers during off peak hours
For those locations that cannot hold the entire library Netflix pushes only the most popular videos which are determined on a daytoday basis
The Netflix CDN design is described in some detail in the YouTube videos Netflix Video and Netflix Video 
Having described the components of the Netflix architecture lets take a closer look at the interaction between the client and the various servers that are involved in movie delivery
As indicated earlier the Web pages for browsing the Netflix video library are served from servers in the Amazon cloud
When a user selects a movie to play the Netflix software running in the Amazon cloud first determines which of its CDN servers have copies of the movie
Among the servers that have the movie the software then determines the best server for that client request
If the client is using a residential ISP that has a Netflix CDN server rack installed in that ISP and this rack has a copy of the requested movie then a server in this rack is typically selected
If not a server at a nearby IXP is typically selected
Once Netflix determines the CDN server that is to deliver the content it sends the client the IP address of the specific server as well as a manifest file which has the URLs for the different versions of the requested movie
The client and that CDN server then directly interact using a proprietary version of DASH
Specifically as described in Section 
the client uses the byterange header in HTTP GET request messages to request chunks from the different versions of the movie
Netflix uses chunks that are approximately fourseconds long Adhikari 
While the chunks are being downloaded the client measures the received throughput and runs a ratedetermination algorithm to determine the quality of the next chunk to request
Netflix embodies many of the key principles discussed earlier in this section including adaptive streaming and CDN distribution
However because Netflix uses its own private CDN which distributes only video and not Web pages Netflix has been able to simplify and tailor its CDN design
In particular Netflix does not need to employ DNS redirect as discussed in Section 
to connect a particular client to a CDN server instead the Netflix software running in the Amazon cloud directly tells the client to use a particular CDN server
Furthermore the Netflix CDN uses push caching rather than pull caching Section 
content is pushed into the servers at scheduled times at offpeak hours rather than dynamically during cache misses
YouTube With hours of video uploaded to YouTube every minute and several billion video views per day YouTube YouTube is indisputably the worlds largest videosharing site
YouTube began its service in April and was acquired by Google in November 
Although the GoogleYouTube design and protocols are proprietary through several independent measurement efforts we can gain a basic understanding about how YouTube operates Zink Torres Adhikari a
As with Netflix YouTube makes extensive use of CDN technology to distribute its videos Torres 
Similar to Netflix Google uses its own private CDN to distribute YouTube videos and has installed server clusters in many hundreds of different IXP and ISP locations
From these locations and directly from its huge data centers Google distributes YouTube videos Adhikari a
Unlike Netflix however Google uses pull caching as described in Section 
and DNS redirect as described in Section 
Most of the time Googles clusterselection strategy directs the client to the cluster for which the RTT between client and cluster is the lowest however in order to balance the load across clusters sometimes the client is directed via DNS to a more distant cluster Torres 
YouTube employs HTTP streaming often making a small number of different versions available for a video each with a different bit rate and corresponding quality level
YouTube does not employ adaptive streaming such as DASH but instead requires the user to manually select a version
In order to save bandwidth and server resources that would be wasted by repositioning or early termination YouTube uses the HTTP byte range request to limit the flow of transmitted data after a target amount of video is prefetched
Several million videos are uploaded to YouTube every day
Not only are YouTube videos streamed from server to client over HTTP but YouTube uploaders also upload their videos from client to server over HTTP
YouTube processes each video it receives converting it to a YouTube video format and creating multiple versions at different bit rates
This processing takes place entirely within Google data centers
See the case study on Googles network infrastructure in Section 
Kankan We just saw that dedicated servers operated by private CDNs stream Netflix and YouTube videos to clients
Netflix and YouTube have to pay not only for the server hardware but also for the bandwidth the servers use to distribute the videos
Given the scale of these services and the amount of bandwidth they are consuming such a CDN deployment can be costly
We conclude this section by describing an entirely different approach for providing video on demand over the Internet at a large scaleone that allows the service provider to significantly reduce its infrastructure and bandwidth costs
As you might suspect this approach uses PP delivery instead of or along with clientserver delivery
Since Kankan owned and operated by Xunlei has been deploying PP video delivery with great success with tens of millions of users every month Zhang 
At a high level PP video streaming is very similar to BitTorrent file downloading
When a peer wants to see a video it contacts a tracker to discover other peers in the system that have a copy of that video
This requesting peer then requests chunks of the video in parallel from the other peers that have the video
Different from downloading with BitTorrent however requests are preferentially made for chunks that are to be played back in the near future in order to ensure continuous playback Dhungel 
Recently Kankan has migrated to a hybrid CDNPP streaming system Zhang 
Specifically Kankan now deploys a few hundred servers within China and pushes video content to these servers
This Kankan CDN plays a major role in the startup stage of video streaming
In most cases the client requests the beginning of the content from CDN servers and in parallel requests content from peers
When the total PP traffic is sufficient for video playback the client will cease streaming from the CDN and only stream from peers
But if the PP streaming traffic becomes insufficient the client will restart CDN connections and return to the mode of hybrid CDNPP streaming
In this manner Kankan can ensure short initial startup delays while minimally relying on costly infrastructure servers and bandwidth
Socket Programming Creating Network Applications Now that weve looked at a number of important network applications lets explore how network application programs are actually created
Recall from Section 
that a typical network application consists of a pair of programsa client program and a server programresiding in two different end systems
When these two programs are executed a client process and a server process are created and these processes communicate with each other by reading from and writing to sockets
When creating a network application the developers main task is therefore to write the code for both the client and server programs
There are two types of network applications
One type is an implementation whose operation is specified in a protocol standard such as an RFC or some other standards document such an application is sometimes referred to as open since the rules specifying its operation are known to all
For such an implementation the client and server programs must conform to the rules dictated by the RFC
For example the client program could be an implementation of the client side of the HTTP protocol described in Section 
and precisely defined in RFC similarly the server program could be an implementation of the HTTP server protocol also precisely defined in RFC 
If one developer writes code for the client program and another developer writes code for the server program and both developers carefully follow the rules of the RFC then the two programs will be able to interoperate
Indeed many of todays network applications involve communication between client and server programs that have been created by independent developersfor example a Google Chrome browser communicating with an Apache Web server or a BitTorrent client communicating with BitTorrent tracker
The other type of network application is a proprietary network application
In this case the client and server programs employ an applicationlayer protocol that has not been openly published in an RFC or elsewhere
A single developer or development team creates both the client and server programs and the developer has complete control over what goes in the code
But because the code does not implement an open protocol other independent developers will not be able to develop code that interoperates with the application
In this section well examine the key issues in developing a clientserver application and well get our hands dirty by looking at code that implements a very simple clientserver application
During the development phase one of the first decisions the developer must make is whether the application is to run over TCP or over UDP
Recall that TCP is connection oriented and provides a reliable bytestream channel through which data flows between two end systems
UDP is connectionless and sends independent packets of data from one end system to the other without any guarantees about delivery
Recall also that when a client or server program implements a protocol defined by an RFC it should use the wellknown port number associated with the protocol conversely when developing a proprietary application the developer must be careful to avoid using such wellknown port numbers
Port numbers were briefly discussed in Section 
They are covered in more detail in Chapter 
We introduce UDP and TCP socket programming by way of a simple UDP application and a simple TCP application
We present the simple UDP and TCP applications in Python 
We could have written the code in Java C or C but we chose Python mostly because Python clearly exposes the key socket concepts
With Python there are fewer lines of code and each line can be explained to the novice programmer without difficulty
But theres no need to be frightened if you are not familiar with Python
You should be able to easily follow the code if you have experience programming in Java C or C
If you are interested in clientserver programming with Java you are encouraged to see the Companion Website for this textbook in fact you can find there all the examples in this section and associated labs in Java
For readers who are interested in clientserver programming in C there are several good references available Donahoo Stevens Frost Kurose our Python examples below have a similar look and feel to C
Socket Programming with UDP In this subsection well write simple clientserver programs that use UDP in the following section well write similar programs that use TCP
Recall from Section 
that processes running on different machines communicate with each other by sending messages into sockets
We said that each process is analogous to a house and the processs socket is analogous to a door
The application resides on one side of the door in the house the transportlayer protocol resides on the other side of the door in the outside world
The application developer has control of everything on the applicationlayer side of the socket however it has little control of the transportlayer side
Now lets take a closer look at the interaction between two communicating processes that use UDP sockets
Before the sending process can push a packet of data out the socket door when using UDP it must first attach a destination address to the packet
After the packet passes through the senders socket the Internet will use this destination address to route the packet through the Internet to the socket in the receiving process
When the packet arrives at the receiving socket the receiving process will retrieve the packet through the socket and then inspect the packets contents and take appropriate action
So you may be now wondering what goes into the destination address that is attached to the packet As you might expect the destination hosts IP address is part of the destination address
By including the destination IP address in the packet the routers in the Internet will be able to route the packet through the Internet to the destination host
But because a host may be running many network application processes each with one or more sockets it is also necessary to identify the particular socket in the destination host
When a socket is created an identifier called a port number is assigned to it
So as you might expect the packets destination address also includes the sockets port number
In summary the sending process attaches to the packet a destination address which consists of the destination hosts IP address and the destination sockets port number
Moreover as we shall soon see the senders source addressconsisting of the IP address of the source host and the port number of the source socketare also attached to the packet
However attaching the source address to the packet is typically not done by the UDP application code instead it is automatically done by the underlying operating system
Well use the following simple clientserver application to demonstrate socket programming for both UDP and TCP 
The client reads a line of characters data from its keyboard and sends the data to the server
The server receives the data and converts the characters to uppercase
The server sends the modified data to the client
The client receives the modified data and displays the line on its screen
highlights the main socketrelated activity of the client and server that communicate over the UDP transport service
Now lets get our hands dirty and take a look at the clientserver program pair for a UDP implementation of this simple application
We also provide a detailed linebyline analysis after each program
Well begin with the UDP client which will send a simple applicationlevel message to the server
In order for Figure 
The clientserver application using UDP the server to be able to receive and reply to the clients message it must be ready and runningthat is it must be running as a process before the client sends its message
The client program is called UDPClient.py and the server program is called UDPServer.py
In order to emphasize the key issues we intentionally provide code that is minimal
Good code would certainly have a few more auxiliary lines in particular for handling error cases
For this application we have arbitrarily chosen for the server port number
UDPClient.py Here is the code for the client side of the application from socket import serverName hostname serverPort clientSocket socketAF_INET SOCK_DGRAM message raw_inputInput lowercase sentence clientSocket.sendtomessage.encodeserverName serverPort modifiedMessage serverAddress clientSocket.recvfrom printmodifiedMessage.decode clientSocket.close Now lets take a look at the various lines of code in UDPClient.py
from socket import The socket module forms the basis of all network communications in Python
By including this line we will be able to create sockets within our program
serverName hostname serverPort The first line sets the variable serverName to the string hostname
Here we provide a string containing either the IP address of the server e.g
or the hostname of the server e.g
If we use the hostname then a DNS lookup will automatically be performed to get the IP address
The second line sets the integer variable serverPort to 
clientSocket socketAF_INET SOCK_DGRAM This line creates the clients socket called clientSocket 
The first parameter indicates the address family in particular AF_INET indicates that the underlying network is using IPv
Do not worry about this nowwe will discuss IPv in Chapter 
The second parameter indicates that the socket is of type SOCK_DGRAM which means it is a UDP socket rather than a TCP socket
Note that we are not specifying the port number of the client socket when we create it we are instead letting the operating system do this for us
Now that the client processs door has been created we will want to create a message to send through the door
message raw_inputInput lowercase sentence raw_input is a builtin function in Python
When this command is executed the user at the client is prompted with the words Input lowercase sentence The user then uses her keyboard to input a line which is put into the variable message 
Now that we have a socket and a message we will want to send the message through the socket to the destination host
clientSocket.sendtomessage.encodeserverName serverPort In the above line we first convert the message from string type to byte type as we need to send bytes into a socket this is done with the encode method
The method sendto attaches the destination address serverName serverPort to the message and sends the resulting packet into the processs socket clientSocket 
As mentioned earlier the source address is also attached to the packet although this is done automatically rather than explicitly by the code
Sending a clienttoserver message via a UDP socket is that simple After sending the packet the client waits to receive data from the server
modifiedMessage serverAddress clientSocket.recvfrom With the above line when a packet arrives from the Internet at the clients socket the packets data is put into the variable modifiedMessage and the packets source address is put into the variable serverAddress 
The variable serverAddress contains both the servers IP address and the servers port number
The program UDPClient doesnt actually need this server address information since it already knows the server address from the outset but this line of Python provides the server address nevertheless
The method recvfrom also takes the buffer size as input
This buffer size works for most purposes
printmodifiedMessage.decode This line prints out modifiedMessage on the users display after converting the message from bytes to string
It should be the original line that the user typed but now capitalized
clientSocket.close This line closes the socket
The process then terminates
UDPServer.py Lets now take a look at the server side of the application from socket import serverPort serverSocket socketAF_INET SOCK_DGRAM serverSocket.bind serverPort printThe server is ready to receive while True message clientAddress serverSocket.recvfrom modifiedMessage message.decode.upper serverSocket.sendtomodifiedMessage.encode clientAddress Note that the beginning of UDPServer is similar to UDPClient
It also imports the socket module also sets the integer variable serverPort to and also creates a socket of type SOCK_DGRAM a UDP socket
The first line of code that is significantly different from UDPClient is serverSocket.bind serverPort The above line binds that is assigns the port number to the servers socket
Thus in UDPServer the code written by the application developer is explicitly assigning a port number to the socket
In this manner when anyone sends a packet to port at the IP address of the server that packet will be directed to this socket
UDPServer then enters a while loop the while loop will allow UDPServer to receive and process packets from clients indefinitely
In the while loop UDPServer waits for a packet to arrive
message clientAddress serverSocket.recvfrom This line of code is similar to what we saw in UDPClient
When a packet arrives at the servers socket the packets data is put into the variable message and the packets source address is put into the variable clientAddress 
The variable clientAddress contains both the clients IP address and the clients port number
Here UDPServer will make use of this address information as it provides a return address similar to the return address with ordinary postal mail
With this source address information the server now knows to where it should direct its reply
modifiedMessage message.decode.upper This line is the heart of our simple application
It takes the line sent by the client and after converting the message to a string uses the method upper to capitalize it
serverSocket.sendtomodifiedMessage.encode clientAddress This last line attaches the clients address IP address and port number to the capitalized message after converting the string to bytes and sends the resulting packet into the servers socket
As mentioned earlier the server address is also attached to the packet although this is done automatically rather than explicitly by the code
The Internet will then deliver the packet to this client address
After the server sends the packet it remains in the while loop waiting for another UDP packet to arrive from any client running on any host
To test the pair of programs you run UDPClient.py on one host and UDPServer.py on another host
Be sure to include the proper hostname or IP address of the server in UDPClient.py
Next you execute UDPServer.py the compiled server program in the server host
This creates a process in the server that idles until it is contacted by some client
Then you execute UDPClient.py the compiled client program in the client
This creates a process in the client
Finally to use the application at the client you type a sentence followed by a carriage return
To develop your own UDP clientserver application you can begin by slightly modifying the client or server programs
For example instead of converting all the letters to uppercase the server could count the number of times the letter s appears and return this number
Or you can modify the client so that after receiving a capitalized sentence the user can continue to send more sentences to the server
Socket Programming with TCP Unlike UDP TCP is a connectionoriented protocol
This means that before the client and server can start to send data to each other they first need to handshake and establish a TCP connection
One end of the TCP connection is attached to the client socket and the other end is attached to a server socket
When creating the TCP connection we associate with it the client socket address IP address and port number and the server socket address IP address and port number
With the TCP connection established when one side wants to send data to the other side it just drops the data into the TCP connection via its socket
This is different from UDP for which the server must attach a destination address to the packet before dropping it into the socket
Now lets take a closer look at the interaction of client and server programs in TCP
The client has the job of initiating contact with the server
In order for the server to be able to react to the clients initial contact the server has to be ready
This implies two things
First as in the case of UDP the TCP server must be running as a process before the client attempts to initiate contact
Second the server program must have a special doormore precisely a special socketthat welcomes some initial contact from a client process running on an arbitrary host
Using our housedoor analogy for a processsocket we will sometimes refer to the clients initial contact as knocking on the welcoming door
With the server process running the client process can initiate a TCP connection to the server
This is done in the client program by creating a TCP socket
When the client creates its TCP socket it specifies the address of the welcoming socket in the server namely the IP address of the server host and the port number of the socket
After creating its socket the client initiates a threeway handshake and establishes a TCP connection with the server
The threeway handshake which takes place within the transport layer is completely invisible to the client and server programs
During the threeway handshake the client process knocks on the welcoming door of the server process
When the server hears the knocking it creates a new doormore precisely a new socket that is dedicated to that particular client
In our example below the welcoming door is a TCP socket serverSocket the newly created socket dedicated to the client making the object that we call connection is called connectionSocket 
Students who are encountering TCP sockets for the first time sometimes confuse the welcoming socket which is the initial point of contact for all clients wanting to communicate with the server and each newly created serverside connection socket that is subsequently created for communicating with each client
From the applications perspective the clients socket and the servers connection socket are directly connected by a pipe
As shown in Figure 
the client process can send arbitrary bytes into its socket and TCP guarantees that the server process will receive through the connection socket each byte in the order sent
TCP thus provides a reliable service between the client and server processes
Furthermore just as people can go in and out the same door the client process not only sends bytes into but also receives bytes from its socket similarly the server process not only receives bytes from but also sends bytes into its connection socket
We use the same simple clientserver application to demonstrate socket programming with TCP The client sends one line of data to the server the server capitalizes the line and sends it back to the client
highlights the main socketrelated activity of the client and server that communicate over the TCP transport service
The TCPServer process has two sockets TCPClient.py Here is the code for the client side of the application from socket import serverName servername serverPort clientSocket socketAF_INET SOCK_STREAM clientSocket.connectserverName serverPort sentence raw_inputInput lowercase sentence clientSocket.sendsentence.encode modifiedSentence clientSocket.recv printFrom Server modifiedSentence.decode clientSocket.close Lets now take a look at the various lines in the code that differ significantly from the UDP implementation
The first such line is the creation of the client socket
clientSocket socketAF_INET SOCK_STREAM This line creates the clients socket called clientSocket 
The first parameter again indicates that the underlying network is using IPv
The second parameter Figure 
The clientserver application using TCP indicates that the socket is of type SOCK_STREAM which means it is a TCP socket rather than a UDP socket
Note that we are again not specifying the port number of the client socket when we create it we are instead letting the operating system do this for us
Now the next line of code is very different from what we saw in UDPClient clientSocket.connectserverName serverPort Recall that before the client can send data to the server or vice versa using a TCP socket a TCP connection must first be established between the client and server
The above line initiates the TCP connection between the client and server
The parameter of the connect method is the address of the server side of the connection
After this line of code is executed the threeway handshake is performed and a TCP connection is established between the client and server
sentence raw_inputInput lowercase sentence As with UDPClient the above obtains a sentence from the user
The string sentence continues to gather characters until the user ends the line by typing a carriage return
The next line of code is also very different from UDPClient clientSocket.sendsentence.encode The above line sends the sentence through the clients socket and into the TCP connection
Note that the program does not explicitly create a packet and attach the destination address to the packet as was the case with UDP sockets
Instead the client program simply drops the bytes in the string sentence into the TCP connection
The client then waits to receive bytes from the server
modifiedSentence clientSocket.recv When characters arrive from the server they get placed into the string modifiedSentence 
Characters continue to accumulate in modifiedSentence until the line ends with a carriage return character
After printing the capitalized sentence we close the clients socket clientSocket.close This last line closes the socket and hence closes the TCP connection between the client and the server
It causes TCP in the client to send a TCP message to TCP in the server see Section 
TCPServer.py Now lets take a look at the server program
from socket import serverPort serverSocket socketAF_INET SOCK_STREAM serverSocket.bind serverPort serverSocket.listen printThe server is ready to receive while True connectionSocket addr serverSocket.accept sentence connectionSocket.recv.decode capitalizedSentence sentence.upper connectionSocket.sendcapitalizedSentence.encode connectionSocket.close Lets now take a look at the lines that differ significantly from UDPServer and TCPClient
As with TCPClient the server creates a TCP socket with serverSocketsocketAF_INET SOCK_STREAM Similar to UDPServer we associate the server port number serverPort with this socket serverSocket.bind serverPort But with TCP serverSocket will be our welcoming socket
After establishing this welcoming door we will wait and listen for some client to knock on the door serverSocket.listen This line has the server listen for TCP connection requests from the client
The parameter specifies the maximum number of queued connections at least 
connectionSocket addr serverSocket.accept When a client knocks on this door the program invokes the accept method for serverSocket which connectionSocket dedicated to this particular client
creates a new socket in the server called The client and server then complete the handshaking creating a TCP connection between the clients clientSocket and the servers connectionSocket 
With the TCP connection established the client and server can now send bytes to each other over the connection
With TCP all bytes sent from one side not are not only guaranteed to arrive at the other side but also guaranteed arrive in order
connectionSocket.close In this program after sending the modified sentence to the client we close the connection socket
But since serverSocket remains open another client can now knock on the door and send the server a sentence to modify
This completes our discussion of socket programming in TCP
You are encouraged to run the two programs in two separate hosts and also to modify them to achieve slightly different goals
You should compare the UDP program pair with the TCP program pair and see how they differ
You should also do many of the socket programming assignments described at the ends of Chapter and 
Finally we hope someday after mastering these and more advanced socket programs you will write your own popular network application become very rich and famous and remember the authors of this textbook 
Summary In this chapter weve studied the conceptual and the implementation aspects of network applications
Weve learned about the ubiquitous clientserver architecture adopted by many Internet applications and seen its use in the HTTP SMTP POP and DNS protocols
Weve studied these important application level protocols and their corresponding associated applications the Web file transfer email and DNS in some detail
Weve learned about the PP architecture and how it is used in many applications
Weve also learned about streaming video and how modern video distribution systems leverage CDNs
Weve examined how the socket API can be used to build network applications
Weve walked through the use of sockets for connectionoriented TCP and connectionless UDP endtoend transport services
The first step in our journey down the layered network architecture is now complete At the very beginning of this book in Section 
we gave a rather vague barebones definition of a protocol the format and the order of messages exchanged between two or more communicating entities as well as the actions taken on the transmission andor receipt of a message or other event
The material in this chapter and in particular our detailed study of the HTTP SMTP POP and DNS protocols has now added considerable substance to this definition
Protocols are a key concept in networking our study of application protocols has now given us the opportunity to develop a more intuitive feel for what protocols are all about
In Section 
we described the service models that TCP and UDP offer to applications that invoke them
We took an even closer look at these service models when we developed simple applications that run over TCP and UDP in Section 
However we have said little about how TCP and UDP provide these service models
For example we know that TCP provides a reliable data service but we havent said yet how it does so
In the next chapter well take a careful look at not only the what but also the how and why of transport protocols
Equipped with knowledge about Internet application structure and applicationlevel protocols were now ready to head further down the protocol stack and examine the transport layer in Chapter 
Homework Problems and Questions Chapter Review Questions SECTION 
List five nonproprietary Internet applications and the applicationlayer protocols that they use
What is the difference between network architecture and application architecture R
For a communication session between a pair of processes which process is the client and which is the server R
For a PP filesharing application do you agree with the statement There is no notion of client and server sides of a communication session Why or why not R
What information is used by a process running on one host to identify a process running on another host R
Suppose you wanted to do a transaction from a remote client to a server as fast as possible
Would you use UDP or TCP Why R
Referring to Figure 
we see that none of the applications listed in Figure 
requires both no data loss and timing
Can you conceive of an application that requires no data loss and that is also highly timesensitive R
List the four broad classes of services that a transport protocol can provide
For each of the service classes indicate if either UDP or TCP or both provides such a service
Recall that TCP can be enhanced with SSL to provide processtoprocess security services including encryption
Does SSL operate at the transport layer or the application layer If the application developer wants TCP to be enhanced with SSL what does the developer have to do SECTION 
What is meant by a handshaking protocol R
Why do HTTP SMTP and POP run on top of TCP rather than on UDP R
Consider an ecommerce site that wants to keep a purchase record for each of its customers
Describe how this can be done with cookies
Describe how Web caching can reduce the delay in receiving a requested object
Will Web caching reduce the delay for all objects requested by a user or for only some of the objects Why R
Telnet into a Web server and send a multiline request message
Include in the request message the Ifmodifiedsince header line to force a response message with the Not Modified status code
List several popular messaging apps
Do they use the same protocols as SMS R
Suppose Alice with a Webbased email account such as Hotmail or Gmail sends a message to Bob who accesses his mail from his mail server using POP
Discuss how the message gets from Alices host to Bobs host
Be sure to list the series of applicationlayer protocols that are used to move the message between the two hosts
Print out the header of an email message you have recently received
How many Received header lines are there Analyze each of the header lines in the message
From a users perspective what is the difference between the downloadanddelete mode and the downloadandkeep mode in POP R
Is it possible for an organizations Web server and mail server to have exactly the same alias for a hostname for example foo.com What would be the type for the RR that contains the hostname of the mail server R
Look over your received emails and examine the header of a message sent from a user with a .edu email address
Is it possible to determine from the header the IP address of the host from which the message was sent Do the same for a message sent from a Gmail account
In BitTorrent suppose Alice provides chunks to Bob throughout a second interval
Will Bob necessarily return the favor and provide chunks to Alice in this same interval Why or why not R
Consider a new peer Alice that joins BitTorrent without possessing any chunks
Without any chunks she cannot become a topfour uploader for any of the other peers since she has nothing to upload
How then will Alice get her first chunk R
What is an overlay network Does it include routers What are the edges in the overlay network SECTION 
CDNs typically adopt one of two different server placement philosophies
Name and briefly describe them
Besides networkrelated considerations such as delay loss and bandwidth performance there are other important factors that go into designing a CDN server selection strategy
What are they SECTION 
In Section 
the UDP server described needed only one socket whereas the TCP server needed two sockets
Why If the TCP server were to support n simultaneous connections each from a different client host how many sockets would the TCP server need R
For the clientserver application over TCP described in Section 
why must the server program be executed before the client program For the clientserver application over UDP why may the client program be executed before the server program Problems P
True or false a
A user requests a Web page that consists of some text and three images
For this page the client will send one request message and receive four response messages
Two distinct Web pages for example www.mit.eduresearch.html and www.mit.edustudents.html can be sent over the same persistent connection
With nonpersistent connections between browser and origin server it is possible for a single TCP segment to carry two distinct HTTP request messages
The Date header in the HTTP response message indicates when the object in the response was last modified
HTTP response messages never have an empty message body
SMS iMessage and WhatsApp are all smartphone realtime messaging systems
After doing some research on the Internet for each of these systems write one paragraph about the protocols they use
Then write a paragraph explaining how they differ
Consider an HTTP client that wants to retrieve a Web document at a given URL
The IP address of the HTTP server is initially unknown
What transport and applicationlayer protocols besides HTTP are needed in this scenario P
Consider the following string of ASCII characters that were captured by Wireshark when the browser sent an HTTP GET message i.e
this is the actual content of an HTTP GET message
The characters crlf are carriage return and linefeed characters that is the italized character string cr in the text below represents the single carriagereturn character that was contained at that point in the HTTP header
Answer the following questions indicating where in the HTTP GET message below you find the answer
GET csindex.html HTTP.crlfHost gai a.cs.umass.educrlfUserAgent Mozilla
WindowsU Windows NT 
Gec ko Netscape
ax crlfAcceptex txml applicationxml applicationxhtmlxml text htmlq
crlfAcceptLanguage enus enq.crlfAccept Encoding zip deflatecrlfAcceptCharset ISO utfq.q.crlfKeepAlive cr lfConnectionkeepalivecrlfcrlf a
What is the URL of the document requested by the browser b
What version of HTTP is the browser running c
Does the browser request a nonpersistent or a persistent connection d
What is the IP address of the host on which the browser is running e
What type of browser initiates this message Why is the browser type needed in an HTTP request message P
The text below shows the reply sent from the server in response to the HTTP GET message in the question above
Answer the following questions indicating where in the message below you find the answer
OKcrlfDate Tue Mar GMTcrlfServer Apache
Fedora crlfLastModified Sat Dec GMTcrlfETag cfaaccrlfAccept Ranges bytescrlfContentLength crlf KeepAlive timeoutmaxcrlfConnection KeepAlivecrlfContentType texthtml charset ISOcrlfcrlfdoctype html public wcdtd html 
transitionalenlfhtmllf headlf meta httpequivContentType contenttexthtml charsetisolf meta nameGENERATOR contentMozilla
en Windows NT 
U Netscapelf titleCMPSCI NTUSTASpring homepagetitlelfheadlf much more document text following here not shown a
Was the server able to successfully find the document or not What time was the document reply provided b
When was the document last modified c
How many bytes are there in the document being returned d
What are the first bytes of the document being returned Did the server agree to a persistent connection P
Obtain the HTTP
specification RFC 
Answer the following questions a
Explain the mechanism used for signaling between the client and server to indicate that a persistent connection is being closed
Can the client the server or both signal the close of a connection b
What encryption services are provided by HTTP c
Can a client open three or more simultaneous connections with a given server d
Either a server or a client may close a transport connection between them if either one detects the connection has been idle for some time
Is it possible that one side starts closing a connection while the other side is transmitting data via this connection Explain
Suppose within your Web browser you click on a link to obtain a Web page
The IP address for the associated URL is not cached in your local host so a DNS lookup is necessary to obtain the IP address
Suppose that n DNS servers are visited before your host receives the IP address from DNS the successive visits incur an RTT of RTT
Further suppose that the Web page associated with the link contains exactly one object consisting of a small amount of HTML text
Let RTT denote the RTT between the local host and the server containing the object
Assuming zero transmission time of the object how much time elapses from when the client clicks on the link until the client receives the object P
Referring to Problem P suppose the HTML file references eight very small objects on the same server
Neglecting transmission times how much time elapses with a
Nonpersistent HTTP with no parallel TCP connections b
Nonpersistent HTTP with the browser configured for parallel connections c
Persistent HTTP P
Consider Figure 
for which there is an institutional network connected to the Internet
Suppose that the average object size is bits and that the average request rate from the institutions browsers to the origin servers is requests per second
Also suppose that the amount of time it takes from when the router on the Internet side of the access link forwards an HTTP request until it receives the response is three seconds on average see Section 
Model the total average response time as the sum of the average access delay that is the delay from Internet router to institution router and the average Internet delay
For the average access delay use ÎÎÎ² where Î is the average time required to send an object over the access link and b is the arrival rate of objects to the access link
Find the total average response time
Now suppose a cache is installed in the institutional LAN
Suppose the miss rate is 
Find the total response time
Consider a short meter link over which a sender can transmit at a rate of bitssec in both directions
Suppose that packets containing data are bits long and packets containing only control e.g
ACK or handshaking are bits long
Assume that N parallel connections each get N of the link bandwidth
Now consider the HTTP protocol and suppose that each downloaded object is Kbits long and that the initial downloaded object contains referenced objects from the same sender
Would parallel downloads via parallel instances of nonpersistent HTTP make sense in this case Now consider persistent HTTP
Do you expect significant gains over the nonpersistent case Justify and explain your answer
Consider the scenario introduced in the previous problem
Now suppose that the link is shared by Bob with four other users
Bob uses parallel instances of nonpersistent HTTP and the other four users use nonpersistent HTTP without parallel downloads
Do Bobs parallel connections help him get Web pages more quickly Why or why not b
If all five users open five parallel instances of nonpersistent HTTP then would Bobs parallel connections still be beneficial Why or why not P
Write a simple TCP program for a server that accepts lines of input from a client and prints the lines onto the servers standard output
You can do this by modifying the TCPServer.py program in the text
Compile and execute your program
On any other machine that contains a Web browser set the proxy server in the browser to the host that is running your server program also configure the port number appropriately
Your browser should now send its GET request messages to your server and your server should display the messages on its standard output
Use this platform to determine whether your browser generates conditional GET messages for objects that are locally cached
What is the difference between MAIL FROM in SMTP and From in the mail message itself P
How does SMTP mark the end of a message body How about HTTP Can HTTP use the same method as SMTP to mark the end of a message body Explain
Read RFC for SMTP
What does MTA stand for Consider the following received spam email modified from a real spam email
Assuming only the originator of this spam email is malicious and all other hosts are honest identify the malacious host that has generated this spam email
From Fri Nov ReturnPath tennispphead.com Received from barmail.cs.umass.edu barmail.cs.umass.edu 
by cs.umass.edu 
for hgcs.umass.edu Fri Nov Received from asususb localhost 
by barmail.cs.umass.edu Spam Firewall for hgcs.umass.edu Fri Nov EST Received from asususb 
by barmail.cs.umass.edu for hgcs.umass.edu Fri Nov EST Received from 
by inbnd.exchangeddd.com Sat Nov From Jonny tennispphead.com To hgcs.umass.edu Subject How to secure your savings P
Read the POP RFC RFC 
What is the purpose of the UIDL POP command P
Consider accessing your email with POP
Suppose you have configured your POP mail client to operate in the downloadand delete mode
Complete the following transaction C list S S S 
C retr S blah blah 
S ..........blah S 
Suppose you have configured your POP mail client to operate in the downloadandkeep mode
Complete the following transaction C list S S S 
C retr S blah blah 
S ..........blah S 
Suppose you have configured your POP mail client to operate in the downloadandkeep mode
Using your transcript in part b suppose you retrieve messages and exit POP and then five minutes later you again access POP to retrieve new email
Suppose that in the fiveminute interval no new messages have been sent to you
Provide a transcript of this second POP session
What is a whois database b
Use various whois databases on the Internet to obtain the names of two DNS servers
Indicate which whois databases you used
Use nslookup on your local host to send DNS queries to three DNS servers your local DNS server and the two DNS servers you found in part b
Try querying for Type A NS and MX reports
Summarize your findings
Use nslookup to find a Web server that has multiple IP addresses
Does the Web server of your institution school or company have multiple IP addresses e
Use the ARIN whois database to determine the IP address range used by your university
Describe how an attacker can use whois databases and the nslookup tool to perform reconnaissance on an institution before launching an attack
Discuss why whois databases should be publicly available
In this problem we use the useful dig tool available on Unix and Linux hosts to explore the hierarchy of DNS servers
Recall that in Figure 
a DNS server in the DNS hierarchy delegates a DNS query to a DNS server lower in the hierarchy by sending back to the DNS client the name of that lowerlevel DNS server
First read the man page for dig and then answer the following questions
Starting with a root DNS server from one of the root servers am.rootservers.net initiate a sequence of queries for the IP address for your departments Web server by using dig
Show the list of the names of DNS servers in the delegation chain in answering your query
Repeat part a for several popular Web sites such as google.com yahoo.com or amazon.com
Suppose you can access the caches in the local DNS servers of your department
Can you propose a way to roughly determine the Web servers outside your department that are most popular among the users in your department Explain
Suppose that your department has a local DNS server for all computers in the department
You are an ordinary user i.e
not a networksystem administrator
Can you determine if an external Web site was likely accessed from a computer in your department a couple of seconds ago Explain
Consider distributing a file of F Gbits to N peers
The server has an upload rate of us Mbps and each peer has a download rate of di Mbps and an upload rate of u
For N and and u Kbps Kbps and Mbps prepare a chart giving the minimum distribution time for each of the combinations of N and u for both clientserver distribution and PP distribution
Consider distributing a file of F bits to N peers using a clientserver architecture
Assume a fluid model where the server can simultaneously transmit to multiple peers transmitting to each peer at different rates as long as the combined rate does not exceed us
Suppose that usNdmin
Specify a distribution scheme that has a distribution time of NFus
Suppose that usNdmin
Specify a distribution scheme that has a distribution time of Fdmin
Conclude that the minimum distribution time is in general given by maxNFus Fdmin
Consider distributing a file of F bits to N peers using a PP architecture
Assume a fluid model
For simplicity assume that dmin is very large so that peer download bandwidth is never a bottleneck
Suppose that ususuuNN
Specify a distribution scheme that has a distribution time of Fus
Suppose that ususuuNN
Specify a distribution scheme that has a distribution time of NFusuuN
Conclude that the minimum distribution time is in general given by maxFus NFusuuN
Consider an overlay network with N active peers with each pair of peers having an active TCP connection
Additionally suppose that the TCP connections pass through a total of M routers
How many nodes and edges are there in the corresponding overlay network P
Suppose Bob joins a BitTorrent torrent but he does not want to upload any data to any other peers so called freeriding
Bob claims that he can receive a complete copy of the file that is shared by the swarm
Is Bobs claim possible Why or why not b
Bob further claims that he can further make his freeriding more efficient by using a collection of multiple computers with distinct IP addresses in the computer lab in his department
How can he do that P
Consider a DASH system for which there are N video versions at N different rates and qualities and N audio versions at N different rates and qualities
Suppose we want to allow the player to choose at any time any of the N video versions and any of the N audio versions
If we create files so that the audio is mixed in with the video so server sends only one media stream at given time how many files will the server need to store each a different URL b
If the server instead sends the audio and video streams separately and has the client synchronize the streams how many files will the server need to store P
Install and compile the Python programs TCPClient and UDPClient on one host and TCPServer and UDPServer on another host
Suppose you run TCPClient before you run TCPServer
What happens Why b
Suppose you run UDPClient before you run UDPServer
What happens Why c
What happens if you use different port numbers for the client and server sides P
Suppose that in UDPClient.py after we create the socket we add the line clientSocket.bind Will it become necessary to change UDPServer.py What are the port numbers for the sockets in UDPClient and UDPServer What were they before making this change P
Can you configure your browser to open multiple simultaneous connections to a Web site What are the advantages and disadvantages of having a large number of simultaneous TCP connections P
We have seen that Internet TCP sockets treat the data being sent as a byte stream but UDP sockets recognize message boundaries
What are one advantage and one disadvantage of byteoriented API versus having the API explicitly recognize and preserve applicationdefined message boundaries P
What is the Apache Web server How much does it cost What functionality does it currently have You may want to look at Wikipedia to answer this question
Socket Programming Assignments The Companion Website includes six socket programming assignments
The first four assignments are summarized below
The fifth assignment makes use of the ICMP protocol and is summarized at the end of Chapter 
The sixth assignment employs multimedia protocols and is summarized at the end of Chapter 
It is highly recommended that students complete several if not all of these assignments
Students can find full details of these assignments as well as important snippets of the Python code at the Web site www.pearsonhighered.comcsresources
Assignment Web Server In this assignment you will develop a simple Web server in Python that is capable of processing only one request
Specifically your Web server will i create a connection socket when contacted by a client browser ii receive the HTTP request from this connection iii parse the request to determine the specific file being requested iv get the requested file from the servers file system v create an HTTP response message consisting of the requested file preceded by header lines and vi send the response over the TCP connection to the requesting browser
If a browser requests a file that is not present in your server your server should return a Not Found error message
In the Companion Website we provide the skeleton code for your server
Your job is to complete the code run your server and then test your server by sending requests from browsers running on different hosts
If you run your server on a host that already has a Web server running on it then you should use a different port than port for your Web server
Assignment UDP Pinger In this programming assignment you will write a client ping program in Python
Your client will send a simple ping message to a server receive a corresponding pong message back from the server and determine the delay between when the client sent the ping message and received the pong message
This delay is called the Round Trip Time RTT
The functionality provided by the client and server is similar to the functionality provided by standard ping program available in modern operating systems
However standard ping programs use the Internet Control Message Protocol ICMP which we will study in Chapter 
Here we will create a nonstandard but simple UDPbased ping program
Your ping program is to send ping messages to the target server over UDP
For each message your client is to determine and print the RTT when the corresponding pong message is returned
Because UDP is an unreliable protocol a packet sent by the client or server may be lost
For this reason the client cannot wait indefinitely for a reply to a ping message
You should have the client wait up to one second for a reply from the server if no reply is received the client should assume that the packet was lost and print a message accordingly
In this assignment you will be given the complete code for the server available in the Companion Website
Your job is to write the client code which will be very similar to the server code
It is recommended that you first study carefully the server code
You can then write your client code liberally cutting and pasting lines from the server code
Assignment Mail Client The goal of this programming assignment is to create a simple mail client that sends email to any recipient
Your client will need to establish a TCP connection with a mail server e.g
a Google mail server dialogue with the mail server using the SMTP protocol send an email message to a recipient e.g
your friend via the mail server and finally close the TCP connection with the mail server
For this assignment the Companion Website provides the skeleton code for your client
Your job is to complete the code and test your client by sending email to different user accounts
You may also try sending through different servers for example through a Google mail server and through your university mail server
Assignment MultiThreaded Web Proxy In this assignment you will develop a Web proxy
When your proxy receives an HTTP request for an object from a browser it generates a new HTTP request for the same object and sends it to the origin server
When the proxy receives the corresponding HTTP response with the object from the origin server it creates a new HTTP response including the object and sends it to the client
This proxy will be multithreaded so that it will be able to handle multiple requests at the same time
For this assignment the Companion Website provides the skeleton code for the proxy server
Your job is to complete the code and then test it by having different browsers request Web objects via your proxy
Wireshark Lab HTTP Having gotten our feet wet with the Wireshark packet sniffer in Lab were now ready to use Wireshark to investigate protocols in operation
In this lab well explore several aspects of the HTTP protocol the basic GETreply interaction HTTP message formats retrieving large HTML files retrieving HTML files with embedded URLs persistent and nonpersistent connections and HTTP authentication and security
As is the case with all Wireshark labs the full description of this lab is available at this books Web site www.pearsonhighered.comcsresources
Wireshark Lab DNS In this lab we take a closer look at the client side of the DNS the protocol that translates Internet hostnames to IP addresses
Recall from Section 
that the clients role in the DNS is relatively simple a client sends a query to its local DNS server and receives a response back
Much can go on under the covers invisible to the DNS clients as the hierarchical DNS servers communicate with each other to either recursively or iteratively resolve the clients DNS query
From the DNS clients standpoint however the protocol is quite simplea query is formulated to the local DNS server and a response is received from that server
We observe DNS in action in this lab
As is the case with all Wireshark labs the full description of this lab is available at this books Web site www.pearsonhighered.comcsresources
An Interview With Marc Andreessen Marc Andreessen is the cocreator of Mosaic the Web browser that popularized the World Wide Web in 
Mosaic had a clean easily understood interface and was the first browser to display images inline with text
In Marc Andreessen and Jim Clark founded Netscape whose browser was by far the most popular browser through the mids
Netscape also developed the Secure Sockets Layer SSL protocol and many Internet server products including mail servers and SSLbased Web servers
He is now a cofounder and general partner of venture capital firm Andreessen Horowitz overseeing portfolio development with holdings that include Facebook Foursquare Groupon Jawbone Twitter and Zynga
He serves on numerous boards including Bump eBay Glam Media Facebook and HewlettPackard
He holds a BS in Computer Science from the University of Illinois at UrbanaChampaign
How did you become interested in computing Did you always know that you wanted to work in information technology The video game and personal computing revolutions hit right when I was growing uppersonal computing was the new technology frontier in the late s and early s
And it wasnt just Apple and the IBM PC but hundreds of new companies like Commodore and Atari as well
I taught myself to program out of a book called Instant FreezeDried BASIC at age and got my first computer a TRS Color Computerlook it up at age 
Please describe one or two of the most exciting projects you have worked on during your career
What were the biggest challenges Undoubtedly the most exciting project was the original Mosaic web browser in and the biggest challenge was getting anyone to take it seriously back then
At the time everyone thought the interactive future would be delivered as interactive television by huge companies not as the Internet by startups
What excites you about the future of networking and the Internet What are your biggest concerns The most exciting thing is the huge unexplored frontier of applications and services that programmers and entrepreneurs are able to explorethe Internet has unleashed creativity at a level that I dont think weve ever seen before
My biggest concern is the principle of unintended consequenceswe dont always know the implications of what we do such as the Internet being used by governments to run a new level of surveillance on citizens
Is there anything in particular students should be aware of as Web technology advances The rate of changethe most important thing to learn is how to learnhow to flexibly adapt to changes in the specific technologies and how to keep an open mind on the new opportunities and possibilities as you move through your career
What people inspired you professionally Vannevar Bush Ted Nelson Doug Engelbart Nolan Bushnell Bill Hewlett and Dave Packard Ken Olsen Steve Jobs Steve Wozniak Andy Grove Grace Hopper Hedy Lamarr Alan Turing Richard Stallman
What are your recommendations for students who want to pursue careers in computing and information technology Go as deep as you possibly can on understanding how technology is created and then complement with learning how business works
Can technology solve the worlds problems No but we advance the standard of living of people through economic growth and most economic growth throughout history has come from technologyso thats as good as it gets
Chapter Transport Layer Residing between the application and network layers the transport layer is a central piece of the layered network architecture
It has the critical role of providing communication services directly to the application processes running on different hosts
The pedagogic approach we take in this chapter is to alternate between discussions of transportlayer principles and discussions of how these principles are implemented in existing protocols as usual particular emphasis will be given to Internet protocols in particular the TCP and UDP transportlayer protocols
Well begin by discussing the relationship between the transport and network layers
This sets the stage for examining the first critical function of the transport layerextending the network layers delivery service between two end systems to a delivery service between two applicationlayer processes running on the end systems
Well illustrate this function in our coverage of the Internets connectionless transport protocol UDP
Well then return to principles and confront one of the most fundamental problems in computer networkinghow two entities can communicate reliably over a medium that may lose and corrupt data
Through a series of increasingly complicated and realistic scenarios well build up an array of techniques that transport protocols use to solve this problem
Well then show how these principles are embodied in TCP the Internets connectionoriented transport protocol
Well next move on to a second fundamentally important problem in networkingcontrolling the transmission rate of transportlayer entities in order to avoid or recover from congestion within the network
Well consider the causes and consequences of congestion as well as commonly used congestioncontrol techniques
After obtaining a solid understanding of the issues behind congestion control well study TCPs approach to congestion control
Introduction and TransportLayer Services In the previous two chapters we touched on the role of the transport layer and the services that it provides
Lets quickly review what we have already learned about the transport layer
A transportlayer protocol provides for logical communication between application processes running on different hosts
By logical communication we mean that from an applications perspective it is as if the hosts running the processes were directly connected in reality the hosts may be on opposite sides of the planet connected via numerous routers and a wide range of link types
Application processes use the logical communication provided by the transport layer to send messages to each other free from the worry of the details of the physical infrastructure used to carry these messages
illustrates the notion of logical communication
As shown in Figure 
transportlayer protocols are implemented in the end systems but not in network routers
On the sending side the transport layer converts the applicationlayer messages it receives from a sending application process into transportlayer packets known as transportlayer segments in Internet terminology
This is done by possibly breaking the application messages into smaller chunks and adding a transportlayer header to each chunk to create the transportlayer segment
The transport layer then passes the segment to the network layer at the sending end system where the segment is encapsulated within a networklayer packet a datagram and sent to the destination
Its important to note that network routers act only on the networklayer fields of the datagram that is they do not examine the fields of the transportlayer segment encapsulated with the datagram
On the receiving side the network layer extracts the transportlayer segment from the datagram and passes the segment up to the transport layer
The transport layer then processes the received segment making the data in the segment available to the receiving application
More than one transportlayer protocol may be available to network applications
For example the Internet has two protocolsTCP and UDP
Each of these protocols provides a different set of transport layer services to the invoking application
Relationship Between Transport and Network Layers Recall that the transport layer lies just above the network layer in the protocol stack
Whereas a transportlayer protocol provides logical communication between Figure 
The transport layer provides logical rather than physical communication between application processes processes running on different hosts a networklayer protocol provides logicalcommunication between hosts
This distinction is subtle but important
Lets examine this distinction with the aid of a household analogy
Consider two houses one on the East Coast and the other on the West Coast with each house being home to a dozen kids
The kids in the East Coast household are cousins of the kids in the West Coast household
The kids in the two households love to write to each othereach kid writes each cousin every week with each letter delivered by the traditional postal service in a separate envelope
Thus each household sends letters to the other household every week
These kids would save a lot of money if they had email In each of the households there is one kidAnn in the West Coast house and Bill in the East Coast houseresponsible for mail collection and mail distribution
Each week Ann visits all her brothers and sisters collects the mail and gives the mail to a postalservice mail carrier who makes daily visits to the house
When letters arrive at the West Coast house Ann also has the job of distributing the mail to her brothers and sisters
Bill has a similar job on the East Coast
In this example the postal service provides logical communication between the two housesthe postal service moves mail from house to house not from person to person
On the other hand Ann and Bill provide logical communication among the cousinsAnn and Bill pick up mail from and deliver mail to their brothers and sisters
Note that from the cousins perspective Ann and Bill are the mail service even though Ann and Bill are only a part the endsystem part of the endtoend delivery process
This household example serves as a nice analogy for explaining how the transport layer relates to the network layer application messages letters in envelopes processes cousins hosts also called end systems houses transportlayer protocol Ann and Bill networklayer protocol postal service including mail carriers Continuing with this analogy note that Ann and Bill do all their work within their respective homes they are not involved for example in sorting mail in any intermediate mail center or in moving mail from one mail center to another
Similarly transportlayer protocols live in the end systems
Within an end system a transport protocol moves messages from application processes to the network edge that is the network layer and vice versa but it doesnt have any say about how the messages are moved within the network core
In fact as illustrated in Figure 
intermediate routers neither act on nor recognize any information that the transport layer may have added to the application messages
Continuing with our family saga suppose now that when Ann and Bill go on vacation another cousin pairsay Susan and Harveysubstitute for them and provide the householdinternal collection and delivery of mail
Unfortunately for the two families Susan and Harvey do not do the collection and delivery in exactly the same way as Ann and Bill
Being younger kids Susan and Harvey pick up and drop off the mail less frequently and occasionally lose letters which are sometimes chewed up by the family dog
Thus the cousinpair Susan and Harvey do not provide the same set of services that is the same service model as Ann and Bill
In an analogous manner a computer network may make available multiple transport protocols with each protocol offering a different service model to applications
The possible services that Ann and Bill can provide are clearly constrained by the possible services that the postal service provides
For example if the postal service doesnt provide a maximum bound on how long it can take to deliver mail between the two houses for example three days then there is no way that Ann and Bill can guarantee a maximum delay for mail delivery between any of the cousin pairs
In a similar manner the services that a transport protocol can provide are often constrained by the service model of the underlying networklayer protocol
If the networklayer protocol cannot provide delay or bandwidth guarantees for transportlayer segments sent between hosts then the transportlayer protocol cannot provide delay or bandwidth guarantees for application messages sent between processes
Nevertheless certain services can be offered by a transport protocol even when the underlying network protocol doesnt offer the corresponding service at the network layer
For example as well see in this chapter a transport protocol can offer reliable data transfer service to an application even when the underlying network protocol is unreliable that is even when the network protocol loses garbles or duplicates packets
As another example which well explore in Chapter when we discuss network security a transport protocol can use encryption to guarantee that application messages are not read by intruders even when the network layer cannot guarantee the confidentiality of transportlayer segments
Overview of the Transport Layer in the Internet Recall that the Internet makes two distinct transportlayer protocols available to the application layer
One of these protocols is UDP User Datagram Protocol which provides an unreliable connectionless service to the invoking application
The second of these protocols is TCP Transmission Control Protocol which provides a reliable connectionoriented service to the invoking application
When designing a network application the application developer must specify one of these two transport protocols
As we saw in Section 
the application developer selects between UDP and TCP when creating sockets
To simplify terminology we refer to the transportlayer packet as a segment
We mention however that the Internet literature for example the RFCs also refers to the transportlayer packet for TCP as a segment but often refers to the packet for UDP as a datagram
But this same Internet literature also uses the term datagram for the networklayer packet For an introductory book on computer networking such as this we believe that it is less confusing to refer to both TCP and UDP packets as segments and reserve the term datagram for the networklayer packet
Before proceeding with our brief introduction of UDP and TCP it will be useful to say a few words about the Internets network layer
Well learn about the network layer in detail in Chapters and 
The Internets networklayer protocol has a nameIP for Internet Protocol
IP provides logical communication between hosts
The IP service model is a besteffort delivery service
This means that IP makes its best effort to deliver segments between communicating hosts but it makes no guarantees
In particular it does not guarantee segment delivery it does not guarantee orderly delivery of segments and it does not guarantee the integrity of the data in the segments
For these reasons IP is said to be an unreliable service
We also mention here that every host has at least one network layer address a socalled IP address
Well examine IP addressing in detail in Chapter for this chapter we need only keep in mind that each host has an IP address
Having taken a glimpse at the IP service model lets now summarize the service models provided by UDP and TCP
The most fundamental responsibility of UDP and TCP is to extend IPs delivery service between two end systems to a delivery service between two processes running on the end systems
Extending hosttohost delivery to processtoprocess delivery is called transportlayer multiplexing and demultiplexing
Well discuss transportlayer multiplexing and demultiplexing in the next section
UDP and TCP also provide integrity checking by including errordetection fields in their segments headers
These two minimal transportlayer servicesprocesstoprocess data delivery and error checkingare the only two services that UDP provides In particular like IP UDP is an unreliable serviceit does not guarantee that data sent by one process will arrive intact or at all to the destination process
UDP is discussed in detail in Section 
TCP on the other hand offers several additional services to applications
First and foremost it provides reliable data transfer
Using flow control sequence numbers acknowledgments and timers techniques well explore in detail in this chapter TCP ensures that data is delivered from sending process to receiving process correctly and in order
TCP thus converts IPs unreliable service between end systems into a reliable data transport service between processes
TCP also provides congestion control
Congestion control is not so much a service provided to the invoking application as it is a service for the Internet as a whole a service for the general good
Loosely speaking TCP congestion control prevents any one TCP connection from swamping the links and routers between communicating hosts with an excessive amount of traffic
TCP strives to give each connection traversing a congested link an equal share of the link bandwidth
This is done by regulating the rate at which the sending sides of TCP connections can send traffic into the network
UDP traffic on the other hand is unregulated
An application using UDP transport can send at any rate it pleases for as long as it pleases
A protocol that provides reliable data transfer and congestion control is necessarily complex
Well need several sections to cover the principles of reliable data transfer and congestion control and additional sections to cover the TCP protocol itself
These topics are investigated in Sections 
The approach taken in this chapter is to alternate between basic principles and the TCP protocol
For example well first discuss reliable data transfer in a general setting and then discuss how TCP specifically provides reliable data transfer
Similarly well first discuss congestion control in a general setting and then discuss how TCP performs congestion control
But before getting into all this good stuff lets first look at transportlayer multiplexing and demultiplexing
Multiplexing and Demultiplexing In this section we discuss transportlayer multiplexing and demultiplexing that is extending the hostto host delivery service provided by the network layer to a processtoprocess delivery service for applications running on the hosts
In order to keep the discussion concrete well discuss this basic transportlayer service in the context of the Internet
We emphasize however that a multiplexingdemultiplexing service is needed for all computer networks
At the destination host the transport layer receives segments from the network layer just below
The transport layer has the responsibility of delivering the data in these segments to the appropriate application process running in the host
Lets take a look at an example
Suppose you are sitting in front of your computer and you are downloading Web pages while running one FTP session and two Telnet sessions
You therefore have four network application processes runningtwo Telnet processes one FTP process and one HTTP process
When the transport layer in your computer receives data from the network layer below it needs to direct the received data to one of these four processes
Lets now examine how this is done
First recall from Section 
that a process as part of a network application can have one or more sockets doors through which data passes from the network to the process and through which data passes from the process to the network
Thus as shown in Figure 
the transport layer in the receiving host does not actually deliver data directly to a process but instead to an intermediary socket
Because at any given time there can be more than one socket in the receiving host each socket has a unique identifier
The format of the identifier depends on whether the socket is a UDP or a TCP socket as well discuss shortly
Now lets consider how a receiving host directs an incoming transportlayer segment to the appropriate socket
Each transportlayer segment has a set of fields in the segment for this purpose
At the receiving end the transport layer examines these fields to identify the receiving socket and then directs the segment to that socket
This job of delivering the data in a transportlayer segment to the correct socket is called demultiplexing
The job of gathering data chunks at the source host from different sockets encapsulating each data chunk with header information that will later be used in demultiplexing to create segments and passing the segments to the network layer is called multiplexing
Note that the transport layer in the middle host Figure 
Transportlayer multiplexing and demultiplexing in Figure 
must demultiplex segments arriving from the network layer below to either process P or P above this is done by directing the arriving segments data to the corresponding processs socket
The transport layer in the middle host must also gather outgoing data from these sockets form transport layer segments and pass these segments down to the network layer
Although we have introduced multiplexing and demultiplexing in the context of the Internet transport protocols its important to realize that they are concerns whenever a single protocol at one layer at the transport layer or elsewhere is used by multiple protocols at the next higher layer
To illustrate the demultiplexing job recall the household analogy in the previous section
Each of the kids is identified by his or her name
When Bill receives a batch of mail from the mail carrier he performs a demultiplexing operation by observing to whom the letters are addressed and then hand delivering the mail to his brothers and sisters
Ann performs a multiplexing operation when she collects letters from her brothers and sisters and gives the collected mail to the mail person
Now that we understand the roles of transportlayer multiplexing and demultiplexing let us examine how it is actually done in a host
From the discussion above we know that transportlayer multiplexing requires that sockets have unique identifiers and that each segment have special fields that indicate the socket to which the segment is to be delivered
These special fields illustrated in Figure 
are the source port number field and the destination port number field
The UDP and TCP segments have other fields as well as discussed in the subsequent sections of this chapter
Each port number is a bit number ranging from to 
The port numbers ranging from to are called wellknown port numbers and are restricted which means that they are reserved for use by wellknown Figure 
Source and destination portnumber fields in a transportlayer segment application protocols such as HTTP which uses port number and FTP which uses port number 
The list of wellknown port numbers is given in RFC and is updated at httpwww.iana.org RFC 
When we develop a new application such as the simple application developed in Section 
we must assign the application a port number
It should now be clear how the transport layer could implement the demultiplexing service Each socket in the host could be assigned a port number and when a segment arrives at the host the transport layer examines the destination port number in the segment and directs the segment to the corresponding socket
The segments data then passes through the socket into the attached process
As well see this is basically how UDP does it
However well also see that multiplexingdemultiplexing in TCP is yet more subtle
Connectionless Multiplexing and Demultiplexing Recall from Section 
that the Python program running in a host can create a UDP socket with the line clientSocket socketAF_INET SOCK_DGRAM When a UDP socket is created in this manner the transport layer automatically assigns a port number to the socket
In particular the transport layer assigns a port number in the range to that is currently not being used by any other UDP port in the host
Alternatively we can add a line into our Python program after we create the socket to associate a specific port number say to this UDP socket via the socket bind method clientSocket.bind If the application developer writing the code were implementing the server side of a wellknown protocol then the developer would have to assign the corresponding wellknown port number
Typically the client side of the application lets the transport layer automatically and transparently assign the port number whereas the server side of the application assigns a specific port number
With port numbers assigned to UDP sockets we can now precisely describe UDP multiplexingdemultiplexing
Suppose a process in Host A with UDP port wants to send a chunk of application data to a process with UDP port in Host B
The transport layer in Host A creates a transportlayer segment that includes the application data the source port number the destination port number and two other values which will be discussed later but are unimportant for the current discussion
The transport layer then passes the resulting segment to the network layer
The network layer encapsulates the segment in an IP datagram and makes a besteffort attempt to deliver the segment to the receiving host
If the segment arrives at the receiving Host B the transport layer at the receiving host examines the destination port number in the segment and delivers the segment to its socket identified by port 
Note that Host B could be running multiple processes each with its own UDP socket and associated port number
As UDP segments arrive from the network Host B directs demultiplexes each segment to the appropriate socket by examining the segments destination port number
It is important to note that a UDP socket is fully identified by a twotuple consisting of a destination IP address and a destination port number
As a consequence if two UDP segments have different source IP addresses andor source port numbers but have the same destination IP address and destination port number then the two segments will be directed to the same destination process via the same destination socket
You may be wondering now what is the purpose of the source port number As shown in Figure 
in the AtoB segment the source port number serves as part of a return addresswhen B wants to send a segment back to A the destination port in the BtoA segment will take its value from the source port value of the AtoB segment
The complete return address is As IP address and the source port number
As an example recall the UDP server program studied in Section 
In UDPServer.py the server uses the recvfrom method to extract the clientside source port number from the segment it receives from the client it then sends a new segment to the client with the extracted source port number serving as the destination port number in this new segment
ConnectionOriented Multiplexing and Demultiplexing In order to understand TCP demultiplexing we have to take a close look at TCP sockets and TCP connection establishment
One subtle difference between a TCP socket and a UDP socket is that a TCP socket is identified by a fourtuple source IP address source port number destination IP address destination port number
Thus when a TCP segment arrives from the network to a host the host uses all four values to direct demultiplex the segment to the appropriate socket
The inversion of source and destination port numbers In particular and in contrast with UDP two arriving TCP segments with different source IP addresses or source port numbers will with the exception of a TCP segment carrying the original connection establishment request be directed to two different sockets
To gain further insight lets reconsider the TCP clientserver programming example in Section 
The TCP server application has a welcoming socket that waits for connectionestablishment requests from TCP clients see Figure 
on port number 
The TCP client creates a socket and sends a connection establishment request segment with the lines clientSocket socketAF_INET SOCK_STREAM clientSocket.connectserverName A connectionestablishment request is nothing more than a TCP segment with destination port number and a special connectionestablishment bit set in the TCP header discussed in Section 
The segment also includes a source port number that was chosen by the client
When the host operating system of the computer running the server process receives the incoming connectionrequest segment with destination port it locates the server process that is waiting to accept a connection on port number 
The server process then creates a new socket connectionSocket addr serverSocket.accept Also the transport layer at the server notes the following four values in the connectionrequest segment the source port number in the segment the IP address of the source host the destination port number in the segment and its own IP address
The newly created connection socket is identified by these four values all subsequently arriving segments whose source port source IP address destination port and destination IP address match these four values will be demultiplexed to this socket
With the TCP connection now in place the client and server can now send data to each other
The server host may support many simultaneous TCP connection sockets with each socket attached to a process and with each socket identified by its own fourtuple
When a TCP segment arrives at the host all four fields source IP address source port destination IP address destination port are used to direct demultiplex the segment to the appropriate socket
FOCUS ON SECURITY Port Scanning Weve seen that a server process waits patiently on an open port for contact by a remote client
Some ports are reserved for wellknown applications e.g
Web FTP DNS and SMTP servers other ports are used by convention by popular applications e.g
the Microsoft SQL server listens for requests on UDP port 
Thus if we determine that a port is open on a host we may be able to map that port to a specific application running on the host
This is very useful for system administrators who are often interested in knowing which network applications are running on the hosts in their networks
But attackers in order to case the joint also want to know which ports are open on target hosts
If a host is found to be running an application with a known security flaw e.g
a SQL server listening on port was subject to a buffer overflow allowing a remote user to execute arbitrary code on the vulnerable host a flaw exploited by the Slammer worm CERT then that host is ripe for attack
Determining which applications are listening on which ports is a relatively easy task
Indeed there are a number of public domain programs called port scanners that do just that
Perhaps the most widely used of these is nmap freely available at httpnmap.org and included in most Linux distributions
For TCP nmap sequentially scans ports looking for ports that are accepting TCP connections
For UDP nmap again sequentially scans ports looking for UDP ports that respond to transmitted UDP segments
In both cases nmap returns a list of open closed or unreachable ports
A host running nmap can attempt to scan any target host anywhere in the Internet
Well revisit nmap in Section 
when we discuss TCP connection management
Two clients using the same destination port number to communicate with the same Web server application The situation is illustrated in Figure 
in which Host C initiates two HTTP sessions to server B and Host A initiates one HTTP session to B
Hosts A and C and server B each have their own unique IP addressA C and B respectively
Host C assigns two different source port numbers and to its two HTTP connections
Because Host A is choosing source port numbers independently of C it might also assign a source port of to its HTTP connection
But this is not a problemserver B will still be able to correctly demultiplex the two connections having the same source port number since the two connections have different source IP addresses
Web Servers and TCP Before closing this discussion its instructive to say a few additional words about Web servers and how they use port numbers
Consider a host running a Web server such as an Apache Web server on port 
When clients for example browsers send segments to the server all segments will have destination port 
In particular both the initial connectionestablishment segments and the segments carrying HTTP request messages will have destination port 
As we have just described the server distinguishes the segments from the different clients using source IP addresses and source port numbers
shows a Web server that spawns a new process for each connection
As shown in Figure 
each of these processes has its own connection socket through which HTTP requests arrive and HTTP responses are sent
We mention however that there is not always a onetoone correspondence between connection sockets and processes
In fact todays highperforming Web servers often use only one process and create a new thread with a new connection socket for each new client connection
A thread can be viewed as a lightweight subprocess
If you did the first programming assignment in Chapter you built a Web server that does just this
For such a server at any given time there may be many connection sockets with different identifiers attached to the same process
If the client and server are using persistent HTTP then throughout the duration of the persistent connection the client and server exchange HTTP messages via the same server socket
However if the client and server use nonpersistent HTTP then a new TCP connection is created and closed for every requestresponse and hence a new socket is created and later closed for every requestresponse
This frequent creating and closing of sockets can severely impact the performance of a busy Web server although a number of operating system tricks can be used to mitigate the problem
Readers interested in the operating system issues surrounding persistent and nonpersistent HTTP are encouraged to see Nielsen Nahum 
Now that weve discussed transportlayer multiplexing and demultiplexing lets move on and discuss one of the Internets transport protocols UDP
In the next section well see that UDP adds little more to the networklayer protocol than a multiplexingdemultiplexing service
Connectionless Transport UDP In this section well take a close look at UDP how it works and what it does
We encourage you to refer back to Section 
which includes an overview of the UDP service model and to Section 
which discusses socket programming using UDP
To motivate our discussion about UDP suppose you were interested in designing a nofrills barebones transport protocol
How might you go about doing this You might first consider using a vacuous transport protocol
In particular on the sending side you might consider taking the messages from the application process and passing them directly to the network layer and on the receiving side you might consider taking the messages arriving from the network layer and passing them directly to the application process
But as we learned in the previous section we have to do a little more than nothing At the very least the transport layer has to provide a multiplexingdemultiplexing service in order to pass data between the network layer and the correct applicationlevel process
UDP defined in RFC does just about as little as a transport protocol can do
Aside from the multiplexingdemultiplexing function and some light error checking it adds nothing to IP
In fact if the application developer chooses UDP instead of TCP then the application is almost directly talking with IP
UDP takes messages from the application process attaches source and destination port number fields for the multiplexingdemultiplexing service adds two other small fields and passes the resulting segment to the network layer
The network layer encapsulates the transportlayer segment into an IP datagram and then makes a besteffort attempt to deliver the segment to the receiving host
If the segment arrives at the receiving host UDP uses the destination port number to deliver the segments data to the correct application process
Note that with UDP there is no handshaking between sending and receiving transportlayer entities before sending a segment
For this reason UDP is said to be connectionless
DNS is an example of an applicationlayer protocol that typically uses UDP
When the DNS application in a host wants to make a query it constructs a DNS query message and passes the message to UDP
Without performing any handshaking with the UDP entity running on the destination end system the hostside UDP adds header fields to the message and passes the resulting segment to the network layer
The network layer encapsulates the UDP segment into a datagram and sends the datagram to a name server
The DNS application at the querying host then waits for a reply to its query
If it doesnt receive a reply possibly because the underlying network lost the query or the reply it might try resending the query try sending the query to another name server or inform the invoking application that it cant get a reply
Now you might be wondering why an application developer would ever choose to build an application over UDP rather than over TCP
Isnt TCP always preferable since TCP provides a reliable data transfer service while UDP does not The answer is no as some applications are better suited for UDP for the following reasons Finer applicationlevel control over what data is sent and when
Under UDP as soon as an application process passes data to UDP UDP will package the data inside a UDP segment and immediately pass the segment to the network layer
TCP on the other hand has a congestion control mechanism that throttles the transportlayer TCP sender when one or more links between the source and destination hosts become excessively congested
TCP will also continue to resend a segment until the receipt of the segment has been acknowledged by the destination regardless of how long reliable delivery takes
Since realtime applications often require a minimum sending rate do not want to overly delay segment transmission and can tolerate some data loss TCPs service model is not particularly well matched to these applications needs
As discussed below these applications can use UDP and implement as part of the application any additional functionality that is needed beyond UDPs nofrills segmentdelivery service
No connection establishment
As well discuss later TCP uses a threeway handshake before it starts to transfer data
UDP just blasts away without any formal preliminaries
Thus UDP does not introduce any delay to establish a connection
This is probably the principal reason why DNS runs over UDP rather than TCPDNS would be much slower if it ran over TCP
HTTP uses TCP rather than UDP since reliability is critical for Web pages with text
But as we briefly discussed in Section 
the TCP connectionestablishment delay in HTTP is an important contributor to the delays associated with downloading Web documents
Indeed the QUIC protocol Quick UDP Internet Connection Iyengar used in Googles Chrome browser uses UDP as its underlying transport protocol and implements reliability in an applicationlayer protocol on top of UDP
No connection state
TCP maintains connection state in the end systems
This connection state includes receive and send buffers congestioncontrol parameters and sequence and acknowledgment number parameters
We will see in Section 
that this state information is needed to implement TCPs reliable data transfer service and to provide congestion control
UDP on the other hand does not maintain connection state and does not track any of these parameters
For this reason a server devoted to a particular application can typically support many more active clients when the application runs over UDP rather than TCP
Small packet header overhead
The TCP segment has bytes of header overhead in every segment whereas UDP has only bytes of overhead
lists popular Internet applications and the transport protocols that they use
As we expect e mail remote terminal access the Web and file transfer run over TCPall these applications need the reliable data transfer service of TCP
Nevertheless many important applications run over UDP rather than TCP
For example UDP is used to carry network management SNMP see Section 
UDP is preferred to TCP in this case since network management applications must often run when the network is in a stressed stateprecisely when reliable congestioncontrolled data transfer is difficult to achieve
Also as we mentioned earlier DNS runs over UDP thereby avoiding TCPs connection establishment delays
As shown in Figure 
both UDP and TCP are somtimes used today with multimedia applications such as Internet phone realtime video conferencing and streaming of stored audio and video
Well take a close look at these applications in Chapter 
We just mention now that all of these applications can tolerate a small amount of packet loss so that reliable data transfer is not absolutely critical for the applications success
Furthermore realtime applications like Internet phone and video conferencing react very poorly to TCPs congestion control
For these reasons developers of multimedia applications may choose to run their applications over UDP instead of TCP
When packet loss rates are low and with some organizations blocking UDP traffic for security reasons see Chapter TCP becomes an increasingly attractive protocol for streaming media transport
Popular Internet applications and their underlying transport protocols Although commonly done today running multimedia applications over UDP is controversial
As we mentioned above UDP has no congestion control
But congestion control is needed to prevent the network from entering a congested state in which very little useful work is done
If everyone were to start streaming highbitrate video without using any congestion control there would be so much packet overflow at routers that very few UDP packets would successfully traverse the sourcetodestination path
Moreover the high loss rates induced by the uncontrolled UDP senders would cause the TCP senders which as well see do decrease their sending rates in the face of congestion to dramatically decrease their rates
Thus the lack of congestion control in UDP can result in high loss rates between a UDP sender and receiver and the crowding out of TCP sessionsa potentially serious problem Floyd 
Many researchers have proposed new mechanisms to force all sources including UDP sources to perform adaptive congestion control Mahdavi Floyd Kohler RFC 
Before discussing the UDP segment structure we mention that it is possible for an application to have reliable data transfer when using UDP
This can be done if reliability is built into the application itself for example by adding acknowledgment and retransmission mechanisms such as those well study in the next section
We mentioned earlier that the QUIC protocol Iyengar used in Googles Chrome browser implements reliability in an applicationlayer protocol on top of UDP
But this is a nontrivial task that would keep an application developer busy debugging for a long time
Nevertheless building reliability directly into the application allows the application to have its cake and eat it too
That is application processes can communicate reliably without being subjected to the transmissionrate constraints imposed by TCPs congestioncontrol mechanism
UDP Segment Structure The UDP segment structure shown in Figure 
is defined in RFC 
The application data occupies the data field of the UDP segment
For example for DNS the data field contains either a query message or a response message
For a streaming audio application audio samples fill the data field
The UDP header has only four fields each consisting of two bytes
As discussed in the previous section the port numbers allow the destination host to pass the application data to the correct process running on the destination end system that is to perform the demultiplexing function
The length field specifies the number of bytes in the UDP segment header plus data
An explicit length value is needed since the size of the data field may differ from one UDP segment to the next
The checksum is used by the receiving host to check whether errors have been introduced into the segment
In truth the checksum is also calculated over a few of the fields in the IP header in addition to the UDP segment
But we ignore this detail in order to see the forest through the trees
Well discuss the checksum calculation below
Basic principles of error detection are described in Section 
The length field specifies the length of the UDP segment including the header in bytes
UDP Checksum The UDP checksum provides for error detection
That is the checksum is used to determine whether bits within the UDP segment have been altered for example by noise in the links or while stored in a router as it moved from source to destination
UDP segment structure UDP at the sender side performs the s complement of the sum of all the bit words in the segment with any overflow encountered during the sum being wrapped around
This result is put in the checksum field of the UDP segment
Here we give a simple example of the checksum calculation
You can find details about efficient implementation of the calculation in RFC and performance over real data in Stone Stone 
As an example suppose that we have the following three bit words The sum of first two of these bit words is Adding the third word to the above sum gives Note that this last addition had overflow which was wrapped around
The s complement is obtained by converting all the s to s and converting all the s to s
Thus the s complement of the sum is which becomes the checksum
At the receiver all four bit words are added including the checksum
If no errors are introduced into the packet then clearly the sum at the receiver will be 
If one of the bits is a then we know that errors have been introduced into the packet
You may wonder why UDP provides a checksum in the first place as many linklayer protocols including the popular Ethernet protocol also provide error checking
The reason is that there is no guarantee that all the links between source and destination provide error checking that is one of the links may use a linklayer protocol that does not provide error checking
Furthermore even if segments are correctly transferred across a link its possible that bit errors could be introduced when a segment is stored in a routers memory
Given that neither linkbylink reliability nor inmemory error detection is guaranteed UDP must provide error detection at the transport layer on an endend basis if the end end data transfer service is to provide error detection
This is an example of the celebrated endend principle in system design Saltzer which states that since certain functionality error detection in this case must be implemented on an endend basis functions placed at the lower levels may be redundant or of little value when compared to the cost of providing them at the higher level
Because IP is supposed to run over just about any layer protocol it is useful for the transport layer to provide error checking as a safety measure
Although UDP provides error checking it does not do anything to recover from an error
Some implementations of UDP simply discard the damaged segment others pass the damaged segment to the application with a warning
That wraps up our discussion of UDP
We will soon see that TCP offers reliable data transfer to its applications as well as other services that UDP doesnt offer
Naturally TCP is also more complex than UDP
Before discussing TCP however it will be useful to step back and first discuss the underlying principles of reliable data transfer
Principles of Reliable Data Transfer In this section we consider the problem of reliable data transfer in a general context
This is appropriate since the problem of implementing reliable data transfer occurs not only at the transport layer but also at the link layer and the application layer as well
The general problem is thus of central importance to networking
Indeed if one had to identify a topten list of fundamentally important problems in all of networking this would be a candidate to lead the list
In the next section well examine TCP and show in particular that TCP exploits many of the principles that we are about to describe
illustrates the framework for our study of reliable data transfer
The service abstraction provided to the upperlayer entities is that of a reliable channel through which data can be transferred
With a reliable channel no transferred data bits are corrupted flipped from to or vice versa or lost and all are delivered in the order in which they were sent
This is precisely the service model offered by TCP to the Internet applications that invoke it
It is the responsibility of a reliable data transfer protocol to implement this service abstraction
This task is made difficult by the fact that the layer below the reliable data transfer protocol may be unreliable
For example TCP is a reliable data transfer protocol that is implemented on top of an unreliable IP endtoend network layer
More generally the layer beneath the two reliably communicating end points might consist of a single physical link as in the case of a linklevel data transfer protocol or a global internetwork as in the case of a transportlevel protocol
For our purposes however we can view this lower layer simply as an unreliable pointtopoint channel
In this section we will incrementally develop the sender and receiver sides of a reliable data transfer protocol considering increasingly complex models of the underlying channel
For example well consider what protocol mechanisms are Figure 
Reliable data transfer Service model and service implementation needed when the underlying channel can corrupt bits or lose entire packets
One assumption well adopt throughout our discussion here is that packets will be delivered in the order in which they were sent with some packets possibly being lost that is the underlying channel will not reorder packets
Figure .b illustrates the interfaces for our data transfer protocol
The sending side of the data transfer protocol will be invoked from above by a call to rdt_send 
It will pass the data to be delivered to the upper layer at the receiving side
Here rdt stands for reliable data transfer protocol and _send indicates that the sending side of rdt is being called
The first step in developing any protocol is to choose a good name On the receiving side rdt_rcv will be called when a packet arrives from the receiving side of the channel
When the rdt protocol wants to deliver data to the upper layer it will do so by calling deliver_data 
In the following we use the terminology packet rather than transportlayer segment
Because the theory developed in this section applies to computer networks in general and not just to the Internet transport layer the generic term packet is perhaps more appropriate here
In this section we consider only the case of unidirectional data transfer that is data transfer from the sending to the receiving side
The case of reliable bidirectional that is fullduplex data transfer is conceptually no more difficult but considerably more tedious to explain
Although we consider only unidirectional data transfer it is important to note that the sending and receiving sides of our protocol will nonetheless need to transmit packets in both directions as indicated in Figure 
We will see shortly that in addition to exchanging packets containing the data to be transferred the sending and receiving sides of rdt will also need to exchange control packets back and forth
Both the send and receive sides of rdt send packets to the other side by a call to udt_send where udt stands for unreliable data transfer
Building a Reliable Data Transfer Protocol We now step through a series of protocols each one becoming more complex arriving at a flawless reliable data transfer protocol
Reliable Data Transfer over a Perfectly Reliable Channel rdt
We first consider the simplest case in which the underlying channel is completely reliable
The protocol itself which well call rdt
The finitestate machine FSM definitions for the rdt
sender and receiver are shown in Figure 
The FSM in Figure .a defines the operation of the sender while the FSM in Figure .b defines the operation of the receiver
It is important to note that there are separate FSMs for the sender and for the receiver
The sender and receiver FSMs in Figure 
each have just one state
The arrows in the FSM description indicate the transition of the protocol from one state to another
Since each FSM in Figure 
has just one state a transition is necessarily from the one state back to itself well see more complicated state diagrams shortly
The event causing the transition is shown above the horizontal line labeling the transition and the actions taken when the event occurs are shown below the horizontal line
When no action is taken on an event or no event occurs and an action is taken well use the symbol Î below or above the horizontal respectively to explicitly denote the lack of an action or event
The initial state of the FSM is indicated by the dashed arrow
Although the FSMs in Figure 
have but one state the FSMs we will see shortly have multiple states so it will be important to identify the initial state of each FSM
The sending side of rdt simply accepts data from the upper layer via the rdt_senddata event creates a packet containing the data via the action make_pktdata and sends the packet into the channel
In practice the rdt_senddata event would result from a procedure call for example to rdt_send by the upperlayer application
A protocol for a completely reliable channel On the receiving side rdt receives a packet from the underlying channel via the rdt_rcvpacket event removes the data from the packet via the action extract packet data and passes the data up to the upper layer via the action deliver_datadata 
In practice the rdt_rcvpacket event would result from a procedure call for example to rdt_rcv from the lowerlayer protocol
In this simple protocol there is no difference between a unit of data and a packet
Also all packet flow is from the sender to receiver with a perfectly reliable channel there is no need for the receiver side to provide any feedback to the sender since nothing can go wrong Note that we have also assumed that the receiver is able to receive data as fast as the sender happens to send data
Thus there is no need for the receiver to ask the sender to slow down Reliable Data Transfer over a Channel with Bit Errors rdt
A more realistic model of the underlying channel is one in which bits in a packet may be corrupted
Such bit errors typically occur in the physical components of a network as a packet is transmitted propagates or is buffered
Well continue to assume for the moment that all transmitted packets are received although their bits may be corrupted in the order in which they were sent
Before developing a protocol for reliably communicating over such a channel first consider how people might deal with such a situation
Consider how you yourself might dictate a long message over the phone
In a typical scenario the message taker might say OK after each sentence has been heard understood and recorded
If the message taker hears a garbled sentence youre asked to repeat the garbled sentence
This messagedictation protocol uses both positive acknowledgments OK and negative acknowledgments Please repeat that
These control messages allow the receiver to let the sender know what has been received correctly and what has been received in error and thus requires repeating
In a computer network setting reliable data transfer protocols based on such retransmission are known as ARQ Automatic Repeat reQuest protocols
Fundamentally three additional protocol capabilities are required in ARQ protocols to handle the presence of bit errors Error detection
First a mechanism is needed to allow the receiver to detect when bit errors have occurred
Recall from the previous section that UDP uses the Internet checksum field for exactly this purpose
In Chapter well examine errordetection and correction techniques in greater detail these techniques allow the receiver to detect and possibly correct packet bit errors
For now we need only know that these techniques require that extra bits beyond the bits of original data to be transferred be sent from the sender to the receiver these bits will be gathered into the packet checksum field of the rdt
Since the sender and receiver are typically executing on different end systems possibly separated by thousands of miles the only way for the sender to learn of the receivers view of the world in this case whether or not a packet was received correctly is for the receiver to provide explicit feedback to the sender
The positive ACK and negative NAK acknowledgment replies in the messagedictation scenario are examples of such feedback
protocol will similarly send ACK and NAK packets back from the receiver to the sender
In principle these packets need only be one bit long for example a value could indicate a NAK and a value of could indicate an ACK
A packet that is received in error at the receiver will be retransmitted by the sender
shows the FSM representation of rdt
a data transfer protocol employing error detection positive acknowledgments and negative acknowledgments
The send side of rdt
has two states
In the leftmost state the sendside protocol is waiting for data to be passed down from the upper layer
When the rdt_senddata event occurs the sender will create a packet sndpkt containing the data to be sent along with a packet checksum for example as discussed in Section 
for the case of a UDP segment and then send the packet via the udt_sendsndpkt operation
In the rightmost state the sender protocol is waiting for an ACK or a NAK packet from the receiver
If an ACK packet is received Figure 
A protocol for a channel with bit errors the notation rdt_rcvrcvpkt isACK rcvpkt in Figure 
corresponds to this event the sender knows that the most recently transmitted packet has been received correctly and thus the protocol returns to the state of waiting for data from the upper layer
If a NAK is received the protocol retransmits the last packet and waits for an ACK or NAK to be returned by the receiver in response to the retransmitted data packet
It is important to note that when the sender is in the waitforACKorNAK state it cannot get more data from the upper layer that is the rdt_send event can not occur that will happen only after the sender receives an ACK and leaves this state
Thus the sender will not send a new piece of data until it is sure that the receiver has correctly received the current packet
Because of this behavior protocols such as rdt
are known as stopandwait protocols
The receiverside FSM for rdt
still has a single state
On packet arrival the receiver replies with either an ACK or a NAK depending on whether or not the received packet is corrupted
In Figure 
the notation rdt_rcvrcvpkt corruptrcvpkt corresponds to the event in which a packet is received and is found to be in error
may look as if it works but unfortunately it has a fatal flaw
In particular we havent accounted for the possibility that the ACK or NAK packet could be corrupted Before proceeding on you should think about how this problem may be fixed
Unfortunately our slight oversight is not as innocuous as it may seem
Minimally we will need to add checksum bits to ACKNAK packets in order to detect such errors
The more difficult question is how the protocol should recover from errors in ACK or NAK packets
The difficulty here is that if an ACK or NAK is corrupted the sender has no way of knowing whether or not the receiver has correctly received the last piece of transmitted data
Consider three possibilities for handling corrupted ACKs or NAKs For the first possibility consider what a human might do in the messagedictation scenario
If the speaker didnt understand the OK or Please repeat that reply from the receiver the speaker would probably ask What did you say thus introducing a new type of sendertoreceiver packet to our protocol
The receiver would then repeat the reply
But what if the speakers What did you say is corrupted The receiver having no idea whether the garbled sentence was part of the dictation or a request to repeat the last reply would probably then respond with What did you say And then of course that response might be garbled
Clearly were heading down a difficult path
A second alternative is to add enough checksum bits to allow the sender not only to detect but also to recover from bit errors
This solves the immediate problem for a channel that can corrupt packets but not lose them
A third approach is for the sender simply to resend the current data packet when it receives a garbled ACK or NAK packet
This approach however introduces duplicate packets into the sendertoreceiver channel
The fundamental difficulty with duplicate packets is that the receiver doesnt know whether the ACK or NAK it last sent was received correctly at the sender
Thus it cannot know a priori whether an arriving packet contains new data or is a retransmission A simple solution to this new problem and one adopted in almost all existing data transfer protocols including TCP is to add a new field to the data packet and have the sender number its data packets by putting a sequence number into this field
The receiver then need only check this sequence number to determine whether or not the received packet is a retransmission
For this simple case of a stopand wait protocol a bit sequence number will suffice since it will allow the receiver to know whether the sender is resending the previously transmitted packet the sequence number of the received packet has the same sequence number as the most recently received packet or a new packet the sequence number changes moving forward in modulo arithmetic
Since we are currently assuming a channel that does not lose packets ACK and NAK packets do not themselves need to indicate the sequence number of the packet they are acknowledging
The sender knows that a received ACK or NAK packet whether garbled or not was generated in response to its most recently transmitted data packet
show the FSM description for rdt
our fixed version of rdt
sender and receiver FSMs each now have twice as many states as before
This is because the protocol state must now reflect whether the packet currently being sent by the sender or expected at the receiver should have a sequence number of or 
Note that the actions in those states where a numbered packet is being sent or expected are mirror images of those where a numbered packet is being sent or expected the only differences have to do with the handling of the sequence number
uses both positive and negative acknowledgments from the receiver to the sender
When an outoforder packet is received the receiver sends a positive acknowledgment for the packet it has received
When a corrupted packet Figure 
sender Figure 
receiver is received the receiver sends a negative acknowledgment
We can accomplish the same effect as a NAK if instead of sending a NAK we send an ACK for the last correctly received packet
A sender that receives two ACKs for the same packet that is receives duplicate ACKs knows that the receiver did not correctly receive the packet following the packet that is being ACKed twice
Our NAKfree reliable data transfer protocol for a channel with bit errors is rdt
shown in Figures 
One subtle change between rtdt
is that the receiver must now include the sequence number of the packet being acknowledged by an ACK message this is done by including the ACK or ACK argument in make_pkt in the receiver FSM and the sender must now check the sequence number of the packet being acknowledged by a received ACK message this is done by including the or argument in isACK in the sender FSM
Reliable Data Transfer over a Lossy Channel with Bit Errors rdt
Suppose now that in addition to corrupting bits the underlying channel can lose packets as well a not uncommon event in todays computer networks including the Internet
Two additional concerns must now be addressed by the protocol how to detect packet loss and what to do when packet loss occurs
The use of checksumming sequence numbers ACK packets and retransmissionsthe techniques Figure 
sender already developed in rdt
will allow us to answer the latter concern
Handling the first concern will require adding a new protocol mechanism
There are many possible approaches toward dealing with packet loss several more of which are explored in the exercises at the end of the chapter
Here well put the burden of detecting and recovering from lost packets on the sender
Suppose that the sender transmits a data packet and either that packet or the receivers ACK of that packet gets lost
In either case no reply is forthcoming at the sender from the receiver
If the sender is willing to wait long enough so that it is certain that a packet has been lost it can simply retransmit the data packet
You should convince yourself that this protocol does indeed work
But how long must the sender wait to be certain that something has been lost The sender must clearly wait at least as long as a roundtrip delay between the sender and receiver which may include buffering at intermediate routers plus whatever amount of time is needed to process a packet at the receiver
In many networks this worstcase maximum delay is very difficult even to estimate much less know with certainty
Moreover the protocol should ideally recover from packet loss as soon as possible waiting for a worstcase delay could mean a long wait until error recovery Figure 
receiver is initiated
The approach thus adopted in practice is for the sender to judiciously choose a time value such that packet loss is likely although not guaranteed to have happened
If an ACK is not received within this time the packet is retransmitted
Note that if a packet experiences a particularly large delay the sender may retransmit the packet even though neither the data packet nor its ACK have been lost
This introduces the possibility of duplicate data packets in the sendertoreceiver channel
Happily protocol rdt
already has enough functionality that is sequence numbers to handle the case of duplicate packets
From the senders viewpoint retransmission is a panacea
The sender does not know whether a data packet was lost an ACK was lost or if the packet or ACK was simply overly delayed
In all cases the action is the same retransmit
Implementing a timebased retransmission mechanism requires a countdown timer that can interrupt the sender after a given amount of time has expired
The sender will thus need to be able to start the timer each time a packet either a firsttime packet or a retransmission is sent respond to a timer interrupt taking appropriate actions and stop the timer
shows the sender FSM for rdt
a protocol that reliably transfers data over a channel that can corrupt or lose packets in the homework problems youll be asked to provide the receiver FSM for rdt
shows how the protocol operates with no lost or delayed packets and how it handles lost data packets
In Figure 
time moves forward from the top of the diagram toward the bottom of the Figure 
sender diagram note that a receive time for a packet is necessarily later than the send time for a packet as a result of transmission and propagation delays
In Figures .bd the sendside brackets indicate the times at which a timer is set and later times out
Several of the more subtle aspects of this protocol are explored in the exercises at the end of this chapter
Because packet sequence numbers alternate between and protocol rdt
is sometimes known as the alternatingbit protocol
We have now assembled the key elements of a data transfer protocol
Checksums sequence numbers timers and positive and negative acknowledgment packets each play a crucial and necessary role in the operation of the protocol
We now have a working reliable data transfer protocol Developing a protocol and FSM representation for a simple applicationlayer protocol 
Pipelined Reliable Data Transfer Protocols Protocol rdt
is a functionally correct protocol but it is unlikely that anyone would be happy with its performance particularly in todays highspeed networks
At the heart of rdt
s performance problem is the fact that it is a stopandwait protocol
Operation of rdt
the alternatingbit protocol Figure 
Stopandwait versus pipelined protocol To appreciate the performance impact of this stopandwait behavior consider an idealized case of two hosts one located on the West Coast of the United States and the other located on the East Coast as shown in Figure 
The speedoflight roundtrip propagation delay between these two end systems RTT is approximately milliseconds
Suppose that they are connected by a channel with a transmission rate R of Gbps bits per second
With a packet size L of bytes bits per packet including both header fields and data the time needed to actually transmit the packet into the Gbps link is dtransLR bitspacket bitssec microseconds Figure .a shows that with our stopandwait protocol if the sender begins sending the packet at t then at tLR microseconds the last bit enters the channel at the sender side
The packet then makes its msec crosscountry journey with the last bit of the packet emerging at the receiver at tRTTLR 
Assuming for simplicity that ACK packets are extremely small so that we can ignore their transmission time and that the receiver can send an ACK as soon as the last bit of a data packet is received the ACK emerges back at the sender at tRTTLR
At this point the sender can now transmit the next message
Thus in 
msec the sender was sending for only 
If we define the utilization of the sender or the channel as the fraction of time the sender is actually busy sending bits into the channel the analysis in Figure .a shows that the stopand wait protocol has a rather dismal sender utilization Usender of UsenderLRRTTLR 
Stopandwait and pipelined sending That is the sender was busy only 
hundredths of one percent of the time Viewed another way the sender was able to send only bytes in 
milliseconds an effective throughput of only kbpseven though a Gbps link was available Imagine the unhappy network manager who just paid a fortune for a gigabit capacity link but manages to get a throughput of only kilobits per second This is a graphic example of how network protocols can limit the capabilities provided by the underlying network hardware
Also we have neglected lowerlayer protocolprocessing times at the sender and receiver as well as the processing and queuing delays that would occur at any intermediate routers between the sender and receiver
Including these effects would serve only to further increase the delay and further accentuate the poor performance
The solution to this particular performance problem is simple Rather than operate in a stopandwait manner the sender is allowed to send multiple packets without waiting for acknowledgments as illustrated in Figure .b
Figure .b shows that if the sender is allowed to transmit three packets before having to wait for acknowledgments the utilization of the sender is essentially tripled
Since the many intransit sendertoreceiver packets can be visualized as filling a pipeline this technique is known as pipelining
Pipelining has the following consequences for reliable data transfer protocols The range of sequence numbers must be increased since each intransit packet not counting retransmissions must have a unique sequence number and there may be multiple intransit unacknowledged packets
The sender and receiver sides of the protocols may have to buffer more than one packet
Minimally the sender will have to buffer packets that have been transmitted but not yet acknowledged
Buffering of correctly received packets may also be needed at the receiver as discussed below
The range of sequence numbers needed and the buffering requirements will depend on the manner in which a data transfer protocol responds to lost corrupted and overly delayed packets
Two basic approaches toward pipelined error recovery can be identified GoBackN and selective repeat
GoBackN GBN In a GoBackN GBN protocol the sender is allowed to transmit multiple packets when available without waiting for an acknowledgment but is constrained to have no more than some maximum allowable number N of unacknowledged packets in the pipeline
We describe the GBN protocol in some detail in this section
But before reading on you are encouraged to play with the GBN applet an awesome applet at the companion Web site
shows the senders view of the range of sequence numbers in a GBN protocol
If we define base to be the sequence number of the oldest unacknowledged Figure 
Senders view of sequence numbers in GoBackN packet and nextseqnum to be the smallest unused sequence number that is the sequence number of the next packet to be sent then four intervals in the range of sequence numbers can be identified
Sequence numbers in the interval base correspond to packets that have already been transmitted and acknowledged
The interval base nextseqnum corresponds to packets that have been sent but not yet acknowledged
Sequence numbers in the interval nextseqnum baseN can be used for packets that can be sent immediately should data arrive from the upper layer
Finally sequence numbers greater than or equal to baseN cannot be used until an unacknowledged packet currently in the pipeline specifically the packet with sequence number base has been acknowledged
As suggested by Figure 
the range of permissible sequence numbers for transmitted but not yet acknowledged packets can be viewed as a window of size N over the range of sequence numbers
As the protocol operates this window slides forward over the sequence number space
For this reason N is often referred to as the window size and the GBN protocol itself as a slidingwindow protocol
You might be wondering why we would even limit the number of outstanding unacknowledged packets to a value of N in the first place
Why not allow an unlimited number of such packets Well see in Section 
that flow control is one reason to impose a limit on the sender
Well examine another reason to do so in Section 
when we study TCP congestion control
In practice a packets sequence number is carried in a fixedlength field in the packet header
If k is the number of bits in the packet sequence number field the range of sequence numbers is thus k
With a finite range of sequence numbers all arithmetic involving sequence numbers must then be done using modulo k arithmetic
That is the sequence number space can be thought of as a ring of size k where sequence number k is immediately followed by sequence number 
Recall that rdt
had a bit sequence number and a range of sequence numbers of 
Several of the problems at the end of this chapter explore the consequences of a finite range of sequence numbers
We will see in Section 
that TCP has a bit sequence number field where TCP sequence numbers count bytes in the byte stream rather than packets
give an extended FSM description of the sender and receiver sides of an ACK based NAKfree GBN protocol
We refer to this FSM Figure 
Extended FSM description of the GBN sender Figure 
Extended FSM description of the GBN receiver description as an extended FSM because we have added variables similar to programminglanguage variables for base and nextseqnum and added operations on these variables and conditional actions involving these variables
Note that the extended FSM specification is now beginning to look somewhat like a programminglanguage specification
Bochman provides an excellent survey of additional extensions to FSM techniques as well as other programminglanguagebased techniques for specifying protocols
The GBN sender must respond to three types of events Invocation from above
When rdt_send is called from above the sender first checks to see if the window is full that is whether there are N outstanding unacknowledged packets
If the window is not full a packet is created and sent and variables are appropriately updated
If the window is full the sender simply returns the data back to the upper layer an implicit indication that the window is full
The upper layer would presumably then have to try again later
In a real implementation the sender would more likely have either buffered but not immediately sent this data or would have a synchronization mechanism for example a semaphore or a flag that would allow the upper layer to call rdt_send only when the window is not full
Receipt of an ACK
In our GBN protocol an acknowledgment for a packet with sequence number n will be taken to be a cumulative acknowledgment indicating that all packets with a sequence number up to and including n have been correctly received at the receiver
Well come back to this issue shortly when we examine the receiver side of GBN
A timeout event
The protocols name GoBackN is derived from the senders behavior in the presence of lost or overly delayed packets
As in the stopandwait protocol a timer will again be used to recover from lost data or acknowledgment packets
If a timeout occurs the sender resends all packets that have been previously sent but that have not yet been acknowledged
Our sender in Figure 
uses only a single timer which can be thought of as a timer for the oldest transmitted but not yet acknowledged packet
If an ACK is received but there are still additional transmitted but not yet acknowledged packets the timer is restarted
If there are no outstanding unacknowledged packets the timer is stopped
The receivers actions in GBN are also simple
If a packet with sequence number n is received correctly and is in order that is the data last delivered to the upper layer came from a packet with sequence number n the receiver sends an ACK for packet n and delivers the data portion of the packet to the upper layer
In all other cases the receiver discards the packet and resends an ACK for the most recently received inorder packet
Note that since packets are delivered one at a time to the upper layer if packet k has been received and delivered then all packets with a sequence number lower than k have also been delivered
Thus the use of cumulative acknowledgments is a natural choice for GBN
In our GBN protocol the receiver discards outoforder packets
Although it may seem silly and wasteful to discard a correctly received but outoforder packet there is some justification for doing so
Recall that the receiver must deliver data in order to the upper layer
Suppose now that packet n is expected but packet n arrives
Because data must be delivered in order the receiver could buffer save packet n and then deliver this packet to the upper layer after it had later received and delivered packet n
However if packet n is lost both it and packet n will eventually be retransmitted as a result of the GBN retransmission rule at the sender
Thus the receiver can simply discard packet n
The advantage of this approach is the simplicity of receiver bufferingthe receiver need not buffer any out oforder packets
Thus while the sender must maintain the upper and lower bounds of its window and the position of nextseqnum within this window the only piece of information the receiver need maintain is the sequence number of the next inorder packet
This value is held in the variable expectedseqnum shown in the receiver FSM in Figure 
Of course the disadvantage of throwing away a correctly received packet is that the subsequent retransmission of that packet might be lost or garbled and thus even more retransmissions would be required
shows the operation of the GBN protocol for the case of a window size of four packets
Because of this window size limitation the sender sends packets through but then must wait for one or more of these packets to be acknowledged before proceeding
As each successive ACK for example ACK and ACK is received the window slides forward and the sender can transmit one new packet pkt and pkt respectively
On the receiver side packet is lost and thus packets and are found to be out of order and are discarded
Before closing our discussion of GBN it is worth noting that an implementation of this protocol in a protocol stack would likely have a structure similar to that of the extended FSM in Figure 
The implementation would also likely be in the form of various procedures that implement the actions to be taken in response to the various events that can occur
In such eventbased programming the various procedures are called invoked either by other procedures in the protocol stack or as the result of an interrupt
In the sender these events would be a call from the upperlayer entity to invoke rdt_send a timer interrupt and a call from the lower layer to invoke rdt_rcv when a packet arrives
The programming exercises at the end of this chapter will give you a chance to actually implement these routines in a simulated but realistic network setting
We note here that the GBN protocol incorporates almost all of the techniques that we will encounter when we study the reliable data transfer components of TCP in Section 
These techniques include the use of sequence numbers cumulative acknowledgments checksums and a timeoutretransmit operation
GoBackN in operation 
Selective Repeat SR The GBN protocol allows the sender to potentially fill the pipeline in Figure 
with packets thus avoiding the channel utilization problems we noted with stopandwait protocols
There are however scenarios in which GBN itself suffers from performance problems
In particular when the window size and bandwidthdelay product are both large many packets can be in the pipeline
A single packet error can thus cause GBN to retransmit a large number of packets many unnecessarily
As the probability of channel errors increases the pipeline can become filled with these unnecessary retransmissions
Imagine in our messagedictation scenario that if every time a word was garbled the surrounding words for example a window size of words had to be repeated
The dictation would be slowed by all of the reiterated words
As the name suggests selectiverepeat protocols avoid unnecessary retransmissions by having the sender retransmit only those packets that it suspects were received in error that is were lost or corrupted at the receiver
This individual asneeded retransmission will require that the receiver individually acknowledge correctly received packets
A window size of N will again be used to limit the number of outstanding unacknowledged packets in the pipeline
However unlike GBN the sender will have already received ACKs for some of the packets in the window
shows the SR senders view of the sequence number space
details the various actions taken by the SR sender
The SR receiver will acknowledge a correctly received packet whether or not it is in order
Outoforder packets are buffered until any missing packets that is packets with lower sequence numbers are received at which point a batch of packets can be delivered in order to the upper layer
itemizes the various actions taken by the SR receiver
shows an example of SR operation in the presence of lost packets
Note that in Figure 
the receiver initially buffers packets and and delivers them together with packet to the upper layer when packet is finally received
Selectiverepeat SR sender and receiver views of sequencenumber space Figure 
SR sender events and actions Figure 
SR receiver events and actions It is important to note that in Step in Figure 
the receiver reacknowledges rather than ignores already received packets with certain sequence numbers below the current window base
You should convince yourself that this reacknowledgment is indeed needed
Given the sender and receiver sequence number spaces in Figure 
for example if there is no ACK for packet send_base propagating from the Figure 
SR operation receiver to the sender the sender will eventually retransmit packet send_base even though it is clear to us not the sender that the receiver has already received that packet
If the receiver were not to acknowledge this packet the senders window would never move forward This example illustrates an important aspect of SR protocols and many other protocols as well
The sender and receiver will not always have an identical view of what has been received correctly and what has not
For SR protocols this means that the sender and receiver windows will not always coincide
The lack of synchronization between sender and receiver windows has important consequences when we are faced with the reality of a finite range of sequence numbers
Consider what could happen for example with a finite range of four packet sequence numbers and a window size of three
Suppose packets through are transmitted and correctly received and acknowledged at the receiver
At this point the receivers window is over the fourth fifth and sixth packets which have sequence numbers and respectively
Now consider two scenarios
In the first scenario shown in Figure .a the ACKs for the first three packets are lost and the sender retransmits these packets
The receiver thus next receives a packet with sequence number a copy of the first packet sent
In the second scenario shown in Figure .b the ACKs for the first three packets are all delivered correctly
The sender thus moves its window forward and sends the fourth fifth and sixth packets with sequence numbers and respectively
The packet with sequence number is lost but the packet with sequence number arrivesa packet containing new data
Now consider the receivers viewpoint in Figure 
which has a figurative curtain between the sender and the receiver since the receiver cannot see the actions taken by the sender
All the receiver observes is the sequence of messages it receives from the channel and sends into the channel
As far as it is concerned the two scenarios in Figure 
There is no way of distinguishing the retransmission of the first packet from an original transmission of the fifth packet
Clearly a window size that is less than the size of the sequence number space wont work
But how small must the window size be A problem at the end of the chapter asks you to show that the window size must be less than or equal to half the size of the sequence number space for SR protocols
At the companion Web site you will find an applet that animates the operation of the SR protocol
Try performing the same experiments that you did with the GBN applet
Do the results agree with what you expect This completes our discussion of reliable data transfer protocols
Weve covered a lot of ground and introduced numerous mechanisms that together provide for reliable data transfer
summarizes these mechanisms
Now that we have seen all of these mechanisms in operation and can see the big picture we encourage you to review this section again to see how these mechanisms were incrementally added to cover increasingly complex and realistic models of the channel connecting the sender and receiver or to improve the performance of the protocols
Lets conclude our discussion of reliable data transfer protocols by considering one remaining assumption in our underlying channel model
Recall that we have assumed that packets cannot be reordered within the channel between the sender and receiver
This is generally a reasonable assumption when the sender and receiver are connected by a single physical wire
However when the channel connecting the two is a network packet reordering can occur
One manifestation of packet reordering is that old copies of a packet with a sequence or acknowledgment Figure 
SR receiver dilemma with toolarge windows A new packet or a retransmission Table 
Summary of reliable data transfer mechanisms and their use Mechanism Use Comments Checksum Used to detect bit errors in a transmitted packet
Timer Used to timeoutretransmit a packet possibly because the packet or its ACK was lost within the channel
Because timeouts can occur when a packet is delayed but not lost premature timeout or when a packet has been received by the receiver but the receivertosender ACK has been lost duplicate copies of a packet may be received by a receiver
Sequence Used for sequential numbering of packets of data flowing from sender to number receiver
Gaps in the sequence numbers of received packets allow the receiver to detect a lost packet
Packets with duplicate sequence numbers allow the receiver to detect duplicate copies of a packet
Acknowledgment Used by the receiver to tell the sender that a packet or set of packets has been received correctly
Acknowledgments will typically carry the sequence number of the packet or packets being acknowledged
Acknowledgments may be individual or cumulative depending on the protocol
Negative Used by the receiver to tell the sender that a packet has not been received acknowledgment correctly
Negative acknowledgments will typically carry the sequence number of the packet that was not received correctly
Window The sender may be restricted to sending only packets with sequence numbers pipelining that fall within a given range
By allowing multiple packets to be transmitted but not yet acknowledged sender utilization can be increased over a stopandwait mode of operation
Well see shortly that the window size may be set on the basis of the receivers ability to receive and buffer messages or the level of congestion in the network or both
number of x can appear even though neither the senders nor the receivers window contains x
With packet reordering the channel can be thought of as essentially buffering packets and spontaneously emitting these packets at any point in the future
Because sequence numbers may be reused some care must be taken to guard against such duplicate packets
The approach taken in practice is to ensure that a sequence number is not reused until the sender is sure that any previously sent packets with sequence number x are no longer in the network
This is done by assuming that a packet cannot live in the network for longer than some fixed maximum amount of time
A maximum packet lifetime of approximately three minutes is assumed in the TCP extensions for highspeed networks RFC 
Sunshine describes a method for using sequence numbers such that reordering problems can be completely avoided
ConnectionOriented Transport TCP Now that we have covered the underlying principles of reliable data transfer lets turn to TCPthe Internets transportlayer connectionoriented reliable transport protocol
In this section well see that in order to provide reliable data transfer TCP relies on many of the underlying principles discussed in the previous section including error detection retransmissions cumulative acknowledgments timers and header fields for sequence and acknowledgment numbers
TCP is defined in RFC RFC RFC RFC and RFC 
The TCP Connection TCP is said to be connectionoriented because before one application process can begin to send data to another the two processes must first handshake with each otherthat is they must send some preliminary segments to each other to establish the parameters of the ensuing data transfer
As part of TCP connection establishment both sides of the connection will initialize many TCP state variables many of which will be discussed in this section and in Section 
associated with the TCP connection
The TCP connection is not an endtoend TDM or FDM circuit as in a circuitswitched network
Instead the connection is a logical one with common state residing only in the TCPs in the two communicating end systems
Recall that because the TCP protocol runs only in the end systems and not in the intermediate network elements routers and linklayer switches the intermediate network elements do not maintain TCP connection state
In fact the intermediate routers are completely oblivious to TCP connections they see datagrams not connections
A TCP connection provides a fullduplex service If there is a TCP connection between Process A on one host and Process B on another host then applicationlayer data can flow from Process A to Process B at the same time as applicationlayer data flows from Process B to Process A
A TCP connection is also always pointtopoint that is between a single sender and a single receiver
So called multicasting see the online supplementary materials for this textthe transfer of data from one sender to many receivers in a single send operationis not possible with TCP
With TCP two hosts are company and three are a crowd Lets now take a look at how a TCP connection is established
Suppose a process running in one host wants to initiate a connection with another process in another host
Recall that the process that is initiating the connection is called the client process while the other process is called the server process
The client application process first informs the client transport layer that it wants to establish a connection CASE HISTORY Vinton Cerf Robert Kahn and TCPIP In the early s packetswitched networks began to proliferate with the ARPAnetthe precursor of the Internetbeing just one of many networks
Each of these networks had its own protocol
Two researchers Vinton Cerf and Robert Kahn recognized the importance of interconnecting these networks and invented a crossnetwork protocol called TCPIP which stands for Transmission Control ProtocolInternet Protocol
Although Cerf and Kahn began by seeing the protocol as a single entity it was later split into its two parts TCP and IP which operated separately
Cerf and Kahn published a paper on TCPIP in May in IEEE Transactions on Communications Technology Cerf 
The TCPIP protocol which is the bread and butter of todays Internet was devised before PCs workstations smartphones and tablets before the proliferation of Ethernet cable and DSL WiFi and other access network technologies and before the Web social media and streaming video
Cerf and Kahn saw the need for a networking protocol that on the one hand provides broad support for yettobedefined applications and on the other hand allows arbitrary hosts and linklayer protocols to interoperate
In Cerf and Kahn received the ACMs Turing Award considered the Nobel Prize of Computing for pioneering work on internetworking including the design and implementation of the Internets basic communications protocols TCPIP and for inspired leadership in networking
to a process in the server
Recall from Section 
a Python client program does this by issuing the command clientSocket.connectserverName serverPort where serverName is the name of the server and serverPort identifies the process on the server
TCP in the client then proceeds to establish a TCP connection with TCP in the server
At the end of this section we discuss in some detail the connectionestablishment procedure
For now it suffices to know that the client first sends a special TCP segment the server responds with a second special TCP segment and finally the client responds again with a third special segment
The first two segments carry no payload that is no applicationlayer data the third of these segments may carry a payload
Because three segments are sent between the two hosts this connectionestablishment procedure is often referred to as a threeway handshake
Once a TCP connection is established the two application processes can send data to each other
Lets consider the sending of data from the client process to the server process
The client process passes a stream of data through the socket the door of the process as described in Section 
Once the data passes through the door the data is in the hands of TCP running in the client
As shown in Figure 
TCP directs this data to the connections send buffer which is one of the buffers that is set aside during the initial threeway handshake
From time to time TCP will grab chunks of data from the send buffer and pass the data to the network layer
Interestingly the TCP specification RFC is very laid back about specifying when TCP should actually send buffered data stating that TCP should send that data in segments at its own convenience
The maximum amount of data that can be grabbed and placed in a segment is limited by the maximum segment size MSS
The MSS is typically set by first determining the length of the largest linklayer frame that can be sent by the local sending host the so called maximum transmission unit MTU and then setting the MSS to ensure that a TCP segment when encapsulated in an IP datagram plus the TCPIP header length typically bytes will fit into a single linklayer frame
Both Ethernet and PPP linklayer protocols have an MTU of bytes
Thus a typical value of MSS is bytes
Approaches have also been proposed for discovering the path MTU the largest linklayer frame that can be sent on all links from source to destination RFC and setting the MSS based on the path MTU value
Note that the MSS is the maximum amount of applicationlayer data in the segment not the maximum size of the TCP segment including headers
This terminology is confusing but we have to live with it as it is well entrenched
TCP pairs each chunk of client data with a TCP header thereby forming TCP segments
The segments are passed down to the network layer where they are separately encapsulated within networklayer IP datagrams
The IP datagrams are then sent into the network
When TCP receives a segment at the other end the segments data is placed in the TCP connections receive buffer as shown in Figure 
The application reads the stream of data from this buffer
Each side of the connection has Figure 
TCP send and receive buffers its own send buffer and its own receive buffer
You can see the online flowcontrol applet at httpwww.awl.comkuroseross which provides an animation of the send and receive buffers
We see from this discussion that a TCP connection consists of buffers variables and a socket connection to a process in one host and another set of buffers variables and a socket connection to a process in another host
As mentioned earlier no buffers or variables are allocated to the connection in the network elements routers switches and repeaters between the hosts
TCP Segment Structure Having taken a brief look at the TCP connection lets examine the TCP segment structure
The TCP segment consists of header fields and a data field
The data field contains a chunk of application data
As mentioned above the MSS limits the maximum size of a segments data field
When TCP sends a large file such as an image as part of a Web page it typically breaks the file into chunks of size MSS except for the last chunk which will often be less than the MSS
Interactive applications however often transmit data chunks that are smaller than the MSS for example with remote login applications like Telnet the data field in the TCP segment is often only one byte
Because the TCP header is typically bytes bytes more than the UDP header segments sent by Telnet may be only bytes in length
shows the structure of the TCP segment
As with UDP the header includes source and destination port numbers which are used for multiplexingdemultiplexing data fromto upperlayer applications
Also as with UDP the header includes a checksum field
A TCP segment header also contains the following fields The bit sequence number field and the bit acknowledgment number field are used by the TCP sender and receiver in implementing a reliable data transfer service as discussed below
The bit receive window field is used for flow control
We will see shortly that it is used to indicate the number of bytes that a receiver is willing to accept
The bit header length field specifies the length of the TCP header in bit words
The TCP header can be of variable length due to the TCP options field
Typically the options field is empty so that the length of the typical TCP header is bytes
The optional and variablelength options field is used when a sender and receiver negotiate the maximum segment size MSS or as a window scaling factor for use in highspeed networks
A time stamping option is also defined
See RFC and RFC for additional details
The flag field contains bits
The ACK bit is used to indicate that the value carried in the acknowledgment field is valid that is the segment contains an acknowledgment for a segment that has been successfully received
The RST Figure 
TCP segment structure SYN and FIN bits are used for connection setup and teardown as we will discuss at the end of this section
The CWR and ECE bits are used in explicit congestion notification as discussed in Section 
Setting the PSH bit indicates that the receiver should pass the data to the upper layer immediately
Finally the URG bit is used to indicate that there is data in this segment that the sendingside upperlayer entity has marked as urgent
The location of the last byte of this urgent data is indicated by the bit urgent data pointer field
TCP must inform the receivingside upper layer entity when urgent data exists and pass it a pointer to the end of the urgent data
In practice the PSH URG and the urgent data pointer are not used
However we mention these fields for completeness
Our experience as teachers is that our students sometimes find discussion of packet formats rather dry and perhaps a bit boring
For a fun and fanciful look at TCP header fields particularly if you love Legos as we do see Pomeranz 
Sequence Numbers and Acknowledgment Numbers Two of the most important fields in the TCP segment header are the sequence number field and the acknowledgment number field
These fields are a critical part of TCPs reliable data transfer service
But before discussing how these fields are used to provide reliable data transfer let us first explain what exactly TCP puts in these fields
Dividing file data into TCP segments TCP views data as an unstructured but ordered stream of bytes
TCPs use of sequence numbers reflects this view in that sequence numbers are over the stream of transmitted bytes and not over the series of transmitted segments
The sequence number for a segment is therefore the bytestream number of the first byte in the segment
Lets look at an example
Suppose that a process in Host A wants to send a stream of data to a process in Host B over a TCP connection
The TCP in Host A will implicitly number each byte in the data stream
Suppose that the data stream consists of a file consisting of bytes that the MSS is bytes and that the first byte of the data stream is numbered 
As shown in Figure 
TCP constructs segments out of the data stream
The first segment gets assigned sequence number the second segment gets assigned sequence number the third segment gets assigned sequence number and so on
Each sequence number is inserted in the sequence number field in the header of the appropriate TCP segment
Now lets consider acknowledgment numbers
These are a little trickier than sequence numbers
Recall that TCP is fullduplex so that Host A may be receiving data from Host B while it sends data to Host B as part of the same TCP connection
Each of the segments that arrive from Host B has a sequence number for the data flowing from B to A
The acknowledgment number that Host A puts in its segment is the sequence number of the next byte Host A is expecting from Host B
It is good to look at a few examples to understand what is going on here
Suppose that Host A has received all bytes numbered through from B and suppose that it is about to send a segment to Host B
Host A is waiting for byte and all the subsequent bytes in Host Bs data stream
So Host A puts in the acknowledgment number field of the segment it sends to B
As another example suppose that Host A has received one segment from Host B containing bytes through and another segment containing bytes through 
For some reason Host A has not yet received bytes through 
In this example Host A is still waiting for byte and beyond in order to recreate Bs data stream
Thus As next segment to B will contain in the acknowledgment number field
Because TCP only acknowledges bytes up to the first missing byte in the stream TCP is said to provide cumulative acknowledgments
This last example also brings up an important but subtle issue
Host A received the third segment bytes through before receiving the second segment bytes through 
Thus the third segment arrived out of order
The subtle issue is What does a host do when it receives outoforder segments in a TCP connection Interestingly the TCP RFCs do not impose any rules here and leave the decision up to the programmers implementing a TCP implementation
There are basically two choices either the receiver immediately discards outoforder segments which as we discussed earlier can simplify receiver design or the receiver keeps the outoforder bytes and waits for the missing bytes to fill in the gaps
Clearly the latter choice is more efficient in terms of network bandwidth and is the approach taken in practice
In Figure 
we assumed that the initial sequence number was zero
In truth both sides of a TCP connection randomly choose an initial sequence number
This is done to minimize the possibility that a segment that is still present in the network from an earlier alreadyterminated connection between two hosts is mistaken for a valid segment in a later connection between these same two hosts which also happen to be using the same port numbers as the old connection Sunshine 
Telnet A Case Study for Sequence and Acknowledgment Numbers Telnet defined in RFC is a popular applicationlayer protocol used for remote login
It runs over TCP and is designed to work between any pair of hosts
Unlike the bulk data transfer applications discussed in Chapter Telnet is an interactive application
We discuss a Telnet example here as it nicely illustrates TCP sequence and acknowledgment numbers
We note that many users now prefer to use the SSH protocol rather than Telnet since data sent in a Telnet connection including passwords are not encrypted making Telnet vulnerable to eavesdropping attacks as discussed in Section 
Suppose Host A initiates a Telnet session with Host B
Because Host A initiates the session it is labeled the client and Host B is labeled the server
Each character typed by the user at the client will be sent to the remote host the remote host will send back a copy of each character which will be displayed on the Telnet users screen
This echo back is used to ensure that characters seen by the Telnet user have already been received and processed at the remote site
Each character thus traverses the network twice between the time the user hits the key and the time the character is displayed on the users monitor
Now suppose the user types a single letter C and then grabs a coffee
Lets examine the TCP segments that are sent between the client and server
As shown in Figure 
we suppose the starting sequence numbers are and for the client and server respectively
Recall that the sequence number of a segment is the sequence number of the first byte in the data field
Thus the first segment sent from the client will have sequence number the first segment sent from the server will have sequence number 
Recall that the acknowledgment number is the sequence Figure 
Sequence and acknowledgment numbers for a simple Telnet application over TCP number of the next byte of data that the host is waiting for
After the TCP connection is established but before any data is sent the client is waiting for byte and the server is waiting for byte 
As shown in Figure 
three segments are sent
The first segment is sent from the client to the server containing the byte ASCII representation of the letter C in its data field
This first segment also has in its sequence number field as we just described
Also because the client has not yet received any data from the server this first segment will have in its acknowledgment number field
The second segment is sent from the server to the client
It serves a dual purpose
First it provides an acknowledgment of the data the server has received
By putting in the acknowledgment field the server is telling the client that it has successfully received everything up through byte and is now waiting for bytes onward
The second purpose of this segment is to echo back the letter C
Thus the second segment has the ASCII representation of C in its data field
This second segment has the sequence number the initial sequence number of the servertoclient data flow of this TCP connection as this is the very first byte of data that the server is sending
Note that the acknowledgment for clienttoserver data is carried in a segment carrying servertoclient data this acknowledgment is said to be piggybacked on the servertoclient data segment
The third segment is sent from the client to the server
Its sole purpose is to acknowledge the data it has received from the server
Recall that the second segment contained datathe letter Cfrom the server to the client
This segment has an empty data field that is the acknowledgment is not being piggybacked with any clienttoserver data
The segment has in the acknowledgment number field because the client has received the stream of bytes up through byte sequence number and it is now waiting for bytes onward
You might think it odd that this segment also has a sequence number since the segment contains no data
But because TCP has a sequence number field the segment needs to have some sequence number
RoundTrip Time Estimation and Timeout TCP like our rdt protocol in Section 
uses a timeoutretransmit mechanism to recover from lost segments
Although this is conceptually simple many subtle issues arise when we implement a timeoutretransmit mechanism in an actual protocol such as TCP
Perhaps the most obvious question is the length of the timeout intervals
Clearly the timeout should be larger than the connections roundtrip time RTT that is the time from when a segment is sent until it is acknowledged
Otherwise unnecessary retransmissions would be sent
But how much larger How should the RTT be estimated in the first place Should a timer be associated with each and every unacknowledged segment So many questions Our discussion in this section is based on the TCP work in Jacobson and the current IETF recommendations for managing TCP timers RFC 
Estimating the RoundTrip Time Lets begin our study of TCP timer management by considering how TCP estimates the roundtrip time between sender and receiver
This is accomplished as follows
The sample RTT denoted SampleRTT for a segment is the amount of time between when the segment is sent that is passed to IP and when an acknowledgment for the segment is received
Instead of measuring a SampleRTT for every transmitted segment most TCP implementations take only one SampleRTT measurement at a time
That is at any point in time the SampleRTT is being estimated for only one of the transmitted but currently unacknowledged segments leading to a new value of SampleRTT approximately once every RTT
Also TCP never computes a SampleRTT for a segment that has been retransmitted it only measures SampleRTT for segments that have been transmitted once Karn 
A problem at the end of the chapter asks you to consider why
Obviously the SampleRTT values will fluctuate from segment to segment due to congestion in the routers and to the varying load on the end systems
Because of this fluctuation any given SampleRTT value may be atypical
In order to estimate a typical RTT it is therefore natural to take some sort of average of the SampleRTT values
TCP maintains an average called EstimatedRTT of the SampleRTT values
Upon obtaining a new SampleRTT TCP updates EstimatedRTT according to the following formula EstimatedRTTÎ±EstimatedRTTÎ±SampleRTT The formula above is written in the form of a programminglanguage statementthe new value of EstimatedRTT is a weighted combination of the previous value of EstimatedRTT and the new value for SampleRTT
The recommended value of Î± is Î± 
that is RFC in which case the formula above becomes EstimatedRTT.EstimatedRTT.SampleRTT Note that EstimatedRTT is a weighted average of the SampleRTT values
As discussed in a homework problem at the end of this chapter this weighted average puts more weight on recent samples than on old samples
This is natural as the more recent samples better reflect the current congestion in the network
In statistics such an average is called an exponential weighted moving average EWMA
The word exponential appears in EWMA because the weight of a given SampleRTT decays exponentially fast as the updates proceed
In the homework problems you will be asked to derive the exponential term in EstimatedRTT 
shows the SampleRTT values and EstimatedRTT for a value of Î± for a TCP connection between gaia.cs.umass.edu in Amherst Massachusetts to fantasia.eurecom.fr in the south of France
Clearly the variations in the SampleRTT are smoothed out in the computation of the EstimatedRTT 
In addition to having an estimate of the RTT it is also valuable to have a measure of the variability of the RTT
RFC defines the RTT variation DevRTT as an estimate of how much SampleRTT typically deviates from EstimatedRTT DevRTTÎ²DevRTTÎ²SampleRTTEstimatedRTT Note that DevRTT is an EWMA of the difference between SampleRTT and EstimatedRTT 
If the SampleRTT values have little fluctuation then DevRTT will be small on the other hand if there is a lot of fluctuation DevRTT will be large
The recommended value of Î² is 
Setting and Managing the Retransmission Timeout Interval Given values of EstimatedRTT and DevRTT what value should be used for TCPs timeout interval Clearly the interval should be greater than or equal to PRINCIPLES IN PRACTICE TCP provides reliable data transfer by using positive acknowledgments and timers in much the same way that we studied in Section 
TCP acknowledges data that has been received correctly and it then retransmits segments when segments or their corresponding acknowledgments are thought to be lost or corrupted
Certain versions of TCP also have an implicit NAK mechanismwith TCPs fast retransmit mechanism the receipt of three duplicate ACKs for a given segment serves as an implicit NAK for the following segment triggering retransmission of that segment before timeout
TCP uses sequences of numbers to allow the receiver to identify lost or duplicate segments
Just as in the case of our reliable data transfer protocol rdt
TCP cannot itself tell for certain if a segment or its ACK is lost corrupted or overly delayed
At the sender TCPs response will be the same retransmit the segment in question
TCP also uses pipelining allowing the sender to have multiple transmitted but yettobe acknowledged segments outstanding at any given time
We saw earlier that pipelining can greatly improve a sessions throughput when the ratio of the segment size to roundtrip delay is small
The specific number of outstanding unacknowledged segments that a sender can have is determined by TCPs flowcontrol and congestioncontrol mechanisms
TCP flow control is discussed at the end of this section TCP congestion control is discussed in Section 
For the time being we must simply be aware that the TCP sender uses pipelining
EstimatedRTT or unnecessary retransmissions would be sent
But the timeout interval should not be too much larger than EstimatedRTT otherwise when a segment is lost TCP would not quickly retransmit the segment leading to large data transfer delays
It is therefore desirable to set the timeout equal to the EstimatedRTT plus some margin
The margin should be large when there is a lot of fluctuation in the SampleRTT values it should be small when there is little fluctuation
The value of DevRTT should thus come into play here
All of these considerations are taken into account in TCPs method for determining the retransmission timeout interval TimeoutIntervalEstimatedRTTDevRTT An initial TimeoutInterval value of second is recommended RFC 
Also when a timeout occurs the value of TimeoutInterval is doubled to avoid a premature timeout occurring for a subsequent segment that will soon be acknowledged
However as soon as a segment is received and EstimatedRTT is updated the TimeoutInterval is again computed using the formula above
RTT samples and RTT estimates 
Reliable Data Transfer Recall that the Internets networklayer service IP service is unreliable
IP does not guarantee datagram delivery does not guarantee inorder delivery of datagrams and does not guarantee the integrity of the data in the datagrams
With IP service datagrams can overflow router buffers and never reach their destination datagrams can arrive out of order and bits in the datagram can get corrupted flipped from to and vice versa
Because transportlayer segments are carried across the network by IP datagrams transportlayer segments can suffer from these problems as well
TCP creates a reliable data transfer service on top of IPs unreliable besteffort service
TCPs reliable data transfer service ensures that the data stream that a process reads out of its TCP receive buffer is uncorrupted without gaps without duplication and in sequence that is the byte stream is exactly the same byte stream that was sent by the end system on the other side of the connection
How TCP provides a reliable data transfer involves many of the principles that we studied in Section 
In our earlier development of reliable data transfer techniques it was conceptually easiest to assume that an individual timer is associated with each transmitted but not yet acknowledged segment
While this is great in theory timer management can require considerable overhead
Thus the recommended TCP timer management procedures RFC use only a single retransmission timer even if there are multiple transmitted but not yet acknowledged segments
The TCP protocol described in this section follows this singletimer recommendation
We will discuss how TCP provides reliable data transfer in two incremental steps
We first present a highly simplified description of a TCP sender that uses only timeouts to recover from lost segments we then present a more complete description that uses duplicate acknowledgments in addition to timeouts
In the ensuing discussion we suppose that data is being sent in only one direction from Host A to Host B and that Host A is sending a large file
presents a highly simplified description of a TCP sender
We see that there are three major events related to data transmission and retransmission in the TCP sender data received from application above timer timeout and ACK Figure 
Simplified TCP sender receipt
Upon the occurrence of the first major event TCP receives data from the application encapsulates the data in a segment and passes the segment to IP
Note that each segment includes a sequence number that is the bytestream number of the first data byte in the segment as described in Section 
Also note that if the timer is already not running for some other segment TCP starts the timer when the segment is passed to IP
It is helpful to think of the timer as being associated with the oldest unacknowledged segment
The expiration interval for this timer is the TimeoutInterval which is calculated from EstimatedRTT and DevRTT as described in Section 
The second major event is the timeout
TCP responds to the timeout event by retransmitting the segment that caused the timeout
TCP then restarts the timer
The third major event that must be handled by the TCP sender is the arrival of an acknowledgment segment ACK from the receiver more specifically a segment containing a valid ACK field value
On the occurrence of this event TCP compares the ACK value y with its variable SendBase 
The TCP state variable SendBase is the sequence number of the oldest unacknowledged byte
Thus SendBase is the sequence number of the last byte that is known to have been received correctly and in order at the receiver
As indicated earlier TCP uses cumulative acknowledgments so that y acknowledges the receipt of all bytes before byte number y 
If y SendBase then the ACK is acknowledging one or more previously unacknowledged segments
Thus the sender updates its SendBase variable it also restarts the timer if there currently are any notyetacknowledged segments
A Few Interesting Scenarios We have just described a highly simplified version of how TCP provides reliable data transfer
But even this highly simplified version has many subtleties
To get a good feeling for how this protocol works lets now walk through a few simple scenarios
depicts the first scenario in which Host A sends one segment to Host B
Suppose that this segment has sequence number and contains bytes of data
After sending this segment Host A waits for a segment from B with acknowledgment number 
Although the segment from A is received at B the acknowledgment from B to A gets lost
In this case the timeout event occurs and Host A retransmits the same segment
Of course when Host B receives the retransmission it observes from the sequence number that the segment contains data that has already been received
Thus TCP in Host B will discard the bytes in the retransmitted segment
In a second scenario shown in Figure 
Host A sends two segments back to back
The first segment has sequence number and bytes of data and the second segment has sequence number and bytes of data
Suppose that both segments arrive intact at B and B sends two separate acknowledgments for each of these segments
The first of these acknowledgments has acknowledgment number the second has acknowledgment number 
Suppose now that neither of the acknowledgments arrives at Host A before the timeout
When the timeout event occurs Host Figure 
Retransmission due to a lost acknowledgment A resends the first segment with sequence number and restarts the timer
As long as the ACK for the second segment arrives before the new timeout the second segment will not be retransmitted
In a third and final scenario suppose Host A sends the two segments exactly as in the second example
The acknowledgment of the first segment is lost in the network but just before the timeout event Host A receives an acknowledgment with acknowledgment number 
Host A therefore knows that Host B has received everything up through byte so Host A does not resend either of the two segments
This scenario is illustrated in Figure 
Doubling the Timeout Interval We now discuss a few modifications that most TCP implementations employ
The first concerns the length of the timeout interval after a timer expiration
In this modification whenever the timeout event occurs TCP retransmits the notyetacknowledged segment with the smallest sequence number as described above
But each time TCP retransmits it sets the next timeout interval to twice the previous value Figure 
Segment not retransmitted rather than deriving it from the last EstimatedRTT and DevRTT as described in Section 
For example suppose TimeoutInterval associated with the oldest not yet acknowledged segment is 
sec when the timer first expires
TCP will then retransmit this segment and set the new expiration time to 
If the timer expires again 
sec later TCP will again retransmit this segment now setting the expiration time to 
Thus the intervals grow exponentially after each retransmission
However whenever the timer is started after either of the two other events that is data received from application above and ACK received the TimeoutInterval is derived from the most recent values of EstimatedRTT and DevRTT 
This modification provides a limited form of congestion control
More comprehensive forms of TCP congestion control will be studied in Section 
The timer expiration is most likely caused by congestion in the network that is too many packets arriving at one or more router queues in the path between the source and destination causing packets to be dropped andor long queuing delays
In times of congestion if the sources continue to retransmit packets persistently the congestion Figure 
A cumulative acknowledgment avoids retransmission of the first segment may get worse
Instead TCP acts more politely with each sender retransmitting after longer and longer intervals
We will see that a similar idea is used by Ethernet when we study CSMACD in Chapter 
Fast Retransmit One of the problems with timeouttriggered retransmissions is that the timeout period can be relatively long
When a segment is lost this long timeout period forces the sender to delay resending the lost packet thereby increasing the endtoend delay
Fortunately the sender can often detect packet loss well before the timeout event occurs by noting socalled duplicate ACKs
A duplicate ACK is an ACK that reacknowledges a segment for which the sender has already received an earlier acknowledgment
To understand the senders response to a duplicate ACK we must look at why the receiver sends a duplicate ACK in the first place
summarizes the TCP receivers ACK generation policy RFC 
When a TCP receiver receives Table 
TCP ACK Generation Recommendation RFC Event TCP Receiver Action Arrival of inorder segment with expected Delayed ACK
Wait up to msec for arrival of sequence number
All data up to expected another inorder segment
If next inorder segment sequence number already acknowledged
does not arrive in this interval send an ACK
Arrival of inorder segment with expected One Immediately send single cumulative ACK sequence number
One other inorder ACKing both inorder segments
segment waiting for ACK transmission
Arrival of outoforder segment with higher Immediately send duplicate ACK indicating thanexpected sequence number
Gap sequence number of next expected byte which is detected
the lower end of the gap
Arrival of segment that partially or completely Immediately send ACK provided that segment fills in gap in received data
starts at the lower end of gap
a segment with a sequence number that is larger than the next expected inorder sequence number it detects a gap in the data streamthat is a missing segment
This gap could be the result of lost or reordered segments within the network
Since TCP does not use negative acknowledgments the receiver cannot send an explicit negative acknowledgment back to the sender
Instead it simply reacknowledges that is generates a duplicate ACK for the last inorder byte of data it has received
Note that Table 
allows for the case that the receiver does not discard outoforder segments
Because a sender often sends a large number of segments back to back if one segment is lost there will likely be many backtoback duplicate ACKs
If the TCP sender receives three duplicate ACKs for the same data it takes this as an indication that the segment following the segment that has been ACKed three times has been lost
In the homework problems we consider the question of why the sender waits for three duplicate ACKs rather than just a single duplicate ACK
In the case that three duplicate ACKs are received the TCP sender performs a fast retransmit RFC retransmitting the missing segment before that segments timer expires
This is shown in Figure 
where the second segment is lost then retransmitted before its timer expires
For TCP with fast retransmit the following code snippet replaces the ACK received event in Figure 
event ACK received with ACK field value of y if y SendBase SendBasey if there are currently any not yet acknowledged segments start timer Figure 
Fast retransmit retransmitting the missing segment before the segments timer expires else a duplicate ACK for already ACKed segment increment number of duplicate ACKs received for y if number of duplicate ACKS received for y TCP fast retransmit resend segment with sequence number y break We noted earlier that many subtle issues arise when a timeoutretransmit mechanism is implemented in an actual protocol such as TCP
The procedures above which have evolved as a result of more than years of experience with TCP timers should convince you that this is indeed the case GoBackN or Selective Repeat Let us close our study of TCPs errorrecovery mechanism by considering the following question Is TCP a GBN or an SR protocol Recall that TCP acknowledgments are cumulative and correctly received but outoforder segments are not individually ACKed by the receiver
Consequently as shown in Figure 
see also Figure 
the TCP sender need only maintain the smallest sequence number of a transmitted but unacknowledged byte SendBase and the sequence number of the next byte to be sent NextSeqNum 
In this sense TCP looks a lot like a GBNstyle protocol
But there are some striking differences between TCP and GoBackN
Many TCP implementations will buffer correctly received but outoforder segments Stevens 
Consider also what happens when the sender sends a sequence of segments 
N and all of the segments arrive in order without error at the receiver
Further suppose that the acknowledgment for packet nN gets lost but the remaining N acknowledgments arrive at the sender before their respective timeouts
In this example GBN would retransmit not only packet n but also all of the subsequent packets nnN
TCP on the other hand would retransmit at most one segment namely segment n
Moreover TCP would not even retransmit segment n if the acknowledgment for segment n arrived before the timeout for segment n
A proposed modification to TCP the socalled selective acknowledgment RFC allows a TCP receiver to acknowledge outoforder segments selectively rather than just cumulatively acknowledging the last correctly received inorder segment
When combined with selective retransmissionskipping the retransmission of segments that have already been selectively acknowledged by the receiverTCP looks a lot like our generic SR protocol
Thus TCPs errorrecovery mechanism is probably best categorized as a hybrid of GBN and SR protocols
Flow Control Recall that the hosts on each side of a TCP connection set aside a receive buffer for the connection
When the TCP connection receives bytes that are correct and in sequence it places the data in the receive buffer
The associated application process will read data from this buffer but not necessarily at the instant the data arrives
Indeed the receiving application may be busy with some other task and may not even attempt to read the data until long after it has arrived
If the application is relatively slow at reading the data the sender can very easily overflow the connections receive buffer by sending too much data too quickly
TCP provides a flowcontrol service to its applications to eliminate the possibility of the sender overflowing the receivers buffer
Flow control is thus a speedmatching servicematching the rate at which the sender is sending against the rate at which the receiving application is reading
As noted earlier a TCP sender can also be throttled due to congestion within the IP network this form of sender control is referred to as congestion control a topic we will explore in detail in Sections 
Even though the actions taken by flow and congestion control are similar the throttling of the sender they are obviously taken for very different reasons
Unfortunately many authors use the terms interchangeably and the savvy reader would be wise to distinguish between them
Lets now discuss how TCP provides its flowcontrol service
In order to see the forest for the trees we suppose throughout this section that the TCP implementation is such that the TCP receiver discards outoforder segments
TCP provides flow control by having the sender maintain a variable called the receive window
Informally the receive window is used to give the sender an idea of how much free buffer space is available at the receiver
Because TCP is fullduplex the sender at each side of the connection maintains a distinct receive window
Lets investigate the receive window in the context of a file transfer
Suppose that Host A is sending a large file to Host B over a TCP connection
Host B allocates a receive buffer to this connection denote its size by RcvBuffer 
From time to time the application process in Host B reads from the buffer
Define the following variables LastByteRead the number of the last byte in the data stream read from the buffer by the application process in B LastByteRcvd the number of the last byte in the data stream that has arrived from the network and has been placed in the receive buffer at B Because TCP is not permitted to overflow the allocated buffer we must have LastByteRcvdLastByteReadRcvBuffer The receive window denoted rwnd is set to the amount of spare room in the buffer rwndRcvBufferLastByteRcvdLastByteRead Because the spare room changes with time rwnd is dynamic
The variable rwnd is illustrated in Figure 
How does the connection use the variable rwnd to provide the flowcontrol service Host B tells Host A how much spare room it has in the connection buffer by placing its current value of rwnd in the receive window field of every segment it sends to A
Initially Host B sets rwnd RcvBuffer 
Note that to pull this off Host B must keep track of several connectionspecific variables
Host A in turn keeps track of two variables LastByteSent and LastByteAcked which have obvious meanings
Note that the difference between these two variables LastByteSent LastByteAcked is the amount of unacknowledged data that A has sent into the connection
By keeping the amount of unacknowledged data less than the value of rwnd Host A is assured that it is not Figure 
The receive window rwnd and the receive buffer RcvBuffer overflowing the receive buffer at Host B
Thus Host A makes sure throughout the connections life that LastByteSentLastByteAckedrwnd There is one minor technical problem with this scheme
To see this suppose Host Bs receive buffer becomes full so that rwnd 
After advertising rwnd to Host A also suppose that B has nothing to send to A
Now consider what happens
As the application process at B empties the buffer TCP does not send new segments with new rwnd values to Host A indeed TCP sends a segment to Host A only if it has data to send or if it has an acknowledgment to send
Therefore Host A is never informed that some space has opened up in Host Bs receive bufferHost A is blocked and can transmit no more data To solve this problem the TCP specification requires Host A to continue to send segments with one data byte when Bs receive window is zero
These segments will be acknowledged by the receiver
Eventually the buffer will begin to empty and the acknowledgments will contain a nonzero rwnd value
The online site at httpwww.awl.comkuroseross for this book provides an interactive Java applet that illustrates the operation of the TCP receive window
Having described TCPs flowcontrol service we briefly mention here that UDP does not provide flow control and consequently segments may be lost at the receiver due to buffer overflow
For example consider sending a series of UDP segments from a process on Host A to a process on Host B
For a typical UDP implementation UDP will append the segments in a finitesized buffer that precedes the corresponding socket that is the door to the process
The process reads one entire segment at a time from the buffer
If the process does not read the segments fast enough from the buffer the buffer will overflow and segments will get dropped
TCP Connection Management In this subsection we take a closer look at how a TCP connection is established and torn down
Although this topic may not seem particularly thrilling it is important because TCP connection establishment can significantly add to perceived delays for example when surfing the Web
Furthermore many of the most common network attacksincluding the incredibly popular SYN flood attackexploit vulnerabilities in TCP connection management
Lets first take a look at how a TCP connection is established
Suppose a process running in one host client wants to initiate a connection with another process in another host server
The client application process first informs the client TCP that it wants to establish a connection to a process in the server
The TCP in the client then proceeds to establish a TCP connection with the TCP in the server in the following manner Step 
The clientside TCP first sends a special TCP segment to the serverside TCP
This special segment contains no applicationlayer data
But one of the flag bits in the segments header see Figure 
the SYN bit is set to 
For this reason this special segment is referred to as a SYN segment
In addition the client randomly chooses an initial sequence number client_isn and puts this number in the sequence number field of the initial TCP SYN segment
This segment is encapsulated within an IP datagram and sent to the server
There has been considerable interest in properly randomizing the choice of the client_isn in order to avoid certain security attacks CERT 
Once the IP datagram containing the TCP SYN segment arrives at the server host assuming it does arrive the server extracts the TCP SYN segment from the datagram allocates the TCP buffers and variables to the connection and sends a connectiongranted segment to the client TCP
Well see in Chapter that the allocation of these buffers and variables before completing the third step of the threeway handshake makes TCP vulnerable to a denialofservice attack known as SYN flooding
This connectiongranted segment also contains no applicationlayer data
However it does contain three important pieces of information in the segment header
First the SYN bit is set to 
Second the acknowledgment field of the TCP segment header is set to client_isn 
Finally the server chooses its own initial sequence number server_isn and puts this value in the sequence number field of the TCP segment header
This connectiongranted segment is saying in effect I received your SYN packet to start a connection with your initial sequence number client_isn 
I agree to establish this connection
My own initial sequence number is server_isn 
The connectiongranted segment is referred to as a SYNACK segment
Upon receiving the SYNACK segment the client also allocates buffers and variables to the connection
The client host then sends the server yet another segment this last segment acknowledges the servers connectiongranted segment the client does so by putting the value server_isn in the acknowledgment field of the TCP segment header
The SYN bit is set to zero since the connection is established
This third stage of the threeway handshake may carry clienttoserver data in the segment payload
Once these three steps have been completed the client and server hosts can send segments containing data to each other
In each of these future segments the SYN bit will be set to zero
Note that in order to establish the connection three packets are sent between the two hosts as illustrated in Figure 
For this reason this connectionestablishment procedure is often referred to as a three way handshake
Several aspects of the TCP threeway handshake are explored in the homework problems Why are initial sequence numbers needed Why is a threeway handshake as opposed to a twoway handshake needed
Its interesting to note that a rock climber and a belayer who is stationed below the rock climber and whose job it is to handle the climbers safety rope use a three wayhandshake communication protocol that is identical to TCPs to ensure that both sides are ready before the climber begins ascent
All good things must come to an end and the same is true with a TCP connection
Either of the two processes participating in a TCP connection can end the connection
When a connection ends the resources that is the buffers and variables Figure 
TCP threeway handshake segment exchange Figure 
Closing a TCP connection in the hosts are deallocated
As an example suppose the client decides to close the connection as shown in Figure 
The client application process issues a close command
This causes the client TCP to send a special TCP segment to the server process
This special segment has a flag bit in the segments header the FIN bit see Figure 
set to 
When the server receives this segment it sends the client an acknowledgment segment in return
The server then sends its own shutdown segment which has the FIN bit set to 
Finally the client acknowledges the servers shutdown segment
At this point all the resources in the two hosts are now deallocated
During the life of a TCP connection the TCP protocol running in each host makes transitions through various TCP states
illustrates a typical sequence of TCP states that are visited by the client TCP
The client TCP begins in the CLOSED state
The application on the client side initiates a new TCP connection by creating a Socket object in our Java examples as in the Python examples from Chapter 
This causes TCP in the client to send a SYN segment to TCP in the server
After having sent the SYN segment the client TCP enters the SYN_SENT state
While in the SYN_SENT state the client TCP waits for a segment from the server TCP that includes an acknowledgment for the clients previous segment and Figure 
A typical sequence of TCP states visited by a client TCP has the SYN bit set to 
Having received such a segment the client TCP enters the ESTABLISHED state
While in the ESTABLISHED state the TCP client can send and receive TCP segments containing payload that is applicationgenerated data
Suppose that the client application decides it wants to close the connection
Note that the server could also choose to close the connection
This causes the client TCP to send a TCP segment with the FIN bit set to and to enter the FIN_WAIT_ state
While in the FIN_WAIT_ state the client TCP waits for a TCP segment from the server with an acknowledgment
When it receives this segment the client TCP enters the FIN_WAIT_ state
While in the FIN_WAIT_ state the client waits for another segment from the server with the FIN bit set to after receiving this segment the client TCP acknowledges the servers segment and enters the TIME_WAIT state
The TIME_WAIT state lets the TCP client resend the final acknowledgment in case the ACK is lost
The time spent in the TIME_WAIT state is implementationdependent but typical values are seconds minute and minutes
After the wait the connection formally closes and all resources on the client side including port numbers are released
illustrates the series of states typically visited by the serverside TCP assuming the client begins connection teardown
The transitions are selfexplanatory
In these two statetransition diagrams we have only shown how a TCP connection is normally established and shut down
We have not described what happens in certain pathological scenarios for example when both sides of a connection want to initiate or shut down at the same time
If you are interested in learning about Figure 
A typical sequence of TCP states visited by a serverside TCP this and other advanced issues concerning TCP you are encouraged to see Stevens comprehensive book Stevens 
Our discussion above has assumed that both the client and server are prepared to communicate i.e
that the server is listening on the port to which the client sends its SYN segment
Lets consider what happens when a host receives a TCP segment whose port numbers or source IP address do not match with any of the ongoing sockets in the host
For example suppose a host receives a TCP SYN packet with destination port but the host is not accepting connections on port that is it is not running a Web server on port 
Then the host will send a special reset segment to the source
This TCP segment has the RST flag bit see Section 
set to 
Thus when a host sends a reset segment it is telling the source I dont have a socket for that segment
Please do not resend the segment
When a host receives a UDP packet whose destination port number doesnt match with an ongoing UDP socket the host sends a special ICMP datagram as discussed in Chapter 
Now that we have a good understanding of TCP connection management lets revisit the nmap port scanning tool and examine more closely how it works
To explore a specific TCP port say port on a target host nmap will send a TCP SYN segment with destination port to that host
There are three possible outcomes The source host receives a TCP SYNACK segment from the target host
Since this means that an application is running with TCP port on the target post nmap returns open
FOCUS ON SECURITY The Syn Flood Attack Weve seen in our discussion of TCPs threeway handshake that a server allocates and initializes connection variables and buffers in response to a received SYN
The server then sends a SYNACK in response and awaits an ACK segment from the client
If the client does not send an ACK to complete the third step of this way handshake eventually often after a minute or more the server will terminate the halfopen connection and reclaim the allocated resources
This TCP connection management protocol sets the stage for a classic Denial of Service DoS attack known as the SYN flood attack
In this attack the attackers send a large number of TCP SYN segments without completing the third handshake step
With this deluge of SYN segments the servers connection resources become exhausted as they are allocated but never used for halfopen connections legitimate clients are then denied service
Such SYN flooding attacks were among the first documented DoS attacks CERT SYN 
Fortunately an effective defense known as SYN cookies RFC are now deployed in most major operating systems
SYN cookies work as follows When the server receives a SYN segment it does not know if the segment is coming from a legitimate user or is part of a SYN flood attack
So instead of creating a halfopen TCP connection for this SYN the server creates an initial TCP sequence number that is a complicated function hash function of source and destination IP addresses and port numbers of the SYN segment as well as a secret number only known to the server
This carefully crafted initial sequence number is the socalled cookie
The server then sends the client a SYNACK packet with this special initial sequence number
Importantly the server does not remember the cookie or any other state information corresponding to the SYN
A legitimate client will return an ACK segment
When the server receives this ACK it must verify that the ACK corresponds to some SYN sent earlier
But how is this done if the server maintains no memory about SYN segments As you may have guessed it is done with the cookie
Recall that for a legitimate ACK the value in the acknowledgment field is equal to the initial sequence number in the SYNACK the cookie value in this case plus one see Figure 
The server can then run the same hash function using the source and destination IP address and port numbers in the SYNACK which are the same as in the original SYN and the secret number
If the result of the function plus one is the same as the acknowledgment cookie value in the clients SYNACK the server concludes that the ACK corresponds to an earlier SYN segment and is hence valid
The server then creates a fully open connection along with a socket
On the other hand if the client does not return an ACK segment then the original SYN has done no harm at the server since the server hasnt yet allocated any resources in response to the original bogus SYN
The source host receives a TCP RST segment from the target host
This means that the SYN segment reached the target host but the target host is not running an application with TCP port 
But the attacker at least knows that the segments destined to the host at port are not blocked by any firewall on the path between source and target hosts
Firewalls are discussed in Chapter 
The source receives nothing
This likely means that the SYN segment was blocked by an intervening firewall and never reached the target host
Nmap is a powerful tool that can case the joint not only for open TCP ports but also for open UDP ports for firewalls and their configurations and even for the versions of applications and operating systems
Most of this is done by manipulating TCP connectionmanagement segments Skoudis 
You can download nmap from www.nmap.org
This completes our introduction to error control and flow control in TCP
In Section 
well return to TCP and look at TCP congestion control in some depth
Before doing so however we first step back and examine congestioncontrol issues in a broader context
Principles of Congestion Control In the previous sections we examined both the general principles and specific TCP mechanisms used to provide for a reliable data transfer service in the face of packet loss
We mentioned earlier that in practice such loss typically results from the overflowing of router buffers as the network becomes congested
Packet retransmission thus treats a symptom of network congestion the loss of a specific transportlayer segment but does not treat the cause of network congestiontoo many sources attempting to send data at too high a rate
To treat the cause of network congestion mechanisms are needed to throttle senders in the face of network congestion
In this section we consider the problem of congestion control in a general context seeking to understand why congestion is a bad thing how network congestion is manifested in the performance received by upperlayer applications and various approaches that can be taken to avoid or react to network congestion
This more general study of congestion control is appropriate since as with reliable data transfer it is high on our topten list of fundamentally important problems in networking
The following section contains a detailed study of TCPs congestioncontrol algorithm
The Causes and the Costs of Congestion Lets begin our general study of congestion control by examining three increasingly complex scenarios in which congestion occurs
In each case well look at why congestion occurs in the first place and at the cost of congestion in terms of resources not fully utilized and poor performance received by the end systems
Well not yet focus on how to react to or avoid congestion but rather focus on the simpler issue of understanding what happens as hosts increase their transmission rate and the network becomes congested
Scenario Two Senders a Router with Infinite Buffers We begin by considering perhaps the simplest congestion scenario possible Two hosts A and B each have a connection that shares a single hop between source and destination as shown in Figure 
Lets assume that the application in Host A is sending data into the connection for example passing data to the transportlevel protocol via a socket at an average rate of Î»in bytessec
These data are original in the sense that each unit of data is sent into the socket only once
The underlying transport level protocol is a simple one
Data is encapsulated and sent no error recovery for example retransmission flow control or congestion control is performed
Ignoring the additional overhead due to adding transport and lowerlayer header information the rate at which Host A offers traffic to the router in this first scenario is thus Î»in bytessec
Host B operates in a similar manner and we assume for simplicity that it too is sending at a rate of Î»in bytessec
Packets from Hosts A and B pass through a router and over a shared outgoing link of capacity R
The router has buffers that allow it to store incoming packets when the packetarrival rate exceeds the outgoing links capacity
In this first scenario we assume that the router has an infinite amount of buffer space
plots the performance of Host As connection under this first scenario
The left graph plots the perconnection throughput number of bytes per Figure 
Congestion scenario Two connections sharing a single hop with infinite buffers Figure 
Congestion scenario Throughput and delay as a function of host sending rate second at the receiver as a function of the connectionsending rate
For a sending rate between and R the throughput at the receiver equals the senders sending rateeverything sent by the sender is received at the receiver with a finite delay
When the sending rate is above R however the throughput is only R
This upper limit on throughput is a consequence of the sharing of link capacity between two connections
The link simply cannot deliver packets to a receiver at a steadystate rate that exceeds R
No matter how high Hosts A and B set their sending rates they will each never see a throughput higher than R
Achieving a perconnection throughput of R might actually appear to be a good thing because the link is fully utilized in delivering packets to their destinations
The righthand graph in Figure 
however shows the consequence of operating near link capacity
As the sending rate approaches R from the left the average delay becomes larger and larger
When the sending rate exceeds R the average number of queued packets in the router is unbounded and the average delay between source and destination becomes infinite assuming that the connections operate at these sending rates for an infinite period of time and there is an infinite amount of buffering available
Thus while operating at an aggregate throughput of near R may be ideal from a throughput standpoint it is far from ideal from a delay standpoint
Even in this extremely idealized scenario weve already found one cost of a congested networklarge queuing delays are experienced as the packetarrival rate nears the link capacity
Scenario Two Senders and a Router with Finite Buffers Lets now slightly modify scenario in the following two ways see Figure 
First the amount of router buffering is assumed to be finite
A consequence of this realworld assumption is that packets will be dropped when arriving to an alreadyfull buffer
Second we assume that each connection is reliable
If a packet containing Figure 
Scenario Two hosts with retransmissions and a router with finite buffers a transportlevel segment is dropped at the router the sender will eventually retransmit it
Because packets can be retransmitted we must now be more careful with our use of the term sending rate
Specifically let us again denote the rate at which the application sends original data into the socket by Î»in bytessec
The rate at which the transport layer sends segments containing original data and retransmitted data into the network will be denoted Î»in bytessec
Î»in is sometimes referred to as the offered load to the network
The performance realized under scenario will now depend strongly on how retransmission is performed
First consider the unrealistic case that Host A is able to somehow magically determine whether or not a buffer is free in the router and thus sends a packet only when a buffer is free
In this case no loss would occur Î»in would be equal to Î»in and the throughput of the connection would be equal to Î»in
This case is shown in Figure .a
From a throughput standpoint performance is ideal everything that is sent is received
Note that the average host sending rate cannot exceed R under this scenario since packet loss is assumed never to occur
Consider next the slightly more realistic case that the sender retransmits only when a packet is known for certain to be lost
Again this assumption is a bit of a stretch
However it is possible that the sending host might set its timeout large enough to be virtually assured that a packet that has not been acknowledged has been lost
In this case the performance might look something like that shown in Figure .b
To appreciate what is happening here consider the case that the offered load Î»in the rate of original data transmission plus retransmissions equals R
According to Figure .b at this value of the offered load the rate at which data Figure 
Scenario performance with finite buffers are delivered to the receiver application is R
Thus out of the .R units of data transmitted .R bytessec on average are original data and .R bytessec on average are retransmitted data
We see here another cost of a congested networkthe sender must perform retransmissions in order to compensate for dropped lost packets due to buffer overflow
Finally let us consider the case that the sender may time out prematurely and retransmit a packet that has been delayed in the queue but not yet lost
In this case both the original data packet and the retransmission may reach the receiver
Of course the receiver needs but one copy of this packet and will discard the retransmission
In this case the work done by the router in forwarding the retransmitted copy of the original packet was wasted as the receiver will have already received the original copy of this packet
The router would have better used the link transmission capacity to send a different packet instead
Here then is yet another cost of a congested networkunneeded retransmissions by the sender in the face of large delays may cause a router to use its link bandwidth to forward unneeded copies of a packet
c shows the throughput versus offered load when each packet is assumed to be forwarded on average twice by the router
Since each packet is forwarded twice the throughput will have an asymptotic value of R as the offered load approaches R
Scenario Four Senders Routers with Finite Buffers and Multihop Paths In our final congestion scenario four hosts transmit packets each over overlapping twohop paths as shown in Figure 
We again assume that each host uses a timeoutretransmission mechanism to implement a reliable data transfer service that all hosts have the same value of Î»in and that all router links have capacity R bytessec
Four senders routers with finite buffers and multihop paths Lets consider the connection from Host A to Host C passing through routers R and R
The AC connection shares router R with the DB connection and shares router R with the BD connection
For extremely small values of Î»in buffer overflows are rare as in congestion scenarios and and the throughput approximately equals the offered load
For slightly larger values of Î»in the corresponding throughput is also larger since more original data is being transmitted into the network and delivered to the destination and overflows are still rare
Thus for small values of Î»in an increase in Î»in results in an increase in Î»out
Having considered the case of extremely low traffic lets next examine the case that Î»in and hence Î»in is extremely large
Consider router R
The AC traffic arriving to router R which arrives at R after being forwarded from R can have an arrival rate at R that is at most R the capacity of the link from R to R regardless of the value of Î»in
If Î»in is extremely large for all connections including the Figure 
Scenario performance with finite buffers and multihop paths BD connection then the arrival rate of BD traffic at R can be much larger than that of the AC traffic
Because the AC and BD traffic must compete at router R for the limited amount of buffer space the amount of AC traffic that successfully gets through R that is is not lost due to buffer overflow becomes smaller and smaller as the offered load from BD gets larger and larger
In the limit as the offered load approaches infinity an empty buffer at R is immediately filled by a BD packet and the throughput of the AC connection at R goes to zero
This in turn implies that the AC endtoend throughput goes to zero in the limit of heavy traffic
These considerations give rise to the offered load versus throughput tradeoff shown in Figure 
The reason for the eventual decrease in throughput with increasing offered load is evident when one considers the amount of wasted work done by the network
In the hightraffic scenario outlined above whenever a packet is dropped at a secondhop router the work done by the firsthop router in forwarding a packet to the secondhop router ends up being wasted
The network would have been equally well off more accurately equally bad off if the first router had simply discarded that packet and remained idle
More to the point the transmission capacity used at the first router to forward the packet to the second router could have been much more profitably used to transmit a different packet
For example when selecting a packet for transmission it might be better for a router to give priority to packets that have already traversed some number of upstream routers
So here we see yet another cost of dropping a packet due to congestionwhen a packet is dropped along a path the transmission capacity that was used at each of the upstream links to forward that packet to the point at which it is dropped ends up having been wasted
Approaches to Congestion Control In Section 
well examine TCPs specific approach to congestion control in great detail
Here we identify the two broad approaches to congestion control that are taken in practice and discuss specific network architectures and congestioncontrol protocols embodying these approaches
At the highest level we can distinguish among congestioncontrol approaches by whether the network layer provides explicit assistance to the transport layer for congestioncontrol purposes Endtoend congestion control
In an endtoend approach to congestion control the network layer provides no explicit support to the transport layer for congestioncontrol purposes
Even the presence of network congestion must be inferred by the end systems based only on observed network behavior for example packet loss and delay
Well see shortly in Section 
that TCP takes this endtoend approach toward congestion control since the IP layer is not required to provide feedback to hosts regarding network congestion
TCP segment loss as indicated by a timeout or the receipt of three duplicate acknowledgments is taken as an indication of network congestion and TCP decreases its window size accordingly
Well also see a more recent proposal for TCP congestion control that uses increasing roundtrip segment delay as an indicator of increased network congestion Networkassisted congestion control
With networkassisted congestion control routers provide explicit feedback to the sender andor receiver regarding the congestion state of the network
This feedback may be as simple as a single bit indicating congestion at a link an approach taken in the early IBM SNA Schwartz DEC DECnet Jain Ramakrishnan architectures and ATM Black network architectures
More sophisticated feedback is also possible
For example in ATM Available Bite Rate ABR congestion control a router informs the sender of the maximum host sending rate it the router can support on an outgoing link
As noted above the Internetdefault versions of IP and TCP adopt an endtoend approach towards congestion control
Well see however in Section 
that more recently IP and TCP may also optionally implement networkassisted congestion control
For networkassisted congestion control congestion information is typically fed back from the network to the sender in one of two ways as shown in Figure 
Direct feedback may be sent from a network router to the sender
This form of notification typically takes the form of a choke packet essentially saying Im congested
The second and more common form of notification occurs when a router marksupdates a field in a packet flowing from sender to receiver to indicate congestion
Upon receipt of a marked packet the receiver then notifies the sender of the congestion indication
This latter form of notification takes a full roundtrip time
Two feedback pathways for networkindicated congestion information 
TCP Congestion Control In this section we return to our study of TCP
As we learned in Section 
TCP provides a reliable transport service between two processes running on different hosts
Another key component of TCP is its congestioncontrol mechanism
As indicated in the previous section TCP must use endtoend congestion control rather than networkassisted congestion control since the IP layer provides no explicit feedback to the end systems regarding network congestion
The approach taken by TCP is to have each sender limit the rate at which it sends traffic into its connection as a function of perceived network congestion
If a TCP sender perceives that there is little congestion on the path between itself and the destination then the TCP sender increases its send rate if the sender perceives that there is congestion along the path then the sender reduces its send rate
But this approach raises three questions
First how does a TCP sender limit the rate at which it sends traffic into its connection Second how does a TCP sender perceive that there is congestion on the path between itself and the destination And third what algorithm should the sender use to change its send rate as a function of perceived endtoend congestion Lets first examine how a TCP sender limits the rate at which it sends traffic into its connection
In Section 
we saw that each side of a TCP connection consists of a receive buffer a send buffer and several variables LastByteRead rwnd and so on
The TCP congestioncontrol mechanism operating at the sender keeps track of an additional variable the congestion window
The congestion window denoted cwnd imposes a constraint on the rate at which a TCP sender can send traffic into the network
Specifically the amount of unacknowledged data at a sender may not exceed the minimum of cwnd and rwnd that is LastByteSentLastByteAckedmincwnd rwnd In order to focus on congestion control as opposed to flow control let us henceforth assume that the TCP receive buffer is so large that the receivewindow constraint can be ignored thus the amount of unacknowledged data at the sender is solely limited by cwnd 
We will also assume that the sender always has data to send i.e
that all segments in the congestion window are sent
The constraint above limits the amount of unacknowledged data at the sender and therefore indirectly limits the senders send rate
To see this consider a connection for which loss and packet transmission delays are negligible
Then roughly at the beginning of every RTT the constraint permits the sender to send cwnd bytes of data into the connection at the end of the RTT the sender receives acknowledgments for the data
Thus the senders send rate is roughly cwndRTT bytessec
By adjusting the value of cwnd the sender can therefore adjust the rate at which it sends data into its connection
Lets next consider how a TCP sender perceives that there is congestion on the path between itself and the destination
Let us define a loss event at a TCP sender as the occurrence of either a timeout or the receipt of three duplicate ACKs from the receiver
Recall our discussion in Section 
of the timeout event in Figure 
and the subsequent modification to include fast retransmit on receipt of three duplicate ACKs
When there is excessive congestion then one or more router buffers along the path overflows causing a datagram containing a TCP segment to be dropped
The dropped datagram in turn results in a loss event at the sendereither a timeout or the receipt of three duplicate ACKs which is taken by the sender to be an indication of congestion on the sendertoreceiver path
Having considered how congestion is detected lets next consider the more optimistic case when the network is congestionfree that is when a loss event doesnt occur
In this case acknowledgments for previously unacknowledged segments will be received at the TCP sender
As well see TCP will take the arrival of these acknowledgments as an indication that all is wellthat segments being transmitted into the network are being successfully delivered to the destinationand will use acknowledgments to increase its congestion window size and hence its transmission rate
Note that if acknowledgments arrive at a relatively slow rate e.g
if the endend path has high delay or contains a lowbandwidth link then the congestion window will be increased at a relatively slow rate
On the other hand if acknowledgments arrive at a high rate then the congestion window will be increased more quickly
Because TCP uses acknowledgments to trigger or clock its increase in congestion window size TCP is said to be selfclocking
Given the mechanism of adjusting the value of cwnd to control the sending rate the critical question remains How should a TCP sender determine the rate at which it should send If TCP senders collectively send too fast they can congest the network leading to the type of congestion collapse that we saw in Figure 
Indeed the version of TCP that well study shortly was developed in response to observed Internet congestion collapse Jacobson under earlier versions of TCP
However if TCP senders are too cautious and send too slowly they could under utilize the bandwidth in the network that is the TCP senders could send at a higher rate without congesting the network
How then do the TCP senders determine their sending rates such that they dont congest the network but at the same time make use of all the available bandwidth Are TCP senders explicitly coordinated or is there a distributed approach in which the TCP senders can set their sending rates based only on local information TCP answers these questions using the following guiding principles A lost segment implies congestion and hence the TCP senders rate should be decreased when a segment is lost
Recall from our discussion in Section 
that a timeout event or the receipt of four acknowledgments for a given segment one original ACK and then three duplicate ACKs is interpreted as an implicit loss event indication of the segment following the quadruply ACKed segment triggering a retransmission of the lost segment
From a congestioncontrol standpoint the question is how the TCP sender should decrease its congestion window size and hence its sending rate in response to this inferred loss event
An acknowledged segment indicates that the network is delivering the senders segments to the receiver and hence the senders rate can be increased when an ACK arrives for a previously unacknowledged segment
The arrival of acknowledgments is taken as an implicit indication that all is wellsegments are being successfully delivered from sender to receiver and the network is thus not congested
The congestion window size can thus be increased
Given ACKs indicating a congestionfree sourcetodestination path and loss events indicating a congested path TCPs strategy for adjusting its transmission rate is to increase its rate in response to arriving ACKs until a loss event occurs at which point the transmission rate is decreased
The TCP sender thus increases its transmission rate to probe for the rate that at which congestion onset begins backs off from that rate and then to begins probing again to see if the congestion onset rate has changed
The TCP senders behavior is perhaps analogous to the child who requests and gets more and more goodies until finally heshe is finally told No backs off a bit but then begins making requests again shortly afterwards
Note that there is no explicit signaling of congestion state by the networkACKs and loss events serve as implicit signalsand that each TCP sender acts on local information asynchronously from other TCP senders
Given this overview of TCP congestion control were now in a position to consider the details of the celebrated TCP congestioncontrol algorithm which was first described in Jacobson and is standardized in RFC 
The algorithm has three major components slow start congestion avoidance and fast recovery
Slow start and congestion avoidance are mandatory components of TCP differing in how they increase the size of cwnd in response to received ACKs
Well see shortly that slow start increases the size of cwnd more rapidly despite its name than congestion avoidance
Fast recovery is recommended but not required for TCP senders
Slow Start When a TCP connection begins the value of cwnd is typically initialized to a small value of MSS RFC resulting in an initial sending rate of roughly MSSRTT
For example if MSS bytes and RTT msec the resulting initial sending rate is only about kbps
Since the available bandwidth to the TCP sender may be much larger than MSSRTT the TCP sender would like to find the amount of available bandwidth quickly
Thus in the slowstart state the value of cwnd begins at MSS and increases by MSS every time a transmitted segment is first acknowledged
In the example of Figure 
TCP sends the first segment into the network Figure 
TCP slow start and waits for an acknowledgment
When this acknowledgment arrives the TCP sender increases the congestion window by one MSS and sends out two maximumsized segments
These segments are then acknowledged with the sender increasing the congestion window by MSS for each of the acknowledged segments giving a congestion window of MSS and so on
This process results in a doubling of the sending rate every RTT
Thus the TCP send rate starts slow but grows exponentially during the slow start phase
But when should this exponential growth end Slow start provides several answers to this question
First if there is a loss event i.e
congestion indicated by a timeout the TCP sender sets the value of cwnd to and begins the slow start process anew
It also sets the value of a second state variable ssthresh shorthand for slow start threshold to cwnd half of the value of the congestion window value when congestion was detected
The second way in which slow start may end is directly tied to the value of ssthresh 
Since ssthresh is half the value of cwnd when congestion was last detected it might be a bit reckless to keep doubling cwnd when it reaches or surpasses the value of ssthresh 
Thus when the value of cwnd equals ssthresh slow start ends and TCP transitions into congestion avoidance mode
As well see TCP increases cwnd more cautiously when in congestionavoidance mode
The final way in which slow start can end is if three duplicate ACKs are detected in which case TCP performs a fast retransmit see Section 
and enters the fast recovery state as discussed below
TCPs behavior in slow start is summarized in the FSM description of TCP congestion control in Figure 
The slowstart algorithm traces it roots to Jacobson an approach similar to slow start was also proposed independently in Jain 
Congestion Avoidance On entry to the congestionavoidance state the value of cwnd is approximately half its value when congestion was last encounteredcongestion could be just around the corner Thus rather than doubling the value of cwnd every RTT TCP adopts a more conservative approach and increases the value of cwnd by just a single MSS every RTT RFC 
This can be accomplished in several ways
A common approach is for the TCP sender to increase cwnd by MSS bytes MSS cwnd whenever a new acknowledgment arrives
For example if MSS is bytes and cwnd is bytes then segments are being sent within an RTT
Each arriving ACK assuming one ACK per segment increases the congestion window size by MSS and thus the value of the congestion window will have increased by one MSS after ACKs when all segments have been received
But when should congestion avoidances linear increase of MSS per RTT end TCPs congestion avoidance algorithm behaves the same when a timeout occurs
As in the case of slow start The value of cwnd is set to MSS and the value of ssthresh is updated to half the value of cwnd when the loss event occurred
Recall however that a loss event also can be triggered by a triple duplicate ACK event
FSM description of TCP congestion control In this case the network is continuing to deliver segments from sender to receiver as indicated by the receipt of duplicate ACKs
So TCPs behavior to this type of loss event should be less drastic than with a timeoutindicated loss TCP halves the value of cwnd adding in MSS for good measure to account for the triple duplicate ACKs received and records the value of ssthresh to be half the value of cwnd when the triple duplicate ACKs were received
The fastrecovery state is then entered
Fast Recovery In fast recovery the value of cwnd is increased by MSS for every duplicate ACK received for the missing segment that caused TCP to enter the fastrecovery state
Eventually when an ACK arrives for the missing segment TCP enters the Examining the behavior of TCP PRINCIPLES IN PRACTICE TCP SPLITTING OPTIMIZING THE PERFORMANCE OF CLOUD SERVICES For cloud services such as search email and social networks it is desirable to provide a high level of responsiveness ideally giving users the illusion that the services are running within their own end systems including their smartphones
This can be a major challenge as users are often located far away from the data centers responsible for serving the dynamic content associated with the cloud services
Indeed if the end system is far from a data center then the RTT will be large potentially leading to poor response time performance due to TCP slow start
As a case study consider the delay in receiving a response for a search query
Typically the server requires three TCP windows during slow start to deliver the response Pathak 
Thus the time from when an end system initiates a TCP connection until the time when it receives the last packet of the response is roughly RTT one RTT to set up the TCP connection plus three RTTs for the three windows of data plus the processing time in the data center
These RTT delays can lead to a noticeable delay in returning search results for a significant fraction of queries
Moreover there can be significant packet loss in access networks leading to TCP retransmissions and even larger delays
One way to mitigate this problem and improve userperceived performance is to deploy front end servers closer to the users and utilize TCP splitting by breaking the TCP connection at the frontend server
With TCP splitting the client establishes a TCP connection to the nearby frontend and the frontend maintains a persistent TCP connection to the data center with a very large TCP congestion window Tariq Pathak Chen 
With this approach the response time roughly becomes RTTFERTTBE processing time where RTTFE is the round trip time between client and frontend server and RTTBE is the roundtrip time between the front end server and the data center backend server
If the frontend server is close to client then this response time approximately becomes RTT plus processing time since RTTFE is negligibly small and RTTBE is approximately RTT
In summary TCP splitting can reduce the networking delay roughly from RTT to RTT significantly improving userperceived performance particularly for users who are far from the nearest data center
TCP splitting also helps reduce TCP retransmission delays caused by losses in access networks
Google and Akamai have made extensive use of their CDN servers in access networks recall our discussion in Section 
to perform TCP splitting for the cloud services they support Chen 
congestionavoidance state after deflating cwnd 
If a timeout event occurs fast recovery transitions to the slowstart state after performing the same actions as in slow start and congestion avoidance The value of cwnd is set to MSS and the value of ssthresh is set to half the value of cwnd when the loss event occurred
Fast recovery is a recommended but not required component of TCP RFC 
It is interesting that an early version of TCP known as TCP Tahoe unconditionally cut its congestion window to MSS and entered the slowstart phase after either a timeoutindicated or tripleduplicateACKindicated loss event
The newer version of TCP TCP Reno incorporated fast recovery
illustrates the evolution of TCPs congestion window for both Reno and Tahoe
In this figure the threshold is initially equal to MSS
For the first eight transmission rounds Tahoe and Reno take identical actions
The congestion window climbs exponentially fast during slow start and hits the threshold at the fourth round of transmission
The congestion window then climbs linearly until a triple duplicate ACK event occurs just after transmission round 
Note that the congestion window is MSS when this loss event occurs
The value of ssthresh is then set to 
Under TCP Reno the congestion window is set to cwnd MSS and then grows linearly
Under TCP Tahoe the congestion window is set to MSS and grows exponentially until it reaches the value of ssthresh at which point it grows linearly
presents the complete FSM description of TCPs congestioncontrol algorithmsslow start congestion avoidance and fast recovery
The figure also indicates where transmission of new segments or retransmitted segments can occur
Although it is important to distinguish between TCP error controlretransmission and TCP congestion control its also important to appreciate how these two aspects of TCP are inextricably linked
TCP Congestion Control Retrospective Having delved into the details of slow start congestion avoidance and fast recovery its worthwhile to now step back and view the forest from the trees
Ignoring the Figure 
Evolution of TCPs congestion window Tahoe and Reno Figure 
Additiveincrease multiplicativedecrease congestion control initial slowstart period when a connection begins and assuming that losses are indicated by triple duplicate ACKs rather than timeouts TCPs congestion control consists of linear additive increase in cwnd of MSS per RTT and then a halving multiplicative decrease of cwnd on a triple duplicateACK event
For this reason TCP congestion control is often referred to as an additiveincrease multiplicativedecrease AIMD form of congestion control
AIMD congestion control gives rise to the saw tooth behavior shown in Figure 
which also nicely illustrates our earlier intuition of TCP probing for bandwidthTCP linearly increases its congestion window size and hence its transmission rate until a triple duplicateACK event occurs
It then decreases its congestion window size by a factor of two but then again begins increasing it linearly probing to see if there is additional available bandwidth
As noted previously many TCP implementations use the Reno algorithm Padhye 
Many variations of the Reno algorithm have been proposed RFC RFC 
The TCP Vegas algorithm Brakmo Ahn attempts to avoid congestion while maintaining good throughput
The basic idea of Vegas is to detect congestion in the routers between source and destination before packet loss occurs and lower the rate linearly when this imminent packet loss is detected
Imminent packet loss is predicted by observing the RTT
The longer the RTT of the packets the greater the congestion in the routers
As of late the Ubuntu Linux implementation of TCP provided slowstart congestion avoidance fast recovery fast retransmit and SACK by default alternative congestion control algorithms such as TCP Vegas and BIC Xu are also provided
For a survey of the many flavors of TCP see Afanasyev 
TCPs AIMD algorithm was developed based on a tremendous amount of engineering insight and experimentation with congestion control in operational networks
Ten years after TCPs development theoretical analyses showed that TCPs congestioncontrol algorithm serves as a distributed asynchronousoptimization algorithm that results in several important aspects of user and network performance being simultaneously optimized Kelly 
A rich theory of congestion control has since been developed Srikant 
Macroscopic Description of TCP Throughput Given the sawtoothed behavior of TCP its natural to consider what the average throughput that is the average rate of a longlived TCP connection might be
In this analysis well ignore the slowstart phases that occur after timeout events
These phases are typically very short since the sender grows out of the phase exponentially fast
During a particular roundtrip interval the rate at which TCP sends data is a function of the congestion window and the current RTT
When the window size is w bytes and the current roundtrip time is RTT seconds then TCPs transmission rate is roughly wRTT
TCP then probes for additional bandwidth by increasing w by MSS each RTT until a loss event occurs
Denote by W the value of w when a loss event occurs
Assuming that RTT and W are approximately constant over the duration of the connection the TCP transmission rate ranges from W RTT to WRTT
These assumptions lead to a highly simplified macroscopic model for the steadystate behavior of TCP
The network drops a packet from the connection when the rate increases to WRTT the rate is then cut in half and then increases by MSSRTT every RTT until it again reaches WRTT
This process repeats itself over and over again
Because TCPs throughput that is rate increases linearly between the two extreme values we have average throughput of a connection.WRTT Using this highly idealized model for the steadystate dynamics of TCP we can also derive an interesting expression that relates a connections loss rate to its available bandwidth Mahdavi 
This derivation is outlined in the homework problems
A more sophisticated model that has been found empirically to agree with measured data is Padhye 
TCP Over HighBandwidth Paths It is important to realize that TCP congestion control has evolved over the years and indeed continues to evolve
For a summary of current TCP variants and discussion of TCP evolution see Floyd RFC Afanasyev 
What was good for the Internet when the bulk of the TCP connections carried SMTP FTP and Telnet traffic is not necessarily good for todays HTTPdominated Internet or for a future Internet with services that are still undreamed of
The need for continued evolution of TCP can be illustrated by considering the highspeed TCP connections that are needed for grid and cloudcomputing applications
For example consider a TCP connection with byte segments and a ms RTT and suppose we want to send data through this connection at Gbps
Following RFC we note that using the TCP throughput formula above in order to achieve a Gbps throughput the average congestion window size would need to be segments
Thats a lot of segments leading us to be rather concerned that one of these inflight segments might be lost
What would happen in the case of a loss Or put another way what fraction of the transmitted segments could be lost that would allow the TCP congestioncontrol algorithm specified in Figure 
still to achieve the desired Gbps rate In the homework questions for this chapter you are led through the derivation of a formula relating the throughput of a TCP connection as a function of the loss rate L the roundtrip time RTT and the maximum segment size MSS average throughput of a connection.MSSRTTL Using this formula we can see that in order to achieve a throughput of Gbps todays TCP congestioncontrol algorithm can only tolerate a segment loss probability of or equivalently one loss event for every segmentsa very low rate
This observation has led a number of researchers to investigate new versions of TCP that are specifically designed for such highspeed environments see Jin Kelly Ha RFC for discussions of these efforts
Fairness Consider K TCP connections each with a different endtoend path but all passing through a bottleneck link with transmission rate R bps
By bottleneck link we mean that for each connection all the other links along the connections path are not congested and have abundant transmission capacity as compared with the transmission capacity of the bottleneck link
Suppose each connection is transferring a large file and there is no UDP traffic passing through the bottleneck link
A congestioncontrol mechanism is said to be fair if the average transmission rate of each connection is approximately RK that is each connection gets an equal share of the link bandwidth
Is TCPs AIMD algorithm fair particularly given that different TCP connections may start at different times and thus may have different window sizes at a given point in time Chiu provides an elegant and intuitive explanation of why TCP congestion control converges to provide an equal share of a bottleneck links bandwidth among competing TCP connections
Lets consider the simple case of two TCP connections sharing a single link with transmission rate R as shown in Figure 
Assume that the two connections Figure 
Two TCP connections sharing a single bottleneck link have the same MSS and RTT so that if they have the same congestion window size then they have the same throughput that they have a large amount of data to send and that no other TCP connections or UDP datagrams traverse this shared link
Also ignore the slowstart phase of TCP and assume the TCP connections are operating in CA mode AIMD at all times
plots the throughput realized by the two TCP connections
If TCP is to share the link bandwidth equally between the two connections then the realized throughput should fall along the degree arrow equal bandwidth share emanating from the origin
Ideally the sum of the two throughputs should equal R
Certainly each connection receiving an equal but zero share of the link capacity is not a desirable situation So the goal should be to have the achieved throughputs fall somewhere near the intersection of the equal bandwidth share line and the full bandwidth utilization line in Figure 
Suppose that the TCP window sizes are such that at a given point in time connections and realize throughputs indicated by point A in Figure 
Because the amount of link bandwidth jointly consumed by the two connections is less than R no loss will occur and both connections will increase their window by MSS per RTT as a result of TCPs congestionavoidance algorithm
Thus the joint throughput of the two connections proceeds along a degree line equal increase for both connections starting from point A
Eventually the link bandwidth jointly consumed by the two connections will be greater than R and eventually packet loss will occur
Suppose that connections and experience packet loss when they realize throughputs indicated by point B
Connections and then decrease their windows by a factor of two
The resulting throughputs realized are thus at point C halfway along a vector starting at B and ending at the origin
Because the joint bandwidth use is less than R at point C the two connections again increase their throughputs along a degree line starting from C
Eventually loss will again occur for example at point D and the two connections again decrease their window sizes by a factor of two and so on
You should convince yourself that the bandwidth realized by the two connections eventually fluctuates along the equal bandwidth share line
You should also convince Figure 
Throughput realized by TCP connections and yourself that the two connections will converge to this behavior regardless of where they are in the two dimensional space Although a number of idealized assumptions lie behind this scenario it still provides an intuitive feel for why TCP results in an equal sharing of bandwidth among connections
In our idealized scenario we assumed that only TCP connections traverse the bottleneck link that the connections have the same RTT value and that only a single TCP connection is associated with a host destination pair
In practice these conditions are typically not met and clientserver applications can thus obtain very unequal portions of link bandwidth
In particular it has been shown that when multiple connections share a common bottleneck those sessions with a smaller RTT are able to grab the available bandwidth at that link more quickly as it becomes free that is open their congestion windows faster and thus will enjoy higher throughput than those connections with larger RTTs Lakshman 
Fairness and UDP We have just seen how TCP congestion control regulates an applications transmission rate via the congestion window mechanism
Many multimedia applications such as Internet phone and video conferencing often do not run over TCP for this very reasonthey do not want their transmission rate throttled even if the network is very congested
Instead these applications prefer to run over UDP which does not have builtin congestion control
When running over UDP applications can pump their audio and video into the network at a constant rate and occasionally lose packets rather than reduce their rates to fair levels at times of congestion and not lose any packets
From the perspective of TCP the multimedia applications running over UDP are not being fairthey do not cooperate with the other connections nor adjust their transmission rates appropriately
Because TCP congestion control will decrease its transmission rate in the face of increasing congestion loss while UDP sources need not it is possible for UDP sources to crowd out TCP traffic
An area of research today is thus the development of congestioncontrol mechanisms for the Internet that prevent UDP traffic from bringing the Internets throughput to a grinding halt Floyd Floyd Kohler RFC 
Fairness and Parallel TCP Connections But even if we could force UDP traffic to behave fairly the fairness problem would still not be completely solved
This is because there is nothing to stop a TCPbased application from using multiple parallel connections
For example Web browsers often use multiple parallel TCP connections to transfer the multiple objects within a Web page
The exact number of multiple connections is configurable in most browsers
When an application uses multiple parallel connections it gets a larger fraction of the bandwidth in a congested link
As an example consider a link of rate R supporting nine ongoing client server applications with each of the applications using one TCP connection
If a new application comes along and also uses one TCP connection then each application gets approximately the same transmission rate of R
But if this new application instead uses parallel TCP connections then the new application gets an unfair allocation of more than R
Because Web traffic is so pervasive in the Internet multiple parallel connections are not uncommon
Explicit Congestion Notification ECN Networkassisted Congestion Control Since the initial standardization of slow start and congestion avoidance in the late s RFC TCP has implemented the form of endend congestion control that we studied in Section 
a TCP sender receives no explicit congestion indications from the network layer and instead infers congestion through observed packet loss
More recently extensions to both IP and TCP RFC have been proposed implemented and deployed that allow the network to explicitly signal congestion to a TCP sender and receiver
This form of networkassisted congestion control is known as Explicit Congestion Notification
As shown in Figure 
the TCP and IP protocols are involved
At the network layer two bits with four possible values overall in the Type of Service field of the IP datagram header which well discuss in Section 
are used for ECN
One setting of the ECN bits is used by a router to indicate that it the Figure 
Explicit Congestion Notification networkassisted congestion control router is experiencing congestion
This congestion indication is then carried in the marked IP datagram to the destination host which then informs the sending host as shown in Figure 
RFC does not provide a definition of when a router is congested that decision is a configuration choice made possible by the router vendor and decided by the network operator
However RFC does recommend that an ECN congestion indication be set only in the face of persistent congestion
A second setting of the ECN bits is used by the sending host to inform routers that the sender and receiver are ECNcapable and thus capable of taking action in response to ECNindicated network congestion
As shown in Figure 
when the TCP in the receiving host receives an ECN congestion indication via a received datagram the TCP in the receiving host informs the TCP in the sending host of the congestion indication by setting the ECE Explicit Congestion Notification Echo bit see Figure 
in a receivertosender TCP ACK segment
The TCP sender in turn reacts to an ACK with an ECE congestion indication by halving the congestion window as it would react to a lost segment using fast retransmit and sets the CWR Congestion Window Reduced bit in the header of the next transmitted TCP sendertoreceiver segment
Other transportlayer protocols besides TCP may also make use of networklayersignaled ECN
The Datagram Congestion Control Protocol DCCP RFC provides a lowoverhead congestion controlled UDPlike unreliable service that utilizes ECN
DCTCP Data Center TCP Alizadeh a version of TCP designed specifically for data center networks also makes use of ECN
Summary We began this chapter by studying the services that a transportlayer protocol can provide to network applications
At one extreme the transportlayer protocol can be very simple and offer a nofrills service to applications providing only a multiplexingdemultiplexing function for communicating processes
The Internets UDP protocol is an example of such a nofrills transportlayer protocol
At the other extreme a transportlayer protocol can provide a variety of guarantees to applications such as reliable delivery of data delay guarantees and bandwidth guarantees
Nevertheless the services that a transport protocol can provide are often constrained by the service model of the underlying networklayer protocol
If the networklayer protocol cannot provide delay or bandwidth guarantees to transportlayer segments then the transportlayer protocol cannot provide delay or bandwidth guarantees for the messages sent between processes
We learned in Section 
that a transportlayer protocol can provide reliable data transfer even if the underlying network layer is unreliable
We saw that providing reliable data transfer has many subtle points but that the task can be accomplished by carefully combining acknowledgments timers retransmissions and sequence numbers
Although we covered reliable data transfer in this chapter we should keep in mind that reliable data transfer can be provided by link network transport or applicationlayer protocols
Any of the upper four layers of the protocol stack can implement acknowledgments timers retransmissions and sequence numbers and provide reliable data transfer to the layer above
In fact over the years engineers and computer scientists have independently designed and implemented link network transport and applicationlayer protocols that provide reliable data transfer although many of these protocols have quietly disappeared
In Section 
we took a close look at TCP the Internets connectionoriented and reliable transport layer protocol
We learned that TCP is complex involving connection management flow control and roundtrip time estimation as well as reliable data transfer
In fact TCP is actually more complex than our descriptionwe intentionally did not discuss a variety of TCP patches fixes and improvements that are widely implemented in various versions of TCP
All of this complexity however is hidden from the network application
If a client on one host wants to send data reliably to a server on another host it simply opens a TCP socket to the server and pumps data into that socket
The clientserver application is blissfully unaware of TCPs complexity
In Section 
we examined congestion control from a broad perspective and in Section 
we showed how TCP implements congestion control
We learned that congestion control is imperative for the wellbeing of the network
Without congestion control a network can easily become gridlocked with little or no data being transported endtoend
In Section 
we learned that TCP implements an end toend congestioncontrol mechanism that additively increases its transmission rate when the TCP connections path is judged to be congestionfree and multiplicatively decreases its transmission rate when loss occurs
This mechanism also strives to give each TCP connection passing through a congested link an equal share of the link bandwidth
We also examined in some depth the impact of TCP connection establishment and slow start on latency
We observed that in many important scenarios connection establishment and slow start significantly contribute to endtoend delay
We emphasize once more that while TCP congestion control has evolved over the years it remains an area of intensive research and will likely continue to evolve in the upcoming years
Our discussion of specific Internet transport protocols in this chapter has focused on UDP and TCPthe two work horses of the Internet transport layer
However two decades of experience with these two protocols has identified circumstances in which neither is ideally suited
Researchers have thus been busy developing additional transportlayer protocols several of which are now IETF proposed standards
The Datagram Congestion Control Protocol DCCP RFC provides a lowoverhead message oriented UDPlike unreliable service but with an applicationselected form of congestion control that is compatible with TCP
If reliable or semireliable data transfer is needed by an application then this would be performed within the application itself perhaps using the mechanisms we have studied in Section 
DCCP is envisioned for use in applications such as streaming media see Chapter that can exploit the tradeoff between timeliness and reliability of data delivery but that want to be responsive to network congestion
Googles QUIC Quick UDP Internet Connections protocol Iyengar implemented in Googles Chromium browser provides reliability via retransmission as well as error correction fastconnection setup and a ratebased congestion control algorithm that aims to be TCP friendlyall implemented as an applicationlevel protocol on top of UDP
In early Google reported that roughly half of all requests from Chrome to Google servers are served over QUIC
DCTCP Data Center TCP Alizadeh is a version of TCP designed specifically for data center networks and uses ECN to better support the mix of short and longlived flows that characterize data center workloads
The Stream Control Transmission Protocol SCTP RFC RFC is a reliable message oriented protocol that allows several different applicationlevel streams to be multiplexed through a single SCTP connection an approach known as multistreaming
From a reliability standpoint the different streams within the connection are handled separately so that packet loss in one stream does not affect the delivery of data in other streams
QUIC provides similar multistream semantics
SCTP also allows data to be transferred over two outgoing paths when a host is connected to two or more networks optional delivery of outoforder data and a number of other features
SCTPs flow and congestioncontrol algorithms are essentially the same as in TCP
The TCPFriendly Rate Control TFRC protocol RFC is a congestioncontrol protocol rather than a fullfledged transportlayer protocol
It specifies a congestioncontrol mechanism that could be used in another transport protocol such as DCCP indeed one of the two applicationselectable protocols available in DCCP is TFRC
The goal of TFRC is to smooth out the saw tooth behavior see Figure 
in TCP congestion control while maintaining a longterm sending rate that is reasonably close to that of TCP
With a smoother sending rate than TCP TFRC is wellsuited for multimedia applications such as IP telephony or streaming media where such a smooth rate is important
TFRC is an equation based protocol that uses the measured packet loss rate as input to an equation Padhye that estimates what TCPs throughput would be if a TCP session experiences that loss rate
This rate is then taken as TFRCs target sending rate
Only the future will tell whether DCCP SCTP QUIC or TFRC will see widespread deployment
While these protocols clearly provide enhanced capabilities over TCP and UDP TCP and UDP have proven themselves good enough over the years
Whether better wins out over good enough will depend on a complex mix of technical social and business considerations
In Chapter we said that a computer network can be partitioned into the network edge and the network core
The network edge covers everything that happens in the end systems
Having now covered the application layer and the transport layer our discussion of the network edge is complete
It is time to explore the network core This journey begins in the next two chapters where well study the network layer and continues into Chapter where well study the link layer
Homework Problems and Questions Chapter Review Questions SECTIONS 
Suppose the network layer provides the following service
The network layer in the source host accepts a segment of maximum size bytes and a destination host address from the transport layer
The network layer then guarantees to deliver the segment to the transport layer at the destination host
Suppose many network application processes can be running at the destination host
Design the simplest possible transportlayer protocol that will get application data to the desired process at the destination host
Assume the operating system in the destination host has assigned a byte port number to each running application process
Modify this protocol so that it provides a return address to the destination process
In your protocols does the transport layer have to do anything in the core of the computer network R
Consider a planet where everyone belongs to a family of six every family lives in its own house each house has a unique address and each person in a given house has a unique name
Suppose this planet has a mail service that delivers letters from source house to destination house
The mail service requires that the letter be in an envelope and that the address of the destination house and nothing more be clearly written on the envelope
Suppose each family has a delegate family member who collects and distributes letters for the other family members
The letters do not necessarily provide any indication of the recipients of the letters
Using the solution to Problem R above as inspiration describe a protocol that the delegates can use to deliver letters from a sending family member to a receiving family member
In your protocol does the mail service ever have to open the envelope and examine the letter in order to provide its service R
Consider a TCP connection between Host A and Host B
Suppose that the TCP segments traveling from Host A to Host B have source port number x and destination port number y
What are the source and destination port numbers for the segments traveling from Host B to Host A R
Describe why an application developer might choose to run an application over UDP rather than TCP
Why is it that voice and video traffic is often sent over TCP rather than UDP in todays Internet Hint The answer we are looking for has nothing to do with TCPs congestioncontrol mechanism
Is it possible for an application to enjoy reliable data transfer even when the application runs over UDP If so how R
Suppose a process in Host C has a UDP socket with port number 
Suppose both Host A and Host B each send a UDP segment to Host C with destination port number 
Will both of these segments be directed to the same socket at Host C If so how will the process at Host C know that these two segments originated from two different hosts R
Suppose that a Web server runs in Host C on port 
Suppose this Web server uses persistent connections and is currently receiving requests from two different Hosts A and B
Are all of the requests being sent through the same socket at Host C If they are being passed through different sockets do both of the sockets have port Discuss and explain
In our rdt protocols why did we need to introduce sequence numbers R
In our rdt protocols why did we need to introduce timers R
Suppose that the roundtrip delay between sender and receiver is constant and known to the sender
Would a timer still be necessary in protocol rdt 
assuming that packets can be lost Explain
Visit the GoBackN Java applet at the companion Web site
Have the source send five packets and then pause the animation before any of the five packets reach the destination
Then kill the first packet and resume the animation
Describe what happens
Repeat the experiment but now let the first packet reach the destination and kill the first acknowledgment
Describe again what happens
Finally try sending six packets
What happens R
Repeat R but now with the Selective Repeat Java applet
How are Selective Repeat and GoBackN different SECTION 
True or false a
Host A is sending Host B a large file over a TCP connection
Assume Host B has no data to send Host A
Host B will not send acknowledgments to Host A because Host B cannot piggyback the acknowledgments on data
The size of the TCP rwnd never changes throughout the duration of the connection
Suppose Host A is sending Host B a large file over a TCP connection
The number of unacknowledged bytes that A sends cannot exceed the size of the receive buffer
Suppose Host A is sending a large file to Host B over a TCP connection
If the sequence number for a segment of this connection is m then the sequence number for the subsequent segment will necessarily be m
The TCP segment has a field in its header for rwnd 
Suppose that the last SampleRTT in a TCP connection is equal to sec
The current value of TimeoutInterval for the connection will necessarily be sec
Suppose Host A sends one segment with sequence number and bytes of data over a TCP connection to Host B
In this same segment the acknowledgment number is necessarily 
Suppose Host A sends two TCP segments back to back to Host B over a TCP connection
The first segment has sequence number the second has sequence number 
How much data is in the first segment b
Suppose that the first segment is lost but the second segment arrives at B
In the acknowledgment that Host B sends to Host A what will be the acknowledgment number R
Consider the Telnet example discussed in Section 
A few seconds after the user types the letter C the user types the letter R
After typing the letter R how many segments are sent and what is put in the sequence number and acknowledgment fields of the segments SECTION 
Suppose two TCP connections are present over some bottleneck link of rate R bps
Both connections have a huge file to send in the same direction over the bottleneck link
The transmissions of the files start at the same time
What transmission rate would TCP like to give to each of the connections R
True or false Consider congestion control in TCP
When the timer expires at the sender the value of ssthresh is set to one half of its previous value
In the discussion of TCP splitting in the sidebar in Section 
it was claimed that the response time with TCP splitting is approximately RTTFERTTBEprocessing time
Justify this claim
Suppose Client A initiates a Telnet session with Server S
At about the same time Client B also initiates a Telnet session with Server S
Provide possible source and destination port numbers for a
The segments sent from A to S
The segments sent from B to S
The segments sent from S to A
The segments sent from S to B
If A and B are different hosts is it possible that the source port number in the segments from A to S is the same as that from B to S f
How about if they are the same host P
Consider Figure 
What are the source and destination port values in the segments flowing from the server back to the clients processes What are the IP addresses in the networklayer datagrams carrying the transportlayer segments P
UDP and TCP use s complement for their checksums
Suppose you have the following three bit bytes 
What is the s complement of the sum of these bit bytes Note that although UDP and TCP use bit words in computing the checksum for this problem you are being asked to consider bit sums
Show all work
Why is it that UDP takes the s complement of the sum that is why not just use the sum With the s complement scheme how does the receiver detect errors Is it possible that a bit error will go undetected How about a bit error P
Suppose you have the following bytes and 
What is the s complement of the sum of these bytes b
Suppose you have the following bytes and 
What is the s complement of the sum of these bytes c
For the bytes in part a give an example where one bit is flipped in each of the bytes and yet the s complement doesnt change
Suppose that the UDP receiver computes the Internet checksum for the received UDP segment and finds that it matches the value carried in the checksum field
Can the receiver be absolutely certain that no bit errors have occurred Explain
Consider our motivation for correcting protocol rdt
Show that the receiver shown in Figure 
when operating with the sender shown in Figure 
can lead the sender and receiver to enter into a deadlock state where each is waiting for an event that will never occur
In protocol rdt
the ACK packets flowing from the receiver to the sender do not have sequence numbers although they do have an ACK field that contains the sequence number of the packet they are acknowledging
Why is it that our ACK packets do not require sequence numbers Figure 
An incorrect receiver for protocol rdt 
Draw the FSM for the receiver side of protocol rdt
Give a trace of the operation of protocol rdt
when data packets and acknowledgment packets are garbled
Your trace should be similar to that used in Figure 
Consider a channel that can lose packets but has a maximum delay that is known
Modify protocol rdt
to include sender timeout and retransmit
Informally argue why your protocol can communicate correctly over this channel
Consider the rdt
receiver in Figure 
and the creation of a new packet in the selftransition i.e
the transition from the state back to itself in the Waitforfrombelow and the Waitforfrombelow states sndpktmake_pktACK checksum and sndpktmake_pktACK checksum 
Would the protocol work correctly if this action were removed from the selftransition in the Waitforfrombelow state Justify your answer
What if this event were removed from the selftransition in the Waitforfrombelow state Hint In this latter case consider what would happen if the first sendertoreceiver packet were corrupted
The sender side of rdt
simply ignores that is takes no action on all received packets that are either in error or have the wrong value in the acknum field of an acknowledgment packet
Suppose that in such circumstances rdt
were simply to retransmit the current data packet
Would the protocol still work Hint Consider what would happen if there were only bit errors there are no packet losses but premature timeouts can occur
Consider how many times the nth packet is sent in the limit as n approaches infinity
Consider the rdt 
Draw a diagram showing that if the network connection between the sender and receiver can reorder messages that is that two messages propagating in the medium between the sender and receiver can be reordered then the alternatingbit protocol will not work correctly make sure you clearly identify the sense in which it will not work correctly
Your diagram should have the sender on the left and the receiver on the right with the time axis running down the page showing data D and acknowledgment A message exchange
Make sure you indicate the sequence number associated with any data or acknowledgment segment
Consider a reliable data transfer protocol that uses only negative acknowledgments
Suppose the sender sends data only infrequently
Would a NAKonly protocol be preferable to a protocol that uses ACKs Why Now suppose the sender has a lot of data to send and the end toend connection experiences few losses
In this second case would a NAKonly protocol be preferable to a protocol that uses ACKs Why P
Consider the crosscountry example shown in Figure 
How big would the window size have to be for the channel utilization to be greater than percent Suppose that the size of a packet is bytes including both header fields and data
Suppose an application uses rdt 
as its transport layer protocol
As the stopandwait protocol has very low channel utilization shown in the crosscountry example the designers of this application let the receiver keep sending back a number more than two of alternating ACK and ACK even if the corresponding data have not arrived at the receiver
Would this application design increase the channel utilization Why Are there any potential problems with this approach Explain
Consider two network entities A and B which are connected by a perfect bidirectional channel i.e
any message sent will be received correctly the channel will not corrupt lose or reorder packets
A and B are to deliver data messages to each other in an alternating manner First A must deliver a message to B then B must deliver a message to A then A must deliver a message to B and so on
If an entity is in a state where it should not attempt to deliver a message to the other side and there is an event like rdt_senddata call from above that attempts to pass data down for transmission to the other side this call from above can simply be ignored with a call to rdt_unable_to_senddata which informs the higher layer that it is currently not able to send data
Note This simplifying assumption is made so you dont have to worry about buffering data
Draw a FSM specification for this protocol one FSM for A and one FSM for B
Note that you do not have to worry about a reliability mechanism here the main point of this question is to create a FSM specification that reflects the synchronized behavior of the two entities
You should use the following events and actions that have the same meaning as protocol rdt
in Figure 
rdt_senddata packet make_pktdata udt_sendpacket rdt_rcvpacket extract packet data deliver_datadata 
Make sure your protocol reflects the strict alternation of sending between A and B
Also make sure to indicate the initial states for A and B in your FSM descriptions
In the generic SR protocol that we studied in Section 
the sender transmits a message as soon as it is available if it is in the window without waiting for an acknowledgment
Suppose now that we want an SR protocol that sends messages two at a time
That is the sender will send a pair of messages and will send the next pair of messages only when it knows that both messages in the first pair have been received correctly
Suppose that the channel may lose messages but will not corrupt or reorder messages
Design an errorcontrol protocol for the unidirectional reliable transfer of messages
Give an FSM description of the sender and receiver
Describe the format of the packets sent between sender and receiver and vice versa
If you use any procedure calls other than those in Section 
for example udt_send start_timer rdt_rcv and so on clearly state their actions
Give an example a timeline trace of sender and receiver showing how your protocol recovers from a lost packet
Consider a scenario in which Host A wants to simultaneously send packets to Hosts B and C
A is connected to B and C via a broadcast channela packet sent by A is carried by the channel to both B and C
Suppose that the broadcast channel connecting A B and C can independently lose and corrupt packets and so for example a packet sent from A might be correctly received by B but not by C
Design a stopandwaitlike errorcontrol protocol for reliably transferring packets from A to B and C such that A will not get new data from the upper layer until it knows that both B and C have correctly received the current packet
Give FSM descriptions of A and C
Hint The FSM for B should be essentially the same as for C
Also give a description of the packet formats used
Consider a scenario in which Host A and Host B want to send messages to Host C
Hosts A and C are connected by a channel that can lose and corrupt but not reorder messages
Hosts B and C are connected by another channel independent of the channel connecting A and C with the same properties
The transport layer at Host C should alternate in delivering messages from A and B to the layer above that is it should first deliver the data from a packet from A then the data from a packet from B and so on
Design a stopandwaitlike errorcontrol protocol for reliably transferring packets from A and B to C with alternating delivery at C as described above
Give FSM descriptions of A and C
Hint The FSM for B should be essentially the same as for A
Also give a description of the packet formats used
Suppose we have two network entities A and B
B has a supply of data messages that will be sent to A according to the following conventions
When A gets a request from the layer above to get the next data D message from B A must send a request R message to B on the AtoB channel
Only when B receives an R message can it send a data D message back to A on the BtoA channel
A should deliver exactly one copy of each D message to the layer above
R messages can be lost but not corrupted in the AtoB channel D messages once sent are always delivered correctly
The delay along both channels is unknown and variable
Design give an FSM description of a protocol that incorporates the appropriate mechanisms to compensate for the lossprone AtoB channel and implements message passing to the layer above at entity A as discussed above
Use only those mechanisms that are absolutely necessary
Consider the GBN protocol with a sender window size of and a sequence number range of 
Suppose that at time t the next inorder packet that the receiver is expecting has a sequence number of k
Assume that the medium does not reorder messages
Answer the following questions a
What are the possible sets of sequence numbers inside the senders window at time t Justify your answer
What are all possible values of the ACK field in all possible messages currently propagating back to the sender at time t Justify your answer
Consider the GBN and SR protocols
Suppose the sequence number space is of size k
What is the largest allowable sender window that will avoid the occurrence of problems such as that in Figure 
for each of these protocols P
Answer true or false to the following questions and briefly justify your answer a
With the SR protocol it is possible for the sender to receive an ACK for a packet that falls outside of its current window
With GBN it is possible for the sender to receive an ACK for a packet that falls outside of its current window
The alternatingbit protocol is the same as the SR protocol with a sender and receiver window size of 
The alternatingbit protocol is the same as the GBN protocol with a sender and receiver window size of 
We have said that an application may choose UDP for a transport protocol because UDP offers finer application control than TCP of what data is sent in a segment and when
Why does an application have more control of what data is sent in a segment b
Why does an application have more control on when the segment is sent P
Consider transferring an enormous file of L bytes from Host A to Host B
Assume an MSS of bytes
What is the maximum value of L such that TCP sequence numbers are not exhausted Recall that the TCP sequence number field has bytes
For the L you obtain in a find how long it takes to transmit the file
Assume that a total of bytes of transport network and datalink header are added to each segment before the resulting packet is sent out over a Mbps link
Ignore flow control and congestion control so A can pump out the segments back to back and continuously
Host A and B are communicating over a TCP connection and Host B has already received from A all bytes up through byte 
Suppose Host A then sends two segments to Host B back toback
The first and second segments contain and bytes of data respectively
In the first segment the sequence number is the source port number is and the destination port number is 
Host B sends an acknowledgment whenever it receives a segment from Host A
In the second segment sent from Host A to B what are the sequence number source port number and destination port number b
If the first segment arrives before the second segment in the acknowledgment of the first arriving segment what is the acknowledgment number the source port number and the destination port number c
If the second segment arrives before the first segment in the acknowledgment of the first arriving segment what is the acknowledgment number d
Suppose the two segments sent by A arrive in order at B
The first acknowledgment is lost and the second acknowledgment arrives after the first timeout interval
Draw a timing diagram showing these segments and all other segments and acknowledgments sent
Assume there is no additional packet loss
For each segment in your figure provide the sequence number and the number of bytes of data for each acknowledgment that you add provide the acknowledgment number
Host A and B are directly connected with a Mbps link
There is one TCP connection between the two hosts and Host A is sending to Host B an enormous file over this connection
Host A can send its application data into its TCP socket at a rate as high as Mbps but Host B can read out of its TCP receive buffer at a maximum rate of Mbps
Describe the effect of TCP flow control
SYN cookies were discussed in Section 
Why is it necessary for the server to use a special initial sequence number in the SYNACK b
Suppose an attacker knows that a target host uses SYN cookies
Can the attacker create halfopen or fully open connections by simply sending an ACK packet to the target Why or why not c
Suppose an attacker collects a large amount of initial sequence numbers sent by the server
Can the attacker cause the server to create many fully open connections by sending ACKs with those initial sequence numbers Why P
Consider the network shown in Scenario in Section 
Suppose both sending hosts A and B have some fixed timeout values
Argue that increasing the size of the finite buffer of the router might possibly decrease the throughput Î»out
Now suppose both hosts dynamically adjust their timeout values like what TCP does based on the buffering delay at the router
Would increasing the buffer size help to increase the throughput Why P
Suppose that the five measured SampleRTT values see Section 
are ms ms ms ms and ms
Compute the EstimatedRTT after each of these SampleRTT values is obtained using a value of Î±
and assuming that the value of EstimatedRTT was ms just before the first of these five samples were obtained
Compute also the DevRTT after each sample is obtained assuming a value of Î²
and assuming the value of DevRTT was ms just before the first of these five samples was obtained
Last compute the TCP TimeoutInterval after each of these samples is obtained
Consider the TCP procedure for estimating RTT
Suppose that Î±
Let SampleRTT be the most recent sample RTT let SampleRTT be the next most recent sample RTT and so on
For a given TCP connection suppose four acknowledgments have been returned with corresponding sample RTTs SampleRTT SampleRTT SampleRTT and SampleRTT 
Express EstimatedRTT in terms of the four sample RTTs
Generalize your formula for n sample RTTs
For the formula in part b let n approach infinity
Comment on why this averaging procedure is called an exponential moving average
In Section 
we discussed TCPs estimation of RTT
Why do you think TCP avoids measuring the SampleRTT for retransmitted segments P
What is the relationship between the variable SendBase in Section 
and the variable LastByteRcvd in Section 
What is the relationship between the variable LastByteRcvd in Section 
and the variable y in Section 
In Section 
we saw that TCP waits until it has received three duplicate ACKs before performing a fast retransmit
Why do you think the TCP designers chose not to perform a fast retransmit after the first duplicate ACK for a segment is received P
Compare GBN SR and TCP no delayed ACK
Assume that the timeout values for all three protocols are sufficiently long such that consecutive data segments and their corresponding ACKs can be received if not lost in the channel by the receiving host Host B and the sending host Host A respectively
Suppose Host A sends data segments to Host B and the nd segment sent from A is lost
In the end all data segments have been correctly received by Host B
How many segments has Host A sent in total and how many ACKs has Host B sent in total What are their sequence numbers Answer this question for all three protocols
If the timeout values for all three protocol are much longer than RTT then which protocol successfully delivers all five data segments in shortest time interval P
In our description of TCP in Figure 
the value of the threshold ssthresh is set as ssthreshcwnd in several places and ssthresh value is referred to as being set to half the window size when a loss event occurred
Must the rate at which the sender is sending when the loss event occurred be approximately equal to cwnd segments per RTT Explain your answer
If your answer is no can you suggest a different manner in which ssthresh should be set P
Consider Figure .b 
If Î»in increases beyond R can Î»out increase beyond R Explain
Now consider Figure .c 
If Î»in increases beyond R can Î»out increase beyond R under the assumption that a packet will be forwarded twice on average from the router to the receiver Explain
Consider Figure 
Assuming TCP Reno is the protocol experiencing the behavior shown above answer the following questions
In all cases you should provide a short discussion justifying your answer
Examining the behavior of TCP a
Identify the intervals of time when TCP slow start is operating
Identify the intervals of time when TCP congestion avoidance is operating
After the th transmission round is segment loss detected by a triple duplicate ACK or by a timeout d
After the nd transmission round is segment loss detected by a triple duplicate ACK or by a timeout Figure 
TCP window size as a function of time e
What is the initial value of ssthresh at the first transmission round f
What is the value of ssthresh at the th transmission round g
What is the value of ssthresh at the th transmission round h
During what transmission round is the th segment sent i
Assuming a packet loss is detected after the th round by the receipt of a triple duplicate ACK what will be the values of the congestion window size and of ssthresh j
Suppose TCP Tahoe is used instead of TCP Reno and assume that triple duplicate ACKs are received at the th round
What are the ssthresh and the congestion window size at the th round k
Again suppose TCP Tahoe is used and there is a timeout event at nd round
How many packets have been sent out from th round till nd round inclusive P
Refer to Figure 
which illustrates the convergence of TCPs AIMD algorithm
Suppose that instead of a multiplicative decrease TCP decreased the window size by a constant amount
Would the resulting AIAD algorithm converge to an equal share algorithm Justify your answer using a diagram similar to Figure 
In Section 
we discussed the doubling of the timeout interval after a timeout event
This mechanism is a form of congestion control
Why does TCP need a windowbased congestioncontrol mechanism as studied in Section 
in addition to this doublingtimeout interval mechanism P
Host A is sending an enormous file to Host B over a TCP connection
Over this connection there is never any packet loss and the timers never expire
Denote the transmission rate of the link connecting Host A to the Internet by R bps
Suppose that the process in Host A is capable of sending data into its TCP socket at a rate S bps where SR
Further suppose that the TCP receive buffer is large enough to hold the entire file and the send buffer can hold only one percent of the file
What would prevent the process in Host A from continuously passing data to its TCP socket at rate S bps TCP flow control TCP congestion control Or something else Elaborate
Consider sending a large file from a host to another over a TCP connection that has no loss
Suppose TCP uses AIMD for its congestion control without slow start
Assuming cwnd increases by MSS every time a batch of ACKs is received and assuming approximately constant roundtrip times how long does it take for cwnd increase from MSS to MSS assuming no loss events b
What is the average throughout in terms of MSS and RTT for this connection up through time RTT P
Recall the macroscopic description of TCP throughput
In the period of time from when the connections rate varies from W RTT to WRTT only one packet is lost at the very end of the period
Show that the loss rate fraction of packets lost is equal to Lloss rateWW b
Use the result above to show that if a connection has loss rate L then its average rate is approximately given by .MSSRTTL P
Consider that only a single TCP Reno connection uses one Mbps link which does not buffer any data
Suppose that this link is the only congested link between the sending and receiving hosts
Assume that the TCP sender has a huge file to send to the receiver and the receivers receive buffer is much larger than the congestion window
We also make the following assumptions each TCP segment size is bytes the twoway propagation delay of this connection is msec and this TCP connection is always in congestion avoidance phase that is ignore slow start
What is the maximum window size in segments that this TCP connection can achieve b
What is the average window size in segments and average throughput in bps of this TCP connection c
How long would it take for this TCP connection to reach its maximum window again after recovering from a packet loss P
Consider the scenario described in the previous problem
Suppose that the Mbps link can buffer a finite number of segments
Argue that in order for the link to always be busy sending data we would like to choose a buffer size that is at least the product of the link speed C and the twoway propagation delay between the sender and the receiver
Repeat Problem but replacing the Mbps link with a Gbps link
Note that in your answer to part c you will realize that it takes a very long time for the congestion window size to reach its maximum window size after recovering from a packet loss
Sketch a solution to solve this problem
Let T measured by RTT denote the time interval that a TCP connection takes to increase its congestion window size from W to W where W is the maximum congestion window size
Argue that T is a function of TCPs average throughput
Consider a simplified TCPs AIMD algorithm where the congestion window size is measured in number of segments not in bytes
In additive increase the congestion window size increases by one segment in each RTT
In multiplicative decrease the congestion window size decreases by half if the result is not an integer round down to the nearest integer
Suppose that two TCP connections C and C share a single congested link of speed segments per second
Assume that both C and C are in the congestion avoidance phase
Connection Cs RTT is msec and connection Cs RTT is msec
Assume that when the data rate in the link exceeds the links speed all TCP connections experience data segment loss
If both C and C at time t have a congestion window of segments what are their congestion window sizes after msec b
In the long run will these two connections get the same share of the bandwidth of the congested link Explain
Consider the network described in the previous problem
Now suppose that the two TCP connections C and C have the same RTT of msec
Suppose that at time t Cs congestion window size is segments but Cs congestion window size is segments
What are their congestion window sizes after msec b
In the long run will these two connections get about the same share of the bandwidth of the congested link c
We say that two connections are synchronized if both connections reach their maximum window sizes at the same time and reach their minimum window sizes at the same time
In the long run will these two connections get synchronized eventually If so what are their maximum window sizes d
Will this synchronization help to improve the utilization of the shared link Why Sketch some idea to break this synchronization
Consider a modification to TCPs congestion control algorithm
Instead of additive increase we can use multiplicative increase
A TCP sender increases its window size by a small positive constant aa whenever it receives a valid ACK
Find the functional relationship between loss rate L and maximum congestion window W
Argue that for this modified TCP regardless of TCPs average throughput a TCP connection always spends the same amount of time to increase its congestion window size from W to W
In our discussion of TCP futures in Section 
we noted that to achieve a throughput of Gbps TCP could only tolerate a segment loss probability of or equivalently one loss event for every segments
Show the derivation for the values of out of for the RTT and MSS values given in Section 
If TCP needed to support a Gbps connection what would the tolerable loss be P
In our discussion of TCP congestion control in Section 
we implicitly assumed that the TCP sender always had data to send
Consider now the case that the TCP sender sends a large amount of data and then goes idle since it has no more data to send at t
TCP remains idle for a relatively long period of time and then wants to send more data at t
What are the advantages and disadvantages of having TCP use the cwnd and ssthresh values from t when starting to send data at t What alternative would you recommend Why P
In this problem we investigate whether either UDP or TCP provides a degree of endpoint authentication
Consider a server that receives a request within a UDP packet and responds to that request within a UDP packet for example as done by a DNS server
If a client with IP address X spoofs its address with address Y where will the server send its response b
Suppose a server receives a SYN with IP source address Y and after responding with a SYNACK receives an ACK with IP source address Y with the correct acknowledgment number
Assuming the server chooses a random initial sequence number and there is no maninthemiddle can the server be certain that the client is indeed at Y and not at some other address X that is spoofing Y P
In this problem we consider the delay introduced by the TCP slowstart phase
Consider a client and a Web server directly connected by one link of rate R
Suppose the client wants to retrieve an object whose size is exactly equal to S where S is the maximum segment size MSS
Denote the roundtrip time between client and server as RTT assumed to be constant
Ignoring protocol headers determine the time to retrieve the object including TCP connection establishment when a
SRRTT SR c
Programming Assignments Implementing a Reliable Transport Protocol In this laboratory programming assignment you will be writing the sending and receiving transportlevel code for implementing a simple reliable data transfer protocol
There are two versions of this lab the alternatingbitprotocol version and the GBN version
This lab should be funyour implementation will differ very little from what would be required in a realworld situation
Since you probably dont have standalone machines with an OS that you can modify your code will have to execute in a simulated hardwaresoftware environment
However the programming interface provided to your routinesthe code that would call your entities from above and from belowis very close to what is done in an actual UNIX environment
Indeed the software interfaces described in this programming assignment are much more realistic than the infinite loop senders and receivers that many texts describe
Stopping and starting timers are also simulated and timer interrupts will cause your timer handling routine to be activated
The full lab assignment as well as code you will need to compile with your own code are available at this books Web site www.pearsonhighered.comcsresources
Wireshark Lab Exploring TCP In this lab youll use your Web browser to access a file from a Web server
As in earlier Wireshark labs youll use Wireshark to capture the packets arriving at your computer
Unlike earlier labs youll also be able to download a Wiresharkreadable packet trace from the Web server from which you downloaded the file
In this server trace youll find the packets that were generated by your own access of the Web server
Youll analyze the client and serverside traces to explore aspects of TCP
In particular youll evaluate the performance of the TCP connection between your computer and the Web server
Youll trace TCPs window behavior and infer packet loss retransmission flow control and congestion control behavior and estimated roundtrip time
As is the case with all Wireshark labs the full description of this lab is available at this books Web site www.pearsonhighered.comcsresources
Wireshark Lab Exploring UDP In this short lab youll do a packet capture and analysis of your favorite application that uses UDP for example DNS or a multimedia application such as Skype
As we learned in Section 
UDP is a simple nofrills transport protocol
In this lab youll investigate the header fields in the UDP segment as well as the checksum calculation
As is the case with all Wireshark labs the full description of this lab is available at this books Web site www.pearsonhighered.comcsresources
AN INTERVIEW WITH
Van Jacobson Van Jacobson works at Google and was previously a Research Fellow at PARC
Prior to that he was cofounder and Chief Scientist of Packet Design
Before that he was Chief Scientist at Cisco
Before joining Cisco he was head of the Network Research Group at Lawrence Berkeley National Laboratory and taught at UC Berkeley and Stanford
Van received the ACM SIGCOMM Award in for outstanding lifetime contribution to the field of communication networks and the IEEE Kobayashi Award in for contributing to the understanding of network congestion and developing congestion control mechanisms that enabled the successful scaling of the Internet
He was elected to the U.S
National Academy of Engineering in 
Please describe one or two of the most exciting projects you have worked on during your career
What were the biggest challenges School teaches us lots of ways to find answers
In every interesting problem Ive worked on the challenge has been finding the right question
When Mike Karels and I started looking at TCP congestion we spent months staring at protocol and packet traces asking Why is it failing
One day in Mikes office one of us said The reason I cant figure out why it fails is because I dont understand how it ever worked to begin with
That turned out to be the right question and it forced us to figure out the ack clocking that makes TCP work
After that the rest was easy
More generally where do you see the future of networking and the Internet For most people the Web is the Internet
Networking geeks smile politely since we know the Web is an application running over the Internet but what if theyre right The Internet is about enabling conversations between pairs of hosts
The Web is about distributed information production and consumption
Information propagation is a very general view of communication of which pairwise conversation is a tiny subset
We need to move into the larger tent
Networking today deals with broadcast media radios PONs etc
by pretending its a pointto point wire
Thats massively inefficient
Terabitspersecond of data are being exchanged all over the World via thumb drives or smart phones but we dont know how to treat that as networking
ISPs are busily setting up caches and CDNs to scalably distribute video and audio
Caching is a necessary part of the solution but theres no part of todays networkingfrom Information Queuing or Traffic Theory down to the Internet protocol specsthat tells us how to engineer and deploy it
I think and hope that over the next few years networking will evolve to embrace the much larger vision of communication that underlies the Web
What people inspired you professionally When I was in grad school Richard Feynman visited and gave a colloquium
He talked about a piece of Quantum theory that Id been struggling with all semester and his explanation was so simple and lucid that what had been incomprehensible gibberish to me became obvious and inevitable
That ability to see and convey the simplicity that underlies our complex world seems to me a rare and wonderful gift
What are your recommendations for students who want careers in computer science and networking Its a wonderful fieldcomputers and networking have probably had more impact on society than any invention since the book
Networking is fundamentally about connecting stuff and studying it helps you make intellectual connections Ant foraging Bee dances demonstrate protocol design better than RFCs traffic jams or people leaving a packed stadium are the essence of congestion and students finding flights back to school in a postThanksgiving blizzard are the core of dynamic routing
If youre interested in lots of stuff and want to have an impact its hard to imagine a better field
Chapter The Network Layer Data Plane We learned in the previous chapter that the transport layer provides various forms of processtoprocess communication by relying on the network layers hosttohost communication service
We also learned that the transport layer does so without any knowledge about how the network layer actually implements this service
So perhaps youre now wondering whats under the hood of the hosttohost communication service what makes it tick In this chapter and the next well learn exactly how the network layer can provide its hosttohost communication service
Well see that unlike the transport and application layers there is a piece of the network layer in each and every host and router in the network
Because of this networklayer protocols are among the most challenging and therefore among the most interesting in the protocol stack
Since the network layer is arguably the most complex layer in the protocol stack well have a lot of ground to cover here
Indeed there is so much to cover that we cover the network layer in two chapters
Well see that the network layer can be decomposed into two interacting parts the data plane and the control plane
In Chapter well first cover the data plane functions of the network layerthe per router functions in the network layer that determine how a datagram that is a networklayer packet arriving on one of a routers input links is forwarded to one of that routers output links
Well cover both traditional IP forwarding where forwarding is based on a datagrams destination address and generalized forwarding where forwarding and other functions may be performed using values in several different fields in the datagrams header
Well study the IPv and IPv protocols and addressing in detail
In Chapter well cover the control plane functions of the network layerthe networkwide logic that controls how a datagram is routed among routers along an endtoend path from the source host to the destination host
Well cover routing algorithms as well as routing protocols such as OSPF and BGP that are in widespread use in todays Internet
Traditionally these controlplane routing protocols and dataplane forwarding functions have been implemented together monolithically within a router
Softwaredefined networking SDN explicitly separates the data plane and control plane by implementing these control plane functions as a separate service typically in a remote controller
Well also cover SDN controllers in Chapter 
This distinction between dataplane and controlplane functions in the network layer is an important concept to keep in mind as you learn about the network layer it will help structure your thinking about the network layer and reflects a modern view of the network layers role in computer networking
Overview of Network Layer Figure 
shows a simple network with two hosts H and H and several routers on the path between H and H
Lets suppose that H is sending information to H and consider the role of the network layer in these hosts and in the intervening routers
The network layer in H takes segments from the transport layer in H encapsulates each segment into a datagram and then sends the datagrams to its nearby router R
At the receiving host H the network layer receives the datagrams from its nearby router R extracts the transportlayer segments and delivers the segments up to the transport layer at H
The primary dataplane role of each router is to forward datagrams from its input links to its output links the primary role of the network control plane is to coordinate these local perrouter forwarding actions so that datagrams are ultimately transferred endtoend along paths of routers between source and destination hosts
Note that the routers in Figure 
are shown with a truncated protocol stack that is with no upper layers above the network layer because routers do not run application and transport layer protocols such as those we examined in Chapters and 
Forwarding and Routing The Data and Control Planes The primary role of the network layer is deceptively simpleto move packets from a sending host to a receiving host
To do so two important networklayer functions can be identified Forwarding
When a packet arrives at a routers input link the router must move the packet to the appropriate output link
For example a packet arriving from Host H to Router R in Figure 
must be forwarded to the next router on a path to H
As we will see forwarding is but one function albeit the most Figure 
The network layer common and important one implemented in the data plane
In the more general case which well cover in Section 
a packet might also be blocked from exiting a router e.g
if the packet originated at a known malicious sending host or if the packet were destined to a forbidden destination host or might be duplicated and sent over multiple outgoing links
The network layer must determine the route or path taken by packets as they flow from a sender to a receiver
The algorithms that calculate these paths are referred to as routing algorithms
A routing algorithm would determine for example the path along which packets flow from H to H in Figure 
Routing is implemented in the control plane of the network layer
The terms forwarding and routing are often used interchangeably by authors discussing the network layer
Well use these terms much more precisely in this book
Forwarding refers to the routerlocal action of transferring a packet from an input link interface to the appropriate output link interface
Forwarding takes place at very short timescales typically a few nanoseconds and thus is typically implemented in hardware
Routing refers to the networkwide process that determines the endtoend paths that packets take from source to destination
Routing takes place on much longer timescales typically seconds and as we will see is often implemented in software
Using our driving analogy consider the trip from Pennsylvania to Florida undertaken by our traveler back in Section 
During this trip our driver passes through many interchanges en route to Florida
We can think of forwarding as the process of getting through a single interchange A car enters the interchange from one road and determines which road it should take to leave the interchange
We can think of routing as the process of planning the trip from Pennsylvania to Florida Before embarking on the trip the driver has consulted a map and chosen one of many paths possible with each path consisting of a series of road segments connected at interchanges
A key element in every network router is its forwarding table
A router forwards a packet by examining the value of one or more fields in the arriving packets header and then using these header values to index into its forwarding table
The value stored in the forwarding table entry for those values indicates the outgoing link interface at that router to which that packet is to be forwarded
For example in Figure 
a packet with header field value of arrives to a router
The router indexes into its forwarding table and determines that the output link interface for this packet is interface 
The router then internally forwards the packet to interface 
In Section 
well look inside a router and examine the forwarding function in much greater detail
Forwarding is the key function performed by the dataplane functionality of the network layer
Control Plane The Traditional Approach But now you are undoubtedly wondering how a routers forwarding tables are configured in the first place
This is a crucial issue one that exposes the important interplay between forwarding in data plane and routing in control plane
As shown Figure 
Routing algorithms determine values in forward tables in Figure 
the routing algorithm determines the contents of the routers forwarding tables
In this example a routing algorithm runs in each and every router and both forwarding and routing functions are contained within a router
As well see in Sections 
the routing algorithm function in one router communicates with the routing algorithm function in other routers to compute the values for its forwarding table
How is this communication performed By exchanging routing messages containing routing information according to a routing protocol Well cover routing algorithms and protocols in Sections 
The distinct and different purposes of the forwarding and routing functions can be further illustrated by considering the hypothetical and unrealistic but technically feasible case of a network in which all forwarding tables are configured directly by human network operators physically present at the routers
In this case no routing protocols would be required Of course the human operators would need to interact with each other to ensure that the forwarding tables were configured in such a way that packets reached their intended destinations
Its also likely that human configuration would be more errorprone and much slower to respond to changes in the network topology than a routing protocol
Were thus fortunate that all networks have both a forwarding and a routing function Control Plane The SDN Approach The approach to implementing routing functionality shown in Figure .with each router having a routing component that communicates with the routing component of other routershas been the traditional approach adopted by routing vendors in their products at least until recently
Our observation that humans could manually configure forwarding tables does suggest however that there may be other ways for controlplane functionality to determine the contents of the dataplane forwarding tables
shows an alternate approach in which a physically separate from the routers remote controller computes and distributes the forwarding tables to be used by each and every router
Note that the data plane components of Figures 
In Figure 
however controlplane routing functionality is separated Figure 
A remote controller determines and distributes values in forwarding tables from the physical routerthe routing device performs forwarding only while the remote controller computes and distributes forwarding tables
The remote controller might be implemented in a remote data center with high reliability and redundancy and might be managed by the ISP or some third party
How might the routers and the remote controller communicate By exchanging messages containing forwarding tables and other pieces of routing information
The controlplane approach shown in Figure 
is at the heart of softwaredefined networking SDN where the network is softwaredefined because the controller that computes forwarding tables and interacts with routers is implemented in software
Increasingly these software implementations are also open i.e
similar to Linux OS code the code is publically available allowing ISPs and networking researchers and students to innovate and propose changes to the software that controls networklayer functionality
We will cover the SDN control plane in Section 
Network Service Model Before delving into the network layers data plane lets wrap up our introduction by taking the broader view and consider the different types of service that might be offered by the network layer
When the transport layer at a sending host transmits a packet into the network that is passes it down to the network layer at the sending host can the transport layer rely on the network layer to deliver the packet to the destination When multiple packets are sent will they be delivered to the transport layer in the receiving host in the order in which they were sent Will the amount of time between the sending of two sequential packet transmissions be the same as the amount of time between their reception Will the network provide any feedback about congestion in the network The answers to these questions and others are determined by the service model provided by the network layer
The network service model defines the characteristics of endtoend delivery of packets between sending and receiving hosts
Lets now consider some possible services that the network layer could provide
These services could include Guaranteed delivery
This service guarantees that a packet sent by a source host will eventually arrive at the destination host
Guaranteed delivery with bounded delay
This service not only guarantees delivery of the packet but delivery within a specified hosttohost delay bound for example within msec
Inorder packet delivery
This service guarantees that packets arrive at the destination in the order that they were sent
Guaranteed minimal bandwidth
This networklayer service emulates the behavior of a transmission link of a specified bit rate for example Mbps between sending and receiving hosts
As long as the sending host transmits bits as part of packets at a rate below the specified bit rate then all packets are eventually delivered to the destination host
The network layer could encrypt all datagrams at the source and decrypt them at the destination thereby providing confidentiality to all transportlayer segments
This is only a partial list of services that a network layer could providethere are countless variations possible
The Internets network layer provides a single service known as besteffort service
With besteffort service packets are neither guaranteed to be received in the order in which they were sent nor is their eventual delivery even guaranteed
There is no guarantee on the endtoend delay nor is there a minimal bandwidth guarantee
It might appear that besteffort service is a euphemism for no service at alla network that delivered no packets to the destination would satisfy the definition of besteffort delivery service Other network architectures have defined and implemented service models that go beyond the Internets besteffort service
For example the ATM network architecture MFA Forum Black provides for guaranteed inorder delay bounded delay and guaranteed minimal bandwidth
There have also been proposed service model extensions to the Internet architecture for example the Intserv architecture RFC aims to provide endend delay guarantees and congestionfree communication
Interestingly in spite of these welldeveloped alternatives the Internets basic besteffort service model combined with adequate bandwidth provisioning have arguably proven to be more than good enough to enable an amazing range of applications including streaming video services such as Netflix and voiceandvideooverIP realtime conferencing applications such as Skype and Facetime
An Overview of Chapter Having now provided an overview of the network layer well cover the dataplane component of the network layer in the following sections in this chapter
well dive down into the internal hardware operations of a router including input and output packet processing the routers internal switching mechanism and packet queueing and scheduling
well take a look at traditional IP forwarding in which packets are forwarded to output ports based on their destination IP addresses
Well encounter IP addressing the celebrated IPv and IPv protocols and more
well cover more generalized forwarding where packets may be forwarded to output ports based on a large number of header values i.e
not only based on destination IP address
Packets may be blocked or duplicated at the router or may have certain header field values rewrittenall under software control
This more generalized form of packet forwarding is a key component of a modern network data plane including the data plane in softwaredefined networks SDN
We mention here in passing that the terms forwarding and switching are often used interchangeably by computernetworking researchers and practitioners well use both terms interchangeably in this textbook as well
While were on the topic of terminology its also worth mentioning two other terms that are often used interchangeably but that we will use more carefully
Well reserve the term packet switch to mean a general packetswitching device that transfers a packet from input link interface to output link interface according to values in a packets header fields
Some packet switches called linklayer switches examined in Chapter base their forwarding decision on values in the fields of the link layer frame switches are thus referred to as linklayer layer devices
Other packet switches called routers base their forwarding decision on header field values in the networklayer datagram
Routers are thus networklayer layer devices
To fully appreciate this important distinction you might want to review Section 
where we discuss networklayer datagrams and linklayer frames and their relationship
Since our focus in this chapter is on the network layer well mostly use the term router in place of packet switch
Whats Inside a Router Now that weve overviewed the data and control planes within the network layer the important distinction between forwarding and routing and the services and functions of the network layer lets turn our attention to its forwarding functionthe actual transfer of packets from a routers incoming links to the appropriate outgoing links at that router
A highlevel view of a generic router architecture is shown in Figure 
Four router components can be identified Figure 
Router architecture Input ports
An input port performs several key functions
It performs the physical layer function of terminating an incoming physical link at a router this is shown in the leftmost box of an input port and the rightmost box of an output port in Figure 
An input port also performs linklayer functions needed to interoperate with the link layer at the other side of the incoming link this is represented by the middle boxes in the input and output ports
Perhaps most crucially a lookup function is also performed at the input port this will occur in the rightmost box of the input port
It is here that the forwarding table is consulted to determine the router output port to which an arriving packet will be forwarded via the switching fabric
Control packets for example packets carrying routing protocol information are forwarded from an input port to the routing processor
Note that the term port here referring to the physical input and output router interfacesis distinctly different from the software ports associated with network applications and sockets discussed in Chapters and 
In practice the number of ports supported by a router can range from a relatively small number in enterprise routers to hundreds of Gbps ports in a router at an ISPs edge where the number of incoming lines tends to be the greatest
The Juniper MX edge router for example supports up to Gbps Ethernet ports with an overall router system capacity of Tbps Juniper MX 
The switching fabric connects the routers input ports to its output ports
This switching fabric is completely contained within the routera network inside of a network router Output ports
An output port stores packets received from the switching fabric and transmits these packets on the outgoing link by performing the necessary linklayer and physicallayer functions
When a link is bidirectional that is carries traffic in both directions an output port will typically be paired with the input port for that link on the same line card
The routing processor performs controlplane functions
In traditional routers it executes the routing protocols which well study in Sections 
maintains routing tables and attached link state information and computes the forwarding table for the router
In SDN routers the routing processor is responsible for communicating with the remote controller in order to among other activities receive forwarding table entries computed by the remote controller and install these entries in the routers input ports
The routing processor also performs the network management functions that well study in Section 
A routers input ports output ports and switching fabric are almost always implemented in hardware as shown in Figure 
To appreciate why a hardware implementation is needed consider that with a Gbps input link and a byte IP datagram the input port has only 
ns to process the datagram before another datagram may arrive
If N ports are combined on a line card as is often done in practice the datagramprocessing pipeline must operate N times fasterfar too fast for software implementation
Forwarding hardware can be implemented either using a router vendors own hardware designs or constructed using purchased merchantsilicon chips e.g
as sold by companies such as Intel and Broadcom
While the data plane operates at the nanosecond time scale a routers control functionsexecuting the routing protocols responding to attached links that go up or down communicating with the remote controller in the SDN case and performing management functionsoperate at the millisecond or second timescale
These control plane functions are thus usually implemented in software and execute on the routing processor typically a traditional CPU
Before delving into the details of router internals lets return to our analogy from the beginning of this chapter where packet forwarding was compared to cars entering and leaving an interchange
Lets suppose that the interchange is a roundabout and that as a car enters the roundabout a bit of processing is required
Lets consider what information is required for this processing Destinationbased forwarding
Suppose the car stops at an entry station and indicates its final destination not at the local roundabout but the ultimate destination of its journey
An attendant at the entry station looks up the final destination determines the roundabout exit that leads to that final destination and tells the driver which roundabout exit to take
The attendant could also determine the cars exit ramp on the basis of many other factors besides the destination
For example the selected exit ramp might depend on the cars origin for example the state that issued the cars license plate
Cars from a certain set of states might be directed to use one exit ramp that leads to the destination via a slow road while cars from other states might be directed to use a different exit ramp that leads to the destination via superhighway
The same decision might be made based on the model make and year of the car
Or a car not deemed roadworthy might be blocked and not be allowed to pass through the roundabout
In the case of generalized forwarding any number of factors may contribute to the attendants choice of the exit ramp for a given car
Once the car enters the roundabout which may be filled with other cars entering from other input roads and heading to other roundabout exits it eventually leaves at the prescribed roundabout exit ramp where it may encounter other cars leaving the roundabout at that exit
We can easily recognize the principal router components in Figure 
in this analogythe entry road and entry station correspond to the input port with a lookup function to determine to local outgoing port the roundabout corresponds to the switch fabric and the roundabout exit road corresponds to the output port
With this analogy its instructive to consider where bottlenecks might occur
What happens if cars arrive blazingly fast for example the roundabout is in Germany or Italy but the station attendant is slow How fast must the attendant work to ensure theres no backup on an entry road Even with a blazingly fast attendant what happens if cars traverse the roundabout slowlycan backups still occur And what happens if most of the cars entering at all of the roundabouts entrance ramps all want to leave the roundabout at the same exit rampcan backups occur at the exit ramp or elsewhere How should the roundabout operate if we want to assign priorities to different cars or block certain cars from entering the roundabout in the first place These are all analogous to critical questions faced by router and switch designers
In the following subsections well look at router functions in more detail
Iyer Chao Chuang Turner McKeown a Partridge Sopranos provide a discussion of specific router architectures
For concreteness and simplicity well initially assume in this section that forwarding decisions are based only on the packets destination address rather than on a generalized set of packet header fields
We will cover the case of more generalized packet forwarding in Section 
Input Port Processing and DestinationBased Forwarding A more detailed view of input processing is shown in Figure 
As just discussed the input ports line termination function and linklayer processing implement the physical and link layers for that individual input link
The lookup performed in the input port is central to the routers operationit is here that the router uses the forwarding table to look up the output port to which an arriving packet will be forwarded via the switching fabric
The forwarding table is either computed and updated by the routing processor using a routing protocol to interact with the routing processors in other network routers or is received from a remote SDN controller
The forwarding table is copied from the routing processor to the line cards over a separate bus e.g
a PCI bus indicated by the dashed line from the routing processor to the input line cards in Figure 
With such a shadow copy at each line card forwarding decisions can be made locally at each input port without invoking the centralized routing processor on a perpacket basis and thus avoiding a centralized processing bottleneck
Lets now consider the simplest case that the output port to which an incoming packet is to be switched is based on the packets destination address
In the case of bit IP addresses a bruteforce implementation of the forwarding table would have one entry for every possible destination address
Since there are more than billion possible addresses this option is totally out of the question
Input port processing As an example of how this issue of scale can be handled lets suppose that our router has four links numbered through and that packets are to be forwarded to the link interfaces as follows Destination Address Range Link Interface through through through Otherwise Clearly for this example it is not necessary to have billion entries in the routers forwarding table
We could for example have the following forwarding table with just four entries Prefix Link Interface Otherwise With this style of forwarding table the router matches a prefix of the packets destination address with the entries in the table if theres a match the router forwards the packet to a link associated with the match
For example suppose the packets destination address is because the bit prefix of this address matches the first entry in the table the router forwards the packet to link interface 
If a prefix doesnt match any of the first three entries then the router forwards the packet to the default interface 
Although this sounds simple enough theres a very important subtlety here
You may have noticed that it is possible for a destination address to match more than one entry
For example the first bits of the address match the second entry in the table and the first bits of the address match the third entry in the table
When there are multiple matches the router uses the longest prefix matching rule that is it finds the longest matching entry in the table and forwards the packet to the link interface associated with the longest prefix match
Well see exactly why this longest prefixmatching rule is used when we study Internet addressing in more detail in Section 
Given the existence of a forwarding table lookup is conceptually simplehardware logic just searches through the forwarding table looking for the longest prefix match
But at Gigabit transmission rates this lookup must be performed in nanoseconds recall our earlier example of a Gbps link and a byte IP datagram
Thus not only must lookup be performed in hardware but techniques beyond a simple linear search through a large table are needed surveys of fast lookup algorithms can be found in Gupta RuizSanchez 
Special attention must also be paid to memory access times resulting in designs with embedded onchip DRAM and faster SRAM used as a DRAM cache memories
In practice Ternary Content Addressable Memories TCAMs are also often used for lookup Yu 
With a TCAM a bit IP address is presented to the memory which returns the content of the forwarding table entry for that address in essentially constant time
The Cisco Catalyst and Series routers and switches can hold upwards of a million TCAM forwarding table entries Cisco TCAM 
Once a packets output port has been determined via the lookup the packet can be sent into the switching fabric
In some designs a packet may be temporarily blocked from entering the switching fabric if packets from other input ports are currently using the fabric
A blocked packet will be queued at the input port and then scheduled to cross the fabric at a later point in time
Well take a closer look at the blocking queuing and scheduling of packets at both input ports and output ports shortly
Although lookup is arguably the most important action in input port processing many other actions must be taken physical and linklayer processing must occur as discussed previously the packets version number checksum and timetolive fieldall of which well study in Section .must be checked and the latter two fields rewritten and counters used for network management such as the number of IP datagrams received must be updated
Lets close our discussion of input port processing by noting that the input port steps of looking up a destination IP address match and then sending the packet into the switching fabric to the specified output port action is a specific case of a more general match plus action abstraction that is performed in many networked devices not just routers
In linklayer switches covered in Chapter linklayer destination addresses are looked up and several actions may be taken in addition to sending the frame into the switching fabric towards the output port
In firewalls covered in Chapter devices that filter out selected incoming packetsan incoming packet whose header matches a given criteria e.g
a combination of sourcedestination IP addresses and transportlayer port numbers may be dropped action
In a network address translator NAT covered in Section 
an incoming packet whose transportlayer port number matches a given value will have its port number rewritten before forwarding action
Indeed the match plus action abstraction is both powerful and prevalent in network devices today and is central to the notion of generalized forwarding that well study in Section 
Switching The switching fabric is at the very heart of a router as it is through this fabric that the packets are actually switched that is forwarded from an input port to an output port
Switching can be accomplished in a number of ways as shown in Figure 
Switching via memory
The simplest earliest routers were traditional computers with switching between input and output ports being done under direct control of the CPU routing processor
Input and output ports functioned as traditional IO devices in a traditional operating system
An input port with an arriving packet first signaled the routing processor via an interrupt
The packet was then copied from the input port into processor memory
The routing processor then extracted the destination address from the header looked up the appropriate output port in the forwarding table and copied the packet to the output ports buffers
In this scenario if the memory bandwidth is such that a maximum of B packets per second can be written into or read from memory then the overall forwarding throughput the total rate at which packets are transferred from input ports to output ports must be less than B
Note also that two packets cannot be forwarded Figure 
Three switching techniques at the same time even if they have different destination ports since only one memory readwrite can be done at a time over the shared system bus
Some modern routers switch via memory
A major difference from early routers however is that the lookup of the destination address and the storing of the packet into the appropriate memory location are performed by processing on the input line cards
In some ways routers that switch via memory look very much like sharedmemory multiprocessors with the processing on a line card switching writing packets into the memory of the appropriate output port
Ciscos Catalyst series switches Cisco internally switches packets via a shared memory
Switching via a bus
In this approach an input port transfers a packet directly to the output port over a shared bus without intervention by the routing processor
This is typically done by having the input port prepend a switchinternal label header to the packet indicating the local output port to which this packet is being transferred and transmitting the packet onto the bus
All output ports receive the packet but only the port that matches the label will keep the packet
The label is then removed at the output port as this label is only used within the switch to cross the bus
If multiple packets arrive to the router at the same time each at a different input port all but one must wait since only one packet can cross the bus at a time
Because every packet must cross the single bus the switching speed of the router is limited to the bus speed in our roundabout analogy this is as if the roundabout could only contain one car at a time
Nonetheless switching via a bus is often sufficient for routers that operate in small local area and enterprise networks
The Cisco router Cisco internally switches packets over a Gbpsbackplane bus
Switching via an interconnection network
One way to overcome the bandwidth limitation of a single shared bus is to use a more sophisticated interconnection network such as those that have been used in the past to interconnect processors in a multiprocessor computer architecture
A crossbar switch is an interconnection network consisting of N buses that connect N input ports to N output ports as shown in Figure 
Each vertical bus intersects each horizontal bus at a crosspoint which can be opened or closed at any time by the switch fabric controller whose logic is part of the switching fabric itself
When a packet arrives from port A and needs to be forwarded to port Y the switch controller closes the crosspoint at the intersection of busses A and Y and port A then sends the packet onto its bus which is picked up only by bus Y
Note that a packet from port B can be forwarded to port X at the same time since the AtoY and BtoX packets use different input and output busses
Thus unlike the previous two switching approaches crossbar switches are capable of forwarding multiple packets in parallel
A crossbar switch is nonblockinga packet being forwarded to an output port will not be blocked from reaching that output port as long as no other packet is currently being forwarded to that output port
However if two packets from two different input ports are destined to that same output port then one will have to wait at the input since only one packet can be sent over any given bus at a time
Cisco series switches Cisco use a crossbar switching network the Cisco series can be configured to use either a bus or crossbar switch Cisco 
More sophisticated interconnection networks use multiple stages of switching elements to allow packets from different input ports to proceed towards the same output port at the same time through the multistage switching fabric
See Tobagi for a survey of switch architectures
The Cisco CRS employs a threestage nonblocking switching strategy
A routers switching capacity can also be scaled by running multiple switching fabrics in parallel
In this approach input ports and output ports are connected to N switching fabrics that operate in parallel
An input port breaks a packet into K smaller chunks and sends sprays the chunks through K of these N switching fabrics to the selected output port which reassembles the K chunks back into the original packet
Output Port Processing Output port processing shown in Figure 
takes packets that have been stored in the output ports memory and transmits them over the output link
This includes selecting and dequeueing packets for transmission and performing the needed linklayer and physicallayer transmission functions
Where Does Queuing Occur If we consider input and output port functionality and the configurations shown in Figure 
its clear that packet queues may form at both the input ports and the output ports just as we identified cases where cars may wait at the inputs and outputs of the traffic intersection in our roundabout analogy
The location and extent of queueing either at the input port queues or the output port queues will depend on the traffic load the relative speed of the switching fabric and the line speed
Lets now consider these queues in a bit more detail since as these queues grow large the routers memory can eventually be exhausted and packet loss will occur when no memory is available to store arriving packets
Recall that in our earlier discussions we said that packets were lost within the network or dropped at a router
It is here at these queues within a router where such packets are actually dropped and lost
Output port processing Suppose that the input and output line speeds transmission rates all have an identical transmission rate of Rline packets per second and that there are N input ports and N output ports
To further simplify the discussion lets assume that all packets have the same fixed length and that packets arrive to input ports in a synchronous manner
That is the time to send a packet on any link is equal to the time to receive a packet on any link and during such an interval of time either zero or one packets can arrive on an input link
Define the switching fabric transfer rate Rswitch as the rate at which packets can be moved from input port to output port
If Rswitch is N times faster than Rline then only negligible queuing will occur at the input ports
This is because even in the worst case where all N input lines are receiving packets and all packets are to be forwarded to the same output port each batch of N packets one packet per input port can be cleared through the switch fabric before the next batch arrives
Input Queueing But what happens if the switch fabric is not fast enough relative to the input line speeds to transfer all arriving packets through the fabric without delay In this case packet queuing can also occur at the input ports as packets must join input port queues to wait their turn to be transferred through the switching fabric to the output port
To illustrate an important consequence of this queuing consider a crossbar switching fabric and suppose that all link speeds are identical that one packet can be transferred from any one input port to a given output port in the same amount of time it takes for a packet to be received on an input link and packets are moved from a given input queue to their desired output queue in an FCFS manner
Multiple packets can be transferred in parallel as long as their output ports are different
However if two packets at the front of two input queues are destined for the same output queue then one of the packets will be blocked and must wait at the input queuethe switching fabric can transfer only one packet to a given output port at a time
shows an example in which two packets darkly shaded at the front of their input queues are destined for the same upperright output port
Suppose that the switch fabric chooses to transfer the packet from the front of the upperleft queue
In this case the darkly shaded packet in the lowerleft queue must wait
But not only must this darkly shaded packet wait so too must the lightly shaded packet that is queued behind that packet in the lowerleft queue even though there is no contention for the middleright output port the destination for the lightly shaded packet
This phenomenon is known as headoftheline HOL blocking in an inputqueued switcha queued packet in an input queue must wait for transfer through the fabric even though its output port is free because it is blocked by another packet at the head of the line
Karol shows that due to HOL blocking the input queue will grow to unbounded length informally this is equivalent to saying that significant packet loss will occur under certain assumptions as soon as the packet arrival rate on the input links reaches only percent of their capacity
A number of solutions to HOL blocking are discussed in McKeown 
HOL blocking at and inputqueued switch Output Queueing Lets next consider whether queueing can occur at a switchs output ports
Suppose that Rswitch is again N times faster than Rline and that packets arriving at each of the N input ports are destined to the same output port
In this case in the time it takes to send a single packet onto the outgoing link N new packets will arrive at this output port one from each of the N input ports
Since the output port can transmit only a single packet in a unit of time the packet transmission time the N arriving packets will have to queue wait for transmission over the outgoing link
Then N more packets can possibly arrive in the time it takes to transmit just one of the N packets that had just previously been queued
And so on
Thus packet queues can form at the output ports even when the switching fabric is N times faster than the port line speeds
Eventually the number of queued packets can grow large enough to exhaust available memory at the output port
Output port queueing When there is not enough memory to buffer an incoming packet a decision must be made to either drop the arriving packet a policy known as droptail or remove one or more alreadyqueued packets to make room for the newly arrived packet
In some cases it may be advantageous to drop or mark the header of a packet before the buffer is full in order to provide a congestion signal to the sender
A number of proactive packetdropping and marking policies which collectively have become known as active queue management AQM algorithms have been proposed and analyzed Labrador Hollot 
One of the most widely studied and implemented AQM algorithms is the Random Early Detection RED algorithm Christiansen Floyd 
Output port queuing is illustrated in Figure 
At time t a packet has arrived at each of the incoming input ports each destined for the uppermost outgoing port
Assuming identical line speeds and a switch operating at three times the line speed one time unit later that is in the time needed to receive or send a packet all three original packets have been transferred to the outgoing port and are queued awaiting transmission
In the next time unit one of these three packets will have been transmitted over the outgoing link
In our example two new packets have arrived at the incoming side of the switch one of these packets is destined for this uppermost output port
A consequence of such queuing is that a packet scheduler at the output port must choose one packet among those queued for transmission a topic well cover in the following section
Given that router buffers are needed to absorb the fluctuations in traffic load a natural question to ask is how much buffering is required
For many years the rule of thumb RFC for buffer sizing was that the amount of buffering B should be equal to an average roundtrip time RTT say msec times the link capacity C
This result is based on an analysis of the queueing dynamics of a relatively small number of TCP flows Villamizar 
Thus a Gbps link with an RTT of msec would need an amount of buffering equal to B RTT C 
Gbits of buffers
More recent theoretical and experimental efforts Appenzeller however suggest that when there are a large number of TCP flows N passing through a link the amount of buffering needed is BRTICN
With a large number of flows typically passing through large backbone router links see e.g
Fraleigh the value of N can be large with the decrease in needed buffer size becoming quite significant
Appenzeller Wischik Beheshti provide very readable discussions of the buffersizing problem from a theoretical implementation and operational standpoint
Packet Scheduling Lets now return to the question of determining the order in which queued packets are transmitted over an outgoing link
Since you yourself have undoubtedly had to wait in long lines on many occasions and observed how waiting customers are served youre no doubt familiar with many of the queueing disciplines commonly used in routers
There is firstcomefirstserved FCFS also known as firstinfirst out FIFO
The British are famous for patient and orderly FCFS queueing at bus stops and in the marketplace Oh are you queueing
Other countries operate on a priority basis with one class of waiting customers given priority service over other waiting customers
There is also roundrobin queueing where customers are again divided into classes as in priority queueing but each class of customer is given service in turn
FirstinFirstOut FIFO Figure 
shows the queuing model abstraction for the FIFO linkscheduling discipline
Packets arriving at the link output queue wait for transmission if the link is currently busy transmitting another packet
If there is not sufficient buffering space to hold the arriving packet the queues packet discarding policy then determines whether the packet will be dropped lost or whether other packets will be removed from the queue to make space for the arriving packet as discussed above
In our discussion below well ignore packet discard
When a packet is completely transmitted over the outgoing link that is receives service it is removed from the queue
The FIFO also known as firstcomefirstserved or FCFS scheduling discipline selects packets for link transmission in the same order in which they arrived at the output link queue
Were all familiar with FIFO queuing from service centers where Figure 
FIFO queueing abstraction arriving customers join the back of the single waiting line remain in order and are then served when they reach the front of the line
shows the FIFO queue in operation
Packet arrivals are indicated by numbered arrows above the upper timeline with the number indicating the order in which the packet arrived
Individual packet departures are shown below the lower timeline
The time that a packet spends in service being transmitted is indicated by the shaded rectangle between the two timelines
In our examples here lets assume that each packet takes three units of time to be transmitted
Under the FIFO discipline packets leave in the same order in which they arrived
Note that after the departure of packet the link remains idle since packets through have been transmitted and removed from the queue until the arrival of packet 
Priority Queuing Under priority queuing packets arriving at the output link are classified into priority classes upon arrival at the queue as shown in Figure 
In practice a network operator may configure a queue so that packets carrying network management information e.g
as indicated by the source or destination TCPUDP port number receive priority over user traffic additionally realtime voiceoverIP packets might receive priority over nonreal traffic such as SMTP or IMAP email packets
Each Figure 
The FIFO queue in operation Figure 
The priority queueing model priority class typically has its own queue
When choosing a packet to transmit the priority queuing discipline will transmit a packet from the highest priority class that has a nonempty queue that is has packets waiting for transmission
The choice among packets in the same priority class is typically done in a FIFO manner
illustrates the operation of a priority queue with two priority classes
Packets and belong to the highpriority class and packets and belong to the lowpriority class
Packet arrives and finding the link idle begins transmission
During the transmission of packet packets and arrive and are queued in the low and highpriority queues respectively
After the transmission of packet packet a highpriority packet is selected for transmission over packet which even though it arrived earlier is a lowpriority packet
At the end of the transmission of packet packet then begins transmission
Packet a highpriority packet arrives during the transmission of packet a lowpriority packet
Under a nonpreemptive priority queuing discipline the transmission of a packet is not interrupted once it has Figure 
The priority queue in operation Figure 
The twoclass robin queue in operation begun
In this case packet queues for transmission and begins being transmitted after the transmission of packet is completed
Round Robin and Weighted Fair Queuing WFQ Under the round robin queuing discipline packets are sorted into classes as with priority queuing
However rather than there being a strict service priority among classes a round robin scheduler alternates service among the classes
In the simplest form of round robin scheduling a class packet is transmitted followed by a class packet followed by a class packet followed by a class packet and so on
A socalled workconserving queuing discipline will never allow the link to remain idle whenever there are packets of any class queued for transmission
A workconserving round robin discipline that looks for a packet of a given class but finds none will immediately check the next class in the round robin sequence
illustrates the operation of a twoclass round robin queue
In this example packets and belong to class and packets and belong to the second class
Packet begins transmission immediately upon arrival at the output queue
Packets and arrive during the transmission of packet and thus queue for transmission
After the transmission of packet the link scheduler looks for a class packet and thus transmits packet 
After the transmission of packet the scheduler looks for a class packet and thus transmits packet 
After the transmission of packet packet is the only queued packet it is thus transmitted immediately after packet 
A generalized form of round robin queuing that has been widely implemented in routers is the socalled weighted fair queuing WFQ discipline Demers Parekh Cisco QoS 
WFQ is illustrated in Figure 
Here arriving packets are classified and queued in the appropriate perclass waiting area
As in round robin scheduling a WFQ scheduler will serve classes in a circular manner first serving class then serving class then serving class and then assuming there are three classes repeating the service pattern
WFQ is also a workconserving Figure 
Weighted fair queueing queuing discipline and thus will immediately move on to the next class in the service sequence when it finds an empty class queue
WFQ differs from round robin in that each class may receive a differential amount of service in any interval of time
Specifically each class i is assigned a weight wi
Under WFQ during any interval of time during which there are class i packets to send class i will then be guaranteed to receive a fraction of service equal to wiwj where the sum in the denominator is taken over all classes that also have packets queued for transmission
In the worst case even if all classes have queued packets class i will still be guaranteed to receive a fraction wiwj of the bandwidth where in this worst case the sum in the denominator is over all classes
Thus for a link with transmission rate R class i will always achieve a throughput of at least Rwiwj
Our description of WFQ has been idealized as we have not considered the fact that packets are discrete and a packets transmission will not be interrupted to begin transmission of another packet Demers Parekh discuss this packetization issue
The Internet Protocol IP IPv Addressing IPv and More Our study of the network layer thus far in Chapter the notion of the data and control plane component of the network layer our distinction between forwarding and routing the identification of various network service models and our look inside a routerhave often been without reference to any specific computer network architecture or protocol
In this section well focus on key aspects of the network layer on todays Internet and the celebrated Internet Protocol IP
There are two versions of IP in use today
Well first examine the widely deployed IP protocol version which is usually referred to simply as IPv RFC Figure 
IPv datagram format in Section 
Well examine IP version RFC RFC which has been proposed to replace IPv in Section 
In between well primarily cover Internet addressinga topic that might seem rather dry and detailoriented but well see is crucial to understanding how the Internets network layer works
To master IP addressing is to master the Internets network layer itself 
IPv Datagram Format Recall that the Internets networklayer packet is referred to as a datagram
We begin our study of IP with an overview of the syntax and semantics of the IPv datagram
You might be thinking that nothing could be drier than the syntax and semantics of a packets bits
Nevertheless the datagram plays a central role in the Internetevery networking student and professional needs to see it absorb it and master it
And just to see that protocol headers can indeed be fun to study check out Pomeranz 
The IPv datagram format is shown in Figure 
The key fields in the IPv datagram are the following Version number
These bits specify the IP protocol version of the datagram
By looking at the version number the router can determine how to interpret the remainder of the IP datagram
Different versions of IP use different datagram formats
The datagram format for IPv is shown in Figure 
The datagram format for the new version of IP IPv is discussed in Section 
Because an IPv datagram can contain a variable number of options which are included in the IPv datagram header these bits are needed to determine where in the IP datagram the payload e.g
the transportlayer segment being encapsulated in this datagram actually begins
Most IP datagrams do not contain options so the typical IP datagram has a byte header
Type of service
The type of service TOS bits were included in the IPv header to allow different types of IP datagrams to be distinguished from each other
For example it might be useful to distinguish realtime datagrams such as those used by an IP telephony application from nonreal time traffic for example FTP
The specific level of service to be provided is a policy issue determined and configured by the network administrator for that router
We also learned in Section 
that two of the TOS bits are used for Explicit Congestion Notification
This is the total length of the IP datagram header plus data measured in bytes
Since this field is bits long the theoretical maximum size of the IP datagram is bytes
However datagrams are rarely larger than bytes which allows an IP datagram to fit in the payload field of a maximally sized Ethernet frame
Identifier flags fragmentation offset
These three fields have to do with socalled IP fragmentation a topic we will consider shortly
Interestingly the new version of IP IPv does not allow for fragmentation
The timetolive TTL field is included to ensure that datagrams do not circulate forever due to for example a longlived routing loop in the network
This field is decremented by one each time the datagram is processed by a router
If the TTL field reaches a router must drop that datagram
This field is typically used only when an IP datagram reaches its final destination
The value of this field indicates the specific transportlayer protocol to which the data portion of this IP datagram should be passed
For example a value of indicates that the data portion is passed to TCP while a value of indicates that the data is passed to UDP
For a list of all possible values see IANA Protocol Numbers 
Note that the protocol number in the IP datagram has a role that is analogous to the role of the port number field in the transportlayer segment
The protocol number is the glue that binds the network and transport layers together whereas the port number is the glue that binds the transport and application layers together
Well see in Chapter that the link layer frame also has a special field that binds the link layer to the network layer
The header checksum aids a router in detecting bit errors in a received IP datagram
The header checksum is computed by treating each bytes in the header as a number and summing these numbers using s complement arithmetic
As discussed in Section 
the s complement of this sum known as the Internet checksum is stored in the checksum field
A router computes the header checksum for each received IP datagram and detects an error condition if the checksum carried in the datagram header does not equal the computed checksum
Routers typically discard datagrams for which an error has been detected
Note that the checksum must be recomputed and stored again at each router since the TTL field and possibly the options field as well will change
An interesting discussion of fast algorithms for computing the Internet checksum is RFC 
A question often asked at this point is why does TCPIP perform error checking at both the transport and network layers There are several reasons for this repetition
First note that only the IP header is checksummed at the IP layer while the TCPUDP checksum is computed over the entire TCPUDP segment
Second TCPUDP and IP do not necessarily both have to belong to the same protocol stack
TCP can in principle run over a different networklayer protocol for example ATM Black and IP can carry data that will not be passed to TCPUDP
Source and destination IP addresses
When a source creates a datagram it inserts its IP address into the source IP address field and inserts the address of the ultimate destination into the destination IP address field
Often the source host determines the destination address via a DNS lookup as discussed in Chapter 
Well discuss IP addressing in detail in Section 
The options fields allow an IP header to be extended
Header options were meant to be used rarelyhence the decision to save overhead by not including the information in options fields in every datagram header
However the mere existence of options does complicate matterssince datagram headers can be of variable length one cannot determine a priori where the data field will start
Also since some datagrams may require options processing and others may not the amount of time needed to process an IP datagram at a router can vary greatly
These considerations become particularly important for IP processing in highperformance routers and hosts
For these reasons and others IP options were not included in the IPv header as discussed in Section 
Finally we come to the last and most important fieldthe raison detre for the datagram in the first place In most circumstances the data field of the IP datagram contains the transportlayer segment TCP or UDP to be delivered to the destination
However the data field can carry other types of data such as ICMP messages discussed in Section 
Note that an IP datagram has a total of bytes of header assuming no options
If the datagram carries a TCP segment then each nonfragmented datagram carries a total of bytes of header bytes of IP header plus bytes of TCP header along with the applicationlayer message
IPv Datagram Fragmentation Well see in Chapter that not all linklayer protocols can carry networklayer packets of the same size
Some protocols can carry big datagrams whereas other protocols can carry only little datagrams
For example Ethernet frames can carry up to bytes of data whereas frames for some widearea links can carry no more than bytes
The maximum amount of data that a linklayer frame can carry is called the maximum transmission unit MTU
Because each IP datagram is encapsulated within the linklayer frame for transport from one router to the next router the MTU of the linklayer protocol places a hard limit on the length of an IP datagram
Having a hard limit on the size of an IP datagram is not much of a problem
What is a problem is that each of the links along the route between sender and destination can use different linklayer protocols and each of these protocols can have different MTUs
To understand the forwarding issue better imagine that you are a router that interconnects several links each running different linklayer protocols with different MTUs
Suppose you receive an IP datagram from one link
You check your forwarding table to determine the outgoing link and this outgoing link has an MTU that is smaller than the length of the IP datagram
Time to panichow are you going to squeeze this oversized IP datagram into the payload field of the linklayer frame The solution is to fragment the payload in the IP datagram into two or more smaller IP datagrams encapsulate each of these smaller IP datagrams in a separate linklayer frame and send these frames over the outgoing link
Each of these smaller datagrams is referred to as a fragment
Fragments need to be reassembled before they reach the transport layer at the destination
Indeed both TCP and UDP are expecting to receive complete unfragmented segments from the network layer
The designers of IPv felt that reassembling datagrams in the routers would introduce significant complication into the protocol and put a damper on router performance
If you were a router would you want to be reassembling fragments on top of everything else you had to do Sticking to the principle of keeping the network core simple the designers of IPv decided to put the job of datagram reassembly in the end systems rather than in network routers
When a destination host receives a series of datagrams from the same source it needs to determine whether any of these datagrams are fragments of some original larger datagram
If some datagrams are fragments it must further determine when it has received the last fragment and how the fragments it has received should be pieced back together to form the original datagram
To allow the destination host to perform these reassembly tasks the designers of IP version put identification flag and fragmentation offset fields in the IP datagram header
When a datagram is created the sending host stamps the datagram with an identification number as well as source and destination addresses
Typically the sending host increments the identification number for each datagram it sends
When a router needs to fragment a datagram each resulting datagram that is fragment is stamped with the source address destination address and identification number of the original datagram
When the destination receives a series of datagrams from the same sending host it can examine the identification numbers of the datagrams to determine which of the datagrams are actually fragments of the same larger datagram
Because IP is an unreliable service one or more of the fragments may never arrive at the destination
For this reason in order for the destination host to be absolutely sure it has received the last fragment of Figure 
IP fragmentation and reassembly the original datagram the last fragment has a flag bit set to whereas all the other fragments have this flag bit set to 
Also in order for the destination host to determine whether a fragment is missing and also to be able to reassemble the fragments in their proper order the offset field is used to specify where the fragment fits within the original IP datagram
illustrates an example
A datagram of bytes bytes of IP header plus bytes of IP payload arrives at a router and must be forwarded to a link with an MTU of bytes
This implies that the data bytes in the original datagram must be allocated to three separate fragments each of which is also an IP datagram
The online material for this book and the problems at the end of this chapter will allow you to explore fragmentation in more detail
Also on this books Web site we provide a Java applet that generates fragments
You provide the incoming datagram size the MTU and the incoming datagram identification
The applet automatically generates the fragments for you
See httpwww.pearsonhighered.comcs resources
IPv Addressing We now turn our attention to IPv addressing
Although you may be thinking that addressing must be a straightforward topic hopefully by the end of this section youll be convinced that Internet addressing is not only a juicy subtle and interesting topic but also one that is of central importance to the Internet
An excellent treatment of IPv addressing can be found in the first chapter in Stewart 
Before discussing IP addressing however well need to say a few words about how hosts and routers are connected into the Internet
A host typically has only a single link into the network when IP in the host wants to send a datagram it does so over this link
The boundary between the host and the physical link is called an interface
Now consider a router and its interfaces
Because a routers job is to receive a datagram on one link and forward the datagram on some other link a router necessarily has two or more links to which it is connected
The boundary between the router and any one of its links is also called an interface
A router thus has multiple interfaces one for each of its links
Because every host and router is capable of sending and receiving IP datagrams IP requires each host and router interface to have its own IP address
Thus an IP address is technically associated with an interface rather than with the host or router containing that interface
Each IP address is bits long equivalently bytes and there are thus a total of or approximately billion possible IP addresses
These addresses are typically written in socalled dotteddecimal notation in which each byte of the address is written in its decimal form and is separated by a period dot from other bytes in the address
For example consider the IP address 
The is the decimal equivalent of the first bits of the address the is the decimal equivalent of the second bits of the address and so on
Thus the address 
in binary notation is Each interface on every host and router in the global Internet must have an IP address that is globally unique except for interfaces behind NATs as discussed in Section 
These addresses cannot be chosen in a willynilly manner however
A portion of an interfaces IP address will be determined by the subnet to which it is connected
provides an example of IP addressing and interfaces
In this figure one router with three interfaces is used to interconnect seven hosts
Take a close look at the IP addresses assigned to the host and router interfaces as there are several things to notice
The three hosts in the upperleft portion of Figure 
and the router interface to which they are connected all have an IP address of the form ...xxx
That is they all have the same leftmost bits in their IP address
These four interfaces are also interconnected to each other by a network that contains no routers
This network could be interconnected by an Ethernet LAN in which case the interfaces would be interconnected by an Ethernet switch as well discuss in Chapter or by a wireless access point as well discuss in Chapter 
Well represent this routerless network connecting these hosts as a cloud for now and dive into the internals of such networks in Chapters and 
In IP terms this network interconnecting three host interfaces and one router interface forms a subnet RFC 
A subnet is also called an IP network or simply Figure 
Interface addresses and subnets a network in the Internet literature
IP addressing assigns an address to this subnet 
where the slash notation sometimes known as a subnet mask indicates that the leftmost bits of the bit quantity define the subnet address
subnet thus consists of the three host interfaces 
and one router interface 
Any additional hosts attached to the 
subnet would be required to have an address of the form ...xxx
There are two additional subnets shown in Figure 
network and the 
illustrates the three IP subnets present in Figure 
The IP definition of a subnet is not restricted to Ethernet segments that connect multiple hosts to a router interface
To get some insight here consider Figure 
which shows three routers that are interconnected with each other by pointtopoint links
Each router has three interfaces one for each pointtopoint link and one for the broadcast link that directly connects the router to a pair of hosts
What subnets are present here Three subnets 
are similar to the subnets we encountered in Figure 
But note that there are three additional subnets in this example as well one subnet 
for the interfaces that connect routers R and R another subnet 
for the interfaces that connect routers R and R and a third subnet 
for the interfaces that connect routers R and R
For a general interconnected system of routers and hosts we can use the following recipe to define the subnets in the system Figure 
Subnet addresses To determine the subnets detach each interface from its host or router creating islands of isolated networks with interfaces terminating the end points of the isolated networks
Each of these isolated networks is called a subnet
If we apply this procedure to the interconnected system in Figure 
we get six islands or subnets
From the discussion above its clear that an organization such as a company or academic institution with multiple Ethernet segments and pointtopoint links will have multiple subnets with all of the devices on a given subnet having the same subnet address
In principle the different subnets could have quite different subnet addresses
In practice however their subnet addresses often have much in common
To understand why lets next turn our attention to how addressing is handled in the global Internet
The Internets address assignment strategy is known as Classless Interdomain Routing CIDR pronounced cider RFC 
CIDR generalizes the notion of subnet addressing
As with subnet addressing the bit IP address is divided into two parts and again has the dotteddecimal form a.b.c.dx where x indicates the number of bits in the first part of the address
The x most significant bits of an address of the form a.b.c.dx constitute the network portion of the IP address and are often referred to as the prefix or network prefix of the address
An organization is typically assigned a block of contiguous addresses that is a range of addresses with a common prefix see the Principles in Practice feature
In this case the IP addresses of devices within the organization will share the common prefix
When we cover the Internets BGP routing protocol in Figure 
Three routers interconnecting six subnets Section 
well see that only these x leading prefix bits are considered by routers outside the organizations network
That is when a router outside the organization forwards a datagram whose destination address is inside the organization only the leading x bits of the address need be considered
This considerably reduces the size of the forwarding table in these routers since a single entry of the form a.b.c.dx will be sufficient to forward packets to any destination within the organization
The remaining x bits of an address can be thought of as distinguishing among the devices within the organization all of which have the same network prefix
These are the bits that will be considered when forwarding packets at routers within the organization
These lowerorder bits may or may not have an additional subnetting structure such as that discussed above
For example suppose the first bits of the CIDRized address a.b.c.d specify the organizations network prefix and are common to the IP addresses of all devices in that organization
The remaining bits then identify the specific hosts in the organization
The organizations internal structure might be such that these rightmost bits are used for subnetting within the organization as discussed above
For example a.b.c.d might refer to a specific subnet within the organization
Before CIDR was adopted the network portions of an IP address were constrained to be or bits in length an addressing scheme known as classful addressing since subnets with and bit subnet addresses were known as class A B and C networks respectively
The requirement that the subnet portion of an IP address be exactly or bytes long turned out to be problematic for supporting the rapidly growing number of organizations with small and mediumsized subnets
A class C subnet could accommodate only up to hosts two of the addresses are reserved for special usetoo small for many organizations
However a class B subnet which supports up to hosts was too large
Under classful addressing an organization with say hosts was typically allocated a class B subnet address
This led to a rapid depletion of the class B address space and poor utilization of the assigned address space
For example the organization that used a class B address for its hosts was allocated enough of the address space for up to interfacesleaving more than addresses that could not be used by other organizations
PRINCIPLES IN PRACTICE This example of an ISP that connects eight organizations to the Internet nicely illustrates how carefully allocated CIDRized addresses facilitate routing
Suppose as shown in Figure 
that the ISP which well call FlyByNightISP advertises to the outside world that it should be sent any datagrams whose first address bits match 
The rest of the world need not know that within the address block 
there are in fact eight other organizations each with its own subnets
This ability to use a single prefix to advertise multiple networks is often referred to as address aggregation also route aggregation or route summarization
Address aggregation works extremely well when addresses are allocated in blocks to ISPs and then from ISPs to client organizations
But what happens when addresses are not allocated in such a hierarchical manner What would happen for example if FlyByNightISP acquires ISPsRUs and then has Organization connect to the Internet through its subsidiary ISPsR Us As shown in Figure 
the subsidiary ISPsRUs owns the address block 
but Organization s IP addresses are unfortunately outside of this address block
What should be done here Certainly Organization could renumber all of its routers and hosts to have addresses within the ISPsRUs address block
But this is a costly solution and Organization might well be reassigned to another subsidiary in the future
The solution typically adopted is for Organization to keep its IP addresses in 
In this case as shown in Figure 
FlyByNightISP continues to advertise the address block 
and ISPsRUs continues to advertise 
However ISPsRUs now also advertises the block of addresses for Organization 
When other routers in the larger Internet see the address blocks 
from FlyByNightISP and 
from ISPsRUs and want to route to an address in the block 
they will use longest prefix matching see Section 
and route toward ISPsRUs as it advertises the longest i.e
mostspecific address prefix that matches the destination address
Hierarchical addressing and route aggregation Figure 
ISPsRUs has a more specific route to Organization We would be remiss if we did not mention yet another type of IP address the IP broadcast address 
When a host sends a datagram with destination address 
the message is delivered to all hosts on the same subnet
Routers optionally forward the message into neighboring subnets as well although they usually dont
Having now studied IP addressing in detail we need to know how hosts and subnets get their addresses in the first place
Lets begin by looking at how an organization gets a block of addresses for its devices and then look at how a device such as a host is assigned an address from within the organizations block of addresses
Obtaining a Block of Addresses In order to obtain a block of IP addresses for use within an organizations subnet a network administrator might first contact its ISP which would provide addresses from a larger block of addresses that had already been allocated to the ISP
For example the ISP may itself have been allocated the address block 
The ISP in turn could divide its address block into eight equalsized contiguous address blocks and give one of these address blocks out to each of up to eight organizations that are supported by this ISP as shown below
We have underlined the subnet part of these addresses for your convenience
ISPs block 
While obtaining a set of addresses from an ISP is one way to get a block of addresses it is not the only way
Clearly there must also be a way for the ISP itself to get a block of addresses
Is there a global authority that has ultimate responsibility for managing the IP address space and allocating address blocks to ISPs and other organizations Indeed there is IP addresses are managed under the authority of the Internet Corporation for Assigned Names and Numbers ICANN ICANN based on guidelines set forth in RFC 
The role of the nonprofit ICANN organization NTIA is not only to allocate IP addresses but also to manage the DNS root servers
It also has the very contentious job of assigning domain names and resolving domain name disputes
The ICANN allocates addresses to regional Internet registries for example ARIN RIPE APNIC and LACNIC which together form the Address Supporting Organization of ICANN ASOICANN and handle the allocationmanagement of addresses within their regions
Obtaining a Host Address The Dynamic Host Configuration Protocol Once an organization has obtained a block of addresses it can assign individual IP addresses to the host and router interfaces in its organization
A system administrator will typically manually configure the IP addresses into the router often remotely with a network management tool
Host addresses can also be configured manually but typically this is done using the Dynamic Host Configuration Protocol DHCP RFC 
DHCP allows a host to obtain be allocated an IP address automatically
A network administrator can configure DHCP so that a given host receives the same IP address each time it connects to the network or a host may be assigned a temporary IP address that will be different each time the host connects to the network
In addition to host IP address assignment DHCP also allows a host to learn additional information such as its subnet mask the address of its firsthop router often called the default gateway and the address of its local DNS server
Because of DHCPs ability to automate the networkrelated aspects of connecting a host into a network it is often referred to as a plugandplay or zeroconf zeroconfiguration protocol
This capability makes it very attractive to the network administrator who would otherwise have to perform these tasks manually DHCP is also enjoying widespread use in residential Internet access networks enterprise networks and in wireless LANs where hosts join and leave the network frequently
Consider for example the student who carries a laptop from a dormitory room to a library to a classroom
It is likely that in each location the student will be connecting into a new subnet and hence will need a new IP address at each location
DHCP is ideally suited to this situation as there are many users coming and going and addresses are needed for only a limited amount of time
The value of DHCPs plugandplay capability is clear since its unimaginable that a system administrator would be able to reconfigure laptops at each location and few students except those taking a computer networking class would have the expertise to configure their laptops manually
DHCP is a clientserver protocol
A client is typically a newly arriving host wanting to obtain network configuration information including an IP address for itself
In the simplest case each subnet in the addressing sense of Figure 
will have a DHCP server
If no server is present on the subnet a DHCP relay agent typically a router that knows the address of a DHCP server for that network is needed
shows a DHCP server attached to subnet 
with the router serving as the relay agent for arriving clients attached to subnets 
In our discussion below well assume that a DHCP server is available on the subnet
For a newly arriving host the DHCP protocol is a fourstep process as shown in Figure 
for the network setting shown in Figure 
In this figure yiaddr as in your Internet address indicates the address being allocated to the newly arriving client
The four steps are Figure 
DHCP client and server DHCP server discovery
The first task of a newly arriving host is to find a DHCP server with which to interact
This is done using a DHCP discover message which a client sends within a UDP packet to port 
The UDP packet is encapsulated in an IP datagram
But to whom should this datagram be sent The host doesnt even know the IP address of the network to which it is attaching much less the address of a DHCP server for this network
Given this the DHCP client creates an IP datagram containing its DHCP discover message along with the broadcast destination IP address of 
and a this host source IP address of 
The DHCP client passes the IP datagram to the link layer which then broadcasts this frame to all nodes attached to the subnet we will cover the details of linklayer broadcasting in Section 
DHCP server offers
A DHCP server receiving a DHCP discover message responds to the client with a DHCP offer message that is broadcast to all nodes on the subnet again using the IP broadcast address of 
You might want to think about why this server reply must also be broadcast
Since several DHCP servers can be present on the subnet the client may find itself in the enviable position of being able to choose from among several offers
Each Figure 
DHCP clientserver interaction server offer message contains the transaction ID of the received discover message the proposed IP address for the client the network mask and an IP address lease timethe amount of time for which the IP address will be valid
It is common for the server to set the lease time to several hours or days Droms 
The newly arriving client will choose from among one or more server offers and respond to its selected offer with a DHCP request message echoing back the configuration parameters
The server responds to the DHCP request message with a DHCP ACK message confirming the requested parameters
Once the client receives the DHCP ACK the interaction is complete and the client can use the DHCP allocated IP address for the lease duration
Since a client may want to use its address beyond the leases expiration DHCP also provides a mechanism that allows a client to renew its lease on an IP address
From a mobility aspect DHCP does have one very significant shortcoming
Since a new IP address is obtained from DHCP each time a node connects to a new subnet a TCP connection to a remote application cannot be maintained as a mobile node moves between subnets
In Chapter we will examine mobile IPan extension to the IP infrastructure that allows a mobile node to use a single permanent address as it moves between subnets
Additional details about DHCP can be found in Droms and dhc 
An open source reference implementation of DHCP is available from the Internet Systems Consortium ISC 
Network Address Translation NAT Given our discussion about Internet addresses and the IPv datagram format were now well aware that every IPcapable device needs an IP address
With the proliferation of small office home office SOHO subnets this would seem to imply that whenever a SOHO wants to install a LAN to connect multiple machines a range of addresses would need to be allocated by the ISP to cover all of the SOHOs IP devices including phones tablets gaming devices IP TVs printers and more
If the subnet grew bigger a larger block of addresses would have to be allocated
But what if the ISP had already allocated the contiguous portions of the SOHO networks current address range And what typical homeowner wants or should need to know how to manage IP addresses in the first place Fortunately there is a simpler approach to address allocation that has found increasingly widespread use in such scenarios network address translation NAT RFC RFC Huston Zhang Cisco NAT 
shows the operation of a NATenabled router
The NATenabled router residing in the home has an interface that is part of the home network on the right of Figure 
Addressing within the home network is exactly as we have seen aboveall four interfaces in the home network have the same subnet address of 
The address space 
is one of three portions of the IP address space that is reserved in RFC for a private network or a realm with private addresses such as the home network in Figure 
A realm with private addresses refers to a network whose addresses only have meaning to devices within that network
To see why this is important consider the fact that there are hundreds of thousands of home networks many using the same address space 
Devices within a given home network can send packets to each other using 
However packets forwarded beyond the home network into the larger global Internet clearly cannot use these addresses as either a source or a destination address because there are hundreds of thousands of networks using this block of addresses
That is the 
addresses can only have meaning within the Figure 
Network address translation given home network
But if private addresses only have meaning within a given network how is addressing handled when packets are sent to or received from the global Internet where addresses are necessarily unique The answer lies in understanding NAT
The NATenabled router does not look like a router to the outside world
Instead the NAT router behaves to the outside world as a single device with a single IP address
In Figure 
all traffic leaving the home router for the larger Internet has a source IP address of 
and all traffic entering the home router must have a destination address of 
In essence the NATenabled router is hiding the details of the home network from the outside world
As an aside you might wonder where the home network computers get their addresses and where the router gets its single IP address
Often the answer is the sameDHCP The router gets its address from the ISPs DHCP server and the router runs a DHCP server to provide addresses to computers within the NATDHCProuter controlled home networks address space
If all datagrams arriving at the NAT router from the WAN have the same destination IP address specifically that of the WANside interface of the NAT router then how does the router know the internal host to which it should forward a given datagram The trick is to use a NAT translation table at the NAT router and to include port numbers as well as IP addresses in the table entries
Consider the example in Figure 
Suppose a user sitting in a home network behind host 
requests a Web page on some Web server port with IP address 
The host 
assigns the arbitrary source port number and sends the datagram into the LAN
The NAT router receives the datagram generates a new source port number for the datagram replaces the source IP address with its WANside IP address 
and replaces the original source port number with the new source port number 
When generating a new source port number the NAT router can select any source port number that is not currently in the NAT translation table
Note that because a port number field is bits long the NAT protocol can support over simultaneous connections with a single WANside IP address for the router NAT in the router also adds an entry to its NAT translation table
The Web server blissfully unaware that the arriving datagram containing the HTTP request has been manipulated by the NAT router responds with a datagram whose destination address is the IP address of the NAT router and whose destination port number is 
When this datagram arrives at the NAT router the router indexes the NAT translation table using the destination IP address and destination port number to obtain the appropriate IP address 
and destination port number for the browser in the home network
The router then rewrites the datagrams destination address and destination port number and forwards the datagram into the home network
NAT has enjoyed widespread deployment in recent years
But NAT is not without detractors
First one might argue that port numbers are meant to be used for addressing processes not for addressing hosts
This violation can indeed cause problems for servers running on the home network since as we have seen in Chapter server processes wait for incoming requests at wellknown port numbers and peers in a PP protocol need to accept incoming connections when acting as servers
Technical solutions to these problems include NAT traversal tools RFC and Universal Plug and Play UPnP a protocol that allows a host to discover and configure a nearby NAT UPnP Forum 
More philosophical arguments have also been raised against NAT by architectural purists
Here the concern is that routers are meant to be layer i.e
networklayer devices and should process packets only up to the network layer
NAT violates this principle that hosts should be talking directly with each other without interfering nodes modifying IP addresses much less port numbers
But like it or not NAT has not become an important component of the Internet as have other socalled middleboxes Sekar that operate at the network layer but have functions that are quite different from routers
Middleboxes do not perform traditional datagram forwarding but instead perform functions such as NAT load balancing of traffic flows traffic firewalling see accompanying sidebar and more
The generalized forwarding paradigm that well study shortly in Section 
allows a number of these middlebox functions as well as traditional router forwarding to be accomplished in a common integrated manner
FOCUS ON SECURITY INSPECTING DATAGRAMS FIREWALLS AND INTRUSION DETECTION SYSTEMS Suppose you are assigned the task of administering a home departmental university or corporate network
Attackers knowing the IP address range of your network can easily send IP datagrams to addresses in your range
These datagrams can do all kinds of devious things including mapping your network with ping sweeps and port scans crashing vulnerable hosts with malformed packets scanning for open TCPUDP ports on servers in your network and infecting hosts by including malware in the packets
As the network administrator what are you going to do about all those bad guys out there each capable of sending malicious packets into your network Two popular defense mechanisms to malicious packet attacks are firewalls and intrusion detection systems IDSs
As a network administrator you may first try installing a firewall between your network and the Internet
Most access routers today have firewall capability
Firewalls inspect the datagram and segment header fields denying suspicious datagrams entry into the internal network
For example a firewall may be configured to block all ICMP echo request packets see Section 
thereby preventing an attacker from doing a traditional port scan across your IP address range
Firewalls can also block packets based on source and destination IP addresses and port numbers
Additionally firewalls can be configured to track TCP connections granting entry only to datagrams that belong to approved connections
Additional protection can be provided with an IDS
An IDS typically situated at the network boundary performs deep packet inspection examining not only header fields but also the payloads in the datagram including applicationlayer data
An IDS has a database of packet signatures that are known to be part of attacks
This database is automatically updated as new attacks are discovered
As packets pass through the IDS the IDS attempts to match header fields and payloads to the signatures in its signature database
If such a match is found an alert is created
An intrusion prevention system IPS is similar to an IDS except that it actually blocks packets in addition to creating alerts
In Chapter well explore firewalls and IDSs in more detail
Can firewalls and IDSs fully shield your network from all attacks The answer is clearly no as attackers continually find new attacks for which signatures are not yet available
But firewalls and traditional signaturebased IDSs are useful in protecting your network from known attacks
IPv In the early s the Internet Engineering Task Force began an effort to develop a successor to the IPv protocol
A prime motivation for this effort was the realization that the bit IPv address space was beginning to be used up with new subnets and IP nodes being attached to the Internet and being allocated unique IP addresses at a breathtaking rate
To respond to this need for a large IP address space a new IP protocol IPv was developed
The designers of IPv also took this opportunity to tweak and augment other aspects of IPv based on the accumulated operational experience with IPv
The point in time when IPv addresses would be completely allocated and hence no new networks could attach to the Internet was the subject of considerable debate
The estimates of the two leaders of the IETFs Address Lifetime Expectations working group were that addresses would become exhausted in and respectively Solensky 
In February IANA allocated out the last remaining pool of unassigned IPv addresses to a regional registry
While these registries still have available IPv addresses within their pool once these addresses are exhausted there are no more available address blocks that can be allocated from a central pool Huston a
A recent survey of IPv addressspace exhaustion and the steps taken to prolong the life of the address space is Richter 
Although the mids estimates of IPv address depletion suggested that a considerable amount of time might be left until the IPv address space was exhausted it was realized that considerable time would be needed to deploy a new technology on such an extensive scale and so the process to develop IP version IPv RFC was begun RFC 
An oftenasked question is what happened to IPv It was initially envisioned that the ST protocol would become IPv but ST was later dropped
An excellent source of information about IPv is Huitema 
IPv Datagram Format The format of the IPv datagram is shown in Figure 
The most important changes introduced in IPv are evident in the datagram format Expanded addressing capabilities
IPv increases the size of the IP address from to bits
This ensures that the world wont run out of IP addresses
Now every grain of sand on the planet can be IPaddressable
In addition to unicast and multicast addresses IPv has introduced a new type of address called an anycast address that allows a datagram to be delivered to any one of a group of hosts
This feature could be used for example to send an HTTP GET to the nearest of a number of mirror sites that contain a given document
A streamlined byte header
As discussed below a number of IPv fields have been dropped or made optional
The resulting byte fixedlength header allows for faster processing of the IP datagram by a router
A new encoding of options allows for more flexible options processing
IPv has an elusive definition of a flow
RFC states that this allows labeling of packets belonging to particular flows for which the sender Figure 
IPv datagram format requests special handling such as a nondefault quality of service or realtime service
For example audio and video transmission might likely be treated as a flow
On the other hand the more traditional applications such as file transfer and email might not be treated as flows
It is possible that the traffic carried by a highpriority user for example someone paying for better service for their traffic might also be treated as a flow
What is clear however is that the designers of IPv foresaw the eventual need to be able to differentiate among the flows even if the exact meaning of a flow had yet to be determined
As noted above a comparison of Figure 
with Figure 
reveals the simpler more streamlined structure of the IPv datagram
The following fields are defined in IPv Version
This bit field identifies the IP version number
Not surprisingly IPv carries a value of in this field
Note that putting a in this field does not create a valid IPv datagram
If it did life would be a lot simplersee the discussion below regarding the transition from IPv to IPv
The bit traffic class field like the TOS field in IPv can be used to give priority to certain datagrams within a flow or it can be used to give priority to datagrams from certain applications for example voiceoverIP over datagrams from other applications for example SMTP email
As discussed above this bit field is used to identify a flow of datagrams
This bit value is treated as an unsigned integer giving the number of bytes in the IPv datagram following the fixedlength byte datagram header
This field identifies the protocol to which the contents data field of this datagram will be delivered for example to TCP or UDP
The field uses the same values as the protocol field in the IPv header
The contents of this field are decremented by one by each router that forwards the datagram
If the hop limit count reaches zero the datagram is discarded
Source and destination addresses
The various formats of the IPv bit address are described in RFC 
This is the payload portion of the IPv datagram
When the datagram reaches its destination the payload will be removed from the IP datagram and passed on to the protocol specified in the next header field
The discussion above identified the purpose of the fields that are included in the IPv datagram
Comparing the IPv datagram format in Figure 
with the IPv datagram format that we saw in Figure 
we notice that several fields appearing in the IPv datagram are no longer present in the IPv datagram Fragmentationreassembly
IPv does not allow for fragmentation and reassembly at intermediate routers these operations can be performed only by the source and destination
If an IPv datagram received by a router is too large to be forwarded over the outgoing link the router simply drops the datagram and sends a Packet Too Big ICMP error message see Section 
back to the sender
The sender can then resend the data using a smaller IP datagram size
Fragmentation and reassembly is a timeconsuming operation removing this functionality from the routers and placing it squarely in the end systems considerably speeds up IP forwarding within the network
Because the transportlayer for example TCP and UDP and linklayer for example Ethernet protocols in the Internet layers perform checksumming the designers of IP probably felt that this functionality was sufficiently redundant in the network layer that it could be removed
Once again fast processing of IP packets was a central concern
Recall from our discussion of IPv in Section 
that since the IPv header contains a TTL field similar to the hop limit field in IPv the IPv header checksum needed to be recomputed at every router
As with fragmentation and reassembly this too was a costly operation in IPv
An options field is no longer a part of the standard IP header
However it has not gone away
Instead the options field is one of the possible next headers pointed to from within the IPv header
That is just as TCP or UDP protocol headers can be the next header within an IP packet so too can an options field
The removal of the options field results in a fixedlength byte IP header
Transitioning from IPv to IPv Now that we have seen the technical details of IPv let us consider a very practical matter How will the public Internet which is based on IPv be transitioned to IPv The problem is that while new IPv capable systems can be made backwardcompatible that is can send route and receive IPv datagrams already deployed IPvcapable systems are not capable of handling IPv datagrams
Several options are possible Huston b RFC 
One option would be to declare a flag daya given time and date when all Internet machines would be turned off and upgraded from IPv to IPv
The last major technology transition from using NCP to using TCP for reliable transport service occurred almost years ago
Even back then RFC when the Internet was tiny and still being administered by a small number of wizards it was realized that such a flag day was not possible
A flag day involving billions of devices is even more unthinkable today
The approach to IPvtoIPv transition that has been most widely adopted in practice involves tunneling RFC 
The basic idea behind tunnelinga key concept with applications in many other scenarios beyond IPvtoIPv transition including wide use in the allIP cellular networks that well cover in Chapter is the following
Suppose two IPv nodes in this example B and E in Figure 
want to interoperate using IPv datagrams but are connected to each other by intervening IPv routers
We refer to the intervening set of IPv routers between two IPv routers as a tunnel as illustrated in Figure 
With tunneling the IPv node on the sending side of the tunnel in this example B takes the entire IPv datagram and puts it in the data payload field of an IPv datagram
This IPv datagram is then addressed to the IPv node on the receiving side of the tunnel in this example E and sent to the first node in the tunnel in this example C
The intervening IPv routers in the tunnel route this IPv datagram among themselves just as they would any other datagram blissfully unaware that the IPv datagram itself contains a complete IPv datagram
The IPv node on the receiving side of the tunnel eventually receives the IPv datagram it is the destination of the IPv datagram determines that the IPv datagram contains an IPv datagram by observing that the protocol number field in the IPv datagram is RFC indicating that the IPv payload is a IPv datagram extracts the IPv datagram and then routes the IPv datagram exactly as it would if it had received the IPv datagram from a directly connected IPv neighbor
We end this section by noting that while the adoption of IPv was initially slow to take off Lawton Huston b momentum has been building
NIST NIST IPv reports that more than a third of US government secondlevel domains are IPvenabled
On the client side Google reports that only about percent of the clients accessing Google services do so via IPv Google IPv 
But other recent measurements Czyz indicate that IPv adoption is accelerating
The proliferation of devices such as IPenabled phones and other portable devices Figure 
Tunneling provides an additional push for more widespread deployment of IPv
Europes Third Generation Partnership Program GPP has specified IPv as the standard addressing scheme for mobile multimedia
One important lesson that we can learn from the IPv experience is that it is enormously difficult to change networklayer protocols
Since the early s numerous new networklayer protocols have been trumpeted as the next major revolution for the Internet but most of these protocols have had limited penetration to date
These protocols include IPv multicast protocols and resource reservation protocols a discussion of these latter two protocols can be found in the online supplement to this text
Indeed introducing new protocols into the network layer is like replacing the foundation of a houseit is difficult to do without tearing the whole house down or at least temporarily relocating the houses residents
On the other hand the Internet has witnessed rapid deployment of new protocols at the application layer
The classic examples of course are the Web instant messaging streaming media distributed games and various forms of social media
Introducing new applicationlayer protocols is like adding a new layer of paint to a houseit is relatively easy to do and if you choose an attractive color others in the neighborhood will copy you
In summary in the future we can certainly expect to see changes in the Internets network layer but these changes will likely occur on a time scale that is much slower than the changes that will occur at the application layer
Generalized Forwarding and SDN In Section 
we noted that an Internet routers forwarding decision has traditionally been based solely on a packets destination address
In the previous section however weve also seen that there has been a proliferation of middleboxes that perform many layer functions
NAT boxes rewrite header IP addresses and port numbers firewalls block traffic based on headerfield values or redirect packets for additional processing such as deep packet inspection DPI
Loadbalancers forward packets requesting a given service e.g
an HTTP request to one of a set of a set of servers that provide that service
RFC lists a number of common middlebox functions
This proliferation of middleboxes layer switches and layer routers Qazi each with its own specialized hardware software and management interfaceshas undoubtedly resulted in costly headaches for many network operators
However recent advances in softwaredefined networking have promised and are now delivering a unified approach towards providing many of these networklayer functions and certain linklayer functions as well in a modern elegant and integrated manner
Recall that Section 
characterized destinationbased forwarding as the two steps of looking up a destination IP address match then sending the packet into the switching fabric to the specified output port action
Lets now consider a significantly more general matchplusaction paradigm where the match can be made over multiple header fields associated with different protocols at different layers in the protocol stack
The action can include forwarding the packet to one or more output ports as in destinationbased forwarding load balancing packets across multiple outgoing interfaces that lead to a service as in load balancing rewriting header values as in NAT purposefully blockingdropping a packet as in a firewall sending a packet to a special server for further processing and action as in DPI and more
In generalized forwarding a matchplusaction table generalizes the notion of the destinationbased forwarding table that we encountered in Section 
Because forwarding decisions may be made using networklayer andor linklayer source and destination addresses the forwarding devices shown in Figure 
are more accurately described as packet switches rather than layer routers or layer switches
Thus in the remainder of this section and in Section 
well refer Figure 
Generalized forwarding Each packet switch contains a matchplusaction table that is computed and distributed by a remote controller to these devices as packet switches adopting the terminology that is gaining widespread adoption in SDN literature
shows a matchplusaction table in each packet switch with the table being computed installed and updated by a remote controller
We note that while it is possible for the control components at the individual packet switch to interact with each other e.g
in a manner similar to that in Figure 
in practice generalized matchplusaction capabilities are implemented via a remote controller that computes installs and updates these tables
You might take a minute to compare Figures 
and .what similarities and differences do you notice between destinationbased forwarding shown in Figure 
and generalized forwarding shown in Figure 
Our following discussion of generalized forwarding will be based on OpenFlow McKeown OpenFlow Casado Tourrilhes a highly visible and successful standard that has pioneered the notion of the matchplusaction forwarding abstraction and controllers as well as the SDN revolution more generally Feamster 
Well primarily consider OpenFlow 
which introduced key SDN abstractions and functionality in a particularly clear and concise manner
Later versions of OpenFlow introduced additional capabilities as a result of experience gained through implementation and use current and earlier versions of the OpenFlow standard can be found at ONF 
Each entry in the matchplusaction forwarding table known as a flow table in OpenFlow includes A set of header field values to which an incoming packet will be matched
As in the case of destinationbased forwarding hardwarebased matching is most rapidly performed in TCAM memory with more than a million destination address entries being possible Bosshart 
A packet that matches no flow table entry can be dropped or sent to the remote controller for more processing
In practice a flow table may be implemented by multiple flow tables for performance or cost reasons Bosshart but well focus here on the abstraction of a single flow table
A set of counters that are updated as packets are matched to flow table entries
These counters might include the number of packets that have been matched by that table entry and the time since the table entry was last updated
A set of actions to be taken when a packet matches a flow table entry
These actions might be to forward the packet to a given output port to drop the packet makes copies of the packet and sent them to multiple output ports andor to rewrite selected header fields
Well explore matching and actions in more detail in Sections 
Well then study how the networkwide collection of perpacket switch matching rules can be used to implement a wide range of functions including routing layer switching firewalling loadbalancing virtual networks and more in Section 
In closing we note that the flow table is essentially an API the abstraction through which an individual packet switchs behavior can be programmed well see in Section 
that networkwide behaviors can similarly be programmed by appropriately programmingconfiguring these tables in a collection of network packet switches Casado 
Match Figure 
shows the eleven packetheader fields and the incoming port ID that can be matched in an OpenFlow 
Recall from Figure 
Packet matching fields OpenFlow 
flow table Section 
that a linklayer layer frame arriving to a packet switch will contain a networklayer layer datagram as its payload which in turn will typically contain a transportlayer layer segment
The first observation we make is that OpenFlows match abstraction allows for a match to be made on selected fields from three layers of protocol headers thus rather brazenly defying the layering principle we studied in Section 
Since weve not yet covered the link layer suffice it to say that the source and destination MAC addresses shown in Figure 
are the linklayer addresses associated with the frames sending and receiving interfaces by forwarding on the basis of Ethernet addresses rather than IP addresses we can see that an OpenFlowenabled device can equally perform as a router layer device forwarding datagrams as well as a switch layer device forwarding frames
The Ethernet type field corresponds to the upper layer protocol e.g
IP to which the frames payload will be de multiplexed and the VLAN fields are concerned with socalled virtual local area networks that well study in Chapter 
The set of twelve values that can be matched in the OpenFlow 
specification has grown to values in more recent OpenFlow specifications Bosshart 
The ingress port refers to the input port at the packet switch on which a packet is received
The packets IP source address IP destination address IP protocol field and IP type of service fields were discussed earlier in Section 
The transportlayer source and destination port number fields can also be matched
Flow table entries may also have wildcards
For example an IP address of 
in a flow table will match the corresponding address field of any datagram that has 
as the first bits of its address
Each flow table entry also has an associated priority
If a packet matches multiple flow table entries the selected match and corresponding action will be that of the highest priority entry with which the packet matches
Lastly we observe that not all fields in an IP header can be matched
For example OpenFlow does not allow matching on the basis of TTL field or datagram length field
Why are some fields allowed for matching while others are not Undoubtedly the answer has to do with the tradeoff between functionality and complexity
The art in choosing an abstraction is to provide for enough functionality to accomplish a task in this case to implement configure and manage a wide range of networklayer functions that had previously been implemented through an assortment of networklayer devices without overburdening the abstraction with so much detail and generality that it becomes bloated and unusable
Butler Lampson has famously noted Lampson Do one thing at a time and do it well
An interface should capture the minimum essentials of an abstraction
Dont generalize generalizations are generally wrong
Given OpenFlows success one can surmise that its designers indeed chose their abstraction well
Additional details of OpenFlow matching can be found in OpenFlow ONF 
Action As shown in Figure 
each flow table entry has a list of zero or more actions that determine the processing that is to be applied to a packet that matches a flow table entry
If there are multiple actions they are performed in the order specified in the list
Among the most important possible actions are Forwarding
An incoming packet may be forwarded to a particular physical output port broadcast over all ports except the port on which it arrived or multicast over a selected set of ports
The packet may be encapsulated and sent to the remote controller for this device
That controller then may or may not take some action on that packet including installing new flow table entries and may return the packet to the device for forwarding under the updated set of flow table rules
A flow table entry with no action indicates that a matched packet should be dropped
The values in ten packet header fields all layer and fields shown in Figure 
except the IP Protocol field may be rewritten before the packet is forwarded to the chosen output port
OpenFlow Examples of Matchplusaction in Action Having now considered both the match and action components of generalized forwarding lets put these ideas together in the context of the sample network shown in Figure 
The network has hosts h h h h h and h and three packet switches s s and s each with four local interfaces numbered through 
Well consider a number of networkwide behaviors that wed like to implement and the flow table entries in s s and s needed to implement this behavior
OpenFlow matchplusaction network with three packet switches hosts and an OpenFlow controller A First Example Simple Forwarding As a very simple example suppose that the desired forwarding behavior is that packets from h or h destined to h or h are to be forwarded from s to s and then from s to s thus completely avoiding the use of the link between s and s
The flow table entry in s would be s Flow Table Example Match Action Ingress Port IP Src 
IP Dst 
Forward Of course well also need a flow table entry in s so that datagrams sent from h or h are forwarded to s over outgoing interface s Flow Table Example Match Action IP Src 
IP Dst 
Forward Lastly well also need a flow table entry in s to complete this first example so that datagrams arriving from s are forwarded to their destination either host h or h s Flow Table Example Match Action Ingress port IP Dst 
Forward Ingress port IP Dst 
Forward A Second Example Load Balancing As a second example lets consider a loadbalancing scenario where datagrams from h destined to 
are to be forwarded over the direct link between s and s while datagrams from h destined to 
are to be forwarded over the link between s and s and then from s to s
Note that this behavior couldnt be achieved with IPs destinationbased forwarding
In this case the flow table in s would be s Flow Table Example Match Action Ingress port IP Dst 
Forward Ingress port IP Dst 
Forward Flow table entries are also needed at s to forward the datagrams received from s to either h or h and flow table entries are needed at s to forward datagrams received on interface from s over interface towards s
See if you can figure out these flow table entries at s and s
A Third Example Firewalling As a third example lets consider a firewall scenario in which s wants only to receive on any of its interfaces traffic sent from hosts attached to s
s Flow Table Example Match Action IP Src 
IP Dst 
Forward IP Src 
IP Dst 
Forward If there were no other entries in ss flow table then only traffic from 
would be forwarded to the hosts attached to s
Although weve only considered a few basic scenarios here the versatility and advantages of generalized forwarding are hopefully apparent
In homework problems well explore how flow tables can be used to create many different logical behaviors including virtual networkstwo or more logically separate networks each with their own independent and distinct forwarding behaviorthat use the same physical set of packet switches and links
In Section 
well return to flow tables when we study the SDN controllers that compute and distribute the flow tables and the protocol used for communicating between a packet switch and its controller
Summary In this chapter weve covered the data plane functions of the network layerthe perrouter functions that determine how packets arriving on one of a routers input links are forwarded to one of that routers output links
We began by taking a detailed look at the internal operations of a router studying input and output port functionality and destinationbased forwarding a routers internal switching mechanism packet queue management and more
We covered both traditional IP forwarding where forwarding is based on a datagrams destination address and generalized forwarding where forwarding and other functions may be performed using values in several different fields in the datagrams header and seen the versatility of the latter approach
We also studied the IPv and IPv protocols in detail and Internet addressing which we found to be much deeper subtler and more interesting than we might have expected
With our newfound understanding of the networklayers data plane were now ready to dive into the network layers control plane in Chapter Homework Problems and Questions Chapter Review Questions SECTION 
Lets review some of the terminology used in this textbook
Recall that the name of a transportlayer packet is segment and that the name of a linklayer packet is frame
What is the name of a networklayer packet Recall that both routers and linklayer switches are called packet switches
What is the fundamental difference between a router and linklayer switch R
We noted that network layer functionality can be broadly divided into data plane functionality and control plane functionality
What are the main functions of the data plane Of the control plane R
We made a distinction between the forwarding function and the routing function performed in the network layer
What are the key differences between routing and forwarding R
What is the role of the forwarding table within a router R
We said that a network layers service model defines the characteristics of endtoend transport of packets between sending and receiving hosts
What is the service model of the Internets network layer What guarantees are made by the Internets service model regarding the hosttohost delivery of datagrams SECTION 
In Section 
we saw that a router typically consists of input ports output ports a switching fabric and a routing processor
Which of these are implemented in hardware and which are implemented in software Why Returning to the notion of the network layers data plane and control plane which are implemented in hardware and which are implemented in software Why R
Discuss why each input port in a highspeed router stores a shadow copy of the forwarding table
What is meant by destinationbased forwarding How does this differ from generalized forwarding assuming youve read Section 
which of the two approaches are adopted by SoftwareDefined Networking R
Suppose that an arriving packet matches two or more entries in a routers forwarding table
With traditional destinationbased forwarding what rule does a router apply to determine which of these rules should be applied to determine the output port to which the arriving packet should be switched R
Three types of switching fabrics are discussed in Section 
List and briefly describe each type
Which if any can send multiple packets across the fabric in parallel R
Describe how packet loss can occur at input ports
Describe how packet loss at input ports can be eliminated without using infinite buffers
Describe how packet loss can occur at output ports
Can this loss be prevented by increasing the switch fabric speed R
What is HOL blocking Does it occur in input ports or output ports R
In Section 
we studied FIFO Priority Round Robin RR and Weighted Fair Queueing WFQ packet scheduling disciplines Which of these queueing disciplines ensure that all packets depart in the order in which they arrived R
Give an example showing why a network operator might want one class of packets to be given priority over another class of packets
What is an essential different between RR and WFQ packet scheduling Is there a case Hint Consider the WFQ weights where RR and WFQ will behave exactly the same SECTION 
Suppose Host A sends Host B a TCP segment encapsulated in an IP datagram
When Host B receives the datagram how does the network layer in Host B know it should pass the segment that is the payload of the datagram to TCP rather than to UDP or to some other upperlayer protocol R
What field in the IP header can be used to ensure that a packet is forwarded through no more than N routers R
Recall that we saw the Internet checksum being used in both transportlayer segment in UDP and TCP headers Figures 
respectively and in networklayer datagrams IP header Figure 
Now consider a transport layer segment encapsulated in an IP datagram
Are the checksums in the segment header and datagram header computed over any common bytes in the IP datagram Explain your answer
When a large datagram is fragmented into multiple smaller datagrams where are these smaller datagrams reassembled into a single larger datagram R
Do routers have IP addresses If so how many R
What is the bit binary equivalent of the IP address 
Visit a host that uses DHCP to obtain its IP address network mask default router and IP address of its local DNS server
List these values
Suppose there are three routers between a source host and a destination host
Ignoring fragmentation an IP datagram sent from the source host to the destination host will travel over how many interfaces How many forwarding tables will be indexed to move the datagram from the source to the destination R
Suppose an application generates chunks of bytes of data every msec and each chunk gets encapsulated in a TCP segment and then an IP datagram
What percentage of each datagram will be overhead and what percentage will be application data R
Suppose you purchase a wireless router and connect it to your cable modem
Also suppose that your ISP dynamically assigns your connected device that is your wireless router one IP address
Also suppose that you have five PCs at home that use 
to wirelessly connect to your wireless router
How are IP addresses assigned to the five PCs Does the wireless router use NAT Why or why not R
What is meant by the term route aggregation Why is it useful for a router to perform route aggregation R
What is meant by a plugandplay or zeroconf protocol R
What is a private network address Should a datagram with a private network address ever be present in the larger public Internet Explain
Compare and contrast the IPv and the IPv header fields
Do they have any fields in common R
It has been said that when IPv tunnels through IPv routers IPv treats the IPv tunnels as linklayer protocols
Do you agree with this statement Why or why not SECTION 
How does generalized forwarding differ from destinationbased forwarding R
What is the difference between a forwarding table that we encountered in destination based forwarding in Section 
and OpenFlows flow table that we encountered in Section 
What is meant by the match plus action operation of a router or switch In the case of destinationbased forwarding packet switch what is matched and what is the action taken In the case of an SDN name three fields that can be matched and three actions that can be taken
Name three header fields in an IP datagram that can be matched in OpenFlow 
What are three IP datagram header fields that cannot be matched in OpenFlow Problems P
Consider the network below
Show the forwarding table in router A such that all traffic destined to host H is forwarded through interface 
Can you write down a forwarding table in router A such that all traffic from H destined to host H is forwarded through interface while all traffic from H destined to host H is forwarded through interface Hint This is a trick question
Suppose two packets arrive to two different input ports of a router at exactly the same time
Also suppose there are no other packets anywhere in the router
Suppose the two packets are to be forwarded to two different output ports
Is it possible to forward the two packets through the switch fabric at the same time when the fabric uses a shared bus b
Suppose the two packets are to be forwarded to two different output ports
Is it possible to forward the two packets through the switch fabric at the same time when the fabric uses switching via memory c
Suppose the two packets are to be forwarded to the same output port
Is it possible to forward the two packets through the switch fabric at the same time when the fabric uses a crossbar P
In Section 
we noted that the maximum queuing delay is nD if the switching fabric is n times faster than the input line rates
Suppose that all packets are of the same length n packets arrive at the same time to the n input ports and all n packets want to be forwarded to different output ports
What is the maximum delay for a packet for the a memory b bus and c crossbar switching fabrics P
Consider the switch shown below
Suppose that all datagrams have the same fixed length that the switch operates in a slotted synchronous manner and that in one time slot a datagram can be transferred from an input port to an output port
The switch fabric is a crossbar so that at most one datagram can be transferred to a given output port in a time slot but different output ports can receive datagrams from different input ports in a single time slot
What is the minimal number of time slots needed to transfer the packets shown from input ports to their output ports assuming any input queue scheduling order you want i.e
it need not have HOL blocking What is the largest number of slots needed assuming the worstcase scheduling order you can devise assuming that a nonempty input queue is never idle P
Consider a datagram network using bit host addresses
Suppose a router has four links numbered through and packets are to be forwarded to the link interfaces as follows Destination Address Range Link Interface through through through otherwise a
Provide a forwarding table that has five entries uses longest prefix matching and forwards packets to the correct link interfaces
Describe how your forwarding table determines the appropriate link interface for datagrams with destination addresses P
Consider a datagram network using bit host addresses
Suppose a router uses longest prefix matching and has the following forwarding table Prefix Match Interface For each of the four interfaces give the associated range of destination host addresses and the number of addresses in the range
Consider a datagram network using bit host addresses
Suppose a router uses longest prefix matching and has the following forwarding table Prefix Match Interface otherwise For each of the four interfaces give the associated range of destination host addresses and the number of addresses in the range
Consider a router that interconnects three subnets Subnet Subnet and Subnet 
Suppose all of the interfaces in each of these three subnets are required to have the prefix 
Also suppose that Subnet is required to support at least interfaces Subnet is to support at least interfaces and Subnet is to support at least interfaces
Provide three network addresses of the form a.b.c.dx that satisfy these constraints
In Section 
an example forwarding table using longest prefix matching is given
Rewrite this forwarding table using the a.b.c.dx notation instead of the binary string notation
In Problem P you are asked to provide a forwarding table using longest prefix matching
Rewrite this forwarding table using the a.b.c.dx notation instead of the binary string notation
Consider a subnet with prefix 
Give an example of one IP address of form xxx.xxx.xxx.xxx that can be assigned to this network
Suppose an ISP owns the block of addresses of the form 
Suppose it wants to create four subnets from this block with each block having the same number of IP addresses
What are the prefixes of form a.b.c.dx for the four subnets P
Consider the topology shown in Figure 
Denote the three subnets with hosts starting clockwise at as Networks A B and C
Denote the subnets without hosts as Networks D E and F
Assign network addresses to each of these six subnets with the following constraints All addresses must be allocated from 
Subnet A should have enough addresses to support interfaces Subnet B should have enough addresses to support interfaces and Subnet C should have enough addresses to support interfaces
Of course subnets D E and F should each be able to support two interfaces
For each subnet the assignment should take the form a.b.c.dx or a.b.c.dx e.f.g.hy
Using your answer to part a provide the forwarding tables using longest prefix matching for each of the three routers
Use the whois service at the American Registry for Internet Numbers httpwww.arin.net whois to determine the IP address blocks for three universities
Can the whois services be used to determine with certainty the geographical location of a specific IP address Use www.maxmind.com to determine the locations of the Web servers at each of these universities
Consider sending a byte datagram into a link that has an MTU of bytes
Suppose the original datagram is stamped with the identification number 
How many fragments are generated What are the values in the various fields in the IP datagrams generated related to fragmentation P
Suppose datagrams are limited to bytes including header between source Host A and destination Host B
Assuming a byte IP header how many datagrams would be required to send an MP consisting of million bytes Explain how you computed your answer
Consider the network setup in Figure 
Suppose that the ISP instead assigns the router the address 
and that the network address of the home network is 
Assign addresses to all interfaces in the home network
Suppose each host has two ongoing TCP connections all to port at host 
Provide the six corresponding entries in the NAT translation table
Suppose you are interested in detecting the number of hosts behind a NAT
You observe that the IP layer stamps an identification number sequentially on each IP packet
The identification number of the first IP packet generated by a host is a random number and the identification numbers of the subsequent IP packets are sequentially assigned
Assume all IP packets generated by hosts behind the NAT are sent to the outside world
Based on this observation and assuming you can sniff all packets sent by the NAT to the outside can you outline a simple technique that detects the number of unique hosts behind a NAT Justify your answer
If the identification numbers are not sequentially assigned but randomly assigned would your technique work Justify your answer
In this problem well explore the impact of NATs on PP applications
Suppose a peer with username Arnold discovers through querying that a peer with username Bernard has a file it wants to download
Also suppose that Bernard and Arnold are both behind a NAT
Try to devise a technique that will allow Arnold to establish a TCP connection with Bernard without application specific NAT configuration
If you have difficulty devising such a technique discuss why
Consider the SDN OpenFlow network shown in Figure 
Suppose that the desired forwarding behavior for datagrams arriving at s is as follows any datagrams arriving on input port from hosts h or h that are destined to hosts h or h should be forwarded over output port any datagrams arriving on input port from hosts h or h that are destined to hosts h or h should be forwarded over output port any arriving datagrams on input ports or and destined to hosts h or h should be delivered to the host specified hosts h and h should be able to send datagrams to each other
Specify the flow table entries in s that implement this forwarding behavior
Consider again the SDN OpenFlow network shown in Figure 
Suppose that the desired forwarding behavior for datagrams arriving from hosts h or h at s is as follows any datagrams arriving from host h and destined for h h h or h should be forwarded in a clockwise direction in the network any datagrams arriving from host h and destined for h h h or h should be forwarded in a counterclockwise direction in the network
Specify the flow table entries in s that implement this forwarding behavior
Consider again the scenario from P above
Give the flow tables entries at packet switches s and s such that any arriving datagrams with a source address of h or h are routed to the destination hosts specified in the destination address field in the IP datagram
Hint Your forwarding table rules should include the cases that an arriving datagram is destined for a directly attached host or should be forwarded to a neighboring router for eventual host delivery there
Consider again the SDN OpenFlow network shown in Figure 
Suppose we want switch s to function as a firewall
Specify the flow table in s that implements the following firewall behaviors specify a different flow table for each of the four firewalling behaviors below for delivery of datagrams destined to h and h
You do not need to specify the forwarding behavior in s that forwards traffic to other routers
Only traffic arriving from hosts h and h should be delivered to hosts h or h i.e
that arriving traffic from hosts h and h is blocked
Only TCP traffic is allowed to be delivered to hosts h or h i.e
that UDP traffic is blocked
Only traffic destined to h is to be delivered i.e
all traffic to h is blocked
Only UDP traffic from h and destined to h is to be delivered
All other traffic is blocked
Wireshark Lab In the Web site for this textbook www.pearsonhighered.comcsresources youll find a Wireshark lab assignment that examines the operation of the IP protocol and the IP datagram format in particular
AN INTERVIEW WITH Vinton G
Cerf Vinton G
Cerf is Vice President and Chief Internet Evangelist for Google
He served for over years at MCI in various positions ending up his tenure there as Senior Vice President for Technology Strategy
He is widely known as the codesigner of the TCPIP protocols and the architecture of the Internet
During his time from to at the US Department of Defense Advanced Research Projects Agency DARPA he played a key role leading the development of Internet and Internetrelated data packet and security techniques
He received the US Presidential Medal of Freedom in and the US National Medal of Technology in 
He holds a BS in Mathematics from Stanford University and an MS and PhD in computer science from UCLA
What brought you to specialize in networking I was working as a programmer at UCLA in the late s
My job was supported by the US Defense Advanced Research Projects Agency called ARPA then called DARPA now
I was working in the laboratory of Professor Leonard Kleinrock on the Network Measurement Center of the newly created ARPAnet
The first node of the ARPAnet was installed at UCLA on September 
I was responsible for programming a computer that was used to capture performance information about the ARPAnet and to report this information back for comparison with mathematical models and predictions of the performance of the network
Several of the other graduate students and I were made responsible for working on the socalled hostlevel protocols of the ARPAnetthe procedures and formats that would allow many different kinds of computers on the network to interact with each other
It was a fascinating exploration into a new world for me of distributed computing and communication
Did you imagine that IP would become as pervasive as it is today when you first designed the protocol When Bob Kahn and I first worked on this in I think we were mostly very focused on the central question How can we make heterogeneous packet networks interoperate with one another assuming we cannot actually change the networks themselves We hoped that we could find a way to permit an arbitrary collection of packetswitched networks to be interconnected in a transparent fashion so that host computers could communicate endtoend without having to do any translations in between
I think we knew that we were dealing with powerful and expandable technology but I doubt we had a clear image of what the world would be like with hundreds of millions of computers all interlinked on the Internet
What do you now envision for the future of networking and the Internet What major challengesobstacles do you think lie ahead in their development I believe the Internet itself and networks in general will continue to proliferate
Already there is convincing evidence that there will be billions of Internetenabled devices on the Internet including appliances like cell phones refrigerators personal digital assistants home servers televisions as well as the usual array of laptops servers and so on
Big challenges include support for mobility battery life capacity of the access links to the network and ability to scale the optical core of the network up in an unlimited fashion
Designing an interplanetary extension of the Internet is a project in which I am deeply engaged at the Jet Propulsion Laboratory
We will need to cut over from IPv bit addresses to IPv bits
The list is long Who has inspired you professionally My colleague Bob Kahn my thesis advisor Gerald Estrin my best friend Steve Crocker we met in high school and he introduced me to computers in and the thousands of engineers who continue to evolve the Internet today
Do you have any advice for students entering the networkingInternet field Think outside the limitations of existing systemsimagine what might be possible but then do the hard work of figuring out how to get there from the current state of affairs
Dare to dream A half dozen colleagues and I at the Jet Propulsion Laboratory have been working on the design of an interplanetary extension of the terrestrial Internet
It may take decades to implement this mission by mission but to paraphrase A mans reach should exceed his grasp or what are the heavens for Chapter The Network Layer Control Plane In this chapter well complete our journey through the network layer by covering the controlplane component of the network layerthe networkwide logic that controls not only how a datagram is forwarded among routers along an endtoend path from the source host to the destination host but also how networklayer components and services are configured and managed
In Section 
well cover traditional routing algorithms for computing least cost paths in a graph these algorithms are the basis for two widely deployed Internet routing protocols OSPF and BGP that well cover in Sections 
As well see OSPF is a routing protocol that operates within a single ISPs network
BGP is a routing protocol that serves to interconnect all of the networks in the Internet BGP is thus often referred to as the glue that holds the Internet together
Traditionally controlplane routing protocols have been implemented together with dataplane forwarding functions monolithically within a router
As we learned in the introduction to Chapter softwaredefined networking SDN makes a clear separation between the data and control planes implementing controlplane functions in a separate controller service that is distinct and remote from the forwarding components of the routers it controls
Well cover SDN controllers in Section 
In Sections 
well cover some of the nuts and bolts of managing an IP network ICMP the Internet Control Message Protocol and SNMP the Simple Network Management Protocol
Introduction Lets quickly set the context for our study of the network control plane by recalling Figures 
There we saw that the forwarding table in the case of destinationbased forwarding and the flow table in the case of generalized forwarding were the principal elements that linked the network layers data and control planes
We learned that these tables specify the local dataplane forwarding behavior of a router
We saw that in the case of generalized forwarding the actions taken Section 
could include not only forwarding a packet to a routers output port but also dropping a packet replicating a packet andor rewriting layer or packetheader fields
In this chapter well study how those forwarding and flow tables are computed maintained and installed
In our introduction to the network layer in Section 
we learned that there are two possible approaches for doing so
illustrates the case where a routing algorithm runs in each and every router both a forwarding and a routing function are contained Figure 
Perrouter control Individual routing algorithm components interact in the control plane within each router
Each router has a routing component that communicates with the routing components in other routers to compute the values for its forwarding table
This perrouter control approach has been used in the Internet for decades
The OSPF and BGP protocols that well study in Sections 
are based on this perrouter approach to control
Logically centralized control
illustrates the case in which a logically centralized controller computes and distributes the forwarding tables to be used by each and every router
As we saw in Section 
the generalized matchplusaction abstraction allows the router to perform traditional IP forwarding as well as a rich set of other functions load sharing firewalling and NAT that had been previously implemented in separate middleboxes
Logically centralized control A distinct typically remote controller interacts with local control agents CAs The controller interacts with a control agent CA in each of the routers via a welldefined protocol to configure and manage that routers flow table
Typically the CA has minimum functionality its job is to communicate with the controller and to do as the controller commands
Unlike the routing algorithms in Figure 
the CAs do not directly interact with each other nor do they actively take part in computing the forwarding table
This is a key distinction between perrouter control and logically centralized control
By logically centralized control Levin we mean that the routing control service is accessed as if it were a single central service point even though the service is likely to be implemented via multiple servers for faulttolerance and performance scalability reasons
As we will see in Section 
SDN adopts this notion of a logically centralized controlleran approach that is finding increased use in production deployments
Google uses SDN to control the routers in its internal B global widearea network that interconnects its data centers Jain 
SWAN Hong from Microsoft Research uses a logically centralized controller to manage routing and forwarding between a wide area network and a data center network
China Telecom and China Unicom are using SDN both within data centers and between data centers Li 
ATT has noted ATT that it supports many SDN capabilities and independently defined proprietary mechanisms that fall under the SDN architectural framework
Routing Algorithms In this section well study routing algorithms whose goal is to determine good paths equivalently routes from senders to receivers through the network of routers
Typically a good path is one that has the least cost
Well see that in practice however realworld concerns such as policy issues for example a rule such as router x belonging to organization Y should not forward any packets originating from the network owned by organization Z also come into play
We note that whether the network control plane adopts a perrouter control approach or a logically centralized approach there must always be a well defined sequence of routers that a packet will cross in traveling from sending to receiving host
Thus the routing algorithms that compute these paths are of fundamental importance and another candidate for our top list of fundamentally important networking concepts
A graph is used to formulate routing problems
Recall that a graph GN E is a set N of nodes and a collection E of edges where each edge is a pair of nodes from N
In the context of networklayer routing the nodes in the graph represent Figure 
Abstract graph model of a computer network routersthe points at which packetforwarding decisions are madeand the edges connecting these nodes represent the physical links between these routers
Such a graph abstraction of a computer network is shown in Figure 
To view some graphs representing real network maps see Dodge Cheswick for a discussion of how well different graphbased models model the Internet see Zegura Faloutsos Li 
As shown in Figure 
an edge also has a value representing its cost
Typically an edges cost may reflect the physical length of the corresponding link for example a transoceanic link might have a higher cost than a shorthaul terrestrial link the link speed or the monetary cost associated with a link
For our purposes well simply take the edge costs as a given and wont worry about how they are determined
For any edge x y in E we denote cx y as the cost of the edge between nodes x and y
If the pair x y does not belong to E we set cx y
Also well only consider undirected graphs i.e
graphs whose edges do not have a direction in our discussion here so that edge x y is the same as edge y x and that cx ycy x however the algorithms well study can be easily extended to the case of directed links with a different cost in each direction
Also a node y is said to be a neighbor of node x if x y belongs to E
Given that costs are assigned to the various edges in the graph abstraction a natural goal of a routing algorithm is to identify the least costly paths between sources and destinations
To make this problem more precise recall that a path in a graph GN E is a sequence of nodes xxxp such that each of the pairs xxxxxpxp are edges in E
The cost of a path xx xp is simply the sum of all the edge costs along the path that is cxxcxxcxpxp
Given any two nodes x and y there are typically many paths between the two nodes with each path having a cost
One or more of these paths is a leastcost path
The leastcost problem is therefore clear Find a path between the source and destination that has least cost
In Figure 
for example the leastcost path between source node u and destination node w is u x y w with a path cost of 
Note that if all edges in the graph have the same cost the leastcost path is also the shortest path that is the path with the smallest number of links between the source and the destination
As a simple exercise try finding the leastcost path from node u to z in Figure 
and reflect for a moment on how you calculated that path
If you are like most people you found the path from u to z by examining Figure 
tracing a few routes from u to z and somehow convincing yourself that the path you had chosen had the least cost among all possible paths
Did you check all of the possible paths between u and z Probably not Such a calculation is an example of a centralized routing algorithmthe routing algorithm was run in one location your brain with complete information about the network
Broadly one way in which we can classify routing algorithms is according to whether they are centralized or decentralized
A centralized routing algorithm computes the leastcost path between a source and destination using complete global knowledge about the network
That is the algorithm takes the connectivity between all nodes and all link costs as inputs
This then requires that the algorithm somehow obtain this information before actually performing the calculation
The calculation itself can be run at one site e.g
a logically centralized controller as in Figure 
or could be replicated in the routing component of each and every router e.g
as in Figure 
The key distinguishing feature here however is that the algorithm has complete information about connectivity and link costs
Algorithms with global state information are often referred to as linkstate LS algorithms since the algorithm must be aware of the cost of each link in the network
Well study LS algorithms in Section 
In a decentralized routing algorithm the calculation of the leastcost path is carried out in an iterative distributed manner by the routers
No node has complete information about the costs of all network links
Instead each node begins with only the knowledge of the costs of its own directly attached links
Then through an iterative process of calculation and exchange of information with its neighboring nodes a node gradually calculates the leastcost path to a destination or set of destinations
The decentralized routing algorithm well study below in Section 
is called a distancevector DV algorithm because each node maintains a vector of estimates of the costs distances to all other nodes in the network
Such decentralized algorithms with interactive message exchange between neighboring routers is perhaps more naturally suited to control planes where the routers interact directly with each other as in Figure 
A second broad way to classify routing algorithms is according to whether they are static or dynamic
In static routing algorithms routes change very slowly over time often as a result of human intervention for example a human manually editing a link costs
Dynamic routing algorithms change the routing paths as the network traffic loads or topology change
A dynamic algorithm can be run either periodically or in direct response to topology or link cost changes
While dynamic algorithms are more responsive to network changes they are also more susceptible to problems such as routing loops and route oscillation
A third way to classify routing algorithms is according to whether they are loadsensitive or load insensitive
In a loadsensitive algorithm link costs vary dynamically to reflect the current level of congestion in the underlying link
If a high cost is associated with a link that is currently congested a routing algorithm will tend to choose routes around such a congested link
While early ARPAnet routing algorithms were loadsensitive McQuillan a number of difficulties were encountered Huitema 
Todays Internet routing algorithms such as RIP OSPF and BGP are loadinsensitive as a links cost does not explicitly reflect its current or recent past level of congestion
The LinkState LS Routing Algorithm Recall that in a linkstate algorithm the network topology and all link costs are known that is available as input to the LS algorithm
In practice this is accomplished by having each node broadcast linkstate packets to all other nodes in the network with each linkstate packet containing the identities and costs of its attached links
In practice for example with the Internets OSPF routing protocol discussed in Section 
this is often accomplished by a linkstate broadcast algorithm Perlman 
The result of the nodes broadcast is that all nodes have an identical and complete view of the network
Each node can then run the LS algorithm and compute the same set of leastcost paths as every other node
The linkstate routing algorithm we present below is known as Dijkstras algorithm named after its inventor
A closely related algorithm is Prims algorithm see Cormen for a general discussion of graph algorithms
Dijkstras algorithm computes the leastcost path from one node the source which we will refer to as u to all other nodes in the network
Dijkstras algorithm is iterative and has the property that after the kth iteration of the algorithm the leastcost paths are known to k destination nodes and among the leastcost paths to all destination nodes these k paths will have the k smallest costs
Let us define the following notation Dv cost of the leastcost path from the source node to destination v as of this iteration of the algorithm
pv previous node neighbor of v along the current leastcost path from the source to v
N subset of nodes v is in N if the leastcost path from the source to v is definitively known
The centralized routing algorithm consists of an initialization step followed by a loop
The number of times the loop is executed is equal to the number of nodes in the network
Upon termination the algorithm will have calculated the shortest paths from the source node u to every other node in the network
LinkState LS Algorithm for Source Node u Initialization N u for all nodes v if v is a neighbor of u then Dv cu v else Dv Loop find w not in N such that Dw is a minimum add w to N update Dv for each neighbor v of w and not in N Dv minDv Dw cw v new cost to v is either old cost to v or known least path cost to w plus cost from w to v until N N As an example lets consider the network in Figure 
and compute the leastcost paths from u to all possible destinations
A tabular summary of the algorithms computation is shown in Table 
where each line in the table gives the values of the algorithms variables at the end of the iteration
Lets consider the few first steps in detail
In the initialization step the currently known leastcost paths from u to its directly attached neighbors v x and w are initialized to and respectively
Note in Table 
Running the linkstate algorithm on the network in Figure 
step N D v p v D w p w D x p x D y p y D z p z u u u u ux u x x uxy u y y uxyv y y uxyvw y uxyvwz particular that the cost to w is set to even though we will soon see that a lessercost path does indeed exist since this is the cost of the direct one hop link from u to w
The costs to y and z are set to infinity because they are not directly connected to u
In the first iteration we look among those nodes not yet added to the set N and find that node with the least cost as of the end of the previous iteration
That node is x with a cost of and thus x is added to the set N
Line of the LS algorithm is then performed to update Dv for all nodes v yielding the results shown in the second line Step in Table 
The cost of the path to v is unchanged
The cost of the path to w which was at the end of the initialization through node x is found to have a cost of 
Hence this lowercost path is selected and ws predecessor along the shortest path from u is set to x
Similarly the cost to y through x is computed to be and the table is updated accordingly
In the second iteration nodes v and y are found to have the leastcost paths and we break the tie arbitrarily and add y to the set N so that N now contains u x and y
The cost to the remaining nodes not yet in N that is nodes v w and z are updated via line of the LS algorithm yielding the results shown in the third row in Table 
And so on 
When the LS algorithm terminates we have for each node its predecessor along the leastcost path from the source node
For each predecessor we also have its predecessor and so in this manner we can construct the entire path from the source to all destinations
The forwarding table in a node say node u can then be constructed from this information by storing for each destination the nexthop node on the leastcost path from u to the destination
shows the resulting leastcost paths and forwarding table in u for the network in Figure 
Least cost path and forwarding table for node u What is the computational complexity of this algorithm That is given n nodes not counting the source how much computation must be done in the worst case to find the leastcost paths from the source to all destinations In the first iteration we need to search through all n nodes to determine the node w not in N that has the minimum cost
In the second iteration we need to check n nodes to determine the minimum cost in the third iteration n nodes and so on
Overall the total number of nodes we need to search through over all the iterations is nn and thus we say that the preceding implementation of the LS algorithm has worstcase complexity of order n squared On
A more sophisticated implementation of this algorithm using a data structure known as a heap can find the minimum in line in logarithmic rather than linear time thus reducing the complexity
Before completing our discussion of the LS algorithm let us consider a pathology that can arise
shows a simple network topology where link costs are equal to the load carried on the link for example reflecting the delay that would be experienced
In this example link costs are not symmetric that is cu v equals cv u only if the load carried on both directions on the link u v is the same
In this example node z originates a unit of traffic destined for w node x also originates a unit of traffic destined for w and node y injects an amount of traffic equal to e also destined for w
The initial routing is shown in Figure .a with the link costs corresponding to the amount of traffic carried
When the LS algorithm is next run node y determines based on the link costs shown in Figure .a that the clockwise path to w has a cost of while the counterclockwise path to w which it had been using has a cost of e
Hence ys leastcost path to w is now clockwise
Similarly x determines that its new leastcost path to w is also clockwise resulting in costs shown in Figure .b
When the LS algorithm is run next nodes x y and z all detect a zerocost path to w in the counterclockwise direction and all route their traffic to the counterclockwise routes
The next time the LS algorithm is run x y and z all then route their traffic to the clockwise routes
What can be done to prevent such oscillations which can occur in any algorithm not just an LS algorithm that uses a congestion or delaybased link metric One solution would be to mandate that link costs not depend on the amount of traffic Figure 
Oscillations with congestionsensitive routing carriedan unacceptable solution since one goal of routing is to avoid highly congested for example highdelay links
Another solution is to ensure that not all routers run the LS algorithm at the same time
This seems a more reasonable solution since we would hope that even if routers ran the LS algorithm with the same periodicity the execution instance of the algorithm would not be the same at each node
Interestingly researchers have found that routers in the Internet can selfsynchronize among themselves Floyd Synchronization 
That is even though they initially execute the algorithm with the same period but at different instants of time the algorithm execution instance can eventually become and remain synchronized at the routers
One way to avoid such selfsynchronization is for each router to randomize the time it sends out a link advertisement
Having studied the LS algorithm lets consider the other major routing algorithm that is used in practice todaythe distancevector routing algorithm
The DistanceVector DV Routing Algorithm Whereas the LS algorithm is an algorithm using global information the distancevector DV algorithm is iterative asynchronous and distributed
It is distributed in that each node receives some information from one or more of its directly attached neighbors performs a calculation and then distributes the results of its calculation back to its neighbors
It is iterative in that this process continues on until no more information is exchanged between neighbors
Interestingly the algorithm is also selfterminatingthere is no signal that the computation should stop it just stops
The algorithm is asynchronous in that it does not require all of the nodes to operate in lockstep with each other
Well see that an asynchronous iterative self terminating distributed algorithm is much more interesting and fun than a centralized algorithm Before we present the DV algorithm it will prove beneficial to discuss an important relationship that exists among the costs of the leastcost paths
Let dxy be the cost of the leastcost path from node x to node y
Then the least costs are related by the celebrated BellmanFord equation namely dxyminvcxvdvy 
where the minv in the equation is taken over all of xs neighbors
The BellmanFord equation is rather intuitive
Indeed after traveling from x to v if we then take the leastcost path from v to y the path cost will be cxvdvy
Since we must begin by traveling to some neighbor v the least cost from x to y is the minimum of cxvdvy taken over all neighbors v
But for those who might be skeptical about the validity of the equation lets check it for source node u and destination node z in Figure 
The source node u has three neighbors nodes v x and w
By walking along various paths in the graph it is easy to see that dvz dxz and dwz
Plugging these values into Equation 
along with the costs cuv cux and cuw gives duzmin which is obviously true and which is exactly what the Dijskstra algorithm gave us for the same network
This quick verification should help relieve any skepticism you may have
The BellmanFord equation is not just an intellectual curiosity
It actually has significant practical importance the solution to the BellmanFord equation provides the entries in node xs forwarding table
To see this let v be any neighboring node that achieves the minimum in Equation 
Then if node x wants to send a packet to node y along a leastcost path it should first forward the packet to node v
Thus node xs forwarding table would specify node v as the nexthop router for the ultimate destination y
Another important practical contribution of the BellmanFord equation is that it suggests the form of the neighbor toneighbor communication that will take place in the DV algorithm
The basic idea is as follows
Each node x begins with Dxy an estimate of the cost of the leastcost path from itself to node y for all nodes y in N
Let DxDxy y in N be node xs distance vector which is the vector of cost estimates from x to all other nodes y in N
With the DV algorithm each node x maintains the following routing information For each neighbor v the cost cx v from x to directly attached neighbor v Node xs distance vector that is DxDxy y in N containing xs estimate of its cost to all destinations y in N The distance vectors of each of its neighbors that is DvDvy y in N for each neighbor v of x In the distributed asynchronous algorithm from time to time each node sends a copy of its distance vector to each of its neighbors
When a node x receives a new distance vector from any of its neighbors w it saves ws distance vector and then uses the BellmanFord equation to update its own distance vector as follows DxyminvcxvDvy for each node y in N If node xs distance vector has changed as a result of this update step node x will then send its updated distance vector to each of its neighbors which can in turn update their own distance vectors
Miraculously enough as long as all the nodes continue to exchange their distance vectors in an asynchronous fashion each cost estimate Dxy converges to dxy the actual cost of the leastcost path from node x to node y Bertsekas DistanceVector DV Algorithm At each node x Initialization for all destinations y in N Dxy cx y if y is not a neighbor then cx y for each neighbor w Dwy for all destinations y in N for each neighbor w send distance vector Dx Dxy y in N to w loop wait until I see a link cost change to some neighbor w or until I receive a distance vector from some neighbor w for each y in N Dxy minvcx v Dvy if Dxy changed for any destination y send distance vector Dx Dxy y in N to all neighbors forever In the DV algorithm a node x updates its distancevector estimate when it either sees a cost change in one of its directly attached links or receives a distancevector update from some neighbor
But to update its own forwarding table for a given destination y what node x really needs to know is not the shortestpath distance to y but instead the neighboring node vy that is the nexthop router along the shortest path to y
As you might expect the nexthop router vy is the neighbor v that achieves the minimum in Line of the DV algorithm
If there are multiple neighbors v that achieve the minimum then vy can be any of the minimizing neighbors
Thus in Lines for each destination y node x also determines vy and updates its forwarding table for destination y
Recall that the LS algorithm is a centralized algorithm in the sense that it requires each node to first obtain a complete map of the network before running the Dijkstra algorithm
The DV algorithm is decentralized and does not use such global information
Indeed the only information a node will have is the costs of the links to its directly attached neighbors and information it receives from these neighbors
Each node waits for an update from any neighbor Lines calculates its new distance vector when receiving an update Line and distributes its new distance vector to its neighbors Lines 
DVlike algorithms are used in many routing protocols in practice including the Internets RIP and BGP ISO IDRP Novell IPX and the original ARPAnet
illustrates the operation of the DV algorithm for the simple threenode network shown at the top of the figure
The operation of the algorithm is illustrated in a synchronous manner where all nodes simultaneously receive distance vectors from their neighbors compute their new distance vectors and inform their neighbors if their distance vectors have changed
After studying this example you should convince yourself that the algorithm operates correctly in an asynchronous manner as well with node computations and update generationreception occurring at any time
The leftmost column of the figure displays three initial routing tables for each of the three nodes
For example the table in the upperleft corner is node xs initial routing table
Within a specific routing table each row is a distance vector specifically each nodes routing table includes its own distance vector and that of each of its neighbors
Thus the first row in node xs initial routing table is DxDxxDxyDxz
The second and third rows in this table are the most recently received distance vectors from nodes y and z respectively
Because at initialization node x has not received anything from node y or z the entries in the second and third rows are initialized to infinity
After initialization each node sends its distance vector to each of its two neighbors
This is illustrated in Figure 
by the arrows from the first column of tables to the second column of tables
For example node x sends its distance vector Dx to both nodes y and z
After receiving the updates each node recomputes its own distance vector
For example node x computes DxxDxymincxyDyycxzDzymin DxzmincxyDyzcxzDzzmin The second column therefore displays for each node the nodes new distance vector along with distance vectors just received from its neighbors
Note for example that Figure 
Distancevector DV algorithm in operation node xs estimate for the least cost to node z Dxz has changed from to 
Also note that for node x neighboring node y achieves the minimum in line of the DV algorithm thus at this stage of the algorithm we have at node x that vyy and vzy
After the nodes recompute their distance vectors they again send their updated distance vectors to their neighbors if there has been a change
This is illustrated in Figure 
by the arrows from the second column of tables to the third column of tables
Note that only nodes x and z send updates node ys distance vector didnt change so node y doesnt send an update
After receiving the updates the nodes then recompute their distance vectors and update their routing tables which are shown in the third column
The process of receiving updated distance vectors from neighbors recomputing routing table entries and informing neighbors of changed costs of the leastcost path to a destination continues until no update messages are sent
At this point since no update messages are sent no further routing table calculations will occur and the algorithm will enter a quiescent state that is all nodes will be performing the wait in Lines of the DV algorithm
The algorithm remains in the quiescent state until a link cost changes as discussed next
DistanceVector Algorithm LinkCost Changes and Link Failure When a node running the DV algorithm detects a change in the link cost from itself to a neighbor Lines it updates its distance vector Lines and if theres a change in the cost of the leastcost path informs its neighbors Lines of its new distance vector
Figure .a illustrates a scenario where the link cost from y to x changes from to 
We focus here only on y and zs distance table entries to destination x
The DV algorithm causes the following sequence of events to occur At time t y detects the linkcost change the cost has changed from to updates its distance vector and informs its neighbors of this change since its distance vector has changed
At time t z receives the update from y and updates its table
It computes a new least cost to x it has decreased from a cost of to a cost of and sends its new distance vector to its neighbors
At time t y receives zs update and updates its distance table
ys least costs do not change and hence y does not send any message to z
The algorithm comes to a quiescent state
Thus only two iterations are required for the DV algorithm to reach a quiescent state
The good news about the decreased cost between x and y has propagated quickly through the network
Changes in link cost Lets now consider what can happen when a link cost increases
Suppose that the link cost between x and y increases from to as shown in Figure .b
Before the link cost changes Dyx Dyz Dzy and Dzx
At time t y detects the link cost change the cost has changed from to 
y computes its new minimumcost path to x to have a cost of DyxmincyxDxx cyzDzxmin Of course with our global view of the network we can see that this new cost via z is wrong
But the only information node y has is that its direct cost to x is and that z has last told y that z could get to x with a cost of 
So in order to get to x y would now route through z fully expecting that z will be able to get to x with a cost of 
As of t we have a routing loopin order to get to x y routes through z and z routes through y
A routing loop is like a black holea packet destined for x arriving at y or z as of t will bounce back and forth between these two nodes forever or until the forwarding tables are changed
Since node y has computed a new minimum cost to x it informs z of its new distance vector at time t
Sometime after t z receives ys new distance vector which indicates that ys minimum cost to x is 
z knows it can get to y with a cost of and hence computes a new least cost to x of Dzxmin
Since zs least cost to x has increased it then informs y of its new distance vector at t
In a similar manner after receiving zs new distance vector y determines Dyx and sends z its distance vector
z then determines Dzx and sends y its distance vector and so on
How long will the process continue You should convince yourself that the loop will persist for iterations message exchanges between y and zuntil z eventually computes the cost of its path via y to be greater than 
At this point z will finally determine that its leastcost path to x is via its direct connection to x
y will then route to x via z
The result of the bad news about the increase in link cost has indeed traveled slowly What would have happened if the link cost cy x had changed from to and the cost cz x had been Because of such scenarios the problem we have seen is sometimes referred to as the counttoinfinity problem
DistanceVector Algorithm Adding Poisoned Reverse The specific looping scenario just described can be avoided using a technique known as poisoned reverse
The idea is simpleif z routes through y to get to destination x then z will advertise to y that its distance to x is infinity that is z will advertise to y that Dzx even though z knows Dzx in truth
z will continue telling this little white lie to y as long as it routes to x via y
Since y believes that z has no path to x y will never attempt to route to x via z as long as z continues to route to x via y and lies about doing so
Lets now see how poisoned reverse solves the particular looping problem we encountered before in Figure .b
As a result of the poisoned reverse ys distance table indicates Dzx
When the cost of the x y link changes from to at time t y updates its table and continues to route directly to x albeit at a higher cost of and informs z of its new cost to x that is Dyx
After receiving the update at t z immediately shifts its route to x to be via the direct z x link at a cost of 
Since this is a new leastcost path to x and since the path no longer passes through y z now informs y that Dzx at t
After receiving the update from z y updates its distance table with Dyx
Also since z is now on ys least cost path to x y poisons the reverse path from z to x by informing z at time t that Dyx even though y knows that Dyx in truth
Does poisoned reverse solve the general counttoinfinity problem It does not
You should convince yourself that loops involving three or more nodes rather than simply two immediately neighboring nodes will not be detected by the poisoned reverse technique
A Comparison of LS and DV Routing Algorithms The DV and LS algorithms take complementary approaches toward computing routing
In the DV algorithm each node talks to only its directly connected neighbors but it provides its neighbors with least cost estimates from itself to all the nodes that it knows about in the network
The LS algorithm requires global information
Consequently when implemented in each and every router e.g
as in Figure 
each node would need to communicate with all other nodes via broadcast but it tells them only the costs of its directly connected links
Lets conclude our study of LS and DV algorithms with a quick comparison of some of their attributes
Recall that N is the set of nodes routers and E is the set of edges links
We have seen that LS requires each node to know the cost of each link in the network
This requires ON E messages to be sent
Also whenever a link cost changes the new link cost must be sent to all nodes
The DV algorithm requires message exchanges between directly connected neighbors at each iteration
We have seen that the time needed for the algorithm to converge can depend on many factors
When link costs change the DV algorithm will propagate the results of the changed link cost only if the new link cost results in a changed leastcost path for one of the nodes attached to that link
Speed of convergence
We have seen that our implementation of LS is an ON algorithm requiring ON E messages
The DV algorithm can converge slowly and can have routing loops while the algorithm is converging
DV also suffers from the counttoinfinity problem
What can happen if a router fails misbehaves or is sabotaged Under LS a router could broadcast an incorrect cost for one of its attached links but no others
A node could also corrupt or drop any packets it received as part of an LS broadcast
But an LS node is computing only its own forwarding tables other nodes are performing similar calculations for themselves
This means route calculations are somewhat separated under LS providing a degree of robustness
Under DV a node can advertise incorrect leastcost paths to any or all destinations
Indeed in a malfunctioning router in a small ISP provided national backbone routers with erroneous routing information
This caused other routers to flood the malfunctioning router with traffic and caused large portions of the Internet to become disconnected for up to several hours Neumann 
More generally we note that at each iteration a nodes calculation in DV is passed on to its neighbor and then indirectly to its neighbors neighbor on the next iteration
In this sense an incorrect node calculation can be diffused through the entire network under DV
In the end neither algorithm is an obvious winner over the other indeed both algorithms are used in the Internet
IntraAS Routing in the Internet OSPF In our study of routing algorithms so far weve viewed the network simply as a collection of interconnected routers
One router was indistinguishable from another in the sense that all routers executed the same routing algorithm to compute routing paths through the entire network
In practice this model and its view of a homogenous set of routers all executing the same routing algorithm is simplistic for two important reasons Scale
As the number of routers becomes large the overhead involved in communicating computing and storing routing information becomes prohibitive
Todays Internet consists of hundreds of millions of routers
Storing routing information for possible destinations at each of these routers would clearly require enormous amounts of memory
The overhead required to broadcast connectivity and link cost updates among all of the routers would be huge A distancevector algorithm that iterated among such a large number of routers would surely never converge
Clearly something must be done to reduce the complexity of route computation in a network as large as the Internet
As described in Section 
the Internet is a network of ISPs with each ISP consisting of its own network of routers
An ISP generally desires to operate its network as it pleases for example to run whatever routing algorithm it chooses within its network or to hide aspects of its networks internal organization from the outside
Ideally an organization should be able to operate and administer its network as it wishes while still being able to connect its network to other outside networks
Both of these problems can be solved by organizing routers into autonomous systems ASs with each AS consisting of a group of routers that are under the same administrative control
Often the routers in an ISP and the links that interconnect them constitute a single AS
Some ISPs however partition their network into multiple ASs
In particular some tier ISPs use one gigantic AS for their entire network whereas others break up their ISP into tens of interconnected ASs
An autonomous system is identified by its globally unique autonomous system number ASN RFC 
AS numbers like IP addresses are assigned by ICANN regional registries ICANN 
Routers within the same AS all run the same routing algorithm and have information about each other
The routing algorithm running within an autonomous system is called an intraautonomous system routing protocol
Open Shortest Path First OSPF OSPF routing and its closely related cousin ISIS are widely used for intraAS routing in the Internet
The Open in OSPF indicates that the routing protocol specification is publicly available for example as opposed to Ciscos EIGRP protocol which was only recently became open Savage after roughly years as a Ciscoproprietary protocol
The most recent version of OSPF version is defined in RFC a public document
OSPF is a linkstate protocol that uses flooding of linkstate information and a Dijkstras leastcost path algorithm
With OSPF each router constructs a complete topological map that is a graph of the entire autonomous system
Each router then locally runs Dijkstras shortestpath algorithm to determine a shortestpath tree to all subnets with itself as the root node
Individual link costs are configured by the network administrator see sidebar Principles and Practice Setting OSPF Weights
The administrator might choose to set all link costs to PRINCIPLES IN PRACTICE SETTING OSPF LINK WEIGHTS Our discussion of linkstate routing has implicitly assumed that link weights are set a routing algorithm such as OSPF is run and traffic flows according to the routing tables computed by the LS algorithm
In terms of cause and effect the link weights are given i.e
they come first and result via Dijkstras algorithm in routing paths that minimize overall cost
In this viewpoint link weights reflect the cost of using a link e.g
if link weights are inversely proportional to capacity then the use of highcapacity links would have smaller weight and thus be more attractive from a routing standpoint and Dijsktras algorithm serves to minimize overall cost
In practice the cause and effect relationship between link weights and routing paths may be reversed with network operators configuring link weights in order to obtain routing paths that achieve certain traffic engineering goals Fortz Fortz 
For example suppose a network operator has an estimate of traffic flow entering the network at each ingress point and destined for each egress point
The operator may then want to put in place a specific routing of ingresstoegress flows that minimizes the maximum utilization over all of the networks links
But with a routing algorithm such as OSPF the operators main knobs for tuning the routing of flows through the network are the link weights
Thus in order to achieve the goal of minimizing the maximum link utilization the operator must find the set of link weights that achieves this goal
This is a reversal of the cause and effect relationshipthe desired routing of flows is known and the OSPF link weights must be found such that the OSPF routing algorithm results in this desired routing of flows
thus achieving minimumhop routing or might choose to set the link weights to be inversely proportional to link capacity in order to discourage traffic from using lowbandwidth links
OSPF does not mandate a policy for how link weights are set that is the job of the network administrator but instead provides the mechanisms protocol for determining leastcost path routing for the given set of link weights
With OSPF a router broadcasts routing information to all other routers in the autonomous system not just to its neighboring routers
A router broadcasts linkstate information whenever there is a change in a links state for example a change in cost or a change in updown status
It also broadcasts a links state periodically at least once every minutes even if the links state has not changed
RFC notes that this periodic updating of link state advertisements adds robustness to the link state algorithm
OSPF advertisements are contained in OSPF messages that are carried directly by IP with an upperlayer protocol of for OSPF
Thus the OSPF protocol must itself implement functionality such as reliable message transfer and linkstate broadcast
The OSPF protocol also checks that links are operational via a HELLO message that is sent to an attached neighbor and allows an OSPF router to obtain a neighboring routers database of networkwide link state
Some of the advances embodied in OSPF include the following Security
Exchanges between OSPF routers for example linkstate updates can be authenticated
With authentication only trusted routers can participate in the OSPF protocol within an AS thus preventing malicious intruders or networking students taking their newfound knowledge out for a joyride from injecting incorrect information into router tables
By default OSPF packets between routers are not authenticated and could be forged
Two types of authentication can be configured simple and MD see Chapter for a discussion on MD and authentication in general
With simple authentication the same password is configured on each router
When a router sends an OSPF packet it includes the password in plaintext
Clearly simple authentication is not very secure
MD authentication is based on shared secret keys that are configured in all the routers
For each OSPF packet that it sends the router computes the MD hash of the content of the OSPF packet appended with the secret key
See the discussion of message authentication codes in Chapter 
Then the router includes the resulting hash value in the OSPF packet
The receiving router using the preconfigured secret key will compute an MD hash of the packet and compare it with the hash value that the packet carries thus verifying the packets authenticity
Sequence numbers are also used with MD authentication to protect against replay attacks
Multiple samecost paths
When multiple paths to a destination have the same cost OSPF allows multiple paths to be used that is a single path need not be chosen for carrying all traffic when multiple equalcost paths exist
Integrated support for unicast and multicast routing
Multicast OSPF MOSPF RFC provides simple extensions to OSPF to provide for multicast routing
MOSPF uses the existing OSPF link database and adds a new type of linkstate advertisement to the existing OSPF linkstate broadcast mechanism
Support for hierarchy within a single AS
An OSPF autonomous system can be configured hierarchically into areas
Each area runs its own OSPF linkstate routing algorithm with each router in an area broadcasting its link state to all other routers in that area
Within each area one or more area border routers are responsible for routing packets outside the area
Lastly exactly one OSPF area in the AS is configured to be the backbone area
The primary role of the backbone area is to route traffic between the other areas in the AS
The backbone always contains all area border routers in the AS and may contain nonborder routers as well
Interarea routing within the AS requires that the packet be first routed to an area border router intraarea routing then routed through the backbone to the area border router that is in the destination area and then routed to the final destination
OSPF is a relatively complex protocol and our coverage here has been necessarily brief Huitema Moy RFC provide additional details
Routing Among the ISPs BGP We just learned that OSPF is an example of an intraAS routing protocol
When routing a packet between a source and destination within the same AS the route the packet follows is entirely determined by the intraAS routing protocol
However to route a packet across multiple ASs say from a smartphone in Timbuktu to a server in a datacenter in Silicon Valley we need an interautonomous system routing protocol
Since an interAS routing protocol involves coordination among multiple ASs communicating ASs must run the same interAS routing protocol
In fact in the Internet all ASs run the same interAS routing protocol called the Border Gateway Protocol more commonly known as BGP RFC Stewart 
BGP is arguably the most important of all the Internet protocols the only other contender would be the IP protocol that we studied in Section 
as it is the protocol that glues the thousands of ISPs in the Internet together
As we will soon see BGP is a decentralized and asynchronous protocol in the vein of distancevector routing described in Section 
Although BGP is a complex and challenging protocol to understand the Internet on a deep level we need to become familiar with its underpinnings and operation
The time we devote to learning BGP will be well worth the effort
The Role of BGP To understand the responsibilities of BGP consider an AS and an arbitrary router in that AS
Recall that every router has a forwarding table which plays the central role in the process of forwarding arriving packets to outbound router links
As we have learned for destinations that are within the same AS the entries in the routers forwarding table are determined by the ASs intraAS routing protocol
But what about destinations that are outside of the AS This is precisely where BGP comes to the rescue
In BGP packets are not routed to a specific destination address but instead to CIDRized prefixes with each prefix representing a subnet or a collection of subnets
In the world of BGP a destination may take the form 
which for this example includes IP addresses
Thus a routers forwarding table will have entries of the form x I where x is a prefix such as 
and I is an interface number for one of the routers interfaces
As an interAS routing protocol BGP provides each router a means to 
Obtain prefix reachability information from neighboring ASs
In particular BGP allows each subnet to advertise its existence to the rest of the Internet
A subnet screams I exist and I am here and BGP makes sure that all the routers in the Internet know about this subnet
If it werent for BGP each subnet would be an isolated islandalone unknown and unreachable by the rest of the Internet
Determine the best routes to the prefixes
A router may learn about two or more different routes to a specific prefix
To determine the best route the router will locally run a BGP route selection procedure using the prefix reachability information it obtained via neighboring routers
The best route will be determined based on policy as well as the reachability information
Let us now delve into how BGP carries out these two tasks
Advertising BGP Route Information Consider the network shown in Figure 
As we can see this simple network has three autonomous systems AS AS and AS
As shown AS includes a subnet with prefix x
For each AS each router is either a gateway router or an internal router
A gateway router is a router on the edge of an AS that directly connects to one or more routers in other ASs
An internal router connects only to hosts and routers within its own AS
In AS for example router c is a gateway router routers a b and d are internal routers
Lets consider the task of advertising reachability information for prefix x to all of the routers shown in Figure 
At a high level this is straightforward
First AS sends a BGP message to AS saying that x exists and is in AS lets denote this message as AS x
Then AS sends a BGP message to AS saying that x exists and that you can get to x by first passing through AS and then going to AS lets denote that message as AS AS x
In this manner each of the autonomous systems will not only learn about the existence of x but also learn about a path of autonomous systems that leads to x
Although the discussion in the above paragraph about advertising BGP reachability information should get the general idea across it is not precise in the sense that autonomous systems do not actually send messages to each other but instead routers do
To understand this lets now reexamine the example in Figure 
In BGP Figure 
Network with three autonomous systems
AS includes a subnet with prefix x pairs of routers exchange routing information over semipermanent TCP connections using port 
Each such TCP connection along with all the BGP messages sent over the connection is called a BGP connection
Furthermore a BGP connection that spans two ASs is called an external BGP eBGP connection and a BGP session between routers in the same AS is called an internal BGP iBGP connection
Examples of BGP connections for the network in Figure 
are shown in Figure 
There is typically one eBGP connection for each link that directly connects gateway routers in different ASs thus in Figure 
there is an eBGP connection between gateway routers c and a and an eBGP connection between gateway routers c and a
There are also iBGP connections between routers within each of the ASs
In particular Figure 
displays a common configuration of one BGP connection for each pair of routers internal to an AS creating a mesh of TCP connections within each AS
In Figure 
the eBGP connections are shown with the long dashes the iBGP connections are shown with the short dashes
Note that iBGP connections do not always correspond to physical links
In order to propagate the reachability information both iBGP and eBGP sessions are used
Consider again advertising the reachability information for prefix x to all routers in AS and AS
In this process gateway router a first sends an eBGP message AS x to gateway router c
Gateway router c then sends the iBGP message AS x to all of the other routers in AS including to gateway router a
Gateway router a then sends the eBGP message AS AS x to gateway router c
eBGP and iBGP connections Finally gateway router c uses iBGP to send the message AS AS x to all the routers in AS
After this process is complete each router in AS and AS is aware of the existence of x and is also aware of an AS path that leads to x
Of course in a real network from a given router there may be many different paths to a given destination each through a different sequence of ASs
For example consider the network in Figure 
which is the original network in Figure 
with an additional physical link from router d to router d
In this case there are two paths from AS to x the path AS AS x via router c and the new path AS x via the router d
Determining the Best Routes As we have just learned there may be many paths from a given router to a destination subnet
In fact in the Internet routers often receive reachability information about dozens of different possible paths
How does a router choose among these paths and then configure its forwarding table accordingly Before addressing this critical question we need to introduce a little more BGP terminology
When a router advertises a prefix across a BGP connection it includes with the prefix several BGP attributes
In BGP jargon a prefix along with its attributes is called a route
Two of the more important attributes are ASPATH and NEXTHOP
The ASPATH attribute contains the list of ASs through which the Figure 
Network augmented with peering link between AS and AS advertisement has passed as weve seen in our examples above
To generate the ASPATH value when a prefix is passed to an AS the AS adds its ASN to the existing list in the ASPATH
For example in Figure 
there are two routes from AS to subnet x one which uses the ASPATH AS AS and another that uses the ASPATH A
BGP routers also use the ASPATH attribute to detect and prevent looping advertisements specifically if a router sees that its own AS is contained in the path list it will reject the advertisement
Providing the critical link between the interAS and intraAS routing protocols the NEXTHOP attribute has a subtle but important use
The NEXTHOP is the IP address of the router interface that begins the ASPATH
To gain insight into this attribute lets again refer to Figure 
As indicated in Figure 
the NEXTHOP attribute for the route AS AS x from AS to x that passes through AS is the IP address of the left interface on router a
The NEXTHOP attribute for the route AS x from AS to x that bypasses AS is the IP address of the leftmost interface of router d
In summary in this toy example each router in AS becomes aware of two BGP routes to prefix x IP address of leftmost interface for router a AS AS x IP address of leftmost interface of router d AS x Here each BGP route is written as a list with three components NEXTHOP ASPATH destination prefix
In practice a BGP route includes additional attributes which we will ignore for the time being
Note that the NEXTHOP attribute is an IP address of a router that does not belong to AS however the subnet that contains this IP address directly attaches to AS
Hot Potato Routing We are now finally in position to talk about BGP routing algorithms in a precise manner
We will begin with one of the simplest routing algorithms namely hot potato routing
Consider router b in the network in Figure 
As just described this router will learn about two possible BGP routes to prefix x
In hot potato routing the route chosen from among all possible routes is that route with the least cost to the NEXTHOP router beginning that route
In this example router b will consult its intraAS routing information to find the leastcost intraAS path to NEXTHOP router a and the leastcost intraAS path to NEXTHOP router d and then select the route with the smallest of these leastcost paths
For example suppose that cost is defined as the number of links traversed
Then the least cost from router b to router a is the least cost from router b to router d is and router a would therefore be selected
Router b would then consult its forwarding table configured by its intraAS algorithm and find the interface I that is on the leastcost path to router a
It then adds x I to its forwarding table
The steps for adding an outsideAS prefix in a routers forwarding table for hot potato routing are summarized in Figure 
It is important to note that when adding an outsideAS prefix into a forwarding table both the interAS routing protocol BGP and the intraAS routing protocol e.g
OSPF are used
The idea behind hotpotato routing is for router b to get packets out of its AS as quickly as possible more specifically with the least cost possible without worrying about the cost of the remaining portions of the path outside of its AS to the destination
In the name hot potato routing a packet is analogous to a hot potato that is burning in your hands
Because it is burning hot you want to pass it off to another person another AS as quickly as possible
Hot potato routing is thus Figure 
Steps in adding outsideAS destination in a routers forwarding table a selfish algorithmit tries to reduce the cost in its own AS while ignoring the other components of the endtoend costs outside its AS
Note that with hot potato routing two routers in the same AS may choose two different AS paths to the same prefix
For example we just saw that router b would send packets through AS to reach x
However router d would bypass AS and send packets directly to AS to reach x
RouteSelection Algorithm In practice BGP uses an algorithm that is more complicated than hot potato routing but nevertheless incorporates hot potato routing
For any given destination prefix the input into BGPs routeselection algorithm is the set of all routes to that prefix that have been learned and accepted by the router
If there is only one such route then BGP obviously selects that route
If there are two or more routes to the same prefix then BGP sequentially invokes the following elimination rules until one route remains 
A route is assigned a local preference value as one of its attributes in addition to the ASPATH and NEXTHOP attributes
The local preference of a route could have been set by the router or could have been learned from another router in the same AS
The value of the local preference attribute is a policy decision that is left entirely up to the ASs network administrator
We will shortly discuss BGP policy issues in some detail
The routes with the highest local preference values are selected
From the remaining routes all with the same highest local preference value the route with the shortest ASPATH is selected
If this rule were the only rule for route selection then BGP would be using a DV algorithm for path determination where the distance metric uses the number of AS hops rather than the number of router hops
From the remaining routes all with the same highest local preference value and the same AS PATH length hot potato routing is used that is the route with the closest NEXTHOP router is selected
If more than one route still remains the router uses BGP identifiers to select the route see Stewart 
As an example lets again consider router b in Figure 
Recall that there are exactly two BGP routes to prefix x one that passes through AS and one that bypasses AS
Also recall that if hot potato routing on its own were used then BGP would route packets through AS to prefix x
But in the above routeselection algorithm rule is applied before rule causing BGP to select the route that bypasses AS since that route has a shorter AS PATH
So we see that with the above routeselection algorithm BGP is no longer a selfish algorithmit first looks for routes with short AS paths thereby likely reducing endtoend delay
As noted above BGP is the de facto standard for interAS routing for the Internet
To see the contents of various BGP routing tables large extracted from routers in tier ISPs see http www.routeviews.org
BGP routing tables often contain over half a million routes that is prefixes and corresponding attributes
Statistics about the size and characteristics of BGP routing tables are presented in Potaroo 
IPAnycast In addition to being the Internets interAS routing protocol BGP is often used to implement the IP anycast service RFC RFC which is commonly used in DNS
To motivate IPanycast consider that in many applications we are interested in replicating the same content on different servers in many different dispersed geographical locations and having each user access the content from the server that is closest
For example a CDN may replicate videos and other objects on servers in different countries
Similarly the DNS system can replicate DNS records on DNS servers throughout the world
When a user wants to access this replicated content it is desirable to point the user to the nearest server with the replicated content
BGPs routeselection algorithm provides an easy and natural mechanism for doing so
To make our discussion concrete lets describe how a CDN might use IPanycast
As shown in Figure 
during the IPanycast configuration stage the CDN company assigns the same IP address to each of its servers and uses standard BGP to advertise this IP address from each of the servers
When a BGP router receives multiple route advertisements for this IP address it treats these advertisements as providing different paths to the same physical location when in fact the advertisements are for different paths to different physical locations
When configuring its routing table each router will locally use the BGP routeselection algorithm to pick the best for example closest as determined by AShop counts route to that IP address
For example if one BGP route corresponding to one location is only one AS hop away from the router and all other BGP routes corresponding to other locations are two or more AS hops away then the BGP router would choose to route packets to the location that is one hop away
After this initial BGP addressadvertisement phase the CDN can do its main job of distributing content
When a client requests the video the CDN returns to the client the common IP address used by the geographically dispersed servers no matter where the client is located
When the client sends a request to that IP address Internet routers then forward the request packet to the closest server as defined by the BGP routeselection algorithm
Although the above CDN example nicely illustrates how IPanycast can be used in practice CDNs generally choose not to use IPanycast because BGP routing changes can result in different packets of the same TCP connection arriving at different instances of the Web server
But IPanycast is extensively used by the DNS system to direct DNS queries to the closest root DNS server
Recall from Section 
there are currently IP addresses for root DNS servers
But corresponding Figure 
Using IPanycast to bring users to the closest CDN server to each of these addresses there are multiple DNS root servers with some of these addresses having over DNS root servers scattered over all corners of the world
When a DNS query is sent to one of these IP addresses IP anycast is used to route the query to the nearest of the DNS root servers that is responsible for that address
Routing Policy When a router selects a route to a destination the AS routing policy can trump all other considerations such as shortest AS path or hot potato routing
Indeed in the routeselection algorithm routes are first selected according to the localpreference attribute whose value is fixed by the policy of the local AS
Lets illustrate some of the basic concepts of BGP routing policy with a simple example
shows six interconnected autonomous systems A B C W X and Y
It is important to note that A B C W X and Y are ASs not routers
Lets Figure 
A simple BGP policy scenario assume that autonomous systems W X and Y are access ISPs and that A B and C are backbone provider networks
Well also assume that A B and C directly send traffic to each other and provide full BGP information to their customer networks
All traffic entering an ISP access network must be destined for that network and all traffic leaving an ISP access network must have originated in that network
W and Y are clearly access ISPs
X is a multihomed access ISP since it is connected to the rest of the network via two different providers a scenario that is becoming increasingly common in practice
However like W and Y X itself must be the sourcedestination of all traffic leavingentering X
But how will this stub network behavior be implemented and enforced How will X be prevented from forwarding traffic between B and C This can easily be accomplished by controlling the manner in which BGP routes are advertised
In particular X will function as an access ISP network if it advertises to its neighbors B and C that it has no paths to any other destinations except itself
That is even though X may know of a path say XCY that reaches network Y it will not advertise this path to B
Since B is unaware that X has a path to Y B would never forward traffic destined to Y or C via X
This simple example illustrates how a selective route advertisement policy can be used to implement customerprovider routing relationships
Lets next focus on a provider network say AS B
Suppose that B has learned from A that A has a path AW to W
B can thus install the route AW into its routing information base
Clearly B also wants to advertise the path BAW to its customer X so that X knows that it can route to W via B
But should B advertise the path BAW to C If it does so then C could route traffic to W via BAW
If A B and C are all backbone providers than B might rightly feel that it should not have to shoulder the burden and cost of carrying transit traffic between A and C
B might rightly feel that it is As and Cs job and cost to make sure that C can route tofrom As customers via a direct connection between A and C
There are currently no official standards that govern how backbone ISPs route among themselves
However a rule of thumb followed by commercial ISPs is that any traffic flowing across an ISPs backbone network must have either a source or a destination or both in a network that is a customer of that ISP otherwise the traffic would be getting a free ride on the ISPs network
Individual peering agreements that would govern questions such as PRINCIPLES IN PRACTICE WHY ARE THERE DIFFERENT INTERAS AND INTRAAS ROUTING PROTOCOLS Having now studied the details of specific interAS and intraAS routing protocols deployed in todays Internet lets conclude by considering perhaps the most fundamental question we could ask about these protocols in the first place hopefully you have been wondering this all along and have not lost the forest for the trees Why are different interAS and intraAS routing protocols used The answer to this question gets at the heart of the differences between the goals of routing within an AS and among ASs Policy
Among ASs policy issues dominate
It may well be important that traffic originating in a given AS not be able to pass through another specific AS
Similarly a given AS may well want to control what transit traffic it carries between other ASs
We have seen that BGP carries path attributes and provides for controlled distribution of routing information so that such policybased routing decisions can be made
Within an AS everything is nominally under the same administrative control and thus policy issues play a much less important role in choosing routes within the AS
The ability of a routing algorithm and its data structures to scale to handle routing toamong large numbers of networks is a critical issue in interAS routing
Within an AS scalability is less of a concern
For one thing if a single ISP becomes too large it is always possible to divide it into two ASs and perform interAS routing between the two new ASs
Recall that OSPF allows such a hierarchy to be built by splitting an AS into areas
Because interAS routing is so policy oriented the quality for example performance of the routes used is often of secondary concern that is a longer or more costly route that satisfies certain policy criteria may well be taken over a route that is shorter but does not meet that criteria
Indeed we saw that among ASs there is not even the notion of cost other than AS hop count associated with routes
Within a single AS however such policy concerns are of less importance allowing routing to focus more on the level of performance realized on a route
those raised above are typically negotiated between pairs of ISPs and are often confidential Huston a provides an interesting discussion of peering agreements
For a detailed description of how routing policy reflects commercial relationships among ISPs see Gao Dmitiropoulos 
For a discussion of BGP routing polices from an ISP standpoint see Caesar b
This completes our brief introduction to BGP
Understanding BGP is important because it plays a central role in the Internet
We encourage you to see the references Griffin Stewart Labovitz Halabi Huitema Gao Feamster Caesar b Li to learn more about BGP
Putting the Pieces Together Obtaining Internet Presence Although this subsection is not about BGP per se it brings together many of the protocols and concepts weve seen thus far including IP addressing DNS and BGP
Suppose you have just created a small company that has a number of servers including a public Web server that describes your companys products and services a mail server from which your employees obtain their email messages and a DNS server
Naturally you would like the entire world to be able to visit your Web site in order to learn about your exciting products and services
Moreover you would like your employees to be able to send and receive email to potential customers throughout the world
To meet these goals you first need to obtain Internet connectivity which is done by contracting with and connecting to a local ISP
Your company will have a gateway router which will be connected to a router in your local ISP
This connection might be a DSL connection through the existing telephone infrastructure a leased line to the ISPs router or one of the many other access solutions described in Chapter 
Your local ISP will also provide you with an IP address range e.g
a address range consisting of addresses
Once you have your physical connectivity and your IP address range you will assign one of the IP addresses in your address range to your Web server one to your mail server one to your DNS server one to your gateway router and other IP addresses to other servers and networking devices in your companys network
In addition to contracting with an ISP you will also need to contract with an Internet registrar to obtain a domain name for your company as described in Chapter 
For example if your companys name is say Xanadu Inc
you will naturally try to obtain the domain name xanadu.com
Your company must also obtain presence in the DNS system
Specifically because outsiders will want to contact your DNS server to obtain the IP addresses of your servers you will also need to provide your registrar with the IP address of your DNS server
Your registrar will then put an entry for your DNS server domain name and corresponding IP address in the .com topleveldomain servers as described in Chapter 
After this step is completed any user who knows your domain name e.g
xanadu.com will be able to obtain the IP address of your DNS server via the DNS system
So that people can discover the IP addresses of your Web server in your DNS server you will need to include entries that map the host name of your Web server e.g
www.xanadu.com to its IP address
You will want to have similar entries for other publicly available servers in your company including your mail server
In this manner if Alice wants to browse your Web server the DNS system will contact your DNS server find the IP address of your Web server and give it to Alice
Alice can then establish a TCP connection directly with your Web server
However there still remains one other necessary and crucial step to allow outsiders from around the world to access your Web server
Consider what happens when Alice who knows the IP address of your Web server sends an IP datagram e.g
a TCP SYN segment to that IP address
This datagram will be routed through the Internet visiting a series of routers in many different ASs and eventually reach your Web server
When any one of the routers receives the datagram it is going to look for an entry in its forwarding table to determine on which outgoing port it should forward the datagram
Therefore each of the routers needs to know about the existence of your companys prefix or some aggregate entry
How does a router become aware of your companys prefix As we have just seen it becomes aware of it from BGP Specifically when your company contracts with a local ISP and gets assigned a prefix i.e
an address range your local ISP will use BGP to advertise your prefix to the ISPs to which it connects
Those ISPs will then in turn use BGP to propagate the advertisement
Eventually all Internet routers will know about your prefix or about some aggregate that includes your prefix and thus be able to appropriately forward datagrams destined to your Web and mail servers
The SDN Control Plane In this section well dive into the SDN control planethe networkwide logic that controls packet forwarding among a networks SDNenabled devices as well as the configuration and management of these devices and their services
Our study here builds on our earlier discussion of generalized SDN forwarding in Section 
so you might want to first review that section as well as Section 
of this chapter before continuing on
As in Section 
well again adopt the terminology used in the SDN literature and refer to the networks forwarding devices as packet switches or just switches with packet being understood since forwarding decisions can be made on the basis of networklayer sourcedestination addresses linklayer sourcedestination addresses as well as many other values in transport network and linklayer packetheader fields
Four key characteristics of an SDN architecture can be identified Kreutz Flowbased forwarding
Packet forwarding by SDNcontrolled switches can be based on any number of header field values in the transportlayer networklayer or linklayer header
We saw in Section 
that the OpenFlow
abstraction allows forwarding based on eleven different header field values
This contrasts sharply with the traditional approach to routerbased forwarding that we studied in Sections 
where forwarding of IP datagrams was based solely on a datagrams destination IP address
Recall from Figure 
that packet forwarding rules are specified in a switchs flow table it is the job of the SDN control plane to compute manage and install flow table entries in all of the networks switches
Separation of data plane and control plane
This separation is shown clearly in Figures 
The data plane consists of the networks switches relatively simple but fast devices that execute the match plus action rules in their flow tables
The control plane consists of servers and software that determine and manage the switches flow tables
Network control functions external to dataplane switches
Given that the S in SDN is for software its perhaps not surprising that the SDN control plane is implemented in software
Unlike traditional routers however this software executes on servers that are both distinct and remote from the networks switches
As shown in Figure 
the control plane itself consists of two components an SDN controller or network operating system Gude and a set of networkcontrol applications
The controller maintains accurate network state information e.g
the state of remote links switches and hosts provides this information to the networkcontrol applications running in the control plane and provides the means through which these applications can monitor program and control the underlying network devices
Although the controller in Figure 
is shown as a single central server in practice the controller is only logically centralized it is typically implemented on several servers that provide coordinated scalable performance and high availability
A programmable network
The network is programmable through the networkcontrol applications running in the control plane
These applications represent the brains of the SDN control plane using the APIs provided by the SDN controller to specify and control the data plane in the network devices
For example a routing networkcontrol application might determine the endend paths between sources and destinations e.g
by executing Dijkstras algorithm using the nodestate and linkstate information maintained by the SDN controller
Another network application might perform access control i.e
determine which packets are to be blocked at a switch as in our third example in Section 
Yet another application might forward packets in a manner that performs server load balancing the second example we considered in Section 
From this discussion we can see that SDN represents a significant unbundling of network functionality data plane switches SDN controllers and networkcontrol applications are separate entities that may each be provided by different vendors and organizations
This contrasts with the preSDN model in which a switchrouter together with its embedded control plane software and protocol implementations was monolithic vertically integrated and sold by a single vendor
This unbundling of network functionality in SDN has been likened to the earlier evolution from mainframe computers where hardware system software and applications were provided by a single vendor to personal computers with their separate hardware operating systems and applications
The unbundling of computing hardware system software and applications has arguably led to a rich open ecosystem driven by innovation in all three of these areas one hope for SDN is that it too will lead to a such rich innovation
Given our understanding of the SDN architecture of Figure 
many questions naturally arise
How and where are the flow tables actually computed How are these tables updated in response to events at SDNcontrolled devices e.g
an attached link going updown And how are the flow table entries at multiple switches coordinated in such a way as to result in orchestrated and consistent networkwide functionality e.g
endtoend paths for forwarding packets from sources to destinations or coordinated distributed firewalls It is the role of the SDN control plane to provide these and many other capabilities
Components of the SDN architecture SDNcontrolled switches the SDN controller networkcontrol applications 
The SDN Control Plane SDN Controller and SDN Networkcontrol Applications Lets begin our discussion of the SDN control plane in the abstract by considering the generic capabilities that the control plane must provide
As well see this abstract first principles approach will lead us to an overall architecture that reflects how SDN control planes have been implemented in practice
As noted above the SDN control plane divides broadly into two componentsthe SDN controller and the SDN networkcontrol applications
Lets explore the controller first
Many SDN controllers have been developed since the earliest SDN controller Gude see Kreutz for an extremely thorough and uptodate survey
provides a more detailed view of a generic SDN controller
A controllers functionality can be broadly organized into three layers
Lets consider these layers in an uncharacteristically bottomup fashion A communication layer communicating between the SDN controller and controlled network devices
Clearly if an SDN controller is going to control the operation of a remote SDNenabled switch host or other device a protocol is needed to transfer information between the controller and that device
In addition a device must be able to communicate locallyobserved events to the controller e.g
a message indicating that an attached link has gone up or down that a device has just joined the network or a heartbeat indicating that a device is up and operational
These events provide the SDN controller with an uptodate view of the networks state
This protocol constitutes the lowest layer of the controller architecture as shown in Figure 
The communication between the controller and the controlled devices cross what has come to be known as the controllers southbound interface
In Section 
well study OpenFlowa specific protocol that provides this communication functionality
OpenFlow is implemented in most if not all SDN controllers
A networkwide statemanagement layer
The ultimate control decisions made by the SDN control planee.g
configuring flow tables in all switches to achieve the desired endend forwarding to implement load balancing or to implement a particular firewalling capabilitywill require that the controller have uptodate information about state of the networks hosts links switches and other SDNcontrolled devices
A switchs flow table contains counters whose values might also be profitably used by networkcontrol applications these values should thus be available to the applications
Since the ultimate aim of the control plane is to determine flow tables for the various controlled devices a controller might also maintain a copy of these tables
These pieces of information all constitute examples of the networkwide state maintained by the SDN controller
The interface to the networkcontrol application layer
The controller interacts with network control applications through its northbound interface
This API Figure 
Components of an SDN controller allows networkcontrol applications to readwrite network state and flow tables within the state management layer
Applications can register to be notified when statechange events occur so that they can take actions in response to network event notifications sent from SDNcontrolled devices
Different types of APIs may be provided well see that two popular SDN controllers communicate with their applications using a REST Fielding requestresponse interface
We have noted several times that an SDN controller can be considered to be logically centralized i.e
that the controller may be viewed externally e.g
from the point of view of SDNcontrolled devices and external networkcontrol applications as a single monolithic service
However these services and the databases used to hold state information are implemented in practice by a distributed set of servers for fault tolerance high availability or for performance reasons
With controller functions being implemented by a set of servers the semantics of the controllers internal operations e.g
maintaining logical time ordering of events consistency consensus and more must be considered Panda 
Such concerns are common across many different distributed systems see Lamport Lampson for elegant solutions to these challenges
Modern controllers such as OpenDaylight OpenDaylight Lithium and ONOS ONOS see sidebar have placed considerable emphasis on architecting a logically centralized but physically distributed controller platform that provides scalable services and high availability to the controlled devices and networkcontrol applications alike
The architecture depicted in Figure 
closely resembles the architecture of the originally proposed NOX controller in Gude as well as that of todays OpenDaylight OpenDaylight Lithium and ONOS ONOS SDN controllers see sidebar
Well cover an example of controller operation in Section 
First however lets examine the OpenFlow protocol which lies in the controllers communication layer
OpenFlow Protocol The OpenFlow protocol OpenFlow ONF operates between an SDN controller and an SDNcontrolled switch or other device implementing the OpenFlow API that we studied earlier in Section 
The OpenFlow protocol operates over TCP with a default port number of 
Among the important messages flowing from the controller to the controlled switch are the following Configuration
This message allows the controller to query and set a switchs configuration parameters
This message is used by a controller to adddelete or modify entries in the switchs flow table and to set switch port properties
This message is used by a controller to collect statistics and counter values from the switchs flow table and ports
This message is used by the controller to send a specific packet out of a specified port at the controlled switch
The message itself contains the packet to be sent in its payload
Among the messages flowing from the SDNcontrolled switch to the controller are the following FlowRemoved
This message informs the controller that a flow table entry has been removed for example by a timeout or as the result of a received modifystate message
This message is used by a switch to inform the controller of a change in port status
Recall from Section 
that a packet arriving at a switch port and not matching any flow table entry is sent to the controller for additional processing
Matched packets may also be sent to the controller as an action to be taken on a match
The packetin message is used to send such packets to the controller
Additional OpenFlow messages are defined in OpenFlow ONF 
Principles in Practice Googles SoftwareDefined Global Network Recall from the case study in Section 
that Google deploys a dedicated widearea network WAN that interconnects its data centers and server clusters in IXPs and ISPs
This network called B has a Googledesigned SDN control plane built on OpenFlow
Googles network is able to drive WAN links at near utilization over the long run a two to three fold increase over typical link utilizations and split application flows among multiple paths based on application priority and existing flow demands Jain 
The Google B network is particularly it wellsuited for SDN i Google controls all devices from the edge servers in IXPs and ISPs to routers in their network core ii the most bandwidth intensive applications are largescale data copies between sites that can defer to higherpriority interactive applications during times of resource congestion iii with only a few dozen data centers being connected centralized control is feasible
Googles B network uses custombuilt switches each implementing a slightly extended version of OpenFlow with a local Open Flow Agent OFA that is similar in spirit to the control agent we encountered in Figure 
Each OFA in turn connects to an Open Flow Controller OFC in the network control server NCS using a separate out of band network distinct from the network that carries datacenter traffic between data centers
The OFC thus provides the services used by the NCS to communicate with its controlled switches similar in spirit to the lowest layer in the SDN architecture shown in Figure 
In B the OFC also performs state management functions keeping node and link status in a Network Information Base NIB
Googles implementation of the OFC is based on the ONIX SDN controller Koponen 
Two routing protocols BGP for routing between the data centers and ISIS a close relative of OSPF for routing within a data center are implemented
Paxos Chandra is used to execute hot replicas of NCS components to protect against failure
A traffic engineering networkcontrol application sitting logically above the set of network control servers interacts with these servers to provide global networkwide bandwidth provisioning for groups of application flows
With B SDN made an important leap forward into the operational networks of a global network provider
See Jain for a detailed description of B
Data and Control Plane Interaction An Example In order to solidify our understanding of the interaction between SDNcontrolled switches and the SDN controller lets consider the example shown in Figure 
in which Dijkstras algorithm which we studied in Section 
is used to determine shortest path routes
The SDN scenario in Figure 
has two important differences from the earlier perroutercontrol scenario of Sections 
where Dijkstras algorithm was implemented in each and every router and linkstate updates were flooded among all network routers Dijkstras algorithm is executed as a separate application outside of the packet switches
Packet switches send link updates to the SDN controller and not to each other
In this example lets assume that the link between switch s and s goes down that shortest path routing is implemented and consequently and that incoming and outgoing flow forwarding rules at s s and s are affected but that ss Figure 
SDN controller scenario Linkstate change operation is unchanged
Lets also assume that OpenFlow is used as the communication layer protocol and that the control plane performs no other function other than linkstate routing
Switch s experiencing a link failure between itself and s notifies the SDN controller of the linkstate change using the OpenFlow portstatus message
The SDN controller receives the OpenFlow message indicating the linkstate change and notifies the linkstate manager which updates a linkstate database
The networkcontrol application that implements Dijkstras linkstate routing has previously registered to be notified when link state changes
That application receives the notification of the linkstate change
The linkstate routing application interacts with the linkstate manager to get updated link state it might also consult other components in the statemanagement layer
It then computes the new leastcost paths
The linkstate routing application then interacts with the flow table manager which determines the flow tables to be updated
The flow table manager then uses the OpenFlow protocol to update flow table entries at affected switchess which will now route packets destined to s via s s which will now begin receiving packets from s via intermediate switch s and s which must now forward packets from s destined to s
This example is simple but illustrates how the SDN control plane provides controlplane services in this case networklayer routing that had been previously implemented with perrouter control exercised in each and every network router
One can now easily appreciate how an SDNenabled ISP could easily switch from leastcost path routing to a more handtailored approach to routing
Indeed since the controller can tailor the flow tables as it pleases it can implement any form of forwarding that it pleases simply by changing its applicationcontrol software
This ease of change should be contrasted to the case of a traditional perrouter control plane where software in all routers which might be provided to the ISP by multiple independent vendors must be changed
SDN Past and Future Although the intense interest in SDN is a relatively recent phenomenon the technical roots of SDN and the separation of the data and control planes in particular go back considerably further
In Feamster Lakshman RFC all argued for the separation of the networks data and control planes
van der Merwe describes a control framework for ATM networks Black with multiple controllers each controlling a number of ATM switches
The Ethane project Casado pioneered the notion of a network of simple flowbased Ethernet switches with matchplusaction flow tables a centralized controller that managed flow admission and routing and the forwarding of unmatched packets from the switch to the controller
A network of more than Ethane switches was operational in 
Ethane quickly evolved into the OpenFlow project and the rest as the saying goes is history Numerous research efforts are aimed at developing future SDN architectures and capabilities
As we have seen the SDN revolution is leading to the disruptive replacement of dedicated monolithic switches and routers with both data and control planes by simple commodity switching hardware and a sophisticated software control plane
A generalization of SDN known as network functions virtualization NFV similarly aims at disruptive replacement of sophisticated middleboxes such as middleboxes with dedicated hardware and proprietary software for media cachingservice with simple commodity servers switching and storage GemberJacobson 
A second area of important research seeks to extend SDN concepts from the intraAS setting to the interAS setting Gupta 
PRINCIPLES IN PRACTICE SDN Controller Case Studies The OpenDaylight and ONOS Controllers In the earliest days of SDN there was a single SDN protocol OpenFlow McKeown OpenFlow and a single SDN controller NOX Gude 
Since then the number of SDN controllers in particular has grown significantly Kreutz 
Some SDN controllers are companyspecific and proprietary e.g
ONIX Koponen Juniper Networks Contrail Juniper Contrail and Googles controller Jain for its B widearea network
But many more controllers are opensource and implemented in a variety of programming languages Erickson 
Most recently the OpenDaylight controller OpenDaylight Lithium and the ONOS controller ONOS have found considerable industry support
They are both opensource and are being developed in partnership with the Linux Foundation
The OpenDaylight Controller Figure 
presents a simplified view of the OpenDaylight Lithium SDN controller platform OpenDaylight Lithium 
ODLs main set of controller components correspond closely to those we developed in Figure 
NetworkService Applications are the applications that determine how dataplane forwarding and other services such as firewalling and load balancing are accomplished in the controlled switches
Unlike the canonical controller in Figure 
the ODL controller has two interfaces through which applications may communicate with native controller services and each other external applications communicate with controller modules using a REST requestresponse API running over HTTP
Internal applications communicate with each other via the Service Abstraction Layer SAL
The choice as to whether a controller application is implemented externally or internally is up to the application designer Figure 
The OpenDaylight controller the particular configuration of applications shown in Figure 
is only meant as an example
ODLs Basic NetworkService Functions are at the heart of the controller and they correspond closely to the networkwide state management capabilities that we encountered in Figure 
The SAL is the controllers nerve center allowing controller components and applications to invoke each others services and to subscribe to events they generate
It also provides a uniform abstract interface to the specific underlying communications protocols in the communication layer including OpenFlow and SNMP the Simple Network Management Protocola network management protocol that we will cover in Section 
OVSDB is a protocol used to manage data center switching an important application area for SDN technology
Well introduce data center networking in Chapter 
ONOS controller architecture The ONOS Controller Figure 
presents a simplified view of the ONOS controller ONOS 
Similar to the canonical controller in Figure 
three layers can be identified in the ONOS controller Northbound abstractions and protocols
A unique feature of ONOS is its intent framework which allows an application to request a highlevel service e.g
to setup a connection between host A and Host B or conversely to not allow Host A and host B to communicate without having to know the details of how this service is performed
State information is provided to networkcontrol applications across the northbound API either synchronously via query or asynchronously via listener callbacks e.g
when network state changes
The state of the networks links hosts and devices is maintained in ONOSs distributed core
ONOS is deployed as a service on a set of interconnected servers with each server running an identical copy of the ONOS software an increased number of servers offers an increased service capacity
The ONOS core provides the mechanisms for service replication and coordination among instances providing the applications above and the network devices below with the abstraction of logically centralized core services
Southbound abstractions and protocols
The southbound abstractions mask the heterogeneity of the underlying hosts links switches and protocols allowing the distributed core to be both device and protocol agnostic
Because of this abstraction the southbound interface below the distributed core is logically higher than in our canonical controller in Figure 
or the ODL controller in Figure 
ICMP The Internet Control Message Protocol The Internet Control Message Protocol ICMP specified in RFC is used by hosts and routers to communicate networklayer information to each other
The most typical use of ICMP is for error reporting
For example when running an HTTP session you may have encountered an error message such as Destination network unreachable
This message had its origins in ICMP
At some point an IP router was unable to find a path to the host specified in your HTTP request
That router created and sent an ICMP message to your host indicating the error
ICMP is often considered part of IP but architecturally it lies just above IP as ICMP messages are carried inside IP datagrams
That is ICMP messages are carried as IP payload just as TCP or UDP segments are carried as IP payload
Similarly when a host receives an IP datagram with ICMP specified as the upperlayer protocol an upperlayer protocol number of it demultiplexes the datagrams contents to ICMP just as it would demultiplex a datagrams content to TCP or UDP
ICMP messages have a type and a code field and contain the header and the first bytes of the IP datagram that caused the ICMP message to be generated in the first place so that the sender can determine the datagram that caused the error
Selected ICMP message types are shown in Figure 
Note that ICMP messages are used not only for signaling error conditions
The wellknown ping program sends an ICMP type code message to the specified host
The destination host seeing the echo request sends back a type code ICMP echo reply
Most TCPIP implementations support the ping server directly in the operating system that is the server is not a process
Chapter of Stevens provides the source code for the ping client program
Note that the client program needs to be able to instruct the operating system to generate an ICMP message of type code 
Another interesting ICMP message is the source quench message
This message is seldom used in practice
Its original purpose was to perform congestion controlto allow a congested router to send an ICMP source quench message to a host to force Figure 
ICMP message types that host to reduce its transmission rate
We have seen in Chapter that TCP has its own congestion control mechanism that operates at the transport layer without the use of networklayer feedback such as the ICMP source quench message
In Chapter we introduced the Traceroute program which allows us to trace a route from a host to any other host in the world
Interestingly Traceroute is implemented with ICMP messages
To determine the names and addresses of the routers between source and destination Traceroute in the source sends a series of ordinary IP datagrams to the destination
Each of these datagrams carries a UDP segment with an unlikely UDP port number
The first of these datagrams has a TTL of the second of the third of and so on
The source also starts timers for each of the datagrams
When the nth datagram arrives at the nth router the nth router observes that the TTL of the datagram has just expired
According to the rules of the IP protocol the router discards the datagram and sends an ICMP warning message to the source type code 
This warning message includes the name of the router and its IP address
When this ICMP message arrives back at the source the source obtains the roundtrip time from the timer and the name and IP address of the nth router from the ICMP message
How does a Traceroute source know when to stop sending UDP segments Recall that the source increments the TTL field for each datagram it sends
Thus one of the datagrams will eventually make it all the way to the destination host
Because this datagram contains a UDP segment with an unlikely port number the destination host sends a port unreachable ICMP message type code back to the source
When the source host receives this particular ICMP message it knows it does not need to send additional probe packets
The standard Traceroute program actually sends sets of three packets with the same TTL thus the Traceroute output provides three results for each TTL
In this manner the source host learns the number and the identities of routers that lie between it and the destination host and the roundtrip time between the two hosts
Note that the Traceroute client program must be able to instruct the operating system to generate UDP datagrams with specific TTL values and must also be able to be notified by its operating system when ICMP messages arrive
Now that you understand how Traceroute works you may want to go back and play with it some more
A new version of ICMP has been defined for IPv in RFC 
In addition to reorganizing the existing ICMP type and code definitions ICMPv also added new types and codes required by the new IPv functionality
These include the Packet Too Big type and an unrecognized IPv options error code
Network Management and SNMP Having now made our way to the end of our study of the network layer with only the linklayer before us were well aware that a network consists of many complex interacting pieces of hardware and software from the links switches routers hosts and other devices that comprise the physical components of the network to the many protocols that control and coordinate these devices
When hundreds or thousands of such components are brought together by an organization to form a network the job of the network administrator to keep the network up and running is surely a challenge
We saw in Section 
that the logically centralized controller can help with this process in an SDN context
But the challenge of network management has been around long before SDN with a rich set of network management tools and approaches that help the network administrator monitor manage and control the network
Well study these tools and techniques in this section
An oftenasked question is What is network management A wellconceived singlesentence albeit a rather long runon sentence definition of network management from Saydam is Network management includes the deployment integration and coordination of the hardware software and human elements to monitor test poll configure analyze evaluate and control the network and element resources to meet the realtime operational performance and Quality of Service requirements at a reasonable cost
Given this broad definition well cover only the rudiments of network management in this sectionthe architecture protocols and information base used by a network administrator in performing their task
Well not cover the administrators decisionmaking processes where topics such as fault identification Labovitz Steinder Feamster Wu Teixeira anomaly detection Lakhina Barford network designengineering to meet contracted Service Level Agreements SLAs Huston a and more come into consideration
Our focus is thus purposefully narrow the interested reader should consult these references the excellent networkmanagement text by Subramanian Subramanian and the more detailed treatment of network management available on the Web site for this text
The Network Management Framework Figure 
shows the key components of network management The managing server is an application typically with a human in the loop running in a centralized network management station in the network operations center NOC
The managing server is the locus of activity for network management it controls the collection processing analysis andor display of network management information
It is here that actions are initiated to control network behavior and here that the human network administrator interacts with the networks devices
A managed device is a piece of network equipment including its software that resides on a managed network
A managed device might be a host router switch middlebox modem thermometer or other networkconnected device
There may be several socalled managed objects within a managed device
These managed objects are the actual pieces of hardware within the managed device for example a network interface card is but one component of a host or router and configuration parameters for these hardware and software components for example an intra AS routing protocol such as OSPF
Each managed object within a managed device associated information that is collected into a Management Information Base MIB well see that the values of these pieces of information are available to and in many cases able to be set by the managing server
A MIB object might be a counter such as the number of IP datagrams discarded at a router due to errors in an IP datagram header or the number of UDP segments received at a host descriptive information such as the version of the software running on a DNS server status information such as whether a particular device is functioning correctly or protocolspecific information such as a routing path to a destination
MIB objects are specified in a data description language known as SMI Structure of Management Information RFC RFC RFC 
A formal definition language is used to ensure that the syntax and semantics of the network management data are well defined and unambiguous
Related MIB objects are gathered into MIB modules
As of mid there were nearly MIB modules defined by RFCs and a much larger number of vendorspecific private MIB modules
Also resident in each managed device is a network management agent a process running in the managed device that communicates with the managing server Figure 
Elements of network management Managing server managed devices MIB data remote agents SNMP taking local actions at the managed device under the command and control of the managing server
The network management agent is similar to the routing agent that we saw in Figure 
The final component of a network management framework is the network management protocol
The protocol runs between the managing server and the managed devices allowing the managing server to query the status of managed devices and indirectly take actions at these devices via its agents
Agents can use the network management protocol to inform the managing server of exceptional events for example component failures or violation of performance thresholds
Its important to note that the network management protocol does not itself manage the network
Instead it provides capabilities that a network administrator can use to manage monitor test poll configure analyze evaluate and control the network
This is a subtle but important distinction
In the following section well cover the Internets SNMP Simple Network Management Protocol protocol
The Simple Network Management Protocol SNMP The Simple Network Management Protocol version SNMPv RFC is an applicationlayer protocol used to convey networkmanagement control and information messages between a managing server and an agent executing on behalf of that managing server
The most common usage of SNMP is in a requestresponse mode in which an SNMP managing server sends a request to an SNMP agent who receives the request performs some action and sends a reply to the request
Typically a request will be used to query retrieve or modify set MIB object values associated with a managed device
A second common usage of SNMP is for an agent to send an unsolicited message known as a trap message to a managing server
Trap messages are used to notify a managing server of an exceptional situation e.g
a link interface going up or down that has resulted in changes to MIB object values
SNMPv defines seven types of messages known generically as protocol data unitsPDUsas shown in Table 
and described below
The format of the PDU is shown in Figure 
The GetRequest GetNextRequest and GetBulkRequest PDUs are all sent from a managing server to an agent to request the value of one or more MIB objects at the agents managed device
The MIB objects whose values are being Table 
SNMPv PDU types SNMPv PDU Senderreceiver Description Type GetRequest managerto get value of one or more MIB object instances agent GetNextRequest managerto get value of next MIB object instance in list or table agent GetBulkRequest managerto get values in large block of data for example values agent in a large table InformRequest managerto inform remote managing entity of MIB values remote manager to its access SetRequest managerto set value of one or more MIB object instances agent Response agentto generated in response to manager or managerto GetRequest manager GetNextRequest GetBulkRequest SetRequest PDU or InformRequest SNMPvTrap agentto inform manager of an exceptional event manager Figure 
SNMP PDU format GetRequest requested are specified in the variable binding portion of the PDU
GetNextRequest and GetBulkRequest differ in the granularity of their data requests
GetRequest can request an arbitrary set of MIB values multiple GetNextRequest s can be used to sequence through a list or table of MIB objects GetBulkRequest allows a large block of data to be returned avoiding the overhead incurred if multiple GetRequest or GetNextRequest messages were to be sent
In all three cases the agent responds with a Response PDU containing the object identifiers and their associated values
The SetRequest PDU is used by a managing server to set the value of one or more MIB objects in a managed device
An agent replies with a Response PDU with the noError error status to confirm that the value has indeed been set
The InformRequest PDU is used by a managing server to notify another managing server of MIB information that is remote to the receiving server
The Response PDU is typically sent from a managed device to the managing server in response to a request message from that server returning the requested information
The final type of SNMPv PDU is the trap message
Trap messages are generated asynchronously that is they are not generated in response to a received request but rather in response to an event for which the managing server requires notification
RFC defines wellknown trap types that include a cold or warm start by a device a link going up or down the loss of a neighbor or an authentication failure event
A received trap request has no required response from a managing server
Given the requestresponse nature of SNMP it is worth noting here that although SNMP PDUs can be carried via many different transport protocols the SNMP PDU is typically carried in the payload of a UDP datagram
Indeed RFC states that UDP is the preferred transport mapping
However since UDP is an unreliable transport protocol there is no guarantee that a request or its response will be received at the intended destination
The request ID field of the PDU see Figure 
is used by the managing server to number its requests to an agent the agents response takes its request ID from that of the received request
Thus the request ID field can be used by the managing server to detect lost requests or replies
It is up to the managing server to decide whether to retransmit a request if no corresponding response is received after a given amount of time
In particular the SNMP standard does not mandate any particular procedure for retransmission or even if retransmission is to be done in the first place
It only requires that the managing server needs to act responsibly in respect to the frequency and duration of retransmissions
This of course leads one to wonder how a responsible protocol should act SNMP has evolved through three versions
The designers of SNMPv have said that SNMPv can be thought of as SNMPv with additional security and administration capabilities RFC 
Certainly there are changes in SNMPv over SNMPv but nowhere are those changes more evident than in the area of administration and security
The central role of security in SNMPv was particularly important since the lack of adequate security resulted in SNMP being used primarily for monitoring rather than control for example SetRequest is rarely used in SNMPv
Once again we see that securitya topic well cover in detail in Chapter is of critical concern but once again a concern whose importance had been realized perhaps a bit late and only then added on
Summary We have now completed our twochapter journey into the network corea journey that began with our study of the network layers data plane in Chapter and finished here with our study of the network layers control plane
We learned that the control plane is the networkwide logic that controls not only how a datagram is forwarded among routers along an endtoend path from the source host to the destination host but also how networklayer components and services are configured and managed
We learned that there are two broad approaches towards building a control plane traditional perrouter control where a routing algorithm runs in each and every router and the routing component in the router communicates with the routing components in other routers and softwaredefined networking SDN control where a logically centralized controller computes and distributes the forwarding tables to be used by each and every router
We studied two fundamental routing algorithms for computing least cost paths in a graphlinkstate routing and distancevector routingin Section 
these algorithms find application in both perrouter control and in SDN control
These algorithms are the basis for two widely deployed Internet routing protocols OSPF and BGP that we covered in Sections 
We covered the SDN approach to the networklayer control plane in Section 
investigating SDN networkcontrol applications the SDN controller and the OpenFlow protocol for communicating between the controller and SDNcontrolled devices
In Sections 
we covered some of the nuts and bolts of managing an IP network ICMP the Internet Control Message Protocol and SNMP the Simple Network Management Protocol
Having completed our study of the network layer our journey now takes us one step further down the protocol stack namely to the link layer
Like the network layer the link layer is part of each and every networkconnected device
But we will see in the next chapter that the link layer has the much more localized task of moving packets between nodes on the same link or LAN
Although this task may appear on the surface to be rather simple compared with that of the network layers tasks we will see that the link layer involves a number of important and fascinating issues that can keep us busy for a long time
Homework Problems and Questions Chapter Review Questions SECTION 
What is meant by a control plane that is based on perrouter control In such cases when we say the network control and data planes are implemented monolithically what do we mean R
What is meant by a control plane that is based on logically centralized control In such cases are the data plane and the control plane implemented within the same device or in separate devices Explain
Compare and contrast the properties of a centralized and a distributed routing algorithm
Give an example of a routing protocol that takes a centralized and a decentralized approach
Compare and contrast linkstate and distancevector routing algorithms
What is the count to infinity problem in distance vector routing R
Is it necessary that every autonomous system use the same intraAS routing algorithm Why or why not SECTIONS 
Why are different interAS and intraAS protocols used in the Internet R
True or false When an OSPF route sends its link state information it is sent only to those nodes directly attached neighbors
What is meant by an area in an OSPF autonomous system Why was the concept of an area introduced R
Define and contrast the following terms subnet prefix and BGP route
How does BGP use the NEXTHOP attribute How does it use the ASPATH attribute R
Describe how a network administrator of an uppertier ISP can implement policy when configuring BGP
True or false When a BGP router receives an advertised path from its neighbor it must add its own identity to the received path and then send that new path on to all of its neighbors
Describe the main role of the communication layer the networkwide statemanagement layer and the networkcontrol application layer in an SDN controller
Suppose you wanted to implement a new routing protocol in the SDN control plane
At which layer would you implement that protocol Explain
What types of messages flow across an SDN controllers northbound and southbound APIs Who is the recipient of these messages sent from the controller across the southbound interface and who sends messages to the controller across the northbound interface R
Describe the purpose of two types of OpenFlow messages of your choosing that are sent from a controlled device to the controller
Describe the purpose of two types of Openflow messages of your choosing that are send from the controller to a controlled device
What is the purpose of the service abstraction layer in the OpenDaylight SDN controller SECTIONS 
Names four different types of ICMP messages R
What two types of ICMP messages are received at the sending host executing the Traceroute program R
Define the following terms in the context of SNMP managing server managed device network management agent and MIB
What are the purposes of the SNMP GetRequest and SetRequest messages R
What is the purpose of the SNMP trap message Problems P
Looking at Figure 
enumerate the paths from y to u that do not contain any loops
Repeat Problem P for paths from x to z z to u and z to w
Consider the following network
With the indicated link costs use Dijkstras shortestpath algorithm to compute the shortest path from x to all network nodes
Show how the algorithm works by computing a table similar to Table 
Dijkstras algorithm discussion and example P
Consider the network shown in Problem P
Using Dijkstras algorithm and showing your work using a table similar to Table 
do the following a
Compute the shortest path from t to all network nodes
Compute the shortest path from u to all network nodes
Compute the shortest path from v to all network nodes
Compute the shortest path from w to all network nodes
Compute the shortest path from y to all network nodes
Compute the shortest path from z to all network nodes
Consider the network shown below and assume that each node initially knows the costs to each of its neighbors
Consider the distancevector algorithm and show the distance table entries at node z
Consider a general topology that is not the specific network shown above and a synchronous version of the distancevector algorithm
Suppose that at each iteration a node exchanges its distance vectors with its neighbors and receives their distance vectors
Assuming that the algorithm begins with each node knowing only the costs to its immediate neighbors what is the maximum number of iterations required before the distributed algorithm converges Justify your answer
Consider the network fragment shown below
x has only two attached neighbors w and y
w has a minimumcost path to destination u not shown of and y has a minimumcost path to u of 
The complete paths from w and y to u and between w and y are not shown
All link costs in the network have strictly positive integer values
Give xs distance vector for destinations w y and u
Give a linkcost change for either cx w or cx y such that x will inform its neighbors of a new minimumcost path to u as a result of executing the distancevector algorithm
Give a linkcost change for either cx w or cx y such that x will not inform its neighbors of a new minimumcost path to u as a result of executing the distancevector algorithm
Consider the threenode topology shown in Figure 
Rather than having the link costs shown in Figure 
the link costs are cxy cyz czx
Compute the distance tables after the initialization step and after each iteration of a synchronous version of the distance vector algorithm as we did in our earlier discussion of Figure 
Consider the counttoinfinity problem in the distance vector routing
Will the counttoinfinity problem occur if we decrease the cost of a link Why How about if we connect two nodes which do not have a link P
Argue that for the distancevector algorithm in Figure 
each value in the distance vector Dx is nonincreasing and will eventually stabilize in a finite number of steps
Consider Figure 
Suppose there is another router w connected to router y and z
The costs of all links are given as follows cxy cxz cyw czw cyz
Suppose that poisoned reverse is used in the distancevector routing algorithm
When the distance vector routing is stabilized router w y and z inform their distances to x to each other
What distance values do they tell each other b
Now suppose that the link cost between x and y increases to 
Will there be a countto infinity problem even if poisoned reverse is used Why or why not If there is a countto infinity problem then how many iterations are needed for the distancevector routing to reach a stable state again Justify your answer
How do you modify cy z such that there is no counttoinfinity problem at all if cyx changes from to P
Describe how loops in paths can be detected in BGP
Will a BGP router always choose the loopfree route with the shortest ASpath length Justify your answer
Consider the network shown below
Suppose AS and AS are running OSPF for their intraAS routing protocol
Suppose AS and AS are running RIP for their intraAS routing protocol
Suppose eBGP and iBGP are used for the interAS routing protocol
Initially suppose there is no physical link between AS and AS
Router c learns about prefix x from which routing protocol OSPF RIP eBGP or iBGP b
Router a learns about x from which routing protocol c
Router c learns about x from which routing protocol d
Router d learns about x from which routing protocol P
Referring to the previous problem once router d learns about x it will put an entry x I in its forwarding table
Will I be equal to I or I for this entry Explain why in one sentence
Now suppose that there is a physical link between AS and AS shown by the dotted line
Suppose router d learns that x is accessible via AS as well as via AS
Will I be set to I or I Explain why in one sentence
Now suppose there is another AS called AS which lies on the path between AS and AS not shown in diagram
Suppose router d learns that x is accessible via AS AS AS as well as via AS AS
Will I be set to I or I Explain why in one sentence
Consider the following network
ISP B provides national backbone service to regional ISP A
ISP C provides national backbone service to regional ISP D
Each ISP consists of one AS
B and C peer with each other in two places using BGP
Consider traffic going from A to D
B would prefer to hand that traffic over to C on the West Coast so that C would have to absorb the cost of carrying the traffic crosscountry while C would prefer to get the traffic via its East Coast peering point with B so that B would have carried the traffic across the country
What BGP mechanism might C use so that B would hand over AtoD traffic at its East Coast peering point To answer this question you will need to dig into the BGP specification
In Figure 
consider the path information that reaches stub networks W X and Y
Based on the information available at W and X what are their respective views of the network topology Justify your answer
The topology view at Y is shown below
Consider Figure 
B would never forward traffic destined to Y via X based on BGP routing
But there are some very popular applications for which data packets go to X first and then flow to Y
Identify one such application and describe how data packets follow a path not given by BGP routing
In Figure 
suppose that there is another stub network V that is a customer of ISP A
Suppose that B and C have a peering relationship and A is a customer of both B and C
Suppose that A would like to have the traffic destined to W to come from B only and the traffic destined to V from either B or C
How should A advertise its routes to B and C What AS routes does C receive P
Suppose ASs X and Z are not directly connected but instead are connected by AS Y
Further suppose that X has a peering agreement with Y and that Y has a peering agreement with Z
Finally suppose that Z wants to transit all of Ys traffic but does not want to transit Xs traffic
Does BGP allow Z to implement this policy P
Consider the two ways in which communication occurs between a managing entity and a managed device requestresponse mode and trapping
What are the pros and cons of these two approaches in terms of overhead notification time when exceptional events occur and robustness with respect to lost messages between the managing entity and the device P
In Section 
we saw that it was preferable to transport SNMP messages in unreliable UDP datagrams
Why do you think the designers of SNMP chose UDP rather than TCP as the transport protocol of choice for SNMP Socket Programming Assignment At the end of Chapter there are four socket programming assignments
Below you will find a fifth assignment which employs ICMP a protocol discussed in this chapter
Assignment ICMP Ping Ping is a popular networking application used to test from a remote location whether a particular host is up and reachable
It is also often used to measure latency between the client host and the target host
It works by sending ICMP echo request packets i.e
ping packets to the target host and listening for ICMP echo response replies i.e
Ping measures the RRT records packet loss and calculates a statistical summary of multiple pingpong exchanges the minimum mean max and standard deviation of the roundtrip times
In this lab you will write your own Ping application in Python
Your application will use ICMP
But in order to keep your program simple you will not exactly follow the official specification in RFC 
Note that you will only need to write the client side of the program as the functionality needed on the server side is built into almost all operating systems
You can find full details of this assignment as well as important snippets of the Python code at the Web site httpwww.pearsonhighered.comcs resources
Programming Assignment In this programming assignment you will be writing a distributed set of procedures that implements a distributed asynchronous distancevector routing for the network shown below
You are to write the following routines that will execute asynchronously within the emulated environment provided for this assignment
For node you will write the routines rtinit
This routine will be called once at the beginning of the emulation
rtinit has no arguments
It should initialize your distance table in node to reflect the direct costs of and to nodes and respectively
In the figure above all links are bidirectional and the costs in both directions are identical
After initializing the distance table and any other data structures needed by your node routines it should then send its directly connected neighbors in this case and the cost of its minimumcost paths to all other network nodes
This minimumcost information is sent to neighboring nodes in a routing update packet by calling the routine tolayer as described in the full assignment
The format of the routing update packet is also described in the full assignment
rtupdatestruct rtpkt rcvdpkt
This routine will be called when node receives a routing packet that was sent to it by one of its directly connected neighbors
The parameter rcvdpkt is a pointer to the packet that was received
rtupdate is the heart of the distancevector algorithm
The values it receives in a routing update packet from some other node i contain is current shortestpath costs to all other network nodes
rtupdate uses these received values to update its own distance table as specified by the distancevector algorithm
If its own minimum cost to another node changes as a result of the update node informs its directly connected neighbors of this change in minimum cost by sending them a routing packet
Recall that in the distancevector algorithm only directly connected nodes will exchange routing packets
Thus nodes and will communicate with each other but nodes and will not communicate with each other
Similar routines are defined for nodes and 
Thus you will write eight procedures in all rtinit rtinit rtinit rtinit rtupdate rtupdate rtupdate and rtupdate
These routines will together implement a distributed asynchronous computation of the distance tables for the topology and costs shown in the figure on the preceding page
You can find the full details of the programming assignment as well as C code that you will need to create the simulated hardwaresoftware environment at httpwww.pearsonhighered.comcsresource
A Java version of the assignment is also available
Wireshark Lab In the Web site for this textbook www.pearsonhighered.comcsresources youll find a Wireshark lab assignment that examines the use of the ICMP protocol in the ping and traceroute commands
An Interview With Jennifer Rexford Jennifer Rexford is a Professor in the Computer Science department at Princeton University
Her research has the broad goal of making computer networks easier to design and manage with particular emphasis on routing protocols
From she was a member of the Network Management and Performance department at ATT LabsResearch
While at ATT she designed techniques and tools for network measurement traffic engineering and router configuration that were deployed in ATTs backbone network
Jennifer is coauthor of the book Web Protocols and Practice Networking Protocols Caching and Traffic Measurement published by AddisonWesley in May 
She served as the chair of ACM SIGCOMM from to 
She received her BSE degree in electrical engineering from Princeton University in and her PhD degree in electrical engineering and computer science from the University of Michigan in 
In Jennifer was the winner of ACMs Grace Murray Hopper Award for outstanding young computer professional and appeared on the MIT TR list of top innovators under the age of 
Please describe one or two of the most exciting projects you have worked on during your career
What were the biggest challenges When I was a researcher at ATT a group of us designed a new way to manage routing in Internet Service Provider backbone networks
Traditionally network operators configure each router individually and these routers run distributed protocols to compute paths through the network
We believed that network management would be simpler and more flexible if network operators could exercise direct control over how routers forward traffic based on a networkwide view of the topology and traffic
The Routing Control Platform RCP we designed and built could compute the routes for all of ATTs backbone on a single commodity computer and could control legacy routers without modification
To me this project was exciting because we had a provocative idea a working system and ultimately a real deployment in an operational network
Fast forward a few years and softwaredefined networking SDN has become a mainstream technology and standard protocols like OpenFlow have made it much easier to tell the underlying switches what to do
How do you think softwaredefined networking should evolve in the future In a major break from the past controlplane software can be created by many different programmers not just at companies selling network equipment
Yet unlike the applications running on a server or a smart phone controller apps must work together to handle the same traffic
Network operators do not want to perform load balancing on some traffic and routing on other traffic instead they want to perform load balancing and routing together on the same traffic
Future SDN controller platforms should offer good programming abstractions for composing independently written multiple controller applications together
More broadly good programming abstractions can make it easier to create controller applications without having to worry about lowlevel details like flow table entries traffic counters bit patterns in packet headers and so on
Also while an SDN controller is logically centralized the network still consists of a distributed collection of devices
Future controllers should offer good abstractions for updating the flow tables across the network so apps can reason about what happens to packets in flight while the devices are updated
Programming abstractions for controlplane software is an exciting area for interdisciplinary research between computer networking distributed systems and programming languages with a real chance for practical impact in the years ahead
Where do you see the future of networking and the Internet Networking is an exciting field because the applications and the underlying technologies change all the time
We are always reinventing ourselves Who would have predicted even ten years ago the dominance of smart phones allowing mobile users to access existing applications as well as new locationbased services The emergence of cloud computing is fundamentally changing the relationship between users and the applications they run and networked sensors and actuators the Internet of Things are enabling a wealth of new applications and security vulnerabilities
The pace of innovation is truly inspiring
The underlying network is a crucial component in all of these innovations
Yet the network is notoriously in the waylimiting performance compromising reliability constraining applications and complicating the deployment and management of services
We should strive to make the network of the future as invisible as the air we breathe so it never stands in the way of new ideas and valuable services
To do this we need to raise the level of abstraction above individual network devices and protocols and their attendant acronyms so we can reason about the network and the users highlevel goals as a whole
What people inspired you professionally Ive long been inspired by Sally Floyd at the International Computer Science Institute
Her research is always purposeful focusing on the important challenges facing the Internet
She digs deeply into hard questions until she understands the problem and the space of solutions completely and she devotes serious energy into making things happen such as pushing her ideas into protocol standards and network equipment
Also she gives back to the community through professional service in numerous standards and research organizations and by creating tools such as the widely used ns and ns simulators that enable other researchers to succeed
She retired in but her influence on the field will be felt for years to come
What are your recommendations for students who want careers in computer science and networking Networking is an inherently interdisciplinary field
Applying techniques from other disciplines breakthroughs in networking come from such diverse areas as queuing theory game theory control theory distributed systems network optimization programming languages machine learning algorithms data structures and so on
I think that becoming conversant in a related field or collaborating closely with experts in those fields is a wonderful way to put networking on a stronger foundation so we can learn how to build networks that are worthy of societys trust
Beyond the theoretical disciplines networking is exciting because we create real artifacts that real people use
Mastering how to design and build systemsby gaining experience in operating systems computer architecture and so onis another fantastic way to amplify your knowledge of networking to help make the world a better place
Chapter The Link Layer and LANs In the previous two chapters we learned that the network layer provides a communication service between any two network hosts
Between the two hosts datagrams travel over a series of communication links some wired and some wireless starting at the source host passing through a series of packet switches switches and routers and ending at the destination host
As we continue down the protocol stack from the network layer to the link layer we naturally wonder how packets are sent across the individual links that make up the endtoend communication path
How are the network layer datagrams encapsulated in the linklayer frames for transmission over a single link Are different linklayer protocols used in the different links along the communication path How are transmission conflicts in broadcast links resolved Is there addressing at the link layer and if so how does the link layer addressing operate with the networklayer addressing we learned about in Chapter And what exactly is the difference between a switch and a router Well answer these and other important questions in this chapter
In discussing the link layer well see that there are two fundamentally different types of linklayer channels
The first type are broadcast channels which connect multiple hosts in wireless LANs satellite networks and hybrid fibercoaxial cable HFC access networks
Since many hosts are connected to the same broadcast communication channel a socalled medium access protocol is needed to coordinate frame transmission
In some cases a central controller may be used to coordinate transmissions in other cases the hosts themselves coordinate transmissions
The second type of linklayer channel is the pointtopoint communication link such as that often found between two routers connected by a longdistance link or between a users office computer and the nearby Ethernet switch to which it is connected
Coordinating access to a pointtopoint link is simpler the reference material on this books Web site has a detailed discussion of the PointtoPoint Protocol PPP which is used in settings ranging from dialup service over a telephone line to highspeed pointtopoint frame transport over fiberoptic links
Well explore several important linklayer concepts and technologies in this chapter
Well dive deeper into error detection and correction a topic we touched on briefly in Chapter 
Well consider multiple access networks and switched LANs including Ethernetby far the most prevalent wired LAN technology
Well also look at virtual LANs and data center networks
Although WiFi and more generally wireless LANs are linklayer topics well postpone our study of these important topics until Chapter 
Introduction to the Link Layer Lets begin with some important terminology
Well find it convenient in this chapter to refer to any device that runs a linklayer i.e
layer protocol as a node
Nodes include hosts routers switches and WiFi access points discussed in Chapter 
We will also refer to the communication channels that connect adjacent nodes along the communication path as links
In order for a datagram to be transferred from source host to destination host it must be moved over each of the individual links in the endtoend path
As an example in the company network shown at the bottom of Figure 
consider sending a datagram from one of the wireless hosts to one of the servers
This datagram will actually pass through six links a WiFi link between sending host and WiFi access point an Ethernet link between the access point and a linklayer switch a link between the linklayer switch and the router a link between the two routers an Ethernet link between the router and a linklayer switch and finally an Ethernet link between the switch and the server
Over a given link a transmitting node encapsulates the datagram in a link layer frame and transmits the frame into the link
In order to gain further insight into the link layer and how it relates to the network layer lets consider a transportation analogy
Consider a travel agent who is planning a trip for a tourist traveling from Princeton New Jersey to Lausanne Switzerland
The travel agent decides that it is most convenient for the tourist to take a limousine from Princeton to JFK airport then a plane from JFK airport to Genevas airport and finally a train from Genevas airport to Lausannes train station
Once the travel agent makes the three reservations it is the responsibility of the Princeton limousine company to get the tourist from Princeton to JFK it is the responsibility of the airline company to get the tourist from JFK to Geneva and it is the responsibility Figure 
Six linklayer hops between wireless host and server of the Swiss train service to get the tourist from Geneva to Lausanne
Each of the three segments of the trip is direct between two adjacent locations
Note that the three transportation segments are managed by different companies and use entirely different transportation modes limousine plane and train
Although the transportation modes are different they each provide the basic service of moving passengers from one location to an adjacent location
In this transportation analogy the tourist is a datagram each transportation segment is a link the transportation mode is a linklayer protocol and the travel agent is a routing protocol
The Services Provided by the Link Layer Although the basic service of any link layer is to move a datagram from one node to an adjacent node over a single communication link the details of the provided service can vary from one linklayer protocol to the next
Possible services that can be offered by a linklayer protocol include Framing
Almost all linklayer protocols encapsulate each networklayer datagram within a linklayer frame before transmission over the link
A frame consists of a data field in which the networklayer datagram is inserted and a number of header fields
The structure of the frame is specified by the linklayer protocol
Well see several different frame formats when we examine specific linklayer protocols in the second half of this chapter
A medium access control MAC protocol specifies the rules by which a frame is transmitted onto the link
For pointtopoint links that have a single sender at one end of the link and a single receiver at the other end of the link the MAC protocol is simple or nonexistentthe sender can send a frame whenever the link is idle
The more interesting case is when multiple nodes share a single broadcast linkthe socalled multiple access problem
Here the MAC protocol serves to coordinate the frame transmissions of the many nodes
When a linklayer protocol provides reliable delivery service it guarantees to move each networklayer datagram across the link without error
Recall that certain transportlayer protocols such as TCP also provide a reliable delivery service
Similar to a transportlayer reliable delivery service a linklayer reliable delivery service can be achieved with acknowledgments and retransmissions see Section 
A linklayer reliable delivery service is often used for links that are prone to high error rates such as a wireless link with the goal of correcting an error locallyon the link where the error occursrather than forcing an endtoend retransmission of the data by a transport or applicationlayer protocol
However linklayer reliable delivery can be considered an unnecessary overhead for low biterror links including fiber coax and many twistedpair copper links
For this reason many wired linklayer protocols do not provide a reliable delivery service
Error detection and correction
The linklayer hardware in a receiving node can incorrectly decide that a bit in a frame is zero when it was transmitted as a one and vice versa
Such bit errors are introduced by signal attenuation and electromagnetic noise
Because there is no need to forward a datagram that has an error many linklayer protocols provide a mechanism to detect such bit errors
This is done by having the transmitting node include errordetection bits in the frame and having the receiving node perform an error check
Recall from Chapters and that the Internets transport layer and network layer also provide a limited form of error detectionthe Internet checksum
Error detection in the link layer is usually more sophisticated and is implemented in hardware
Error correction is similar to error detection except that a receiver not only detects when bit errors have occurred in the frame but also determines exactly where in the frame the errors have occurred and then corrects these errors
Where Is the Link Layer Implemented Before diving into our detailed study of the link layer lets conclude this introduction by considering the question of where the link layer is implemented
Well focus here on an end system since we learned in Chapter that the link layer is implemented in a routers line card
Is a hosts link layer implemented in hardware or software Is it implemented on a separate card or chip and how does it interface with the rest of a hosts hardware and operating system components Figure 
shows a typical host architecture
For the most part the link layer is implemented in a network adapter also sometimes known as a network interface card NIC
At the heart of the network adapter is the linklayer controller usually a single specialpurpose chip that implements many of the linklayer services framing link access error detection and so on
Thus much of a linklayer controllers functionality is implemented in hardware
For example Intels adapter Intel implements the Ethernet protocols well study in Section 
the Atheros AR Atheros controller implements the 
WiFi protocols well study in Chapter 
Until the late s most network adapters were physically separate cards such as a PCMCIA card or a plugin card fitting into a PCs PCI card slot but increasingly network adapters are being integrated onto the hosts motherboard a socalled LANonmotherboard configuration
On the sending side the controller takes a datagram that has been created and stored in host memory by the higher layers of the protocol stack encapsulates the datagram in a linklayer frame filling in the frames various fields and then transmits the frame into the communication link following the link access protocol
On the receiving side a controller receives the entire frame and extracts the network layer datagram
If the link layer performs error detection then it is the sending controller that sets the errordetection bits in the frame header and it is the receiving controller that performs error detection
shows a network adapter attaching to a hosts bus e.g
a PCI or PCIX bus where it looks much like any other IO device to the other host Figure 
Network adapter Its relationship to other host components and to protocol stack functionality components
also shows that while most of the link layer is implemented in hardware part of the link layer is implemented in software that runs on the hosts CPU
The software components of the link layer implement higherlevel linklayer functionality such as assembling linklayer addressing information and activating the controller hardware
On the receiving side linklayer software responds to controller interrupts e.g
due to the receipt of one or more frames handling error conditions and passing a datagram up to the network layer
Thus the link layer is a combination of hardware and softwarethe place in the protocol stack where software meets hardware
Intel provides a readable overview as well as a detailed description of the XL controller from a software programming point of view
ErrorDetection and Correction Techniques In the previous section we noted that bitlevel error detection and correctiondetecting and correcting the corruption of bits in a linklayer frame sent from one node to another physically connected neighboring nodeare two services often provided by the link layer
We saw in Chapter that error detection and correction services are also often offered at the transport layer as well
In this section well examine a few of the simplest techniques that can be used to detect and in some cases correct such bit errors
A full treatment of the theory and implementation of this topic is itself the topic of many textbooks for example Schwartz or Bertsekas and our treatment here is necessarily brief
Our goal here is to develop an intuitive feel for the capabilities that errordetection and correction techniques provide and to see how a few simple techniques work and are used in practice in the link layer
illustrates the setting for our study
At the sending node data D to be protected against bit errors is augmented with errordetection and correction bits EDC
Typically the data to be protected includes not only the datagram passed down from the network layer for transmission across the link but also linklevel addressing information sequence numbers and other fields in the link frame header
Both D and EDC are sent to the receiving node in a linklevel frame
At the receiving node a sequence of bits D and EDC is received
Note that D and EDC may differ from the original D and EDC as a result of intransit bit flips
The receivers challenge is to determine whether or not D is the same as the original D given that it has only received D and EDC
The exact wording of the receivers decision in Figure 
we ask whether an error is detected not whether an error has occurred is important
Errordetection and correction techniques allow the receiver to sometimes but not always detect that bit errors have occurred
Even with the use of errordetection bits there still may be undetected bit errors that is the receiver may be unaware that the received information contains bit errors
As a Figure 
Errordetection and correction scenario consequence the receiver might deliver a corrupted datagram to the network layer or be unaware that the contents of a field in the frames header has been corrupted
We thus want to choose an error detection scheme that keeps the probability of such occurrences small
Generally more sophisticated errordetection andcorrection techniques that is those that have a smaller probability of allowing undetected bit errors incur a larger overheadmore computation is needed to compute and transmit a larger number of errordetection and correction bits
Lets now examine three techniques for detecting errors in the transmitted dataparity checks to illustrate the basic ideas behind error detection and correction checksumming methods which are more typically used in the transport layer and cyclic redundancy checks which are more typically used in the link layer in an adapter
Parity Checks Perhaps the simplest form of error detection is the use of a single parity bit
Suppose that the information to be sent D in Figure 
has d bits
In an even parity scheme the sender simply includes one additional bit and chooses its value such that the total number of s in the d bits the original information plus a parity bit is even
For odd parity schemes the parity bit value is chosen such that there is an odd number of s
illustrates an even parity scheme with the single parity bit being stored in a separate field
Receiver operation is also simple with a single parity bit
The receiver need only count the number of s in the received d bits
If an odd number of valued bits are found with an even parity scheme the receiver knows that at least one bit error has occurred
More precisely it knows that some odd number of bit errors have occurred
But what happens if an even number of bit errors occur You should convince yourself that this would result in an undetected error
If the probability of bit errors is small and errors can be assumed to occur independently from one bit to the next the probability of multiple bit errors in a packet would be extremely small
In this case a single parity bit might suffice
However measurements have shown that rather than occurring independently errors are often clustered together in bursts
Under burst error conditions the probability of undetected errors in a frame protected by singlebit parity can approach percent Spragins 
Clearly a more robust errordetection scheme is needed and fortunately is used in practice
But before examining errordetection schemes that are used in practice lets consider a simple Figure 
Onebit even parity generalization of onebit parity that will provide us with insight into errorcorrection techniques
shows a twodimensional generalization of the singlebit parity scheme
Here the d bits in D are divided into i rows and j columns
A parity value is computed for each row and for each column
The resulting ij parity bits comprise the linklayer frames errordetection bits
Suppose now that a single bit error occurs in the original d bits of information
With this two dimensional parity scheme the parity of both the column and the row containing the flipped bit will be in error
The receiver can thus not only detect the fact that a single bit error has occurred but can use the column and row indices of the column and row with parity errors to actually identify the bit that was corrupted and correct that error Figure 
shows an example in which the valued bit in position is corrupted and switched to a an error that is both detectable and correctable at the receiver
Although our discussion has focused on the original d bits of information a single error in the parity bits themselves is also detectable and correctable
Twodimensional parity can also detect but not correct any combination of two errors in a packet
Other properties of the twodimensional parity scheme are explored in the problems at the end of the chapter
Twodimensional even parity The ability of the receiver to both detect and correct errors is known as forward error correction FEC
These techniques are commonly used in audio storage and playback devices such as audio CDs
In a network setting FEC techniques can be used by themselves or in conjunction with linklayer ARQ techniques similar to those we examined in Chapter 
FEC techniques are valuable because they can decrease the number of sender retransmissions required
Perhaps more important they allow for immediate correction of errors at the receiver
This avoids having to wait for the roundtrip propagation delay needed for the sender to receive a NAK packet and for the retransmitted packet to propagate back to the receivera potentially important advantage for realtime network applications Rubenstein or links such as deepspace links with long propagation delays
Research examining the use of FEC in errorcontrol protocols includes Biersack Nonnenmacher Byers Shacham 
Checksumming Methods In checksumming techniques the d bits of data in Figure 
are treated as a sequence of kbit integers
One simple checksumming method is to simply sum these kbit integers and use the resulting sum as the errordetection bits
The Internet checksum is based on this approachbytes of data are treated as bit integers and summed
The s complement of this sum then forms the Internet checksum that is carried in the segment header
As discussed in Section 
the receiver checks the checksum by taking the s complement of the sum of the received data including the checksum and checking whether the result is all bits
If any of the bits are an error is indicated
RFC discusses the Internet checksum algorithm and its implementation in detail
In the TCP and UDP protocols the Internet checksum is computed over all fields header and data fields included
In IP the checksum is computed over the IP header since the UDP or TCP segment has its own checksum
In other protocols for example XTP Strayer one checksum is computed over the header and another checksum is computed over the entire packet
Checksumming methods require relatively little packet overhead
For example the checksums in TCP and UDP use only bits
However they provide relatively weak protection against errors as compared with cyclic redundancy check which is discussed below and which is often used in the link layer
A natural question at this point is Why is checksumming used at the transport layer and cyclic redundancy check used at the link layer Recall that the transport layer is typically implemented in software in a host as part of the hosts operating system
Because transportlayer error detection is implemented in software it is important to have a simple and fast errordetection scheme such as checksumming
On the other hand error detection at the link layer is implemented in dedicated hardware in adapters which can rapidly perform the more complex CRC operations
Feldmeier Feldmeier presents fast software implementation techniques for not only weighted checksum codes but CRC see below and other codes as well
Cyclic Redundancy Check CRC An errordetection technique used widely in todays computer networks is based on cyclic redundancy check CRC codes
CRC codes are also known as polynomial codes since it is possible to view the bit string to be sent as a polynomial whose coefficients are the and values in the bit string with operations on the bit string interpreted as polynomial arithmetic
CRC codes operate as follows
Consider the dbit piece of data D that the sending node wants to send to the receiving node
The sender and receiver must first agree on an r bit pattern known as a generator which we will denote as G
We will require that the most significant leftmost bit of G be a 
The key idea behind CRC codes is shown in Figure 
For a given piece of data D the sender will choose r additional bits R and append them to D such that the resulting dr bit pattern interpreted as a binary number is exactly divisible by G i.e
has no remainder using modulo arithmetic
The process of error checking with CRCs is thus simple The receiver divides the dr received bits by G
If the remainder is nonzero the receiver knows that an error has occurred otherwise the data is accepted as being correct
All CRC calculations are done in modulo arithmetic without carries in addition or borrows in subtraction
This means that addition and subtraction are identical and both are equivalent to the bitwise exclusiveor XOR of the operands
Thus for example XOR XOR Also we similarly have Multiplication and division are the same as in base arithmetic except that any required addition or subtraction is done without carries or borrows
As in regular Figure 
CRC binary arithmetic multiplication by k left shifts a bit pattern by k places
Thus given D and R the quantity DrXOR R yields the dr bit pattern shown in Figure 
Well use this algebraic characterization of the dr bit pattern from Figure 
in our discussion below
Let us now turn to the crucial question of how the sender computes R
Recall that we want to find R such that there is an n such that DrXOR RnG That is we want to choose R such that G divides into DrXOR R without remainder
If we XOR that is add modulo without carry R to both sides of the above equation we get DrnG XOR R This equation tells us that if we divide Dr by G the value of the remainder is precisely R
In other words we can calculate R as RremainderDrG Figure 
illustrates this calculation for the case of D d G and r
The bits transmitted in this case are 
You should check these calculations for yourself and also check that indeed DrG XOR R
A sample CRC calculation International standards have been defined for and bit generators G
The CRC bit standard which has been adopted in a number of linklevel IEEE protocols uses a generator of GCRC Each of the CRC standards can detect burst errors of fewer than r bits
This means that all consecutive bit errors of r bits or fewer will be detected
Furthermore under appropriate assumptions a burst of length greater than r bits is detected with probability .r
Also each of the CRC standards can detect any odd number of bit errors
See Williams for a discussion of implementing CRC checks
The theory behind CRC codes and even more powerful codes is beyond the scope of this text
The text Schwartz provides an excellent introduction to this topic
Multiple Access Links and Protocols In the introduction to this chapter we noted that there are two types of network links pointtopoint links and broadcast links
A pointtopoint link consists of a single sender at one end of the link and a single receiver at the other end of the link
Many linklayer protocols have been designed for pointtopoint links the pointtopoint protocol PPP and highlevel data link control HDLC are two such protocols
The second type of link a broadcast link can have multiple sending and receiving nodes all connected to the same single shared broadcast channel
The term broadcast is used here because when any one node transmits a frame the channel broadcasts the frame and each of the other nodes receives a copy
Ethernet and wireless LANs are examples of broadcast linklayer technologies
In this section well take a step back from specific linklayer protocols and first examine a problem of central importance to the link layer how to coordinate the access of multiple sending and receiving nodes to a shared broadcast channelthe multiple access problem
Broadcast channels are often used in LANs networks that are geographically concentrated in a single building or on a corporate or university campus
Thus well look at how multiple access channels are used in LANs at the end of this section
We are all familiar with the notion of broadcastingtelevision has been using it since its invention
But traditional television is a oneway broadcast that is one fixed node transmitting to many receiving nodes while nodes on a computer network broadcast channel can both send and receive
Perhaps a more apt human analogy for a broadcast channel is a cocktail party where many people gather in a large room the air providing the broadcast medium to talk and listen
A second good analogy is something many readers will be familiar witha classroomwhere teachers and students similarly share the same single broadcast medium
A central problem in both scenarios is that of determining who gets to talk that is transmit into the channel and when
As humans weve evolved an elaborate set of protocols for sharing the broadcast channel Give everyone a chance to speak
Dont speak until you are spoken to
Dont monopolize the conversation
Raise your hand if you have a question
Dont interrupt when someone is speaking
Dont fall asleep when someone is talking
Computer networks similarly have protocolssocalled multiple access protocolsby which nodes regulate their transmission into the shared broadcast channel
As shown in Figure 
multiple access protocols are needed in a wide variety of network settings including both wired and wireless access networks and satellite networks
Although technically each node accesses the broadcast channel through its adapter in this section we will refer to the node as the sending and Figure 
Various multiple access channels receiving device
In practice hundreds or even thousands of nodes can directly communicate over a broadcast channel
Because all nodes are capable of transmitting frames more than two nodes can transmit frames at the same time
When this happens all of the nodes receive multiple frames at the same time that is the transmitted frames collide at all of the receivers
Typically when there is a collision none of the receiving nodes can make any sense of any of the frames that were transmitted in a sense the signals of the colliding frames become inextricably tangled together
Thus all the frames involved in the collision are lost and the broadcast channel is wasted during the collision interval
Clearly if many nodes want to transmit frames frequently many transmissions will result in collisions and much of the bandwidth of the broadcast channel will be wasted
In order to ensure that the broadcast channel performs useful work when multiple nodes are active it is necessary to somehow coordinate the transmissions of the active nodes
This coordination job is the responsibility of the multiple access protocol
Over the past years thousands of papers and hundreds of PhD dissertations have been written on multiple access protocols a comprehensive survey of the first years of this body of work is Rom 
Furthermore active research in multiple access protocols continues due to the continued emergence of new types of links particularly new wireless links
Over the years dozens of multiple access protocols have been implemented in a variety of linklayer technologies
Nevertheless we can classify just about any multiple access protocol as belonging to one of three categories channel partitioning protocols random access protocols and takingturns protocols
Well cover these categories of multiple access protocols in the following three subsections
Lets conclude this overview by noting that ideally a multiple access protocol for a broadcast channel of rate R bits per second should have the following desirable characteristics 
When only one node has data to send that node has a throughput of R bps
When M nodes have data to send each of these nodes has a throughput of RM bps
This need not necessarily imply that each of the M nodes always has an instantaneous rate of RM but rather that each node should have an average transmission rate of RM over some suitably defined interval of time
The protocol is decentralized that is there is no master node that represents a single point of failure for the network
The protocol is simple so that it is inexpensive to implement
Channel Partitioning Protocols Recall from our early discussion back in Section 
that timedivision multiplexing TDM and frequencydivision multiplexing FDM are two techniques that can Figure 
A fournode TDM and FDM example be used to partition a broadcast channels bandwidth among all nodes sharing that channel
As an example suppose the channel supports N nodes and that the transmission rate of the channel is R bps
TDM divides time into time frames and further divides each time frame into N time slots
The TDM time frame should not be confused with the linklayer unit of data exchanged between sending and receiving adapters which is also called a frame
In order to reduce confusion in this subsection well refer to the linklayer unit of data exchanged as a packet
Each time slot is then assigned to one of the N nodes
Whenever a node has a packet to send it transmits the packets bits during its assigned time slot in the revolving TDM frame
Typically slot sizes are chosen so that a single packet can be transmitted during a slot time
shows a simple fournode TDM example
Returning to our cocktail party analogy a TDMregulated cocktail party would allow one partygoer to speak for a fixed period of time then allow another partygoer to speak for the same amount of time and so on
Once everyone had had a chance to talk the pattern would repeat
TDM is appealing because it eliminates collisions and is perfectly fair Each node gets a dedicated transmission rate of RN bps during each frame time
However it has two major drawbacks
First a node is limited to an average rate of RN bps even when it is the only node with packets to send
A second drawback is that a node must always wait for its turn in the transmission sequenceagain even when it is the only node with a frame to send
Imagine the partygoer who is the only one with anything to say and imagine that this is the even rarer circumstance where everyone wants to hear what that one person has to say
Clearly TDM would be a poor choice for a multiple access protocol for this particular party
While TDM shares the broadcast channel in time FDM divides the R bps channel into different frequencies each with a bandwidth of RN and assigns each frequency to one of the N nodes
FDM thus creates N smaller channels of RN bps out of the single larger R bps channel
FDM shares both the advantages and drawbacks of TDM
It avoids collisions and divides the bandwidth fairly among the N nodes
However FDM also shares a principal disadvantage with TDMa node is limited to a bandwidth of RN even when it is the only node with packets to send
A third channel partitioning protocol is code division multiple access CDMA
While TDM and FDM assign time slots and frequencies respectively to the nodes CDMA assigns a different code to each node
Each node then uses its unique code to encode the data bits it sends
If the codes are chosen carefully CDMA networks have the wonderful property that different nodes can transmit simultaneously and yet have their respective receivers correctly receive a senders encoded data bits assuming the receiver knows the senders code in spite of interfering transmissions by other nodes
CDMA has been used in military systems for some time due to its antijamming properties and now has widespread civilian use particularly in cellular telephony
Because CDMAs use is so tightly tied to wireless channels well save our discussion of the technical details of CDMA until Chapter 
For now it will suffice to know that CDMA codes like time slots in TDM and frequencies in FDM can be allocated to the multiple access channel users
Random Access Protocols The second broad class of multiple access protocols are random access protocols
In a random access protocol a transmitting node always transmits at the full rate of the channel namely R bps
When there is a collision each node involved in the collision repeatedly retransmits its frame that is packet until its frame gets through without a collision
But when a node experiences a collision it doesnt necessarily retransmit the frame right away
Instead it waits a random delay before retransmitting the frame
Each node involved in a collision chooses independent random delays
Because the random delays are independently chosen it is possible that one of the nodes will pick a delay that is sufficiently less than the delays of the other colliding nodes and will therefore be able to sneak its frame into the channel without a collision
There are dozens if not hundreds of random access protocols described in the literature Rom Bertsekas 
In this section well describe a few of the most commonly used random access protocolsthe ALOHA protocols Abramson Abramson Abramson and the carrier sense multiple access CSMA protocols Kleinrock b
Ethernet Metcalfe is a popular and widely deployed CSMA protocol
Slotted ALOHA Lets begin our study of random access protocols with one of the simplest random access protocols the slotted ALOHA protocol
In our description of slotted ALOHA we assume the following All frames consist of exactly L bits
Time is divided into slots of size LR seconds that is a slot equals the time to transmit one frame
Nodes start to transmit frames only at the beginnings of slots
The nodes are synchronized so that each node knows when the slots begin
If two or more frames collide in a slot then all the nodes detect the collision event before the slot ends
Let p be a probability that is a number between and 
The operation of slotted ALOHA in each node is simple When the node has a fresh frame to send it waits until the beginning of the next slot and transmits the entire frame in the slot
If there isnt a collision the node has successfully transmitted its frame and thus need not consider retransmitting the frame
The node can prepare a new frame for transmission if it has one
If there is a collision the node detects the collision before the end of the slot
The node retransmits its frame in each subsequent slot with probability p until the frame is transmitted without a collision
By retransmitting with probability p we mean that the node effectively tosses a biased coin the event heads corresponds to retransmit which occurs with probability p
The event tails corresponds to skip the slot and toss the coin again in the next slot this occurs with probability p
All nodes involved in the collision toss their coins independently
Slotted ALOHA would appear to have many advantages
Unlike channel partitioning slotted ALOHA allows a node to transmit continuously at the full rate R when that node is the only active node
A node is said to be active if it has frames to send
Slotted ALOHA is also highly decentralized because each node detects collisions and independently decides when to retransmit
Slotted ALOHA does however require the slots to be synchronized in the nodes shortly well discuss an unslotted version of the ALOHA protocol as well as CSMA protocols none of which require such synchronization
Slotted ALOHA is also an extremely simple protocol
Slotted ALOHA works well when there is only one active node but how efficient is it when there are multiple active nodes There are two possible efficiency Figure 
Nodes and collide in the first slot
Node finally succeeds in the fourth slot node in the eighth slot and node in the ninth slot concerns here
First as shown in Figure 
when there are multiple active nodes a certain fraction of the slots will have collisions and will therefore be wasted
The second concern is that another fraction of the slots will be empty because all active nodes refrain from transmitting as a result of the probabilistic transmission policy
The only unwasted slots will be those in which exactly one node transmits
A slot in which exactly one node transmits is said to be a successful slot
The efficiency of a slotted multiple access protocol is defined to be the longrun fraction of successful slots in the case when there are a large number of active nodes each always having a large number of frames to send
Note that if no form of access control were used and each node were to immediately retransmit after each collision the efficiency would be zero
Slotted ALOHA clearly increases the efficiency beyond zero but by how much We now proceed to outline the derivation of the maximum efficiency of slotted ALOHA
To keep this derivation simple lets modify the protocol a little and assume that each node attempts to transmit a frame in each slot with probability p
That is we assume that each node always has a frame to send and that the node transmits with probability p for a fresh frame as well as for a frame that has already suffered a collision
Suppose there are N nodes
Then the probability that a given slot is a successful slot is the probability that one of the nodes transmits and that the remaining N nodes do not transmit
The probability that a given node transmits is p the probability that the remaining nodes do not transmit is pN
Therefore the probability a given node has a success is ppN
Because there are N nodes the probability that any one of the N nodes has a success is NppN
Thus when there are N active nodes the efficiency of slotted ALOHA is NppN
To obtain the maximum efficiency for N active nodes we have to find the p that maximizes this expression
See the homework problems for a general outline of this derivation
And to obtain the maximum efficiency for a large number of active nodes we take the limit of NppN as N approaches infinity
Again see the homework problems
After performing these calculations well find that the maximum efficiency of the protocol is given by e
That is when a large number of nodes have many frames to transmit then at best only percent of the slots do useful work
Thus the effective transmission rate of the channel is not R bps but only 
R bps A similar analysis also shows that percent of the slots go empty and percent of slots have collisions
Imagine the poor network administrator who has purchased a Mbps slotted ALOHA system expecting to be able to use the network to transmit data among a large number of users at an aggregate rate of say Mbps Although the channel is capable of transmitting a given frame at the full channel rate of Mbps in the long run the successful throughput of this channel will be less than Mbps
ALOHA The slotted ALOHA protocol required that all nodes synchronize their transmissions to start at the beginning of a slot
The first ALOHA protocol Abramson was actually an unslotted fully decentralized protocol
In pure ALOHA when a frame first arrives that is a networklayer datagram is passed down from the network layer at the sending node the node immediately transmits the frame in its entirety into the broadcast channel
If a transmitted frame experiences a collision with one or more other transmissions the node will then immediately after completely transmitting its collided frame retransmit the frame with probability p
Otherwise the node waits for a frame transmission time
After this wait it then transmits the frame with probability p or waits remaining idle for another frame time with probability p
To determine the maximum efficiency of pure ALOHA we focus on an individual node
Well make the same assumptions as in our slotted ALOHA analysis and take the frame transmission time to be the unit of time
At any given time the probability that a node is transmitting a frame is p
Suppose this frame begins transmission at time t
As shown in Figure 
in order for this frame to be successfully transmitted no other nodes can begin their transmission in the interval of time tt
Such a transmission would overlap with the beginning of the transmission of node is frame
The probability that all other nodes do not begin a transmission in this interval is pN
Similarly no other node can begin a transmission while node i is transmitting as such a transmission would overlap with the latter part of node is transmission
The probability that all other nodes do not begin a transmission in this interval is also pN
Thus the probability that a given node has a successful transmission is ppN
By taking limits as in the slotted ALOHA case we find that the maximum efficiency of the pure ALOHA protocol is only eexactly half that of slotted ALOHA
This then is the price to be paid for a fully decentralized ALOHA protocol
Interfering transmissions in pure ALOHA Carrier Sense Multiple Access CSMA In both slotted and pure ALOHA a nodes decision to transmit is made independently of the activity of the other nodes attached to the broadcast channel
In particular a node neither pays attention to whether another node happens to be transmitting when it begins to transmit nor stops transmitting if another node begins to interfere with its transmission
In our cocktail party analogy ALOHA protocols are quite like a boorish partygoer who continues to chatter away regardless of whether other people are talking
As humans we have human protocols that allow us not only to behave with more civility but also to decrease the amount of time spent colliding with each other in conversation and consequently to increase the amount of data we exchange in our conversations
Specifically there are two important rules for polite human conversation Listen before speaking
If someone else is speaking wait until they are finished
In the networking world this is called carrier sensinga node listens to the channel before transmitting
If a frame from another node is currently being transmitted into the channel a node then waits until it detects no transmissions for a short amount of time and then begins transmission
If someone else begins talking at the same time stop talking
In the networking world this is called collision detectiona transmitting node listens to the channel while it is transmitting
If it detects that another node is transmitting an interfering frame it stops transmitting and waits a random amount of time before repeating the senseandtransmitwhenidle cycle
These two rules are embodied in the family of carrier sense multiple access CSMA and CSMA with collision detection CSMACD protocols Kleinrock b Metcalfe Lam Rom 
Many variations on CSMA and CASE HISTORY NORM ABRAMSON AND ALOHANET Norm Abramson a PhD engineer had a passion for surfing and an interest in packet switching
This combination of interests brought him to the University of Hawaii in 
Hawaii consists of many mountainous islands making it difficult to install and operate landbased networks
When not surfing Abramson thought about how to design a network that does packet switching over radio
The network he designed had one central host and several secondary nodes scattered over the Hawaiian Islands
The network had two channels each using a different frequency band
The downlink channel broadcasted packets from the central host to the secondary hosts and the upstream channel sent packets from the secondary hosts to the central host
In addition to sending informational packets the central host also sent on the downstream channel an acknowledgment for each packet successfully received from the secondary hosts
Because the secondary hosts transmitted packets in a decentralized fashion collisions on the upstream channel inevitably occurred
This observation led Abramson to devise the pure ALOHA protocol as described in this chapter
In with continued funding from ARPA Abramson connected his ALOHAnet to the ARPAnet
Abramsons work is important not only because it was the first example of a radio packet network but also because it inspired Bob Metcalfe
A few years later Metcalfe modified the ALOHA protocol to create the CSMACD protocol and the Ethernet LAN
CSMACD have been proposed
Here well consider a few of the most important and fundamental characteristics of CSMA and CSMACD
The first question that you might ask about CSMA is why if all nodes perform carrier sensing do collisions occur in the first place After all a node will refrain from transmitting whenever it senses that another node is transmitting
The answer to the question can best be illustrated using spacetime diagrams Molle 
shows a spacetime diagram of four nodes A B C D attached to a linear broadcast bus
The horizontal axis shows the position of each node in space the vertical axis represents time
At time t node B senses the channel is idle as no other nodes are currently transmitting
Node B thus begins transmitting with its bits propagating in both directions along the broadcast medium
The downward propagation of Bs bits in Figure 
with increasing time indicates that a nonzero amount of time is needed for Bs bits actually to propagate albeit at near the speed of light along the broadcast medium
At time ttt node D has a frame to send
Although node B is currently transmitting at time t the bits being transmitted by B have yet to reach D and thus D senses Figure 
Spacetime diagram of two CSMA nodes with colliding transmissions the channel idle at t
In accordance with the CSMA protocol D thus begins transmitting its frame
A short time later Bs transmission begins to interfere with Ds transmission at D
From Figure 
it is evident that the endtoend channel propagation delay of a broadcast channelthe time it takes for a signal to propagate from one of the nodes to anotherwill play a crucial role in determining its performance
The longer this propagation delay the larger the chance that a carriersensing node is not yet able to sense a transmission that has already begun at another node in the network
Carrier Sense Multiple Access with Collision Dection CSMACD In Figure 
nodes do not perform collision detection both B and D continue to transmit their frames in their entirety even though a collision has occurred
When a node performs collision detection it ceases transmission as soon as it detects a collision
shows the same scenario as in Figure 
except that the two Figure 
CSMA with collision detection nodes each abort their transmission a short time after detecting a collision
Clearly adding collision detection to a multiple access protocol will help protocol performance by not transmitting a useless damaged by interference with a frame from another node frame in its entirety
Before analyzing the CSMACD protocol let us now summarize its operation from the perspective of an adapter in a node attached to a broadcast channel 
The adapter obtains a datagram from the network layer prepares a linklayer frame and puts the frame adapter buffer
If the adapter senses that the channel is idle that is there is no signal energy entering the adapter from the channel it starts to transmit the frame
If on the other hand the adapter senses that the channel is busy it waits until it senses no signal energy and then starts to transmit the frame
While transmitting the adapter monitors for the presence of signal energy coming from other adapters using the broadcast channel
If the adapter transmits the entire frame without detecting signal energy from other adapters the adapter is finished with the frame
If on the other hand the adapter detects signal energy from other adapters while transmitting it aborts the transmission that is it stops transmitting its frame
After aborting the adapter waits a random amount of time and then returns to step 
The need to wait a random rather than fixed amount of time is hopefully clearif two nodes transmitted frames at the same time and then both waited the same fixed amount of time theyd continue colliding forever
But what is a good interval of time from which to choose the random backoff time If the interval is large and the number of colliding nodes is small nodes are likely to wait a large amount of time with the channel remaining idle before repeating the senseandtransmitwhenidle step
On the other hand if the interval is small and the number of colliding nodes is large its likely that the chosen random values will be nearly the same and transmitting nodes will again collide
What wed like is an interval that is short when the number of colliding nodes is small and long when the number of colliding nodes is large
The binary exponential backoff algorithm used in Ethernet as well as in DOCSIS cable network multiple access protocols DOCSIS elegantly solves this problem
Specifically when transmitting a frame that has already experienced n collisions a node chooses the value of K at random from n
Thus the more collisions experienced by a frame the larger the interval from which K is chosen
For Ethernet the actual amount of time a node waits is K bit times i.e
K times the amount of time needed to send bits into the Ethernet and the maximum value that n can take is capped at 
Lets look at an example
Suppose that a node attempts to transmit a frame for the first time and while transmitting it detects a collision
The node then chooses K with probability 
or chooses K with probability 
If the node chooses K then it immediately begins sensing the channel
If the node chooses K it waits bit times e.g
microseconds for a Mbps Ethernet before beginning the senseandtransmitwhenidle cycle
After a second collision K is chosen with equal probability from 
After three collisions K is chosen with equal probability from 
After or more collisions K is chosen with equal probability from 
Thus the size of the sets from which K is chosen grows exponentially with the number of collisions for this reason this algorithm is referred to as binary exponential backoff
We also note here that each time a node prepares a new frame for transmission it runs the CSMACD algorithm not taking into account any collisions that may have occurred in the recent past
So it is possible that a node with a new frame will immediately be able to sneak in a successful transmission while several other nodes are in the exponential backoff state
CSMACD Efficiency When only one node has a frame to send the node can transmit at the full channel rate e.g
for Ethernet typical rates are Mbps Mbps or Gbps
However if many nodes have frames to transmit the effective transmission rate of the channel can be much less
We define the efficiency of CSMACD to be the longrun fraction of time during which frames are being transmitted on the channel without collisions when there is a large number of active nodes with each node having a large number of frames to send
In order to present a closedform approximation of the efficiency of Ethernet let dprop denote the maximum time it takes signal energy to propagate between any two adapters
Let dtrans be the time to transmit a maximumsize frame approximately 
msecs for a Mbps Ethernet
A derivation of the efficiency of CSMACD is beyond the scope of this book see Lam and Bertsekas 
Here we simply state the following approximation Efficiencydpropdtrans We see from this formula that as dprop approaches the efficiency approaches 
This matches our intuition that if the propagation delay is zero colliding nodes will abort immediately without wasting the channel
Also as dtrans becomes very large efficiency approaches 
This is also intuitive because when a frame grabs the channel it will hold on to the channel for a very long time thus the channel will be doing productive work most of the time
TakingTurns Protocols Recall that two desirable properties of a multiple access protocol are when only one node is active the active node has a throughput of R bps and when M nodes are active then each active node has a throughput of nearly RM bps
The ALOHA and CSMA protocols have this first property but not the second
This has motivated researchers to create another class of protocolsthe takingturns protocols
As with random access protocols there are dozens of takingturns protocols and each one of these protocols has many variations
Well discuss two of the more important protocols here
The first one is the polling protocol
The polling protocol requires one of the nodes to be designated as a master node
The master node polls each of the nodes in a roundrobin fashion
In particular the master node first sends a message to node saying that it node can transmit up to some maximum number of frames
After node transmits some frames the master node tells node it node can transmit up to the maximum number of frames
The master node can determine when a node has finished sending its frames by observing the lack of a signal on the channel
The procedure continues in this manner with the master node polling each of the nodes in a cyclic manner
The polling protocol eliminates the collisions and empty slots that plague random access protocols
This allows polling to achieve a much higher efficiency
But it also has a few drawbacks
The first drawback is that the protocol introduces a polling delaythe amount of time required to notify a node that it can transmit
If for example only one node is active then the node will transmit at a rate less than R bps as the master node must poll each of the inactive nodes in turn each time the active node has sent its maximum number of frames
The second drawback which is potentially more serious is that if the master node fails the entire channel becomes inoperative
protocol and the Bluetooth protocol we will study in Section 
are examples of polling protocols
The second takingturns protocol is the tokenpassing protocol
In this protocol there is no master node
A small specialpurpose frame known as a token is exchanged among the nodes in some fixed order
For example node might always send the token to node node might always send the token to node and node N might always send the token to node 
When a node receives a token it holds onto the token only if it has some frames to transmit otherwise it immediately forwards the token to the next node
If a node does have frames to transmit when it receives the token it sends up to a maximum number of frames and then forwards the token to the next node
Token passing is decentralized and highly efficient
But it has its problems as well
For example the failure of one node can crash the entire channel
Or if a node accidentally neglects to release the token then some recovery procedure must be invoked to get the token back in circulation
Over the years many tokenpassing protocols have been developed including the fiber distributed data interface FDDI protocol Jain and the IEEE 
token ring protocol IEEE 
and each one had to address these as well as other sticky issues
DOCSIS The LinkLayer Protocol for Cable Internet Access In the previous three subsections weve learned about three broad classes of multiple access protocols channel partitioning protocols random access protocols and taking turns protocols
A cable access network will make for an excellent case study here as well find aspects of each of these three classes of multiple access protocols with the cable access network Recall from Section 
that a cable access network typically connects several thousand residential cable modems to a cable modem termination system CMTS at the cable network headend
The Data OverCable Service Interface Specifications DOCSIS DOCSIS specifies the cable data network architecture and its protocols
DOCSIS uses FDM to divide the downstream CMTS to modem and upstream modem to CMTS network segments into multiple frequency channels
Each downstream channel is MHz wide with a maximum throughput of approximately Mbps per channel although this data rate is seldom seen at a cable modem in practice each upstream channel has a maximum channel width of 
MHz and a maximum upstream throughput of approximately Mbps
Each upstream and Figure 
Upstream and downstream channels between CMTS and cable modems downstream channel is a broadcast channel
Frames transmitted on the downstream channel by the CMTS are received by all cable modems receiving that channel since there is just a single CMTS transmitting into the downstream channel however there is no multiple access problem
The upstream direction however is more interesting and technically challenging since multiple cable modems share the same upstream channel frequency to the CMTS and thus collisions can potentially occur
As illustrated in Figure 
each upstream channel is divided into intervals of time TDMlike each containing a sequence of minislots during which cable modems can transmit to the CMTS
The CMTS explicitly grants permission to individual cable modems to transmit during specific minislots
The CMTS accomplishes this by sending a control message known as a MAP message on a downstream channel to specify which cable modem with data to send can transmit during which minislot for the interval of time specified in the control message
Since minislots are explicitly allocated to cable modems the CMTS can ensure there are no colliding transmissions during a minislot
But how does the CMTS know which cable modems have data to send in the first place This is accomplished by having cable modems send minislotrequest frames to the CMTS during a special set of interval minislots that are dedicated for this purpose as shown in Figure 
These minislot request frames are transmitted in a random access manner and so may collide with each other
A cable modem can neither sense whether the upstream channel is busy nor detect collisions
Instead the cable modem infers that its minislotrequest frame experienced a collision if it does not receive a response to the requested allocation in the next downstream control message
When a collision is inferred a cable modem uses binary exponential backoff to defer the retransmission of its minislotrequest frame to a future time slot
When there is little traffic on the upstream channel a cable modem may actually transmit data frames during slots nominally assigned for minislotrequest frames and thus avoid having to wait for a minislot assignment
A cable access network thus serves as a terrific example of multiple access protocols in actionFDM TDM random access and centrally allocated time slots all within one network 
Switched Local Area Networks Having covered broadcast networks and multiple access protocols in the previous section lets turn our attention next to switched local networks
shows a switched local network connecting three departments two servers and a router with four switches
Because these switches operate at the link layer they switch linklayer frames rather than networklayer datagrams dont recognize networklayer addresses and dont use routing algorithms like RIP or OSPF to determine Figure 
An institutional network connected together by four switches paths through the network of layer switches
Instead of using IP addresses we will soon see that they use linklayer addresses to forward linklayer frames through the network of switches
Well begin our study of switched LANs by first covering linklayer addressing Section 
We then examine the celebrated Ethernet protocol Section 
After examining linklayer addressing and Ethernet well look at how linklayer switches operate Section 
and then see Section 
how these switches are often used to build largescale LANs
LinkLayer Addressing and ARP Hosts and routers have linklayer addresses
Now you might find this surprising recalling from Chapter that hosts and routers have networklayer addresses as well
You might be asking why in the world do we need to have addresses at both the network and link layers In addition to describing the syntax and function of the linklayer addresses in this section we hope to shed some light on why the two layers of addresses are useful and in fact indispensable
Well also cover the Address Resolution Protocol ARP which provides a mechanism to translate IP addresses to linklayer addresses
MAC Addresses In truth it is not hosts and routers that have linklayer addresses but rather their adapters that is network interfaces that have linklayer addresses
A host or router with multiple network interfaces will thus have multiple linklayer addresses associated with it just as it would also have multiple IP addresses associated with it
Its important to note however that linklayer switches do not have link layer addresses associated with their interfaces that connect to hosts and routers
This is because the job of the linklayer switch is to carry datagrams between hosts and routers a switch does this job transparently that is without the host or router having to explicitly address the frame to the intervening switch
This is illustrated in Figure 
A linklayer address is variously called a LAN address a physical address or a MAC address
Because MAC address seems to be the most popular term well henceforth refer to linklayer addresses as MAC addresses
For most LANs including Ethernet and 
wireless LANs the MAC address is bytes long giving possible MAC addresses
As shown in Figure 
these byte addresses are typically expressed in hexadecimal notation with each byte of the address expressed as a pair of hexadecimal numbers
Although MAC addresses were designed to be permanent it is now possible to change an adapters MAC address via software
For the rest of this section however well assume that an adapters MAC address is fixed
One interesting property of MAC addresses is that no two adapters have the same address
This might seem surprising given that adapters are manufactured in many countries by many companies
How does a company manufacturing adapters in Taiwan make sure that it is using different addresses from a company manufacturing Figure 
Each interface connected to a LAN has a unique MAC address adapters in Belgium The answer is that the IEEE manages the MAC address space
In particular when a company wants to manufacture adapters it purchases a chunk of the address space consisting of addresses for a nominal fee
IEEE allocates the chunk of addresses by fixing the first bits of a MAC address and letting the company create unique combinations of the last bits for each adapter
An adapters MAC address has a flat structure as opposed to a hierarchical structure and doesnt change no matter where the adapter goes
A laptop with an Ethernet interface always has the same MAC address no matter where the computer goes
A smartphone with an 
interface always has the same MAC address no matter where the smartphone goes
Recall that in contrast IP addresses have a hierarchical structure that is a network part and a host part and a hosts IP addresses needs to be changed when the host moves i.e
changes the network to which it is attached
An adapters MAC address is analogous to a persons social security number which also has a flat addressing structure and which doesnt change no matter where the person goes
An IP address is analogous to a persons postal address which is hierarchical and which must be changed whenever a person moves
Just as a person may find it useful to have both a postal address and a social security number it is useful for a host and router interfaces to have both a networklayer address and a MAC address
When an adapter wants to send a frame to some destination adapter the sending adapter inserts the destination adapters MAC address into the frame and then sends the frame into the LAN
As we will soon see a switch occasionally broadcasts an incoming frame onto all of its interfaces
Well see in Chapter that 
also broadcasts frames
Thus an adapter may receive a frame that isnt addressed to it
Thus when an adapter receives a frame it will check to see whether the destination MAC address in the frame matches its own MAC address
If there is a match the adapter extracts the enclosed datagram and passes the datagram up the protocol stack
If there isnt a match the adapter discards the frame without passing the networklayer datagram up
Thus the destination only will be interrupted when the frame is received
However sometimes a sending adapter does want all the other adapters on the LAN to receive and process the frame it is about to send
In this case the sending adapter inserts a special MAC broadcast address into the destination address field of the frame
For LANs that use byte addresses such as Ethernet and 
the broadcast address is a string of consecutive s that is FFFFFFFFFF FF in hexadecimal notation
Address Resolution Protocol ARP Because there are both networklayer addresses for example Internet IP addresses and linklayer addresses that is MAC addresses there is a need to translate between them
For the Internet this is the job of the Address Resolution Protocol ARP RFC 
To understand the need for a protocol such as ARP consider the network shown in Figure 
In this simple example each host and router has a single IP address and single MAC address
As usual IP addresses are shown in dotteddecimal PRINCIPLES IN PRACTICE KEEPING THE LAYERS INDEPENDENT There are several reasons why hosts and router interfaces have MAC addresses in addition to networklayer addresses
First LANs are designed for arbitrary networklayer protocols not just for IP and the Internet
If adapters were assigned IP addresses rather than neutral MAC addresses then adapters would not easily be able to support other networklayer protocols for example IPX or DECnet
Second if adapters were to use networklayer addresses instead of MAC addresses the networklayer address would have to be stored in the adapter RAM and reconfigured every time the adapter was moved or powered up
Another option is to not use any addresses in the adapters and have each adapter pass the data typically an IP datagram of each frame it receives up the protocol stack
The network layer could then check for a matching networklayer address
One problem with this option is that the host would be interrupted by every frame sent on the LAN including by frames that were destined for other hosts on the same broadcast LAN
In summary in order for the layers to be largely independent building blocks in a network architecture different layers need to have their own addressing scheme
We have now seen three types of addresses host names for the application layer IP addresses for the network layer and MAC addresses for the link layer
Each interface on a LAN has an IP address and a MAC address notation and MAC addresses are shown in hexadecimal notation
For the purposes of this discussion we will assume in this section that the switch broadcasts all frames that is whenever a switch receives a frame on one interface it forwards the frame on all of its other interfaces
In the next section we will provide a more accurate explanation of how switches operate
Now suppose that the host with IP address 
wants to send an IP datagram to host 
In this example both the source and destination are in the same subnet in the addressing sense of Section 
To send a datagram the source must give its adapter not only the IP datagram but also the MAC address for destination 
The sending adapter will then construct a linklayer frame containing the destinations MAC address and send the frame into the LAN
The important question addressed in this section is How does the sending host determine the MAC address for the destination host with IP address 
As you might have guessed it uses ARP
An ARP module in the sending host takes any IP address on the same LAN as input and returns the corresponding MAC address
In the example at hand sending host 
provides its ARP module the IP address 
and the ARP module returns the corresponding MAC address BDDCA
So we see that ARP resolves an IP address to a MAC address
In many ways it is analogous to DNS studied in Section 
which resolves host names to IP addresses
However one important difference between the two resolvers is that DNS resolves host names for hosts anywhere in the Internet whereas ARP resolves IP addresses only for hosts and router interfaces on the same subnet
If a node in California were to try to use ARP to resolve the IP address for a node in Mississippi ARP would return with an error
A possible ARP table in 
Now that we have explained what ARP does lets look at how it works
Each host and router has an ARP table in its memory which contains mappings of IP addresses to MAC addresses
shows what an ARP table in host 
might look like
The ARP table also contains a time tolive TTL value which indicates when each mapping will be deleted from the table
Note that a table does not necessarily contain an entry for every host and router on the subnet some may have never been entered into the table and others may have expired
A typical expiration time for an entry is minutes from when an entry is placed in an ARP table
Now suppose that host 
wants to send a datagram that is IPaddressed to another host or router on that subnet
The sending host needs to obtain the MAC address of the destination given the IP address
This task is easy if the senders ARP table has an entry for the destination node
But what if the ARP table doesnt currently have an entry for the destination In particular suppose 
wants to send a datagram to 
In this case the sender uses the ARP protocol to resolve the address
First the sender constructs a special packet called an ARP packet
An ARP packet has several fields including the sending and receiving IP and MAC addresses
Both ARP query and response packets have the same format
The purpose of the ARP query packet is to query all the other hosts and routers on the subnet to determine the MAC address corresponding to the IP address that is being resolved
Returning to our example 
passes an ARP query packet to the adapter along with an indication that the adapter should send the packet to the MAC broadcast address namely FFFFFF FFFFFF
The adapter encapsulates the ARP packet in a linklayer frame uses the broadcast address for the frames destination address and transmits the frame into the subnet
Recalling our social security numberpostal address analogy an ARP query is equivalent to a person shouting out in a crowded room of cubicles in some company say AnyCorp What is the social security number of the person whose postal address is Cubicle Room AnyCorp Palo Alto California The frame containing the ARP query is received by all the other adapters on the subnet and because of the broadcast address each adapter passes the ARP packet within the frame up to its ARP module
Each of these ARP modules checks to see if its IP address matches the destination IP address in the ARP packet
The one with a match sends back to the querying host a response ARP packet with the desired mapping
The querying host 
can then update its ARP table and send its IP datagram encapsulated in a linklayer frame whose destination MAC is that of the host or router responding to the earlier ARP query
There are a couple of interesting things to note about the ARP protocol
First the query ARP message is sent within a broadcast frame whereas the response ARP message is sent within a standard frame
Before reading on you should think about why this is so
Second ARP is plugandplay that is an ARP table gets built automaticallyit doesnt have to be configured by a system administrator
And if a host becomes disconnected from the subnet its entry is eventually deleted from the other ARP tables in the subnet
Students often wonder if ARP is a linklayer protocol or a networklayer protocol
As weve seen an ARP packet is encapsulated within a linklayer frame and thus lies architecturally above the link layer
However an ARP packet has fields containing linklayer addresses and thus is arguably a linklayer protocol but it also contains networklayer addresses and thus is also arguably a networklayer protocol
In the end ARP is probably best considered a protocol that straddles the boundary between the link and network layersnot fitting neatly into the simple layered protocol stack we studied in Chapter 
Such are the complexities of realworld protocols Sending a Datagram off the Subnet It should now be clear how ARP operates when a host wants to send a datagram to another host on the same subnet
But now lets look at the more complicated situation when a host on a subnet wants to send a networklayer datagram to a host off the subnet that is across a router onto another subnet
Lets discuss this issue in the context of Figure 
which shows a simple network consisting of two subnets interconnected by a router
There are several interesting things to note about Figure 
Each host has exactly one IP address and one adapter
But as discussed in Chapter a router has an IP address for each of its interfaces
For each router interface there is also an ARP module in the router and an adapter
Because the router in Figure 
has two interfaces it has two IP addresses two ARP modules and two adapters
Of course each adapter in the network has its own MAC address
Two subnets interconnected by a router Also note that Subnet has the network address 
and that Subnet has the network address 
Thus all of the interfaces connected to Subnet have addresses of the form ...xxx and all of the interfaces connected to Subnet have addresses of the form ...xxx
Now lets examine how a host on Subnet would send a datagram to a host on Subnet 
Specifically suppose that host 
wants to send an IP datagram to a host 
The sending host passes the datagram to its adapter as usual
But the sending host must also indicate to its adapter an appropriate destination MAC address
What MAC address should the adapter use One might be tempted to guess that the appropriate MAC address is that of the adapter for host 
This guess however would be wrong If the sending adapter were to use that MAC address then none of the adapters on Subnet would bother to pass the IP datagram up to its network layer since the frames destination address would not match the MAC address of any adapter on Subnet 
The datagram would just die and go to datagram heaven
If we look carefully at Figure 
we see that in order for a datagram to go from 
to a host on Subnet the datagram must first be sent to the router interface 
which is the IP address of the firsthop router on the path to the final destination
Thus the appropriate MAC address for the frame is the address of the adapter for router interface 
namely EE BBB
How does the sending host acquire the MAC address for 
By using ARP of course Once the sending adapter has this MAC address it creates a frame containing the datagram addressed to 
and sends the frame into Subnet 
The router adapter on Subnet sees that the linklayer frame is addressed to it and therefore passes the frame to the network layer of the router
Hooraythe IP datagram has successfully been moved from source host to the router But we are not finished
We still have to move the datagram from the router to the destination
The router now has to determine the correct interface on which the datagram is to be forwarded
As discussed in Chapter this is done by consulting a forwarding table in the router
The forwarding table tells the router that the datagram is to be forwarded via router interface 
This interface then passes the datagram to its adapter which encapsulates the datagram in a new frame and sends the frame into Subnet 
This time the destination MAC address of the frame is indeed the MAC address of the ultimate destination
And how does the router obtain this destination MAC address From ARP of course ARP for Ethernet is defined in RFC 
A nice introduction to ARP is given in the TCPIP tutorial RFC 
Well explore ARP in more detail in the homework problems
Ethernet Ethernet has pretty much taken over the wired LAN market
In the s and the early s Ethernet faced many challenges from other LAN technologies including token ring FDDI and ATM
Some of these other technologies succeeded in capturing a part of the LAN market for a few years
But since its invention in the mids Ethernet has continued to evolve and grow and has held on to its dominant position
Today Ethernet is by far the most prevalent wired LAN technology and it is likely to remain so for the foreseeable future
One might say that Ethernet has been to local area networking what the Internet has been to global networking
There are many reasons for Ethernets success
First Ethernet was the first widely deployed highspeed LAN
Because it was deployed early network administrators became intimately familiar with Ethernet its wonders and its quirksand were reluctant to switch over to other LAN technologies when they came on the scene
Second token ring FDDI and ATM were more complex and expensive than Ethernet which further discouraged network administrators from switching over
Third the most compelling reason to switch to another LAN technology such as FDDI or ATM was usually the higher data rate of the new technology however Ethernet always fought back producing versions that operated at equal data rates or higher
Switched Ethernet was also introduced in the early s which further increased its effective data rates
Finally because Ethernet has been so popular Ethernet hardware in particular adapters and switches has become a commodity and is remarkably cheap
The original Ethernet LAN was invented in the mids by Bob Metcalfe and David Boggs
The original Ethernet LAN used a coaxial bus to interconnect the nodes
Bus topologies for Ethernet actually persisted throughout the s and into the mids
Ethernet with a bus topology is a broadcast LAN all transmitted frames travel to and are processed by all adapters connected to the bus
Recall that we covered Ethernets CSMACD multiple access protocol with binary exponential backoff in Section 
By the late s most companies and universities had replaced their LANs with Ethernet installations using a hubbased star topology
In such an installation the hosts and routers are directly connected to a hub with twistedpair copper wire
A hub is a physicallayer device that acts on individual bits rather than frames
When a bit representing a zero or a one arrives from one interface the hub simply re creates the bit boosts its energy strength and transmits the bit onto all the other interfaces
Thus Ethernet with a hubbased star topology is also a broadcast LANwhenever a hub receives a bit from one of its interfaces it sends a copy out on all of its other interfaces
In particular if a hub receives frames from two different interfaces at the same time a collision occurs and the nodes that created the frames must retransmit
In the early s Ethernet experienced yet another major evolutionary change
Ethernet installations continued to use a star topology but the hub at the center was replaced with a switch
Well be examining switched Ethernet in depth later in this chapter
For now we only mention that a switch is not only collisionless but is also a bonafide storeandforward packet switch but unlike routers which operate up through layer a switch operates only up through layer 
Ethernet frame structure Ethernet Frame Structure We can learn a lot about Ethernet by examining the Ethernet frame which is shown in Figure 
To give this discussion about Ethernet frames a tangible context lets consider sending an IP datagram from one host to another host with both hosts on the same Ethernet LAN for example the Ethernet LAN in Figure 
Although the payload of our Ethernet frame is an IP datagram we note that an Ethernet frame can carry other networklayer packets as well
Let the sending adapter adapter A have the MAC address AAAAAAAAAAAA and the receiving adapter adapter B have the MAC address BBBBBBBBBBBB
The sending adapter encapsulates the IP datagram within an Ethernet frame and passes the frame to the physical layer
The receiving adapter receives the frame from the physical layer extracts the IP datagram and passes the IP datagram to the network layer
In this context lets now examine the six fields of the Ethernet frame as shown in Figure 
Data field to bytes
This field carries the IP datagram
The maximum transmission unit MTU of Ethernet is bytes
This means that if the IP datagram exceeds bytes then the host has to fragment the datagram as discussed in Section 
The minimum size of the data field is bytes
This means that if the IP datagram is less than bytes the data field has to be stuffed to fill it out to bytes
When stuffing is used the data passed to the network layer contains the stuffing as well as an IP datagram
The network layer uses the length field in the IP datagram header to remove the stuffing
Destination address bytes
This field contains the MAC address of the destination adapter BB BBBBBBBBBB
When adapter B receives an Ethernet frame whose destination address is either BBBBBBBBBBBB or the MAC broadcast address it passes the contents of the frames data field to the network layer if it receives a frame with any other MAC address it discards the frame
Source address bytes
This field contains the MAC address of the adapter that transmits the frame onto the LAN in this example AAAAAAAAAAAA
Type field bytes
The type field permits Ethernet to multiplex networklayer protocols
To understand this we need to keep in mind that hosts can use other networklayer protocols besides IP
In fact a given host may support multiple networklayer protocols using different protocols for different applications
For this reason when the Ethernet frame arrives at adapter B adapter B needs to know to which networklayer protocol it should pass that is demultiplex the contents of the data field
IP and other networklayer protocols for example Novell IPX or AppleTalk each have their own standardized type number
Furthermore the ARP protocol discussed in the previous section has its own type number and if the arriving frame contains an ARP packet i.e
has a type field of hexadecimal the ARP packet will be demultiplexed up to the ARP protocol
Note that the type field is analogous to the protocol field in the networklayer datagram and the portnumber fields in the transportlayer segment all of these fields serve to glue a protocol at one layer to a protocol at the layer above
Cyclic redundancy check CRC bytes
As discussed in Section 
the purpose of the CRC field is to allow the receiving adapter adapter B to detect bit errors in the frame
The Ethernet frame begins with an byte preamble field
Each of the first bytes of the preamble has a value of the last byte is 
The first bytes of the preamble serve to wake up the receiving adapters and to synchronize their clocks to that of the senders clock
Why should the clocks be out of synchronization Keep in mind that adapter A aims to transmit the frame at Mbps Mbps or Gbps depending on the type of Ethernet LAN
However because nothing is absolutely perfect adapter A will not transmit the frame at exactly the target rate there will always be some drift from the target rate a drift which is not known a priori by the other adapters on the LAN
A receiving adapter can lock onto adapter As clock simply by locking onto the bits in the first bytes of the preamble
The last bits of the eighth byte of the preamble the first two consecutive s alert adapter B that the important stuff is about to come
All of the Ethernet technologies provide connectionless service to the network layer
That is when adapter A wants to send a datagram to adapter B adapter A encapsulates the datagram in an Ethernet frame and sends the frame into the LAN without first handshaking with adapter B
This layer connectionless service is analogous to IPs layer datagram service and UDPs layer connectionless service
Ethernet technologies provide an unreliable service to the network layer
Specifically when adapter B receives a frame from adapter A it runs the frame through a CRC check but neither sends an acknowledgment when a frame passes the CRC check nor sends a negative acknowledgment when a frame fails the CRC check
When a frame fails the CRC check adapter B simply discards the frame
Thus adapter A has no idea whether its transmitted frame reached adapter B and passed the CRC check
This lack of reliable transport at the link layer helps to make Ethernet simple and cheap
But it also means that the stream of datagrams passed to the network layer can have gaps
CASE HISTORY BOB METCALFE AND ETHERNET As a PhD student at Harvard University in the early s Bob Metcalfe worked on the ARPAnet at MIT
During his studies he also became exposed to Abramsons work on ALOHA and random access protocols
After completing his PhD and just before beginning a job at Xerox Palo Alto Research Center Xerox PARC he visited Abramson and his University of Hawaii colleagues for three months getting a firsthand look at ALOHAnet
At Xerox PARC Metcalfe became exposed to Alto computers which in many ways were the forerunners of the personal computers of the s
Metcalfe saw the need to network these computers in an inexpensive manner
So armed with his knowledge about ARPAnet ALOHAnet and random access protocols Metcalfealong with colleague David Boggsinvented Ethernet
Metcalfe and Boggss original Ethernet ran at 
Mbps and linked up to hosts separated by up to one mile
Metcalfe and Boggs succeeded at getting most of the researchers at Xerox PARC to communicate through their Alto computers
Metcalfe then forged an alliance between Xerox Digital and Intel to establish Ethernet as a Mbps Ethernet standard ratified by the IEEE
Xerox did not show much interest in commercializing Ethernet
In Metcalfe formed his own company Com which developed and commercialized networking technology including Ethernet technology
In particular Com developed and marketed Ethernet cards in the early s for the immensely popular IBM PCs
If there are gaps due to discarded Ethernet frames does the application at Host B see gaps as well As we learned in Chapter this depends on whether the application is using UDP or TCP
If the application is using UDP then the application in Host B will indeed see gaps in the data
On the other hand if the application is using TCP then TCP in Host B will not acknowledge the data contained in discarded frames causing TCP in Host A to retransmit
Note that when TCP retransmits data the data will eventually return to the Ethernet adapter at which it was discarded
Thus in this sense Ethernet does retransmit data although Ethernet is unaware of whether it is transmitting a brandnew datagram with brandnew data or a datagram that contains data that has already been transmitted at least once
Ethernet Technologies In our discussion above weve referred to Ethernet as if it were a single protocol standard
But in fact Ethernet comes in many different flavors with somewhat bewildering acronyms such as BASET BASE BASET BASELX GBASET and GBASET
These and many other Ethernet technologies have been standardized over the years by the IEEE 
CSMACD Ethernet working group IEEE 
While these acronyms may appear bewildering there is actually considerable order here
The first part of the acronym refers to the speed of the standard or G for Megabit per second Megabit Gigabit Gigabit and Gigibit Ethernet respectively
BASE refers to baseband Ethernet meaning that the physical media only carries Ethernet traffic almost all of the 
standards are for baseband Ethernet
The final part of the acronym refers to the physical media itself Ethernet is both a linklayer and a physicallayer specification and is carried over a variety of physical media including coaxial cable copper wire and fiber
Generally a T refers to twistedpair copper wires
Historically an Ethernet was initially conceived of as a segment of coaxial cable
The early BASE and BASE standards specify Mbps Ethernet over two types of coaxial cable each limited in length to meters
Longer runs could be obtained by using a repeatera physicallayer device that receives a signal on the input side and regenerates the signal on the output side
A coaxial cable corresponds nicely to our view of Ethernet as a broadcast mediumall frames transmitted by one interface are received at other interfaces and Ethernets CDMACD protocol nicely solves the multiple access problem
Nodes simply attach to the cable and voila we have a local area network Ethernet has passed through a series of evolutionary steps over the years and todays Ethernet is very different from the original bustopology designs using coaxial cable
In most installations today nodes are connected to a switch via pointtopoint segments made of twistedpair copper wires or fiberoptic cables as shown in Figures 
In the mids Ethernet was standardized at Mbps times faster than Mbps Ethernet
The original Ethernet MAC protocol and frame format were preserved but higherspeed physical layers were defined for copper wire BASET and fiber BASEFX BASESX BASEBX
shows these different standards and the common Ethernet MAC protocol and frame format
Mbps Ethernet is limited to a meter distance over twisted pair and to Figure 
Mbps Ethernet standards A common link layer different physical layers several kilometers over fiber allowing Ethernet switches in different buildings to be connected
Gigabit Ethernet is an extension to the highly successful Mbps and Mbps Ethernet standards
Offering a raw data rate of Mbps Gigabit Ethernet maintains full compatibility with the huge installed base of Ethernet equipment
The standard for Gigabit Ethernet referred to as IEEE .z does the following Uses the standard Ethernet frame format Figure 
and is backward compatible with BASET and BASET technologies
This allows for easy integration of Gigabit Ethernet with the existing installed base of Ethernet equipment
Allows for pointtopoint links as well as shared broadcast channels
Pointtopoint links use switches while broadcast channels use hubs as described earlier
In Gigabit Ethernet jargon hubs are called buffered distributors
Uses CSMACD for shared broadcast channels
In order to have acceptable efficiency the maximum distance between nodes must be severely restricted
Allows for fullduplex operation at Gbps in both directions for pointtopoint channels
Initially operating over optical fiber Gigabit Ethernet is now able to run over category UTP cabling
Lets conclude our discussion of Ethernet technology by posing a question that may have begun troubling you
In the days of bus topologies and hubbased star topologies Ethernet was clearly a broadcast link as defined in Section 
in which frame collisions occurred when nodes transmitted at the same time
To deal with these collisions the Ethernet standard included the CSMACD protocol which is particularly effective for a wired broadcast LAN spanning a small geographical region
But if the prevalent use of Ethernet today is a switchbased star topology using storeandforward packet switching is there really a need anymore for an Ethernet MAC protocol As well see shortly a switch coordinates its transmissions and never forwards more than one frame onto the same interface at any time
Furthermore modern switches are fullduplex so that a switch and a node can each send frames to each other at the same time without interference
In other words in a switchbased Ethernet LAN there are no collisions and therefore there is no need for a MAC protocol As weve seen todays Ethernets are very different from the original Ethernet conceived by Metcalfe and Boggs more than years agospeeds have increased by three orders of magnitude Ethernet frames are carried over a variety of media switchedEthernets have become dominant and now even the MAC protocol is often unnecessary Is all of this really still Ethernet The answer of course is yes by definition
It is interesting to note however that through all of these changes there has indeed been one enduring constant that has remained unchanged over yearsEthernets frame format
Perhaps this then is the one true and timeless centerpiece of the Ethernet standard
LinkLayer Switches Up until this point we have been purposefully vague about what a switch actually does and how it works
The role of the switch is to receive incoming linklayer frames and forward them onto outgoing links well study this forwarding function in detail in this subsection
Well see that the switch itself is transparent to the hosts and routers in the subnet that is a hostrouter addresses a frame to another hostrouter rather than addressing the frame to the switch and happily sends the frame into the LAN unaware that a switch will be receiving the frame and forwarding it
The rate at which frames arrive to any one of the switchs output interfaces may temporarily exceed the link capacity of that interface
To accommodate this problem switch output interfaces have buffers in much the same way that router output interfaces have buffers for datagrams
Lets now take a closer look at how switches operate
Forwarding and Filtering Filtering is the switch function that determines whether a frame should be forwarded to some interface or should just be dropped
Forwarding is the switch function that determines the interfaces to which a frame should be directed and then moves the frame to those interfaces
Switch filtering and forwarding are done with a switch table
The switch table contains entries for some but not necessarily all of the hosts and routers on a LAN
An entry in the switch table contains a MAC address the switch interface that leads toward that MAC address and the time at which the entry was placed in the table
An example switch table for the uppermost switch in Figure 
is shown in Figure 
This description of frame forwarding may sound similar to our discussion of datagram forwarding Figure 
Portion of a switch table for the uppermost switch in Figure 
in Chapter 
Indeed in our discussion of generalized forwarding in Section 
we learned that many modern packet switches can be configured to forward on the basis of layer destination MAC addresses i.e
function as a layer switch or layer IP destination addresses i.e
function as a layer router
Nonetheless well make the important distinction that switches forward packets based on MAC addresses rather than on IP addresses
We will also see that a traditional i.e
in a nonSDN context switch table is constructed in a very different manner from a routers forwarding table
To understand how switch filtering and forwarding work suppose a frame with destination address DD DDDDDDDDDD arrives at the switch on interface x
The switch indexes its table with the MAC address DDDDDDDDDDDD
There are three possible cases There is no entry in the table for DDDDDDDDDDDD
In this case the switch forwards copies of the frame to the output buffers preceding all interfaces except for interface x
In other words if there is no entry for the destination address the switch broadcasts the frame
There is an entry in the table associating DDDDDDDDDDDD with interface x
In this case the frame is coming from a LAN segment that contains adapter DDDDDDDDDDDD
There being no need to forward the frame to any of the other interfaces the switch performs the filtering function by discarding the frame
There is an entry in the table associating DDDDDDDDDDDD with interface yx
In this case the frame needs to be forwarded to the LAN segment attached to interface y
The switch performs its forwarding function by putting the frame in an output buffer that precedes interface y
Lets walk through these rules for the uppermost switch in Figure 
and its switch table in Figure 
Suppose that a frame with destination address FEFA arrives at the switch from interface 
The switch examines its table and sees that the destination is on the LAN segment connected to interface that is Electrical Engineering
This means that the frame has already been broadcast on the LAN segment that contains the destination
The switch therefore filters that is discards the frame
Now suppose a frame with the same destination address arrives from interface 
The switch again examines its table and sees that the destination is in the direction of interface it therefore forwards the frame to the output buffer preceding interface 
It should be clear from this example that as long as the switch table is complete and accurate the switch forwards frames toward destinations without any broadcasting
In this sense a switch is smarter than a hub
But how does this switch table get configured in the first place Are there linklayer equivalents to networklayer routing protocols Or must an overworked manager manually configure the switch table SelfLearning A switch has the wonderful property particularly for the alreadyoverworked network administrator that its table is built automatically dynamically and autonomouslywithout any intervention from a network administrator or from a configuration protocol
In other words switches are selflearning
This capability is accomplished as follows 
The switch table is initially empty
For each incoming frame received on an interface the switch stores in its table the MAC address in the frames source address field the interface from which the frame arrived and the current time
In this manner the switch records in its table the LAN segment on which the sender resides
If every host in the LAN eventually sends a frame then every host will eventually get recorded in the table
The switch deletes an address in the table if no frames are received with that address as the source address after some period of time the aging time
In this manner if a PC is replaced by another PC with a different adapter the MAC address of the original PC will eventually be purged from the switch table
Lets walk through the selflearning property for the uppermost switch in Figure 
and its corresponding switch table in Figure 
Suppose at time a frame with source address arrives from interface 
Suppose that this address is not in the switch table
Then the switch adds a new entry to the table as shown in Figure 
Continuing with this same example suppose that the aging time for this switch is minutes and no frames with source address FEFA arrive to the switch between and 
Then at time the switch removes this address from its table
Switch learns about the location of an adapter with address Switches are plugandplay devices because they require no intervention from a network administrator or user
A network administrator wanting to install a switch need do nothing more than connect the LAN segments to the switch interfaces
The administrator need not configure the switch tables at the time of installation or when a host is removed from one of the LAN segments
Switches are also fullduplex meaning any switch interface can send and receive at the same time
Properties of LinkLayer Switching Having described the basic operation of a linklayer switch lets now consider their features and properties
We can identify several advantages of using switches rather than broadcast links such as buses or hubbased star topologies Elimination of collisions
In a LAN built from switches and without hubs there is no wasted bandwidth due to collisions The switches buffer frames and never transmit more than one frame on a segment at any one time
As with a router the maximum aggregate throughput of a switch is the sum of all the switch interface rates
Thus switches provide a significant performance improvement over LANs with broadcast links
Because a switch isolates one link from another the different links in the LAN can operate at different speeds and can run over different media
For example the uppermost switch in Figure 
might have three Gbps BASET copper links two Mbps BASE FX fiber links and one BASET copper link
Thus a switch is ideal for mixing legacy equipment with new equipment
In addition to providing enhanced security see sidebar on Focus on Security a switch also eases network management
For example if an adapter malfunctions and continually sends Ethernet frames called a jabbering adapter a switch can detect the problem and internally disconnect the malfunctioning adapter
With this feature the network administrator need not get out of bed and drive back to work in order to correct the problem
Similarly a cable cut disconnects only that host that was using the cut cable to connect to the switch
In the days of coaxial cable many a network manager spent hours walking the line or more accurately crawling the floor to find the cable break that brought down the entire network
Switches also gather statistics on bandwidth usage collision rates and traffic types and make this information available to the network manager
This information can be used to debug and correct problems and to plan how the LAN should evolve in the future
Researchers are exploring adding yet more management functionality into Ethernet LANs in prototype deployments Casado Koponen 
FOCUS ON SECURITY SNIFFING A SWITCHED LAN SWITCH POISONING When a host is connected to a switch it typically only receives frames that are intended for it
For example consider a switched LAN in Figure 
When host A sends a frame to host B and there is an entry for host B in the switch table then the switch will forward the frame only to host B
If host C happens to be running a sniffer host C will not be able to sniff this AtoB frame
Thus in a switchedLAN environment in contrast to a broadcast link environment such as 
LANs or hubbased Ethernet LANs it is more difficult for an attacker to sniff frames
However because the switch broadcasts frames that have destination addresses that are not in the switch table the sniffer at C can still sniff some frames that are not intended for C
Furthermore a sniffer will be able sniff all Ethernet broadcast frames with broadcast destination address FFFFFFFFFFFF
A wellknown attack against a switch called switch poisoning is to send tons of packets to the switch with many different bogus source MAC addresses thereby filling the switch table with bogus entries and leaving no room for the MAC addresses of the legitimate hosts
This causes the switch to broadcast most frames which can then be picked up by the sniffer Skoudis 
As this attack is rather involved even for a sophisticated attacker switches are significantly less vulnerable to sniffing than are hubs and wireless LANs
Switches Versus Routers As we learned in Chapter routers are storeandforward packet switches that forward packets using networklayer addresses
Although a switch is also a storeandforward packet switch it is fundamentally different from a router in that it forwards packets using MAC addresses
Whereas a router is a layer packet switch a switch is a layer packet switch
Recall however that we learned in Section 
that modern switches using the match plus action operation can be used to forward a layer frame based on the frames destination MAC address as well as a layer datagram using the datagrams destination IP address
Indeed we saw that switches using the OpenFlow standard can perform generalized packet forwarding based on any of eleven different frame datagram and transport layer header fields
Even though switches and routers are fundamentally different network administrators must often choose between them when installing an interconnection device
For example for the network in Figure 
the network administrator could just as easily have used a router instead of a switch to connect the department LANs servers and internet gateway router
Indeed a router would permit interdepartmental communication without creating collisions
Given that both switches and routers are candidates for interconnection devices what are the pros and cons of the two approaches Figure 
Packet processing in switches routers and hosts First consider the pros and cons of switches
As mentioned above switches are plugandplay a property that is cherished by all the overworked network administrators of the world
Switches can also have relatively high filtering and forwarding ratesas shown in Figure 
switches have to process frames only up through layer whereas routers have to process datagrams up through layer 
On the other hand to prevent the cycling of broadcast frames the active topology of a switched network is restricted to a spanning tree
Also a large switched network would require large ARP tables in the hosts and routers and would generate substantial ARP traffic and processing
Furthermore switches are susceptible to broadcast stormsif one host goes haywire and transmits an endless stream of Ethernet broadcast frames the switches will forward all of these frames causing the entire network to collapse
Now consider the pros and cons of routers
Because network addressing is often hierarchical and not flat as is MAC addressing packets do not normally cycle through routers even when the network has redundant paths
However packets can cycle when router tables are misconfigured but as we learned in Chapter IP uses a special datagram header field to limit the cycling
Thus packets are not restricted to a spanning tree and can use the best path between source and destination
Because routers do not have the spanning tree restriction they have allowed the Internet to be built with a rich topology that includes for example multiple active links between Europe and North America
Another feature of routers is that they provide firewall protection against layer broadcast storms
Perhaps the most significant drawback of routers though is that they are not plugandplaythey and the hosts that connect to them need their IP addresses to be configured
Also routers often have a larger perpacket processing time than switches because they have to process up through the layer fields
Finally there are two different ways to pronounce the word router either as rootor or as rowter and people waste a lot of time arguing over the proper pronunciation Perlman 
Given that both switches and routers have their pros and cons as summarized in Table 
when should an institutional network for example a university campus Table 
Comparison of the typical features of popular interconnection devices Hubs Routers Switches Traffic isolation No Yes Yes Plug and play Yes No Yes Optimal routing No Yes No network or a corporate campus network use switches and when should it use routers Typically small networks consisting of a few hundred hosts have a few LAN segments
Switches suffice for these small networks as they localize traffic and increase aggregate throughput without requiring any configuration of IP addresses
But larger networks consisting of thousands of hosts typically include routers within the network in addition to switches
The routers provide a more robust isolation of traffic control broadcast storms and use more intelligent routes among the hosts in the network
For more discussion of the pros and cons of switched versus routed networks as well as a discussion of how switched LAN technology can be extended to accommodate two orders of magnitude more hosts than todays Ethernets see Meyers Kim 
Virtual Local Area Networks VLANs In our earlier discussion of Figure 
we noted that modern institutional LANs are often configured hierarchically with each workgroup department having its own switched LAN connected to the switched LANs of other groups via a switch hierarchy
While such a configuration works well in an ideal world the real world is often far from ideal
Three drawbacks can be identified in the configuration in Figure 
Lack of traffic isolation
Although the hierarchy localizes group traffic to within a single switch broadcast traffic e.g
frames carrying ARP and DHCP messages or frames whose destination has not yet been learned by a selflearning switch must still traverse the entire institutional network
Limiting the scope of such broadcast traffic would improve LAN performance
Perhaps more importantly it also may be desirable to limit LAN broadcast traffic for securityprivacy reasons
For example if one group contains the companys executive management team and another group contains disgruntled employees running Wireshark packet sniffers the network manager may well prefer that the executives traffic never even reaches employee hosts
This type of isolation could be provided by replacing the center switch in Figure 
with a router
Well see shortly that this isolation also can be achieved via a switched layer solution
Inefficient use of switches
If instead of three groups the institution had groups then first level switches would be required
If each group were small say less than people then a single port switch would likely be large enough to accommodate everyone but this single switch would not provide traffic isolation
If an employee moves between groups the physical cabling must be changed to connect the employee to a different switch in Figure 
Employees belonging to two groups make the problem even harder
Fortunately each of these difficulties can be handled by a switch that supports virtual local area networks VLANs
As the name suggests a switch that supports VLANs allows multiple virtual local area networks to be defined over a single physical local area network infrastructure
Hosts within a VLAN communicate with each other as if they and no other hosts were connected to the switch
In a portbased VLAN the switchs ports interfaces are divided into groups by the network manager
Each group constitutes a VLAN with the ports in each VLAN forming a broadcast domain i.e
broadcast traffic from one port can only reach other ports in the group
shows a single switch with ports
Ports to belong to the EE VLAN while ports to belong to the CS VLAN ports and are unassigned
This VLAN solves all of the difficulties noted aboveEE and CS VLAN frames are isolated from each other the two switches in Figure 
have been replaced by a single switch and if the user at switch port joins the CS Department the network operator simply reconfigures the VLAN software so that port is now associated with the CS VLAN
One can easily imagine how the VLAN switch is configured and operatesthe network manager declares a port to belong Figure 
A single switch with two configured VLANs to a given VLAN with undeclared ports belonging to a default VLAN using switch management software a table of porttoVLAN mappings is maintained within the switch and switch hardware only delivers frames between ports belonging to the same VLAN
But by completely isolating the two VLANs we have introduced a new difficulty How can traffic from the EE Department be sent to the CS Department One way to handle this would be to connect a VLAN switch port e.g
port in Figure 
to an external router and configure that port to belong both the EE and CS VLANs
In this case even though the EE and CS departments share the same physical switch the logical configuration would look as if the EE and CS departments had separate switches connected via a router
An IP datagram going from the EE to the CS department would first cross the EE VLAN to reach the router and then be forwarded by the router back over the CS VLAN to the CS host
Fortunately switch vendors make such configurations easy for the network manager by building a single device that contains both a VLAN switch and a router so a separate external router is not needed
A homework problem at the end of the chapter explores this scenario in more detail
Returning again to Figure 
lets now suppose that rather than having a separate Computer Engineering department some EE and CS faculty are housed in a separate building where of course they need network access and of course theyd like to be part of their departments VLAN
shows a second port switch where the switch ports have been defined as belonging to the EE or the CS VLAN as needed
But how should these two switches be interconnected One easy solution would be to define a port belonging to the CS VLAN on each switch similarly for the EE VLAN and to connect these ports to each other as shown in Figure .a
This solution doesnt scale however since N VLANS would require N ports on each switch simply to interconnect the two switches
A more scalable approach to interconnecting VLAN switches is known as VLAN trunking
In the VLAN trunking approach shown in Figure .b a special port on each switch port on the left switch and port on the right switch is configured as a trunk port to interconnect the two VLAN switches
The trunk port belongs to all VLANs and frames sent to any VLAN are forwarded over the trunk link to the other switch
But this raises yet another question How does a switch know that a frame arriving on a trunk port belongs to a particular VLAN The IEEE has defined an extended Ethernet frame format .Q for frames crossing a VLAN trunk
As shown in Figure 
the .Q frame consists of the standard Ethernet frame with a fourbyte VLAN tag added into the header that carries the identity of the VLAN to which the frame belongs
The VLAN tag is added into a frame by the switch at the sending side of a VLAN trunk parsed and removed by the switch at the receiving side of the trunk
The VLAN tag itself consists of a byte Tag Protocol Identifier TPID field with a fixed hexadecimal value of a byte Tag Control Information field that contains a bit VLAN identifier field and a bit priority field that is similar in intent to the IP datagram TOS field
Connecting two VLAN switches with two VLANs a two cables b trunked Figure 
Original Ethernet frame top .Qtagged Ethernet VLAN frame below In this discussion weve only briefly touched on VLANs and have focused on portbased VLANs
We should also mention that VLANs can be defined in several other ways
In MACbased VLANs the network manager specifies the set of MAC addresses that belong to each VLAN whenever a device attaches to a port the port is connected into the appropriate VLAN based on the MAC address of the device
VLANs can also be defined based on networklayer protocols e.g
IPv IPv or Appletalk and other criteria
It is also possible for VLANs to be extended across IP routers allowing islands of LANs to be connected together to form a single VLAN that could span the globe Yu 
See the .Q standard IEEE .q for more details
Link Virtualization A Network as a Link Layer Because this chapter concerns linklayer protocols and given that were now nearing the chapters end lets reflect on how our understanding of the term link has evolved
We began this chapter by viewing the link as a physical wire connecting two communicating hosts
In studying multiple access protocols we saw that multiple hosts could be connected by a shared wire and that the wire connecting the hosts could be radio spectra or other media
This led us to consider the link a bit more abstractly as a channel rather than as a wire
In our study of Ethernet LANs Figure 
we saw that the interconnecting media could actually be a rather complex switched infrastructure
Throughout this evolution however the hosts themselves maintained the view that the interconnecting medium was simply a linklayer channel connecting two or more hosts
We saw for example that an Ethernet host can be blissfully unaware of whether it is connected to other LAN hosts by a single short LAN segment Figure 
or by a geographically dispersed switched LAN Figure 
or by a VLAN Figure 
In the case of a dialup modem connection between two hosts the link connecting the two hosts is actually the telephone networka logically separate global telecommunications network with its own switches links and protocol stacks for data transfer and signaling
From the Internet linklayer point of view however the dialup connection through the telephone network is viewed as a simple wire
In this sense the Internet virtualizes the telephone network viewing the telephone network as a linklayer technology providing linklayer connectivity between two Internet hosts
You may recall from our discussion of overlay networks in Chapter that an overlay network similarly views the Internet as a means for providing connectivity between overlay nodes seeking to overlay the Internet in the same way that the Internet overlays the telephone network
In this section well consider Multiprotocol Label Switching MPLS networks
Unlike the circuitswitched telephone network MPLS is a packetswitched virtualcircuit network in its own right
It has its own packet formats and forwarding behaviors
Thus from a pedagogical viewpoint a discussion of MPLS fits well into a study of either the network layer or the link layer
From an Internet viewpoint however we can consider MPLS like the telephone network and switchedEthernets as a linklayer technology that serves to interconnect IP devices
Thus well consider MPLS in our discussion of the link layer
Frame relay and ATM networks can also be used to interconnect IP devices though they represent a slightly older but still deployed technology and will not be covered here see the very readable book Goralski for details
Our treatment of MPLS will be necessarily brief as entire books could be and have been written on these networks
We recommend Davie for details on MPLS
Well focus here primarily on how MPLS servers interconnect to IP devices although well dive a bit deeper into the underlying technologies as well
Multiprotocol Label Switching MPLS Multiprotocol Label Switching MPLS evolved from a number of industry efforts in the midtolate s to improve the forwarding speed of IP routers by adopting a key concept from the world of virtualcircuit networks a fixedlength label
The goal was not to abandon the destinationbased IP datagram forwarding infrastructure for one based on fixedlength labels and virtual circuits but to augment it by selectively labeling datagrams and allowing routers to forward datagrams based on fixedlength labels rather than destination IP addresses when possible
Importantly these techniques work handinhand with IP using IP addressing and routing
The IETF unified these efforts in the MPLS protocol RFC RFC effectively blending VC techniques into a routed datagram network
Lets begin our study of MPLS by considering the format of a linklayer frame that is handled by an MPLScapable router
shows that a linklayer frame transmitted between MPLScapable devices has a small MPLS header added between the layer e.g
Ethernet header and layer i.e
RFC defines the format of the MPLS header for such links headers are defined for ATM and framerelayed networks as well in other RFCs
Among the fields in the MPLS Figure 
MPLS header Located between link and networklayer headers header are the label bits reserved for experimental use a single S bit which is used to indicate the end of a series of stacked MPLS headers an advanced topic that well not cover here and a timeto live field
Its immediately evident from Figure 
that an MPLSenhanced frame can only be sent between routers that are both MPLS capable since a nonMPLScapable router would be quite confused when it found an MPLS header where it had expected to find the IP header
An MPLScapable router is often referred to as a labelswitched router since it forwards an MPLS frame by looking up the MPLS label in its forwarding table and then immediately passing the datagram to the appropriate output interface
Thus the MPLScapable router need not extract the destination IP address and perform a lookup of the longest prefix match in the forwarding table
But how does a router know if its neighbor is indeed MPLS capable and how does a router know what label to associate with the given IP destination To answer these questions well need to take a look at the interaction among a group of MPLScapable routers
In the example in Figure 
routers R through R are MPLS capable
R and R are standard IP routers
R has advertised to R and R that it R can route to destination A and that a received frame with MPLS label will be forwarded to destination A
Router R has advertised to router R that it can route to destinations A and D and that incoming frames with MPLS labels and respectively will be switched toward those destinations
Router R has also advertised to router R that it R can reach destination A and that a received frame with MPLS label will be switched toward A
Note that router R is now in the interesting position of having Figure 
MPLSenhanced forwarding two MPLS paths to reach A via interface with outbound MPLS label and via interface with an MPLS label of 
The broad picture painted in Figure 
is that IP devices R R A and D are connected together via an MPLS infrastructure MPLScapable routers R R R and R in much the same way that a switched LAN or an ATM network can connect together IP devices
And like a switched LAN or ATM network the MPLScapable routers R through R do so without ever touching the IP header of a packet
In our discussion above weve not specified the specific protocol used to distribute labels among the MPLScapable routers as the details of this signaling are well beyond the scope of this book
We note however that the IETF working group on MPLS has specified in RFC that an extension of the RSVP protocol known as RSVPTE RFC will be the focus of its efforts for MPLS signaling
Weve also not discussed how MPLS actually computes the paths for packets among MPLS capable routers nor how it gathers linkstate information e.g
amount of link bandwidth unreserved by MPLS to use in these path computations
Existing linkstate routing algorithms e.g
OSPF have been extended to flood this information to MPLScapable routers
Interestingly the actual path computation algorithms are not standardized and are currently vendorspecific
Thus far the emphasis of our discussion of MPLS has been on the fact that MPLS performs switching based on labels without needing to consider the IP address of a packet
The true advantages of MPLS and the reason for current interest in MPLS however lie not in the potential increases in switching speeds but rather in the new traffic management capabilities that MPLS enables
As noted above R has two MPLS paths to A
If forwarding were performed up at the IP layer on the basis of IP address the IP routing protocols we studied in Chapter would specify only a single leastcost path to A
Thus MPLS provides the ability to forward packets along routes that would not be possible using standard IP routing protocols
This is one simple form of traffic engineering using MPLS RFC RFC RFC Xiao in which a network operator can override normal IP routing and force some of the traffic headed toward a given destination along one path and other traffic destined toward the same destination along another path whether for policy performance or some other reason
It is also possible to use MPLS for many other purposes as well
It can be used to perform fast restoration of MPLS forwarding paths e.g
to reroute traffic over a precomputed failover path in response to link failure Kar Huang RFC 
Finally we note that MPLS can and has been used to implement socalled virtual private networks VPNs
In implementing a VPN for a customer an ISP uses its MPLSenabled network to connect together the customers various networks
MPLS can be used to isolate both the resources and addressing used by the customers VPN from that of other users crossing the ISPs network see DeClercq for details
Our discussion of MPLS has been brief and we encourage you to consult the references weve mentioned
We note that with so many possible uses for MPLS it appears that it is rapidly becoming the Swiss Army knife of Internet traffic engineering 
Data Center Networking In recent years Internet companies such as Google Microsoft Facebook and Amazon as well as their counterparts in Asia and Europe have built massive data centers each housing tens to hundreds of thousands of hosts and concurrently supporting many distinct cloud applications e.g
search email social networking and ecommerce
Each data center has its own data center network that interconnects its hosts with each other and interconnects the data center with the Internet
In this section we provide a brief introduction to data center networking for cloud applications
The cost of a large data center is huge exceeding million per month for a host data center Greenberg a
Of these costs about percent can be attributed to the hosts themselves which need to be replaced every years percent to infrastructure including transformers uninterruptable power supplies UPS systems generators for longterm outages and cooling systems percent for electric utility costs for the power draw and percent for networking including network gear switches routers and load balancers external links and transit traffic costs
In these percentages costs for equipment are amortized so that a common cost metric is applied for onetime purchases and ongoing expenses such as power
While networking is not the largest cost networking innovation is the key to reducing overall cost and maximizing performance Greenberg a
The worker bees in a data center are the hosts They serve content e.g
Web pages and videos store emails and documents and collectively perform massively distributed computations e.g
distributed index computations for search engines
The hosts in data centers called blades and resembling pizza boxes are generally commodity hosts that include CPU memory and disk storage
The hosts are stacked in racks with each rack typically having to blades
At the top of each rack there is a switch aptly named the Top of Rack TOR switch that interconnects the hosts in the rack with each other and with other switches in the data center
Specifically each host in the rack has a network interface card that connects to its TOR switch and each TOR switch has additional ports that can be connected to other switches
Today hosts typically have Gbps Ethernet connections to their TOR switches Greenberg 
Each host is also assigned its own datacenterinternal IP address
The data center network supports two types of traffic traffic flowing between external clients and internal hosts and traffic flowing between internal hosts
To handle flows between external clients and internal hosts the data center network includes one or more border routers connecting the data center network to the public Internet
The data center network therefore interconnects the racks with each other and connects the racks to the border routers
shows an example of a data center network
Data center network design the art of designing the interconnection network and protocols that connect the racks with each other and with the border routers has become an important branch of computer networking research in recent years AlFares Greenberg a Greenberg b Mysore Guo Wang 
A data center network with a hierarchical topology Load Balancing A cloud data center such as a Google or Microsoft data center provides many applications concurrently such as search email and video applications
To support requests from external clients each application is associated with a publicly visible IP address to which clients send their requests and from which they receive responses
Inside the data center the external requests are first directed to a load balancer whose job it is to distribute requests to the hosts balancing the load across the hosts as a function of their current load
A large data center will often have several load balancers each one devoted to a set of specific cloud applications
Such a load balancer is sometimes referred to as a layer switch since it makes decisions based on the destination port number layer as well as destination IP address in the packet
Upon receiving a request for a particular application the load balancer forwards it to one of the hosts that handles the application
A host may then invoke the services of other hosts to help process the request
When the host finishes processing the request it sends its response back to the load balancer which in turn relays the response back to the external client
The load balancer not only balances the work load across hosts but also provides a NATlike function translating the public external IP address to the internal IP address of the appropriate host and then translating back for packets traveling in the reverse direction back to the clients
This prevents clients from contacting hosts directly which has the security benefit of hiding the internal network structure and preventing clients from directly interacting with the hosts
Hierarchical Architecture For a small data center housing only a few thousand hosts a simple network consisting of a border router a load balancer and a few tens of racks all interconnected by a single Ethernet switch could possibly suffice
But to scale to tens to hundreds of thousands of hosts a data center often employs a hierarchy of routers and switches such as the topology shown in Figure 
At the top of the hierarchy the border router connects to access routers only two are shown in Figure 
but there can be many more
Below each access router there are three tiers of switches
Each access router connects to a toptier switch and each toptier switch connects to multiple secondtier switches and a load balancer
Each secondtier switch in turn connects to multiple racks via the racks TOR switches thirdtier switches
All links typically use Ethernet for their linklayer and physicallayer protocols with a mix of copper and fiber cabling
With such a hierarchical design it is possible to scale a data center to hundreds of thousands of hosts
Because it is critical for a cloud application provider to continually provide applications with high availability data centers also include redundant network equipment and redundant links in their designs not shown in Figure 
For example each TOR switch can connect to two tier switches and each access router tier switch and tier switch can be duplicated and integrated into the design Cisco Greenberg b
In the hierarchical design in Figure 
observe that the hosts below each access router form a single subnet
In order to localize ARP broadcast traffic each of these subnets is further partitioned into smaller VLAN subnets each comprising a few hundred hosts Greenberg a
Although the conventional hierarchical architecture just described solves the problem of scale it suffers from limited hosttohost capacity Greenberg b
To understand this limitation consider again Figure 
and suppose each host connects to its TOR switch with a Gbps link whereas the links between switches are Gbps Ethernet links
Two hosts in the same rack can always communicate at a full Gbps limited only by the rate of the hosts network interface cards
However if there are many simultaneous flows in the data center network the maximum rate between two hosts in different racks can be much less
To gain insight into this issue consider a traffic pattern consisting of simultaneous flows between pairs of hosts in different racks
Specifically suppose each of hosts in rack in Figure 
sends a flow to a corresponding host in rack 
Similarly there are ten simultaneous flows between pairs of hosts in racks and ten simultaneous flows between racks and and ten simultaneous flows between racks and 
If each flow evenly shares a links capacity with other flows traversing that link then the flows crossing the Gbps AtoB link as well as the Gbps BtoC link will each only receive Gbps Mbps which is significantly less than the Gbps network interface card rate
The problem becomes even more acute for flows between hosts that need to travel higher up the hierarchy
One possible solution to this limitation is to deploy higherrate switches and routers
But this would significantly increase the cost of the data center because switches and routers with high port speeds are very expensive
Supporting highbandwidth hosttohost communication is important because a key requirement in data centers is flexibility in placement of computation and services Greenberg b Farrington 
For example a largescale Internet search engine may run on thousands of hosts spread across multiple racks with significant bandwidth requirements between all pairs of hosts
Similarly a cloud computing service such as EC may wish to place the multiple virtual machines comprising a customers service on the physical hosts with the most capacity irrespective of their location in the data center
If these physical hosts are spread across multiple racks network bottlenecks as described above may result in poor performance
Trends in Data Center Networking In order to reduce the cost of data centers and at the same time improve their delay and throughput performance Internet cloud giants such as Google Facebook Amazon and Microsoft are continually deploying new data center network designs
Although these designs are proprietary many important trends can nevertheless be identified
One such trend is to deploy new interconnection architectures and network protocols that overcome the drawbacks of the traditional hierarchical designs
One such approach is to replace the hierarchy of switches and routers with a fully connected topology Facebook AlFares Greenberg b Guo such as the topology shown in Figure 
In this design each tier switch connects to all of the tier switches so that hosttohost traffic never has to rise above the switch tiers and with n tier switches between any two tier switches there are n disjoint paths
Such a design can significantly improve the hosttohost capacity
To see this consider again our example of flows
The topology in Figure 
can handle such a flow pattern since there are four distinct paths between the first tier switch and the second tier switch together providing an aggregate capacity of Gbps between the first two tier switches
Such a design not only alleviates the hosttohost capacity limitation but also creates a more flexible computation and service environment in which communication between any two racks not connected to the same switch is logically equivalent irrespective of their locations in the data center
Another major trend is to employ shipping containerbased modular data centers MDCs YouTube Waldrop 
In an MDC a factory builds within a Figure 
Highly interconnected data network topology standard meter shipping container a mini data center and ships the container to the data center location
Each container has up to a few thousand hosts stacked in tens of racks which are packed closely together
At the data center location multiple containers are interconnected with each other and also with the Internet
Once a prefabricated container is deployed at a data center it is often difficult to service
Thus each container is designed for graceful performance degradation as components servers and switches fail over time the container continues to operate but with degraded performance
When many components have failed and performance has dropped below a threshold the entire container is removed and replaced with a fresh one
Building a data center out of containers creates new networking challenges
With an MDC there are two types of networks the containerinternal networks within each of the containers and the core network connecting each container Guo Farrington 
Within each container at the scale of up to a few thousand hosts it is possible to build a fully connected network as described above using inexpensive commodity Gigabit Ethernet switches
However the design of the core network interconnecting hundreds to thousands of containers while providing high hosttohost bandwidth across containers for typical workloads remains a challenging problem
A hybrid electricaloptical switch architecture for interconnecting the containers is proposed in Farrington 
When using highly interconnected topologies one of the major issues is designing routing algorithms among the switches
One possibility Greenberg b is to use a form of random routing
Another possibility Guo is to deploy multiple network interface cards in each host connect each host to multiple lowcost commodity switches and allow the hosts themselves to intelligently route traffic among the switches
Variations and extensions of these approaches are currently being deployed in contemporary data centers
Another important trend is that large cloud providers are increasingly building or customizing just about everything that is in their data centers including network adapters switches routers TORs software and networking protocols Greenberg Singh 
Another trend pioneered by Amazon is to improve reliability with availability zones which essentially replicate distinct data centers in different nearby buildings
By having the buildings nearby a few kilometers apart transactional data can be synchronized across the data centers in the same availability zone while providing fault tolerance Amazon 
Many more innovations in data center design are likely to continue to come interested readers are encouraged to see the recent papers and videos on data center network design
Retrospective A Day in the Life of a Web Page Request Now that weve covered the link layer in this chapter and the network transport and application layers in earlier chapters our journey down the protocol stack is complete In the very beginning of this book Section 
we wrote much of this book is concerned with computer network protocols and in the first five chapters weve certainly seen that this is indeed the case Before heading into the topical chapters in second part of this book wed like to wrap up our journey down the protocol stack by taking an integrated holistic view of the protocols weve learned about so far
One way then to take this big picture view is to identify the many many protocols that are involved in satisfying even the simplest request downloading a Web page
illustrates our setting a student Bob connects a laptop to his schools Ethernet switch and downloads a Web page say the home page of www.google.com
As we now know theres a lot going on under the hood to satisfy this seemingly simple request
A Wireshark lab at the end of this chapter examines trace files containing a number of the packets involved in similar scenarios in more detail
Getting Started DHCP UDP IP and Ethernet Lets suppose that Bob boots up his laptop and then connects it to an Ethernet cable connected to the schools Ethernet switch which in turn is connected to the schools router as shown in Figure 
The schools router is connected to an ISP in this example comcast.net
In this example comcast.net is providing the DNS service for the school thus the DNS server resides in the Comcast network rather than the school network
Well assume that the DHCP server is running within the router as is often the case
When Bob first connects his laptop to the network he cant do anything e.g
download a Web page without an IP address
Thus the first networkrelated Figure 
A day in the life of a Web page request Network setting and actions action taken by Bobs laptop is to run the DHCP protocol to obtain an IP address as well as other information from the local DHCP server 
The operating system on Bobs laptop creates a DHCP request message Section 
and puts this message within a UDP segment Section 
with destination port DHCP server and source port DHCP client
The UDP segment is then placed within an IP datagram Section 
with a broadcast IP destination address 
and a source IP address of 
since Bobs laptop doesnt yet have an IP address
The IP datagram containing the DHCP request message is then placed within an Ethernet frame Section 
The Ethernet frame has a destination MAC addresses of FFFFFFFFFFFF so that the frame will be broadcast to all devices connected to the switch hopefully including a DHCP server the frames source MAC address is that of Bobs laptop DA
The broadcast Ethernet frame containing the DHCP request is the first frame sent by Bobs laptop to the Ethernet switch
The switch broadcasts the incoming frame on all outgoing ports including the port connected to the router
The router receives the broadcast Ethernet frame containing the DHCP request on its interface with MAC address BFB and the IP datagram is extracted from the Ethernet frame
The datagrams broadcast IP destination address indicates that this IP datagram should be processed by upper layer protocols at this node so the datagrams payload a UDP segment is thus demultiplexed Section 
up to UDP and the DHCP request message is extracted from the UDP segment
The DHCP server now has the DHCP request message
Lets suppose that the DHCP server running within the router can allocate IP addresses in the CIDR Section 
In this example all IP addresses used within the school are thus within Comcasts address block
Lets suppose the DHCP server allocates address 
to Bobs laptop
The DHCP server creates a DHCP ACK message Section 
containing this IP address as well as the IP address of the DNS server 
the IP address for the default gateway router 
and the subnet block 
equivalently the network mask
The DHCP message is put inside a UDP segment which is put inside an IP datagram which is put inside an Ethernet frame
The Ethernet frame has a source MAC address of the routers interface to the home network BFB and a destination MAC address of Bobs laptop DA
The Ethernet frame containing the DHCP ACK is sent unicast by the router to the switch
Because the switch is selflearning Section 
and previously received an Ethernet frame containing the DHCP request from Bobs laptop the switch knows to forward a frame addressed to DA only to the output port leading to Bobs laptop
Bobs laptop receives the Ethernet frame containing the DHCP ACK extracts the IP datagram from the Ethernet frame extracts the UDP segment from the IP datagram and extracts the DHCP ACK message from the UDP segment
Bobs DHCP client then records its IP address and the IP address of its DNS server
It also installs the address of the default gateway into its IP forwarding table Section 
Bobs laptop will send all datagrams with destination address outside of its subnet 
to the default gateway
At this point Bobs laptop has initialized its networking components and is ready to begin processing the Web page fetch
Note that only the last two DHCP steps of the four presented in Chapter are actually necessary
Still Getting Started DNS and ARP When Bob types the URL for www.google.com into his Web browser he begins the long chain of events that will eventually result in Googles home page being displayed by his Web browser
Bobs Web browser begins the process by creating a TCP socket Section 
that will be used to send the HTTP request Section 
In order to create the socket Bobs laptop will need to know the IP address of www.google.com
We learned in Section 
that the DNS protocol is used to provide this nametoIPaddress translation service
The operating system on Bobs laptop thus creates a DNS query message Section 
putting the string www.google.com in the question section of the DNS message
This DNS message is then placed within a UDP segment with a destination port of DNS server
The UDP segment is then placed within an IP datagram with an IP destination address of 
the address of the DNS server returned in the DHCP ACK in step and a source IP address of 
Bobs laptop then places the datagram containing the DNS query message in an Ethernet frame
This frame will be sent addressed at the link layer to the gateway router in Bobs schools network
However even though Bobs laptop knows the IP address of the schools gateway router 
via the DHCP ACK message in step above it doesnt know the gateway routers MAC address
In order to obtain the MAC address of the gateway router Bobs laptop will need to use the ARP protocol Section 
Bobs laptop creates an ARP query message with a target IP address of 
the default gateway places the ARP message within an Ethernet frame with a broadcast destination address FFFFFFFFFFFF and sends the Ethernet frame to the switch which delivers the frame to all connected devices including the gateway router
The gateway router receives the frame containing the ARP query message on the interface to the school network and finds that the target IP address of 
in the ARP message matches the IP address of its interface
The gateway router thus prepares an ARP reply indicating that its MAC address of BFB corresponds to IP address 
It places the ARP reply message in an Ethernet frame with a destination address of DA Bobs laptop and sends the frame to the switch which delivers the frame to Bobs laptop
Bobs laptop receives the frame containing the ARP reply message and extracts the MAC address of the gateway router BFB from the ARP reply message
Bobs laptop can now finally address the Ethernet frame containing the DNS query to the gateway routers MAC address
Note that the IP datagram in this frame has an IP destination address of 
the DNS server while the frame has a destination address of BFB the gateway router
Bobs laptop sends this frame to the switch which delivers the frame to the gateway router
Still Getting Started IntraDomain Routing to the DNS Server 
The gateway router receives the frame and extracts the IP datagram containing the DNS query
The router looks up the destination address of this datagram 
and determines from its forwarding table that the datagram should be sent to the leftmost router in the Comcast network in Figure 
The IP datagram is placed inside a linklayer frame appropriate for the link connecting the schools router to the leftmost Comcast router and the frame is sent over this link
The leftmost router in the Comcast network receives the frame extracts the IP datagram examines the datagrams destination address 
and determines the outgoing interface on which to forward the datagram toward the DNS server from its forwarding table which has been filled in by Comcasts intradomain protocol such as RIP OSPF or ISIS Section 
as well as the Internets interdomain protocol BGP Section 
Eventually the IP datagram containing the DNS query arrives at the DNS server
The DNS server extracts the DNS query message looks up the name www.google.com in its DNS database Section 
and finds the DNS resource record that contains the IP address 
assuming that it is currently cached in the DNS server
Recall that this cached data originated in the authoritative DNS server Section 
The DNS server forms a DNS reply message containing this hostnametoIP address mapping and places the DNS reply message in a UDP segment and the segment within an IP datagram addressed to Bobs laptop 
This datagram will be forwarded back through the Comcast network to the schools router and from there via the Ethernet switch to Bobs laptop
Bobs laptop extracts the IP address of the server www.google.com from the DNS message
Finally after a lot of work Bobs laptop is now ready to contact the www.google.com server 
Web ClientServer Interaction TCP and HTTP 
Now that Bobs laptop has the IP address of www.google.com it can create the TCP socket Section 
that will be used to send the HTTP GET message Section 
When Bob creates the TCP socket the TCP in Bobs laptop must first perform a threeway handshake Section 
with the TCP in www.google.com
Bobs laptop thus first creates a TCP SYN segment with destination port for HTTP places the TCP segment inside an IP datagram with a destination IP address of 
www.google.com places the datagram inside a frame with a destination MAC address of BFB the gateway router and sends the frame to the switch
The routers in the school network Comcasts network and Googles network forward the datagram containing the TCP SYN toward www.google.com using the forwarding table in each router as in steps above
Recall that the router forwarding table entries governing forwarding of packets over the interdomain link between the Comcast and Google networks are determined by the BGP protocol Chapter 
Eventually the datagram containing the TCP SYN arrives at www.google.com
The TCP SYN message is extracted from the datagram and demultiplexed to the welcome socket associated with port 
A connection socket Section 
is created for the TCP connection between the Google HTTP server and Bobs laptop
A TCP SYNACK Section 
segment is generated placed inside a datagram addressed to Bobs laptop and finally placed inside a linklayer frame appropriate for the link connecting www.google.com to its firsthop router
The datagram containing the TCP SYNACK segment is forwarded through the Google Comcast and school networks eventually arriving at the Ethernet card in Bobs laptop
The datagram is demultiplexed within the operating system to the TCP socket created in step which enters the connected state
With the socket on Bobs laptop now finally ready to send bytes to www.google.com Bobs browser creates the HTTP GET message Section 
containing the URL to be fetched
The HTTP GET message is then written into the socket with the GET message becoming the payload of a TCP segment
The TCP segment is placed in a datagram and sent and delivered to www.google.com as in steps above
The HTTP server at www.google.com reads the HTTP GET message from the TCP socket creates an HTTP response message Section 
places the requested Web page content in the body of the HTTP response message and sends the message into the TCP socket
The datagram containing the HTTP reply message is forwarded through the Google Comcast and school networks and arrives at Bobs laptop
Bobs Web browser program reads the HTTP response from the socket extracts the html for the Web page from the body of the HTTP response and finally finally displays the Web page Our scenario above has covered a lot of networking ground If youve understood most or all of the above example then youve also covered a lot of ground since you first read Section 
where we wrote much of this book is concerned with computer network protocols and you may have wondered what a protocol actually was As detailed as the above example might seem weve omitted a number of possible additional protocols e.g
NAT running in the schools gateway router wireless access to the schools network security protocols for accessing the school network or encrypting segments or datagrams network management protocols and considerations Web caching the DNS hierarchy that one would encounter in the public Internet
Well cover a number of these topics and more in the second part of this book
Lastly we note that our example above was an integrated and holistic but also very nuts and bolts view of many of the protocols that weve studied in the first part of this book
The example focused more on the how than the why
For a broader more reflective view on the design of network protocols in general see Clark RFC 
Summary In this chapter weve examined the link layerits services the principles underlying its operation and a number of important specific protocols that use these principles in implementing linklayer services
We saw that the basic service of the link layer is to move a networklayer datagram from one node host switch router WiFi access point to an adjacent node
We saw that all linklayer protocols operate by encapsulating a networklayer datagram within a linklayer frame before transmitting the frame over the link to the adjacent node
Beyond this common framing function however we learned that different linklayer protocols provide very different link access delivery and transmission services
These differences are due in part to the wide variety of link types over which linklayer protocols must operate
A simple pointtopoint link has a single sender and receiver communicating over a single wire
A multiple access link is shared among many senders and receivers consequently the linklayer protocol for a multiple access channel has a protocol its multiple access protocol for coordinating link access
In the case of MPLS the link connecting two adjacent nodes for example two IP routers that are adjacent in an IP sensethat they are nexthop IP routers toward some destination may actually be a network in and of itself
In one sense the idea of a network being considered as a link should not seem odd
A telephone link connecting a home modemcomputer to a remote modemrouter for example is actually a path through a sophisticated and complex telephone network
Among the principles underlying linklayer communication we examined errordetection and correction techniques multiple access protocols linklayer addressing virtualization VLANs and the construction of extended switched LANs and data center networks
Much of the focus today at the link layer is on these switched networks
In the case of error detectioncorrection we examined how it is possible to add additional bits to a frames header in order to detect and in some cases correct bitflip errors that might occur when the frame is transmitted over the link
We covered simple parity and checksumming schemes as well as the more robust cyclic redundancy check
We then moved on to the topic of multiple access protocols
We identified and studied three broad approaches for coordinating access to a broadcast channel channel partitioning approaches TDM FDM random access approaches the ALOHA protocols and CSMA protocols and takingturns approaches polling and token passing
We studied the cable access network and found that it uses many of these multiple access methods
We saw that a consequence of having multiple nodes share a single broadcast channel was the need to provide node addresses at the link layer
We learned that linklayer addresses were quite different from networklayer addresses and that in the case of the Internet a special protocol ARPthe Address Resolution Protocol is used to translate between these two forms of addressing and studied the hugely successful Ethernet protocol in detail
We then examined how nodes sharing a broadcast channel form a LAN and how multiple LANs can be connected together to form larger LANsall without the intervention of networklayer routing to interconnect these local nodes
We also learned how multiple virtual LANs can be created on a single physical LAN infrastructure
We ended our study of the link layer by focusing on how MPLS networks provide linklayer services when they interconnect IP routers and an overview of the network designs for todays massive data centers
We wrapped up this chapter and indeed the first five chapters by identifying the many protocols that are needed to fetch a simple Web page
Having covered the link layer our journey down the protocol stack is now over Certainly the physical layer lies below the link layer but the details of the physical layer are probably best left for another course for example in communication theory rather than computer networking
We have however touched upon several aspects of the physical layer in this chapter and in Chapter our discussion of physical media in Section 
Well consider the physical layer again when we study wireless link characteristics in the next chapter
Although our journey down the protocol stack is over our study of computer networking is not yet at an end
In the following three chapters we cover wireless networking network security and multimedia networking
These four topics do not fit conveniently into any one layer indeed each topic crosscuts many layers
Understanding these topics billed as advanced topics in some networking texts thus requires a firm foundation in all layers of the protocol stacka foundation that our study of the link layer has now completed Homework Problems and Questions Chapter Review Questions SECTIONS 
Consider the transportation analogy in Section 
If the passenger is analagous to a datagram what is analogous to the link layer frame R
If all the links in the Internet were to provide reliable delivery service would the TCP reliable delivery service be redundant Why or why not R
What are some of the possible services that a linklayer protocol can offer to the network layer Which of these linklayer services have corresponding services in IP In TCP SECTION 
Suppose two nodes start to transmit at the same time a packet of length L over a broadcast channel of rate R
Denote the propagation delay between the two nodes as dprop
Will there be a collision if dpropLR Why or why not R
In Section 
we listed four desirable characteristics of a broadcast channel
Which of these characteristics does slotted ALOHA have Which of these characteristics does token passing have R
In CSMACD after the fifth collision what is the probability that a node chooses K The result K corresponds to a delay of how many seconds on a Mbps Ethernet R
Describe polling and tokenpassing protocols using the analogy of cocktail party interactions
Why would the tokenring protocol be inefficient if a LAN had a very large perimeter SECTION 
How big is the MAC address space The IPv address space The IPv address space R
Suppose nodes A B and C each attach to the same broadcast LAN through their adapters
If A sends thousands of IP datagrams to B with each encapsulating frame addressed to the MAC address of B will Cs adapter process these frames If so will Cs adapter pass the IP datagrams in these frames to the network layer C How would your answers change if A sends frames with the MAC broadcast address R
Why is an ARP query sent within a broadcast frame Why is an ARP response sent within a frame with a specific destination MAC address R
For the network in Figure 
the router has two ARP modules each with its own ARP table
Is it possible that the same MAC address appears in both tables R
Compare the frame structures for BASET BASET and Gigabit Ethernet
How do they differ R
Consider Figure 
How many subnetworks are there in the addressing sense of Section 
What is the maximum number of VLANs that can be configured on a switch supporting the .Q protocol Why R
Suppose that N switches supporting K VLAN groups are to be connected via a trunking protocol
How many ports are needed to connect the switches Justify your answer
Suppose the information content of a packet is the bit pattern and an even parity scheme is being used
What would the value of the field containing the parity bits be for the case of a twodimensional parity scheme Your answer should be such that a minimum length checksum field is used
Show give an example other than the one in Figure 
that twodimensional parity checks can correct and detect a single bit error
Show give an example of a doublebit error that can be detected but not corrected
Suppose the information portion of a packet D in Figure 
contains bytes consisting of the bit unsigned binary ASCII representation of string Networking
Compute the Internet checksum for this data
Consider the previous problem but instead suppose these bytes contain a
the binary representation of the numbers through 
the ASCII representation of the letters B through K uppercase
the ASCII representation of the letters b through k lowercase
Compute the Internet checksum for this data
Consider the bit generator G and suppose that D has the value 
What is the value of R P
Consider the previous problem but suppose that D has the value a
In this problem we explore some of the properties of the CRC
For the generator G given in Section 
answer the following questions
Why can it detect any single bit error in data D b
Can the above G detect any odd number of bit errors Why P
In Section 
we provided an outline of the derivation of the efficiency of slotted ALOHA
In this problem well complete the derivation
Recall that when there are N active nodes the efficiency of slotted ALOHA is NppN
Find the value of p that maximizes this expression
Using the value of p found in a find the efficiency of slotted ALOHA by letting N approach infinity
Hint NN approaches e as N approaches infinity
Show that the maximum efficiency of pure ALOHA is e
Note This problem is easy if you have completed the problem above P 
Consider two nodes A and B that use the slotted ALOHA protocol to contend for a channel
Suppose node A has more data to transmit than node B and node As retransmission probability pA is greater than node Bs retransmission probability pB
Provide a formula for node As average throughput
What is the total efficiency of the protocol with these two nodes b
If pApB is node As average throughput twice as large as that of node B Why or why not If not how can you choose pA and pB to make that happen c
In general suppose there are N nodes among which node A has retransmission probability p and all other nodes have retransmission probability p
Provide expressions to compute the average throughputs of node A and of any other node
Suppose four active nodesnodes A B C and Dare competing for access to a channel using slotted ALOHA
Assume each node has an infinite number of packets to send
Each node attempts to transmit in each slot with probability p
The first slot is numbered slot the second slot is numbered slot and so on
What is the probability that node A succeeds for the first time in slot b
What is the probability that some node either A B C or D succeeds in slot c
What is the probability that the first success occurs in slot d
What is the efficiency of this fournode system P
Graph the efficiency of slotted ALOHA and pure ALOHA as a function of p for the following values of N a
Consider a broadcast channel with N nodes and a transmission rate of R bps
Suppose the broadcast channel uses polling with an additional polling node for multiple access
Suppose the amount of time from when a node completes transmission until the subsequent node is permitted to transmit that is the polling delay is dpoll
Suppose that within a polling round a given node is allowed to transmit at most Q bits
What is the maximum throughput of the broadcast channel P
Consider three LANs interconnected by two routers as shown in Figure 
Assign IP addresses to all of the interfaces
For Subnet use addresses of the form ...xxx for Subnet uses addresses of the form ...xxx and for Subnet use addresses of the form ...xxx
Assign MAC addresses to all of the adapters
Consider sending an IP datagram from Host E to Host B
Suppose all of the ARP tables are up to date
Enumerate all the steps as done for the singlerouter example in Section 
Repeat c now assuming that the ARP table in the sending host is empty and the other tables are up to date
Consider Figure 
Now we replace the router between subnets and with a switch S and label the router between subnets and as R
Three subnets interconnected by routers a
Consider sending an IP datagram from Host E to Host F
Will Host E ask router R to help forward the datagram Why In the Ethernet frame containing the IP datagram what are the source and destination IP and MAC addresses b
Suppose E would like to send an IP datagram to B and assume that Es ARP cache does not contain Bs MAC address
Will E perform an ARP query to find Bs MAC address Why In the Ethernet frame containing the IP datagram destined to B that is delivered to router R what are the source and destination IP and MAC addresses c
Suppose Host A would like to send an IP datagram to Host B and neither As ARP cache contains Bs MAC address nor does Bs ARP cache contain As MAC address
Further suppose that the switch Ss forwarding table contains entries for Host B and router R only
Thus A will broadcast an ARP request message
What actions will switch S perform once it receives the ARP request message Will router R also receive this ARP request message If so will R forward the message to Subnet Once Host B receives this ARP request message it will send back to Host A an ARP response message
But will it send an ARP query message to ask for As MAC address Why What will switch S do once it receives an ARP response message from Host B P
Consider the previous problem but suppose now that the router between subnets and is replaced by a switch
Answer questions ac in the previous problem in this new context
Recall that with the CSMACD protocol the adapter waits K bit times after a collision where K is drawn randomly
For K how long does the adapter wait until returning to Step for a Mbps broadcast channel For a Mbps broadcast channel P
Suppose nodes A and B are on the same Mbps broadcast channel and the propagation delay between the two nodes is bit times
Suppose CSMACD and Ethernet packets are used for this broadcast channel
Suppose node A begins transmitting a frame and before it finishes node B begins transmitting a frame
Can A finish transmitting before it detects that B has transmitted Why or why not If the answer is yes then A incorrectly believes that its frame was successfully transmitted without a collision
Hint Suppose at time t bits A begins transmitting a frame
In the worst case A transmits a minimumsized frame of bit times
So A would finish transmitting the frame at t bit times
Thus the answer is no if Bs signal reaches A before bit time t bits
In the worst case when does Bs signal reach A P
Suppose nodes A and B are on the same Mbps broadcast channel and the propagation delay between the two nodes is bit times
Suppose A and B send Ethernet frames at the same time the frames collide and then A and B choose different values of K in the CSMACD algorithm
Assuming no other nodes are active can the retransmissions from A and B collide For our purposes it suffices to work out the following example
Suppose A and B begin transmission at t bit times
They both detect collisions at t t bit times
Suppose KA and KB
At what time does B schedule its retransmission At what time does A begin transmission Note The nodes must wait for an idle channel after returning to Step see protocol
At what time does As signal reach B Does B refrain from transmitting at its scheduled time P
In this problem you will derive the efficiency of a CSMACDlike multiple access protocol
In this protocol time is slotted and all adapters are synchronized to the slots
Unlike slotted ALOHA however the length of a slot in seconds is much less than a frame time the time to transmit a frame
Let S be the length of a slot
Suppose all frames are of constant length LkRS where R is the transmission rate of the channel and k is a large integer
Suppose there are N nodes each with an infinite number of frames to send
We also assume that dpropS so that all nodes can detect a collision before the end of a slot time
The protocol is as follows If for a given slot no node has possession of the channel all nodes contend for the channel in particular each node transmits in the slot with probability p
If exactly one node transmits in the slot that node takes possession of the channel for the subsequent k slots and transmits its entire frame
If some node has possession of the channel all other nodes refrain from transmitting until the node that possesses the channel has finished transmitting its frame
Once this node has transmitted its frame all nodes contend for the channel
Note that the channel alternates between two states the productive state which lasts exactly k slots and the nonproductive state which lasts for a random number of slots
Clearly the channel efficiency is the ratio of kkx where x is the expected number of consecutive unproductive slots
For fixed N and p determine the efficiency of this protocol
For fixed N determine the p that maximizes the efficiency
Using the p which is a function of N found in b determine the efficiency as N approaches infinity
Show that this efficiency approaches as the frame length becomes large
Consider Figure 
in problem P
Provide MAC addresses and IP addresses for the interfaces at Host A both routers and Host F
Suppose Host A sends a datagram to Host F
Give the source and destination MAC addresses in the frame encapsulating this IP datagram as the frame is transmitted i from A to the left router ii from the left router to the right router iii from the right router to F
Also give the source and destination IP addresses in the IP datagram encapsulated within the frame at each of these points in time
Suppose now that the leftmost router in Figure 
is replaced by a switch
Hosts A B C and D and the right router are all starconnected into this switch
Give the source and destination MAC addresses in the frame encapsulating this IP datagram as the frame is transmitted i from A to the switch ii from the switch to the right router iii from the right router to F
Also give the source and destination IP addresses in the IP datagram encapsulated within the frame at each of these points in time
Consider Figure 
Suppose that all links are Mbps
What is the maximum total aggregate throughput that can be achieved among the hosts and servers in this network You can assume that any host or server can send to any other host or server
Suppose the three departmental switches in Figure 
are replaced by hubs
All links are Mbps
Now answer the questions posed in problem P
Suppose that all the switches in Figure 
are replaced by hubs
All links are Mbps
Now answer the questions posed in problem P
Lets consider the operation of a learning switch in the context of a network in which nodes labeled A through F are star connected into an Ethernet switch
Suppose that i B sends a frame to E ii E replies with a frame to B iii A sends a frame to B iv B replies with a frame to A
The switch table is initially empty
Show the state of the switch table before and after each of these events
For each of these events identify the links on which the transmitted frame will be forwarded and briefly justify your answers
In this problem we explore the use of small packets for VoiceoverIP applications
One of the drawbacks of a small packet size is that a large fraction of link bandwidth is consumed by overhead bytes
To this end suppose that the packet consists of P bytes and bytes of header
Consider sending a digitally encoded voice source directly
Suppose the source is encoded at a constant rate of kbps
Assume each packet is entirely filled before the source sends the packet into the network
The time required to fill a packet is the packetization delay
In terms of L determine the packetization delay in milliseconds
Packetization delays greater than msec can cause a noticeable and unpleasant echo
Determine the packetization delay for L bytes roughly corresponding to a maximumsized Ethernet packet and for L corresponding to an ATM packet
Calculate the storeandforward delay at a single switch for a link rate of R Mbps for L bytes and for L bytes
Comment on the advantages of using a small packet size
Consider the single switch VLAN in Figure 
and assume an external router is connected to switch port 
Assign IP addresses to the EE and CS hosts and router interface
Trace the steps taken at both the network layer and the link layer to transfer an IP datagram from an EE host to a CS host Hint Reread the discussion of Figure 
in the text
Consider the MPLS network shown in Figure 
and suppose that routers R and R are now MPLS enabled
Suppose that we want to perform traffic engineering so that packets from R destined for A are switched to A via RRRR and packets from R destined for A are switched via RRRR
Show the MPLS tables in R and R as well as the modified table in R that would make this possible
Consider again the same scenario as in the previous problem but suppose that packets from R destined for D are switched via RRR while packets from R destined to D are switched via RRRR
Show the MPLS tables in all routers that would make this possible
In this problem you will put together much of what you have learned about Internet protocols
Suppose you walk into a room connect to Ethernet and want to download a Web page
What are all the protocol steps that take place starting from powering on your PC to getting the Web page Assume there is nothing in our DNS or browser caches when you power on your PC
Hint The steps include the use of Ethernet DHCP ARP DNS TCP and HTTP protocols
Explicitly indicate in your steps how you obtain the IP and MAC addresses of a gateway router
Consider the data center network with hierarchical topology in Figure 
Suppose now there are pairs of flows with ten flows between the first and ninth rack ten flows between the second and tenth rack and so on
Further suppose that all links in the network are Gbps except for the links between hosts and TOR switches which are Gbps
Each flow has the same data rate determine the maximum rate of a flow
For the same traffic pattern determine the maximum rate of a flow for the highly interconnected topology in Figure 
Now suppose there is a similar traffic pattern but involving hosts on each rack and pairs of flows
Determine the maximum flow rates for the two topologies
Consider the hierarchical network in Figure 
and suppose that the data center needs to support email and video distribution among other applications
Suppose four racks of servers are reserved for email and four racks are reserved for video
For each of the applications all four racks must lie below a single tier switch since the tier to tier links do not have sufficient bandwidth to support the intraapplication traffic
For the email application suppose that for 
percent of the time only three racks are used and that the video application has identical usage patterns
For what fraction of time does the email application need to use a fourth rack How about for the video application b
Assuming email usage and video usage are independent for what fraction of time do equivalently what is the probability that both applications need their fourth rack c
Suppose that it is acceptable for an application to have a shortage of servers for 
percent of time or less causing rare periods of performance degradation for users
Discuss how the topology in Figure 
can be used so that only seven racks are collectively assigned to the two applications assuming that the topology can support all the traffic
Wireshark Labs At the Companion website for this textbook httpwww.pearsonhighered.comcsresources youll find a Wireshark lab that examines the operation of the IEEE 
protocol and the Wireshark frame format
A second Wireshark lab examines packet traces taken in a home network scenario
AN INTERVIEW WITH Simon S
Lam Simon S
Lam is Professor and Regents Chair in Computer Sciences at the University of Texas at Austin
From to he was with the ARPA Network Measurement Center at UCLA where he worked on satellite and radio packet switching
He led a research group that invented secure sockets and prototyped in the first secure sockets layer named Secure Network Programming which won the ACM Software System Award
His research interests are in design and analysis of network protocols and security services
He received his BSEE from Washington State University and his MS and PhD from UCLA
He was elected to the National Academy of Engineering in 
Why did you decide to specialize in networking When I arrived at UCLA as a new graduate student in Fall my intention was to study control theory
Then I took the queuing theory classes of Leonard Kleinrock and was very impressed by him
For a while I was working on adaptive control of queuing systems as a possible thesis topic
In early Larry Roberts initiated the ARPAnet Satellite System project later called Packet Satellite
Professor Kleinrock asked me to join the project
The first thing we did was to introduce a simple yet realistic backoff algorithm to the slotted ALOHA protocol
Shortly thereafter I found many interesting research problems such as ALOHAs instability problem and need for adaptive backoff which would form the core of my thesis
You were active in the early days of the Internet in the s beginning with your student days at UCLA
What was it like then Did people have any inkling of what the Internet would become The atmosphere was really no different from other systembuilding projects I have seen in industry and academia
The initially stated goal of the ARPAnet was fairly modest that is to provide access to expensive computers from remote locations so that many more scientists could use them
However with the startup of the Packet Satellite project in and the Packet Radio project in ARPAs goal had expanded substantially
By ARPA was building three different packet networks at the same time and it became necessary for Vint Cerf and Bob Kahn to develop an interconnection strategy
Back then all of these progressive developments in networking were viewed I believe as logical rather than magical
No one could have envisioned the scale of the Internet and power of personal computers today
It was a decade before appearance of the first PCs
To put things in perspective most students submitted their computer programs as decks of punched cards for batch processing
Only some students had direct access to computers which were typically housed in a restricted area
Modems were slow and still a rarity
As a graduate student I had only a phone on my desk and I used pencil and paper to do most of my work
Where do you see the field of networking and the Internet heading in the future In the past the simplicity of the Internets IP protocol was its greatest strength in vanquishing competition and becoming the de facto standard for internetworking
Unlike competitors such as X
in the s and ATM in the s IP can run on top of any linklayer networking technology because it offers only a besteffort datagram service
Thus any packet network can connect to the Internet
Today IPs greatest strength is actually a shortcoming
IP is like a straitjacket that confines the Internets development to specific directions
In recent years many researchers have redirected their efforts to the application layer only
There is also a great deal of research on wireless ad hoc networks sensor networks and satellite networks
These networks can be viewed either as standalone systems or linklayer systems which can flourish because they are outside of the IP straitjacket
Many people are excited about the possibility of PP systems as a platform for novel Internet applications
However PP systems are highly inefficient in their use of Internet resources
A concern of mine is whether the transmission and switching capacity of the Internet core will continue to increase faster than the traffic demand on the Internet as it grows to interconnect all kinds of devices and support future PPenabled applications
Without substantial overprovisioning of capacity ensuring network stability in the presence of malicious attacks and congestion will continue to be a significant challenge
The Internets phenomenal growth also requires the allocation of new IP addresses at a rapid rate to network operators and enterprises worldwide
At the current rate the pool of unallocated IPv addresses would be depleted in a few years
When that happens large contiguous blocks of address space can only be allocated from the IPv address space
Since adoption of IPv is off to a slow start due to lack of incentives for early adopters IPv and IPv will most likely co exist on the Internet for many years to come
Successful migration from an IPvdominant Internet to an IPvdominant Internet will require a substantial global effort
What is the most challenging part of your job The most challenging part of my job as a professor is teaching and motivating every student in my class and every doctoral student under my supervision rather than just the high achievers
The very bright and motivated may require a little guidance but not much else
I often learn more from these students than they learn from me
Educating and motivating the underachievers present a major challenge
What impacts do you foresee technology having on learning in the future Eventually almost all human knowledge will be accessible through the Internet which will be the most powerful tool for learning
This vast knowledge base will have the potential of leveling the playing field for students all over the world
For example motivated students in any country will be able to access the bestclass Web sites multimedia lectures and teaching materials
Already it was said that the IEEE and ACM digital libraries have accelerated the development of computer science researchers in China
In time the Internet will transcend all geographic barriers to learning
Chapter Wireless and Mobile Networks In the telephony world the past years have arguably been the golden years of cellular telephony
The number of worldwide mobile cellular subscribers increased from million in to nearly 
billion subscribers by with the number of cellular subscribers now surpassing the number of wired telephone lines
There are now a larger number of mobile phone subscriptions than there are people on our planet
The many advantages of cell phones are evident to allanywhere anytime untethered access to the global telephone network via a highly portable lightweight device
More recently laptops smartphones and tablets are wirelessly connected to the Internet via a cellular or WiFi network
And increasingly devices such as gaming consoles thermostats home security systems home appliances watches eye glasses cars traffic control systems and more are being wirelessly connected to the Internet
From a networking standpoint the challenges posed by networking these wireless and mobile devices particularly at the link layer and the network layer are so different from traditional wired computer networks that an individual chapter devoted to the study of wireless and mobile networks i.e
this chapter is appropriate
Well begin this chapter with a discussion of mobile users wireless links and networks and their relationship to the larger typically wired networks to which they connect
Well draw a distinction between the challenges posed by the wireless nature of the communication links in such networks and by the mobility that these wireless links enable
Making this important distinctionbetween wireless and mobilitywill allow us to better isolate identify and master the key concepts in each area
Note that there are indeed many networked environments in which the network nodes are wireless but not mobile e.g
wireless home or office networks with stationary workstations and large displays and that there are limited forms of mobility that do not require wireless links e.g
a worker who uses a wired laptop at home shuts down the laptop drives to work and attaches the laptop to the companys wired network
Of course many of the most exciting networked environments are those in which users are both wireless and mobilefor example a scenario in which a mobile user say in the back seat of car maintains a VoiceoverIP call and multiple ongoing TCP connections while racing down the autobahn at kilometers per hour soon in an autonomous vehicle
It is here at the intersection of wireless and mobility that well find the most interesting technical challenges Well begin by illustrating the setting in which well consider wireless communication and mobilitya network in which wireless and possibly mobile users are connected into the larger network infrastructure by a wireless link at the networks edge
Well then consider the characteristics of this wireless link in Section 
We include a brief introduction to code division multiple access CDMA a sharedmedium access protocol that is often used in wireless networks in Section 
In Section 
well examine the linklevel aspects of the IEEE 
WiFi wireless LAN standard in some depth well also say a few words about Bluetooth and other wireless personal area networks
In Section 
well provide an overview of cellular Internet access including G and emerging G cellular technologies that provide both voice and highspeed Internet access
In Section 
well turn our attention to mobility focusing on the problems of locating a mobile user routing to the mobile user and handing off the mobile user who dynamically moves from one point of attachment to the network to another
Well examine how these mobility services are implemented in the mobile IP standard in enterprise 
networks and in LTE cellular networks in Sections 
Finally well consider the impact of wireless links and mobility on transportlayer protocols and networked applications in Section 
Introduction Figure 
shows the setting in which well consider the topics of wireless data communication and mobility
Well begin by keeping our discussion general enough to cover a wide range of networks including both wireless LANs such as IEEE 
and cellular networks such as a G network well drill down into a more detailed discussion of specific wireless architectures in later sections
We can identify the following elements in a wireless network Wireless hosts
As in the case of wired networks hosts are the endsystem devices that run applications
A wireless host might be a laptop tablet smartphone or desktop computer
The hosts themselves may or may not be mobile
Elements of a wireless network Wireless links
A host connects to a base station defined below or to another wireless host through a wireless communication link
Different wireless link technologies have different transmission rates and can transmit over different distances
shows two key characteristics coverage area and link rate of the more popular wireless network standards
The figure is only meant to provide a rough idea of these characteristics
For example some of these types of networks are only now being deployed and some link rates can increase or decrease beyond the values shown depending on distance channel conditions and the number of users in the wireless network
Well cover these standards later in the first half of this chapter well also consider other wireless link characteristics such as their bit error rates and the causes of bit errors in Section 
In Figure 
wireless links connect wireless hosts located at the edge of the network into the larger network infrastructure
We hasten to add that wireless links are also sometimes used within a network to connect routers switches and Figure 
Link characteristics of selected wireless network standards other network equipment
However our focus in this chapter will be on the use of wireless communication at the network edge as it is here that many of the most exciting technical challenges and most of the growth are occurring
The base station is a key part of the wireless network infrastructure
Unlike the wireless host and wireless link a base station has no obvious counterpart in a wired network
A base station is responsible for sending and receiving data e.g
packets to and from a wireless host that is associated with that base station
A base station will often be responsible for coordinating the transmission of multiple wireless hosts with which it is associated
When we say a wireless host is associated with a base station we mean that the host is within the wireless communication distance of the base station and the host uses that base station to relay data between it the host and the larger network
Cell towers in cellular networks and access points in 
wireless LANs are examples of base stations
In Figure 
the base station is connected to the larger network e.g
the Internet corporate or home network or telephone network thus functioning as a linklayer relay between the wireless host and the rest of the world with which the host communicates
Hosts associated with a base station are often referred to as operating in infrastructure mode since all traditional network services e.g
address assignment and routing are provided by the network to which a host is connected via CASE HISTORY PUBLIC WIFI ACCESS COMING SOON TO A LAMP POST NEAR YOU WiFi hotspotspublic locations where users can find 
wireless accessare becoming increasingly common in hotels airports and cafÃ©s around the world
Most college campuses offer ubiquitous wireless access and its hard to find a hotel that doesnt offer wireless Internet access
Over the past decade a number of cities have designed deployed and operated municipal WiFi networks
The vision of providing ubiquitous WiFi access to the community as a public service much like streetlightshelping to bridge the digital divide by providing Internet access to all citizens and to promote economic developmentis compelling
Many cities around the world including Philadelphia Toronto Hong Kong Minneapolis London and Auckland have plans to provide ubiquitous wireless within the city or have already done so to varying degrees
The goal in Philadelphia was to turn Philadelphia into the nations largest WiFi hotspot and help to improve education bridge the digital divide enhance neighborhood development and reduce the costs of government
The ambitious program an agreement between the city Wireless Philadelphia a nonprofit entity and the Internet Service Provider Earthlinkbuilt an operational network of .b hotspots on streetlamp pole arms and traffic control devices that covered percent of the city
But financial and operational concerns caused the network to be sold to a group of private investors in who later sold the network back to the city in 
Other cities such as Minneapolis Toronto Hong Kong and Auckland have had success with smallerscale efforts
The fact that 
networks operate in the unlicensed spectrum and hence can be deployed without purchasing expensive spectrum use rights would seem to make them financially attractive
access points see Section 
have much shorter ranges than G cellular base stations see Section 
requiring a larger number of deployed endpoints to cover the same geographic region
Cellular data networks providing Internet access on the other hand operate in the licensed spectrum
Cellular providers pay billions of dollars for spectrum access rights for their networks making cellular data networks a business rather than municipal undertaking
the base station
In ad hoc networks wireless hosts have no such infrastructure with which to connect
In the absence of such infrastructure the hosts themselves must provide for services such as routing address assignment DNSlike name translation and more
When a mobile host moves beyond the range of one base station and into the range of another it will change its point of attachment into the larger network i.e
change the base station with which it is associateda process referred to as handoff
Such mobility raises many challenging questions
If a host can move how does one find the mobile hosts current location in the network so that data can be forwarded to that mobile host How is addressing performed given that a host can be in one of many possible locations If the host moves during a TCP connection or phone call how is data routed so that the connection continues uninterrupted These and many many other questions make wireless and mobile networking an area of exciting networking research
This is the larger network with which a wireless host may wish to communicate
Having discussed the pieces of a wireless network we note that these pieces can be combined in many different ways to form different types of wireless networks
You may find a taxonomy of these types of wireless networks useful as you read on in this chapter or readlearn more about wireless networks beyond this book
At the highest level we can classify wireless networks according to two criteria i whether a packet in the wireless network crosses exactly one wireless hop or multiple wireless hops and ii whether there is infrastructure such as a base station in the network Singlehop infrastructurebased
These networks have a base station that is connected to a larger wired network e.g
Furthermore all communication is between this base station and a wireless host over a single wireless hop
networks you use in the classroom cafÃ© or library and the G LTE data networks that we will learn about shortly all fall in this category
The vast majority of our daily interactions are with singlehop infrastructurebased wireless networks
In these networks there is no base station that is connected to a wireless network
However as we will see one of the nodes in this singlehop network may coordinate the transmissions of the other nodes
Bluetooth networks that connect small wireless devices such as keyboards speakers and headsets and which we will study in Section 
networks in ad hoc mode are singlehop infrastructureless networks
In these networks a base station is present that is wired to the larger network
However some wireless nodes may have to relay their communication through other wireless nodes in order to communicate via the base station
Some wireless sensor networks and socalled wireless mesh networks fall in this category
There is no base station in these networks and nodes may have to relay messages among several other nodes in order to reach a destination
Nodes may also be mobile with connectivity changing among nodesa class of networks known as mobile ad hoc networks MANETs
If the mobile nodes are vehicles the network is a vehicular ad hoc network VANET
As you might imagine the development of protocols for such networks is challenging and is the subject of much ongoing research
In this chapter well mostly confine ourselves to singlehop networks and then mostly to infrastructure based networks
Lets now dig deeper into the technical challenges that arise in wireless and mobile networks
Well begin by first considering the individual wireless link deferring our discussion of mobility until later in this chapter
Wireless Links and Network Characteristics Lets begin by considering a simple wired network say a home network with a wired Ethernet switch see Section 
interconnecting the hosts
If we replace the wired Ethernet with a wireless 
network a wireless network interface would replace the hosts wired Ethernet interface and an access point would replace the Ethernet switch but virtually no changes would be needed at the network layer or above
This suggests that we focus our attention on the link layer when looking for important differences between wired and wireless networks
Indeed we can find a number of important differences between a wired link and a wireless link Decreasing signal strength
Electromagnetic radiation attenuates as it passes through matter e.g
a radio signal passing through a wall
Even in free space the signal will disperse resulting in decreased signal strength sometimes referred to as path loss as the distance between sender and receiver increases
Interference from other sources
Radio sources transmitting in the same frequency band will interfere with each other
For example 
GHz wireless phones and .b wireless LANs transmit in the same frequency band
Thus the .b wireless LAN user talking on a 
GHz wireless phone can expect that neither the network nor the phone will perform particularly well
In addition to interference from transmitting sources electromagnetic noise within the environment e.g
a nearby motor a microwave can result in interference
Multipath propagation occurs when portions of the electromagnetic wave reflect off objects and the ground taking paths of different lengths between a sender and receiver
This results in the blurring of the received signal at the receiver
Moving objects between the sender and receiver can cause multipath propagation to change over time
For a detailed discussion of wireless channel characteristics models and measurements see Anderson 
The discussion above suggests that bit errors will be more common in wireless links than in wired links
For this reason it is perhaps not surprising that wireless link protocols such as the 
protocol well examine in the following section employ not only powerful CRC error detection codes but also linklevel reliabledatatransfer protocols that retransmit corrupted frames
Having considered the impairments that can occur on a wireless channel lets next turn our attention to the host receiving the wireless signal
This host receives an electromagnetic signal that is a combination of a degraded form of the original signal transmitted by the sender degraded due to the attenuation and multipath propagation effects that we discussed above among others and background noise in the environment
The signaltonoise ratio SNR is a relative measure of the strength of the received signal i.e
the information being transmitted and this noise
The SNR is typically measured in units of decibels dB a unit of measure that some think is used by electrical engineers primarily to confuse computer scientists
The SNR measured in dB is twenty times the ratio of the base logarithm of the amplitude of the received signal to the amplitude of the noise
For our purposes here we need only know that a larger SNR makes it easier for the receiver to extract the transmitted signal from the background noise
adapted from Holland shows the bit error rate BERroughly speaking the probability that a transmitted bit is received in error at the receiverversus the SNR for three different modulation techniques for encoding information for transmission on an idealized wireless channel
The theory of modulation and coding as well as signal extraction and BER is well beyond the scope of Figure 
Bit error rate transmission rate and SNR Figure 
Hidden terminal problem caused by obstacle a and fading b this text see Schwartz for a discussion of these topics
Nonetheless Figure 
illustrates several physicallayer characteristics that are important in understanding higherlayer wireless communication protocols For a given modulation scheme the higher the SNR the lower the BER
Since a sender can increase the SNR by increasing its transmission power a sender can decrease the probability that a frame is received in error by increasing its transmission power
Note however that there is arguably little practical gain in increasing the power beyond a certain threshold say to decrease the BER from to 
There are also disadvantages associated with increasing the transmission power More energy must be expended by the sender an important concern for batterypowered mobile users and the senders transmissions are more likely to interfere with the transmissions of another sender see Figure .b
For a given SNR a modulation technique with a higher bit transmission rate whether in error or not will have a higher BER
For example in Figure 
with an SNR of dB BPSK modulation with a transmission rate of Mbps has a BER of less than while with QAM modulation with a transmission rate of Mbps the BER is far too high to be practically useful
However with an SNR of dB QAM modulation has a transmission rate of Mbps and a BER of while BPSK modulation has a transmission rate of only Mbps and a BER that is so low as to be literally off the charts
If one can tolerate a BER of the higher transmission rate offered by QAM would make it the preferred modulation technique in this situation
These considerations give rise to the final characteristic described next
Dynamic selection of the physicallayer modulation technique can be used to adapt the modulation technique to channel conditions
The SNR and hence the BER may change as a result of mobility or due to changes in the environment
Adaptive modulation and coding are used in cellular data systems and in the 
WiFi and G cellular data networks that well study in Sections 
This allows for example the selection of a modulation technique that provides the highest transmission rate possible subject to a constraint on the BER for given channel characteristics
A higher and timevarying bit error rate is not the only difference between a wired and wireless link
Recall that in the case of wired broadcast links all nodes receive the transmissions from all other nodes
In the case of wireless links the situation is not as simple as shown in Figure 
Suppose that Station A is transmitting to Station B
Suppose also that Station C is transmitting to Station B
With the socalled hidden terminal problem physical obstructions in the environment for example a mountain or a building may prevent A and C from hearing each others transmissions even though As and Cs transmissions are indeed interfering at the destination B
This is shown in Figure .a
A second scenario that results in undetectable collisions at the receiver results from the fading of a signals strength as it propagates through the wireless medium
Figure .b illustrates the case where A and C are placed such that their signals are not strong enough to detect each others transmissions yet their signals are strong enough to interfere with each other at station B
As well see in Section 
the hidden terminal problem and fading make multiple access in a wireless network considerably more complex than in a wired network
CDMA Recall from Chapter that when hosts communicate over a shared medium a protocol is needed so that the signals sent by multiple senders do not interfere at the receivers
In Chapter we described three classes of medium access protocols channel partitioning random access and taking turns
Code division multiple access CDMA belongs to the family of channel partitioning protocols
It is prevalent in wireless LAN and cellular technologies
Because CDMA is so important in the wireless world well take a quick look at CDMA now before getting into specific wireless access technologies in the subsequent sections
In a CDMA protocol each bit being sent is encoded by multiplying the bit by a signal the code that changes at a much faster rate known as the chipping rate than the original sequence of data bits
shows a simple idealized CDMA encodingdecoding scenario
Suppose that the rate at which original data bits reach the CDMA encoder defines the unit of time that is each original data bit to be transmitted requires a onebit slot time
Let di be the value of the data bit for the ith bit slot
For mathematical convenience we represent a data bit with a value as 
Each bit slot is further subdivided into M minislots in Figure 
M Figure 
A simple CDMA example Sender encoding receiver decoding although in practice M is much larger
The CDMA code used by the sender consists of a sequence of M values cm m M each taking a or value
In the example in Figure 
the Mbit CDMA code being used by the sender is 
To illustrate how CDMA works let us focus on the ith data bit di
For the mth minislot of the bit transmission time of di the output of the CDMA encoder Zim is the value of di multiplied by the mth bit in the assigned CDMA code cm Zimdicm 
In a simple world with no interfering senders the receiver would receive the encoded bits Zim and recover the original data bit di by computing diMmMZimcm 
The reader might want to work through the details of the example in Figure 
to see that the original data bits are indeed correctly recovered at the receiver using Equation 
The world is far from ideal however and as noted above CDMA must work in the presence of interfering senders that are encoding and transmitting their data using a different assigned code
But how can a CDMA receiver recover a senders original data bits when those data bits are being tangled with bits being transmitted by other senders CDMA works under the assumption that the interfering transmitted bit signals are additive
This means for example that if three senders send a value and a fourth sender sends a value during the same minislot then the received signal at all receivers during that minislot is a since 
In the presence of multiple senders sender s computes its encoded transmissions Zims in exactly the same manner as in Equation 
The value received at a receiver during the mth minislot of the ith bit slot however is now the sum of the transmitted bits from all N senders during that minislot ZimsNZims Amazingly if the senders codes are chosen carefully each receiver can recover the data sent by a given sender out of the aggregate signal simply by using the senders code in exactly the same manner as in Equation 
as shown in Figure 
for a twosender CDMA example
The Mbit CDMA code being used by the upper sender is while the CDMA code being used by the lower sender is 
illustrates a receiver recovering the original data bits from the upper sender
Note that the receiver is able to extract the data from sender in spite of the interfering transmission from sender 
Recall our cocktail analogy from Chapter 
A CDMA protocol is similar to having partygoers speaking in multiple languages in such circumstances humans are actually quite good at locking into the conversation in the language they understand while filtering out the remaining conversations
We see here that CDMA is a partitioning protocol in that it partitions the codespace as opposed to time or frequency and assigns each node a dedicated piece of the codespace
Our discussion here of CDMA is necessarily brief in practice a number of difficult issues must be addressed
First in order for the CDMA receivers to be able Figure 
A twosender CDMA example to extract a particular senders signal the CDMA codes must be carefully chosen
Second our discussion has assumed that the received signal strengths from various senders are the same in reality this can be difficult to achieve
There is a considerable body of literature addressing these and other issues related to CDMA see Pickholtz Viterbi for details
Wireless LANs Pervasive in the workplace the home educational institutions cafÃ©s airports and street corners wireless LANs are now one of the most important access network technologies in the Internet today
Although many technologies and standards for wireless LANs were developed in the s one particular class of standards has clearly emerged as the winner the IEEE 
wireless LAN also known as WiFi
In this section well take a close look at 
wireless LANs examining its frame structure its medium access protocol and its internetworking of 
LANs with wired Ethernet LANs
There are several 
standards for wireless LAN technology in the IEEE 
WiFi family as summarized in Table 
The different 
standards all share some common characteristics
They all use the same medium access protocol CSMACA which well discuss shortly
All three use the same frame structure for their linklayer frames as well
All three standards have the ability to reduce their transmission rate in order to reach out over greater distances
And importantly 
products are also all backwards compatible meaning for example that a mobile capable only of .g may still interact with a newer .ac base station
However as shown in Table 
the standards have some major differences at the physical layer
devices operate in two difference frequency ranges 
GHz referred to as the 
GHz range and 
GHz referred to as the GHz range
GHz range is an unlicensed frequency band where 
devices may compete for frequency spectrum with 
GHz phones and microwave ovens
At GHz 
LANs have a shorter transmission distance for a given power level and suffer more from multipath propagation
The two most recent standards .n IEEE .n and .ac IEEE .ac Cisco .ac uses multiple input multipleoutput MIMO antennas i.e
two or more antennas on the sending side and two or more antennas on the receiving side that are transmittingreceiving different signals Diggavi 
.ac base Table 
Summary of IEEE 
standards Standard Frequency Range Data Rate .b 
GHz up to Mbps .a GHz up to Mbps .g 
GHz up to Mbps .n 
GHz and GHz up to Mbps .ac GHz up to Mbps stations may transmit to multiple stations simultaneously and use smart antennas to adaptively beamform to target transmissions in the direction of a receiver
This decreases interference and increases the distance reached at a given data rate
The data rates shown in Table 
are for an idealized environment e.g
a receiver placed meter away from the base station with no interference a scenario that were unlikely to experience in practice So as the saying goes YMMV Your Mileage or in this case your wireless data rate May Vary
Architecture Figure 
illustrates the principal components of the 
wireless LAN architecture
The fundamental building block of the 
architecture is the basic service set BSS
A BSS contains one or more wireless stations and a central base station known as an access point AP in 
shows the AP in each of two BSSs connecting to an interconnection device such as a switch or router which in turn leads to the Internet
In a typical home network there is one AP and one router typically integrated together as one unit that connects the BSS to the Internet
As with Ethernet devices each 
wireless station has a byte MAC address that is stored in the firmware of the stations adapter that is 
network interface card
Each AP also has a MAC address for its wireless interface
As with Ethernet these MAC addresses are administered by IEEE and are in theory globally unique
LAN architecture Figure 
An IEEE 
ad hoc network As noted in Section 
wireless LANs that deploy APs are often referred to as infrastructure wireless LANs with the infrastructure being the APs along with the wired Ethernet infrastructure that interconnects the APs and a router
shows that IEEE 
stations can also group themselves together to form an ad hoc networka network with no central control and with no connections to the outside world
Here the network is formed on the fly by mobile devices that have found themselves in proximity to each other that have a need to communicate and that find no preexisting network infrastructure in their location
An ad hoc network might be formed when people with laptops get together for example in a conference room a train or a car and want to exchange data in the absence of a centralized AP
There has been tremendous interest in ad hoc networking as communicating portable devices continue to proliferate
In this section though well focus our attention on infrastructure wireless LANs
Channels and Association In 
each wireless station needs to associate with an AP before it can send or receive network layer data
Although all of the 
standards use association well discuss this topic specifically in the context of IEEE .bg
When a network administrator installs an AP the administrator assigns a one or twoword Service Set Identifier SSID to the access point
When you choose WiFi under Setting on your iPhone for example a list is displayed showing the SSID of each AP in range
The administrator must also assign a channel number to the AP
To understand channel numbers recall that 
operates in the frequency range of 
GHz to 
Within this MHz band 
defines partially overlapping channels
Any two channels are nonoverlapping if and only if they are separated by four or more channels
In particular the set of channels and is the only set of three nonoverlapping channels
This means that an administrator could create a wireless LAN with an aggregate maximum transmission rate of Mbps by installing three .b APs at the same physical location assigning channels and to the APs and interconnecting each of the APs with a switch
Now that we have a basic understanding of 
channels lets describe an interesting and not completely uncommon situationthat of a WiFi jungle
A WiFi jungle is any physical location where a wireless station receives a sufficiently strong signal from two or more APs
For example in many cafÃ©s in New York City a wireless station can pick up a signal from numerous nearby APs
One of the APs might be managed by the cafÃ© while the other APs might be in residential apartments near the cafÃ©
Each of these APs would likely be located in a different IP subnet and would have been independently assigned a channel
Now suppose you enter such a WiFi jungle with your phone tablet or laptop seeking wireless Internet access and a blueberry muffin
Suppose there are five APs in the WiFi jungle
To gain Internet access your wireless device needs to join exactly one of the subnets and hence needs to associate with exactly one of the APs
Associating means the wireless device creates a virtual wire between itself and the AP
Specifically only the associated AP will send data frames that is frames containing data such as a datagram to your wireless device and your wireless device will send data frames into the Internet only through the associated AP
But how does your wireless device associate with a particular AP And more fundamentally how does your wireless device know which APs if any are out there in the jungle The 
standard requires that an AP periodically send beacon frames each of which includes the APs SSID and MAC address
Your wireless device knowing that APs are sending out beacon frames scans the channels seeking beacon frames from any APs that may be out there some of which may be transmitting on the same channelits a jungle out there
Having learned about available APs from the beacon frames you or your wireless device select one of the APs for association
standard does not specify an algorithm for selecting which of the available APs to associate with that algorithm is left up to the designers of the 
firmware and software in your wireless device
Typically the device chooses the AP whose beacon frame is received with the highest signal strength
While a high signal strength is good see e.g
signal strength is not the only AP characteristic that will determine the performance a device receives
In particular its possible that the selected AP may have a strong signal but may be overloaded with other affiliated devices that will need to share the wireless bandwidth at that AP while an unloaded AP is not selected due to a slightly weaker signal
A number of alternative ways of choosing APs have thus recently been proposed Vasudevan Nicholson Sundaresan 
For an interesting and downtoearth discussion of how signal strength is measured see Bardwell 
Active and passive scanning for access points The process of scanning channels and listening for beacon frames is known as passive scanning see Figure .a
A wireless device can also perform active scanning by broadcasting a probe frame that will be received by all APs within the wireless devices range as shown in Figure .b
APs respond to the probe request frame with a probe response frame
The wireless device can then choose the AP with which to associate from among the responding APs
After selecting the AP with which to associate the wireless device sends an association request frame to the AP and the AP responds with an association response frame
Note that this second requestresponse handshake is needed with active scanning since an AP responding to the initial probe request frame doesnt know which of the possibly many responding APs the device will choose to associate with in much the same way that a DHCP client can choose from among multiple DHCP servers see Figure 
Once associated with an AP the device will want to join the subnet in the IP addressing sense of Section 
to which the AP belongs
Thus the device will typically send a DHCP discovery message see Figure 
into the subnet via the AP in order to obtain an IP address on the subnet
Once the address is obtained the rest of the world then views that device simply as another host with an IP address in that subnet
In order to create an association with a particular AP the wireless device may be required to authenticate itself to the AP
wireless LANs provide a number of alternatives for authentication and access
One approach used by many companies is to permit access to a wireless network based on a devices MAC address
A second approach used by many Internet cafÃ©s employs usernames and passwords
In both cases the AP typically communicates with an authentication server relaying information between the wireless device and the authentication server using a protocol such as RADIUS RFC or DIAMETER RFC 
Separating the authentication server from the AP allows one authentication server to serve many APs centralizing the often sensitive decisions of authentication and access within the single server and keeping AP costs and complexity low
Well see in Chapter that the new IEEE .i protocol defining security aspects of the 
protocol family takes precisely this approach
MAC Protocol Once a wireless device is associated with an AP it can start sending and receiving data frames to and from the access point
But because multiple wireless devices or the AP itself may want to transmit data frames at the same time over the same channel a multiple access protocol is needed to coordinate the transmissions
In the following well refer to the devices or the AP as wireless stations that share the multiple access channel
As discussed in Chapter and Section 
broadly speaking there are three classes of multiple access protocols channel partitioning including CDMA random access and taking turns
Inspired by the huge success of Ethernet and its random access protocol the designers of 
chose a random access protocol for 
This random access protocol is referred to as CSMA with collision avoidance or more succinctly as CSMACA
As with Ethernets CSMACD the CSMA in CSMACA stands for carrier sense multiple access meaning that each station senses the channel before transmitting and refrains from transmitting when the channel is sensed busy
Although both Ethernet and 
use carriersensing random access the two MAC protocols have important differences
First instead of using collision detection 
uses collision avoidance techniques
Second because of the relatively high bit error rates of wireless channels 
unlike Ethernet uses a linklayer acknowledgmentretransmission ARQ scheme
Well describe .s collisionavoidance and linklayer acknowledgment schemes below
Recall from Sections 
that with Ethernets collisiondetection algorithm an Ethernet station listens to the channel as it transmits
If while transmitting it detects that another station is also transmitting it aborts its transmission and tries to transmit again after waiting a small random amount of time
Unlike the 
Ethernet protocol the 
MAC protocol does not implement collision detection
There are two important reasons for this The ability to detect collisions requires the ability to send the stations own signal and receive to determine whether another station is also transmitting at the same time
Because the strength of the received signal is typically very small compared to the strength of the transmitted signal at the 
adapter it is costly to build hardware that can detect a collision
More importantly even if the adapter could transmit and listen at the same time and presumably abort transmission when it senses a busy channel the adapter would still not be able to detect all collisions due to the hidden terminal problem and fading as discussed in Section 
Because .wireless LANs do not use collision detection once a station begins to transmit a frame it transmits the frame in its entirety that is once a station gets started there is no turning back
As one might expect transmitting entire frames particularly long frames when collisions are prevalent can significantly degrade a multiple access protocols performance
In order to reduce the likelihood of collisions 
employs several collisionavoidance techniques which well shortly discuss
Before considering collision avoidance however well first need to examine .s linklayer acknowledgment scheme
Recall from Section 
that when a station in a wireless LAN sends a frame the frame may not reach the destination station intact for a variety of reasons
To deal with this nonnegligible chance of failure the 
MAC protocol uses linklayer acknowledgments
As shown in Figure 
when the destination station receives a frame that passes the CRC it waits a short period of time known as the Short Interframe Spacing SIFS and then sends back Figure 
uses linklayer acknowledgments an acknowledgment frame
If the transmitting station does not receive an acknowledgment within a given amount of time it assumes that an error has occurred and retransmits the frame using the CSMACA protocol to access the channel
If an acknowledgment is not received after some fixed number of retransmissions the transmitting station gives up and discards the frame
Having discussed how 
uses linklayer acknowledgments were now in a position to describe the 
Suppose that a station wireless device or an AP has a frame to transmit
If initially the station senses the channel idle it transmits its frame after a short period of time known as the Distributed Interframe Space DIFS see Figure 
Otherwise the station chooses a random backoff value using binary exponential backoff as we encountered in Section 
and counts down this value after DIFS when the channel is sensed idle
While the channel is sensed busy the counter value remains frozen
When the counter reaches zero note that this can only occur while the channel is sensed idle the station transmits the entire frame and then waits for an acknowledgment
If an acknowledgment is received the transmitting station knows that its frame has been correctly received at the destination station
If the station has another frame to send it begins the CSMACA protocol at step 
If the acknowledgment isnt received the transmitting station reenters the backoff phase in step with the random value chosen from a larger interval
Recall that under Ethernets CSMACD multiple access protocol Section 
a station begins transmitting as soon as the channel is sensed idle
With CSMACA however the station refrains from transmitting while counting down even when it senses the channel to be idle
Why do CSMACD and CDMACA take such different approaches here To answer this question lets consider a scenario in which two stations each have a data frame to transmit but neither station transmits immediately because each senses that a third station is already transmitting
With Ethernets CSMACD the two stations would each transmit as soon as they detect that the third station has finished transmitting
This would cause a collision which isnt a serious issue in CSMACD since both stations would abort their transmissions and thus avoid the useless transmissions of the remainders of their frames
however the situation is quite different
does not detect a collision and abort transmission a frame suffering a collision will be transmitted in its entirety
The goal in 
is thus to avoid collisions whenever possible
if the two stations sense the channel busy they both immediately enter random backoff hopefully choosing different backoff values
If these values are indeed different once the channel becomes idle one of the two stations will begin transmitting before the other and if the two stations are not hidden from each other the losing station will hear the winning stations signal freeze its counter and refrain from transmitting until the winning station has completed its transmission
In this manner a costly collision is avoided
Of course collisions can still occur with 
in this scenario The two stations could be hidden from each other or the two stations could choose random backoff values that are close enough that the transmission from the station starting first have yet to reach the second station
Recall that we encountered this problem earlier in our discussion of random access algorithms in the context of Figure 
Dealing with Hidden Terminals RTS and CTS The 
MAC protocol also includes a nifty but optional reservation scheme that helps avoid collisions even in the presence of hidden terminals
Lets investigate this scheme in the context of Figure 
which shows two wireless stations and one access point
Both of the wireless stations are within range of the AP whose coverage is shown as a shaded circle and both have associated with the AP
However due to fading the signal ranges of wireless stations are limited to the interiors of the shaded circles shown in Figure 
Thus each of the wireless stations is hidden from the other although neither is hidden from the AP
Lets now consider why hidden terminals can be problematic
Suppose Station H is transmitting a frame and halfway through Hs transmission Station H wants to send a frame to the AP
H not hearing the transmission from H will first wait a DIFS interval and then transmit the frame resulting in a collision
The channel will therefore be wasted during the entire period of Hs transmission as well as during Hs transmission
In order to avoid this problem the IEEE 
protocol allows a station to use a short Request to Send RTS control frame and a short Clear to Send CTS control frame to reserve access to the channel
When a sender wants to send a DATA Figure 
Hidden terminal example H is hidden from H and vice versa frame it can first send an RTS frame to the AP indicating the total time required to transmit the DATA frame and the acknowledgment ACK frame
When the AP receives the RTS frame it responds by broadcasting a CTS frame
This CTS frame serves two purposes It gives the sender explicit permission to send and also instructs the other stations not to send for the reserved duration
Thus in Figure 
before transmitting a DATA frame H first broadcasts an RTS frame which is heard by all stations in its circle including the AP
The AP then responds Figure 
Collision avoidance using the RTS and CTS frames with a CTS frame which is heard by all stations within its range including H and H
Station H having heard the CTS refrains from transmitting for the time specified in the CTS frame
The RTS CTS DATA and ACK frames are shown in Figure 
The use of the RTS and CTS frames can improve performance in two important ways The hidden station problem is mitigated since a long DATA frame is transmitted only after the channel has been reserved
Because the RTS and CTS frames are short a collision involving an RTS or CTS frame will last only for the duration of the short RTS or CTS frame
Once the RTS and CTS frames are correctly transmitted the following DATA and ACK frames should be transmitted without collisions
You are encouraged to check out the 
applet in the textbooks Web site
This interactive applet illustrates the CSMACA protocol including the RTSCTS exchange sequence
Although the RTSCTS exchange can help reduce collisions it also introduces delay and consumes channel resources
For this reason the RTSCTS exchange is only used if at all to reserve the channel for the transmission of a long DATA frame
In practice each wireless station can set an RTS threshold such that the RTSCTS sequence is used only when the frame is longer than the threshold
For many wireless stations the default RTS threshold value is larger than the maximum frame length so the RTSCTS sequence is skipped for all DATA frames sent
as a PointtoPoint Link Our discussion so far has focused on the use of 
in a multiple access setting
We should mention that if two nodes each have a directional antenna they can point their directional antennas at each other and run the 
protocol over what is essentially a pointtopoint link
Given the low cost of commodity 
hardware the use of directional antennas and an increased transmission power allow 
to be used as an inexpensive means of providing wireless pointtopoint connections over tens of kilometers distance
Raman describes one of the first such multihop wireless networks operating in the rural Ganges plains in India using pointtopoint 
The IEEE 
Frame Although the 
frame shares many similarities with an Ethernet frame it also contains a number of fields that are specific to its use for wireless links
frame is shown in Figure 
The numbers above each of the fields in the frame represent the lengths of the fields in bytes the numbers above each of the subfields in the frame control field represent the lengths of the subfields in bits
Lets now examine the fields in the frame as well as some of the more important subfields in the frames control field
frame Payload and CRC Fields At the heart of the frame is the payload which typically consists of an IP datagram or an ARP packet
Although the field is permitted to be as long as bytes it is typically fewer than bytes holding an IP datagram or an ARP packet
As with an Ethernet frame an 
frame includes a bit cyclic redundancy check CRC so that the receiver can detect bit errors in the received frame
As weve seen bit errors are much more common in wireless LANs than in wired LANs so the CRC is even more useful here
Address Fields Perhaps the most striking difference in the 
frame is that it has four address fields each of which can hold a byte MAC address
But why four address fields Doesnt a source MAC field and destination MAC field suffice as they do for Ethernet It turns out that three address fields are needed for internetworking purposesspecifically for moving the networklayer datagram from a wireless station through an AP to a router interface
The fourth address field is used when APs forward frames to each other in ad hoc mode
Since we are only considering infrastructure networks here lets focus our attention on the first three address fields
standard defines these fields as follows Address is the MAC address of the station that transmits the frame
Thus if a wireless station transmits the frame that stations MAC address is inserted in the address field
Similarly if an AP transmits the frame the APs MAC address is inserted in the address field
Address is the MAC address of the wireless station that is to receive the frame
Thus if a mobile wireless station transmits the frame address contains the MAC address of the destination AP
Similarly if an AP transmits the frame address contains the MAC address of the destination wireless station
The use of address fields in 
frames Sending frames between H and R To understand address recall that the BSS consisting of the AP and wireless stations is part of a subnet and that this subnet connects to other subnets via some router interface
Address contains the MAC address of this router interface
To gain further insight into the purpose of address lets walk through an internetworking example in the context of Figure 
In this figure there are two APs each of which is responsible for a number of wireless stations
Each of the APs has a direct connection to a router which in turn connects to the global Internet
We should keep in mind that an AP is a linklayer device and thus neither speaks IP nor understands IP addresses
Consider now moving a datagram from the router interface R to the wireless Station H
The router is not aware that there is an AP between it and H from the routers perspective H is just a host in one of the subnets to which it the router is connected
The router which knows the IP address of H from the destination address of the datagram uses ARP to determine the MAC address of H just as in an ordinary Ethernet LAN
After obtaining Hs MAC address router interface R encapsulates the datagram within an Ethernet frame
The source address field of this frame contains Rs MAC address and the destination address field contains Hs MAC address
When the Ethernet frame arrives at the AP the AP converts the 
Ethernet frame to an 
frame before transmitting the frame into the wireless channel
The AP fills in address and address with Hs MAC address and its own MAC address respectively as described above
For address the AP inserts the MAC address of R
In this manner H can determine from address the MAC address of the router interface that sent the datagram into the subnet
Now consider what happens when the wireless station H responds by moving a datagram from H to R
H creates an 
frame filling the fields for address and address with the APs MAC address and Hs MAC address respectively as described above
For address H inserts Rs MAC address
When the AP receives the 
frame it converts the frame to an Ethernet frame
The source address field for this frame is Hs MAC address and the destination address field is Rs MAC address
Thus address allows the AP to determine the appropriate destination MAC address when constructing the Ethernet frame
In summary address plays a crucial role for internetworking the BSS with a wired LAN
Sequence Number Duration and Frame Control Fields Recall that in 
whenever a station correctly receives a frame from another station it sends back an acknowledgment
Because acknowledgments can get lost the sending station may send multiple copies of a given frame
As we saw in our discussion of the rdt
protocol Section 
the use of sequence numbers allows the receiver to distinguish between a newly transmitted frame and the retransmission of a previous frame
The sequence number field in the 
frame thus serves exactly the same purpose here at the link layer as it did in the transport layer in Chapter 
Recall that the 
protocol allows a transmitting station to reserve the channel for a period of time that includes the time to transmit its data frame and the time to transmit an acknowledgment
This duration value is included in the frames duration field both for data frames and for the RTS and CTS frames
As shown in Figure 
the frame control field includes many subfields
Well say just a few words about some of the more important subfields for a more complete discussion you are encouraged to consult the 
specification Held Crow IEEE 
The type and subtype fields are used to distinguish the association RTS CTS ACK and data frames
The to and from fields are used to define the meanings of the different address fields
These meanings change depending on whether ad hoc or infrastructure modes are used and in the case of infrastructure mode whether a wireless station or an AP is sending the frame
Finally the WEP field indicates whether encryption is being used or not WEP is discussed in Chapter 
Mobility in the Same IP Subnet In order to increase the physical range of a wireless LAN companies and universities will often deploy multiple BSSs within the same IP subnet
This naturally raises the issue of mobility among the BSSs how do wireless stations seamlessly move from one BSS to another while maintaining ongoing TCP sessions As well see in this subsection mobility can be handled in a relatively straightforward manner when the BSSs are part of the subnet
When stations move between subnets more sophisticated mobility management protocols will be needed such as those well study in Sections 
Lets now look at a specific example of mobility between BSSs in the same subnet
shows two interconnected BSSs with a host H moving from BSS to BSS
Because in this example the interconnection device that connects the two BSSs is not a router all of the stations in the two BSSs including the APs belong to the same IP subnet
Thus when H moves from BSS to BSS it may keep its IP address and all of its ongoing TCP connections
If the interconnection device were a router then H would have to obtain a new IP address in the subnet in which it was moving
This address change would disrupt and eventually terminate any ongoing TCP connections at H
In Section 
well see how a networklayer mobility protocol such as mobile IP can be used to avoid this problem
But what specifically happens when H moves from BSS to BSS As H wanders away from AP H detects a weakening signal from AP and starts to scan for a stronger signal
H receives beacon frames from AP which in many corporate and university settings will have the same SSID as AP
H then disassociates with AP and associates with AP while keeping its IP address and maintaining its ongoing TCP sessions
This addresses the handoff problem from the host and AP viewpoint
But what about the switch in Figure 
How does it know that the host has moved from one AP to another As you may recall from Chapter switches are selflearning and automatically build their forwarding tables
This self learning feature nicely handles Figure 
Mobility in the same subnet occasional moves for example when an employee gets transferred from one department to another however switches were not designed to support highly mobile users who want to maintain TCP connections while moving between BSSs
To appreciate the problem here recall that before the move the switch has an entry in its forwarding table that pairs Hs MAC address with the outgoing switch interface through which H can be reached
If H is initially in BSS then a datagram destined to H will be directed to H via AP
Once H associates with BSS however its frames should be directed to AP
One solution a bit of a hack really is for AP to send a broadcast Ethernet frame with Hs source address to the switch just after the new association
When the switch receives the frame it updates its forwarding table allowing H to be reached via AP
The .f standards group is developing an interAP protocol to handle these and related issues
Our discussion above has focused on mobility with the same LAN subnet
Recall that VLANs which we studied in Section 
can be used to connect together islands of LANs into a large virtual LAN that can span a large geographical region
Mobility among base stations within such a VLAN can be handled in exactly the same manner as above Yu 
Advanced Features in 
Well wrap up our coverage of 
with a short discussion of two advanced capabilities found in 
As well see these capabilities are not completely specified in the 
standard but rather are made possible by mechanisms specified in the standard
This allows different vendors to implement these capabilities using their own proprietary approaches presumably giving them an edge over the competition
Rate Adaptation We saw earlier in Figure 
that different modulation techniques with the different transmission rates that they provide are appropriate for different SNR scenarios
Consider for example a mobile 
user who is initially meters away from the base station with a high signaltonoise ratio
Given the high SNR the user can communicate with the base station using a physicallayer modulation technique that provides high transmission rates while maintaining a low BER
This is one happy user Suppose now that the user becomes mobile walking away from the base station with the SNR falling as the distance from the base station increases
In this case if the modulation technique used in the 
protocol operating between the base station and the user does not change the BER will become unacceptably high as the SNR decreases and eventually no transmitted frames will be received correctly
For this reason some 
implementations have a rate adaptation capability that adaptively selects the underlying physicallayer modulation technique to use based on current or recent channel characteristics
If a node sends two frames in a row without receiving an acknowledgment an implicit indication of bit errors on the channel the transmission rate falls back to the next lower rate
If frames in a row are acknowledged or if a timer that tracks the time since the last fallback expires the transmission rate increases to the next higher rate
This rate adaptation mechanism shares the same probing philosophy as TCPs congestioncontrol mechanismwhen conditions are good reflected by ACK receipts the transmission rate is increased until something bad happens the lack of ACK receipts when something bad happens the transmission rate is reduced
rate adaptation and TCP congestion control are thus similar to the young child who is constantly pushing hisher parents for more and more say candy for a young child later curfew hours for the teenager until the parents finally say Enough and the child backs off only to try again later after conditions have hopefully improved
A number of other schemes have also been proposed to improve on this basic automatic rate adjustment scheme Kamerman Holland Lacage 
Power Management Power is a precious resource in mobile devices and thus the 
standard provides power management capabilities that allow 
nodes to minimize the amount of time that their sense transmit and receive functions and other circuitry need to be on
power management operates as follows
A node is able to explicitly alternate between sleep and wake states not unlike a sleepy student in a classroom
A node indicates to the access point that it will be going to sleep by setting the powermanagement bit in the header of an 
frame to 
A timer in the node is then set to wake up the node just before the AP is scheduled to send its beacon frame recall that an AP typically sends a beacon frame every msec
Since the AP knows from the set powertransmission bit that the node is going to sleep it the AP knows that it should not send any frames to that node and will buffer any frames destined for the sleeping host for later transmission
A node will wake up just before the AP sends a beacon frame and quickly enter the fully active state unlike the sleepy student this wakeup requires only microseconds Kamerman 
The beacon frames sent out by the AP contain a list of nodes whose frames have been buffered at the AP
If there are no buffered frames for the node it can go back to sleep
Otherwise the node can explicitly request that the buffered frames be sent by sending a polling message to the AP
With an interbeacon time of msec a wakeup time of microseconds and a similarly small time to receive a beacon frame and check to ensure that there are no buffered frames a node that has no frames to send or receive can be asleep of the time resulting in a significant energy savings
Personal Area Networks Bluetooth and Zigbee As illustrated in Figure 
the IEEE 
WiFi standard is aimed at communication among devices separated by up to meters except when 
is used in a pointtopoint configuration with a directional antenna
Two other wireless protocols in the IEEE family are Bluetooth and Zigbee defined in the IEEE 
and IEEE 
standards IEEE 
Bluetooth An IEEE 
network operates over a short range at low power and at low cost
It is essentially a lowpower shortrange lowrate cable replacement technology for interconnecting a computer with its wireless keyboard mouse or other peripheral device cellular phones speakers headphones and many other devices whereas 
is a higherpower mediumrange higherrate access technology
For this reason 
networks are sometimes referred to as wireless personal area networks WPANs
The link and physical layers of 
are based on the earlier Bluetooth specification for personal area networks Held Bisdikian 
networks operate in the 
GHz unlicensed radio band in a TDM manner with time slots of microseconds
During each time slot a sender transmits on one of channels with the channel changing in a known but pseudorandom manner from slot to slot
This form of channel hopping known as frequencyhopping spread spectrum FHSS spreads transmissions in time over the frequency spectrum
can provide data rates up to Mbps
networks are ad hoc networks No network infrastructure e.g
an access point is needed to interconnect 
devices must organize themselves
devices are first organized into a piconet of up to eight active devices as shown in Figure 
One of these devices is designated as the master with the remaining devices acting as slaves
The master node truly rules the piconetits clock determines time in the piconet it can transmit in each oddnumbered slot and a Figure 
A Bluetooth piconet slave can transmit only after the master has communicated with it in the previous slot and even then the slave can only transmit to the master
In addition to the slave devices there can also be up to parked devices in the network
These devices cannot communicate until their status has been changed from parked to active by the master node
For more information about WPANs the interested reader should consult the Bluetooth references Held Bisdikian or the official IEEE 
Web site IEEE 
Zigbee A second personal area network standardized by the IEEE is the 
standard IEEE 
known as Zigbee
While Bluetooth networks provide a cable replacement data rate of over a Megabit per second Zigbee is targeted at lowerpowered lowerdatarate lowerdutycycle applications than Bluetooth
While we may tend to think that bigger and faster is better not all network applications need high bandwidth and the consequent higher costs both economic and power costs
For example home temperature and light sensors security devices and wallmounted switches are all very simple low power lowdutycycle lowcost devices
Zigbee is thus wellsuited for these devices
Zigbee defines channel rates of and Kbps depending on the channel frequency
Nodes in a Zigbee network come in two flavors
Socalled reducedfunction devices operate as slave devices under the control of a single fullfunction device much as Bluetooth slave devices
A full function device can operate as a master device as in Bluetooth by controlling multiple slave devices and multiple fullfunction devices can additionally be configured into a mesh network in which full function devices route frames amongst themselves
Zigbee shares many protocol mechanisms that weve already encountered in other linklayer protocols beacon frames and linklayer acknowledgments similar to 
carriersense random access protocols with binary exponential backoff similar to 
and Ethernet and fixed guaranteed allocation of time slots similar to DOCSIS
Zigbee networks can be configured in many different ways
Lets consider the simple case of a single fullfunction device controlling multiple reducedfunction devices in a timeslotted manner using beacon frames
shows the case Figure 
superframe structure where the Zigbee network divides time into recurring super frames each of which begins with a beacon frame
Each beacon frame divides the super frame into an active period during which devices may transmit and an inactive period during which all devices including the controller can sleep and thus conserve power
The active period consists of time slots some of which are used by devices in a CSMACA random access manner and some of which are allocated by the controller to specific devices thus providing guaranteed channel access for those devices
More details about Zigbee networks can be found at Baronti IEEE 
Cellular Internet Access In the previous section we examined how an Internet host can access the Internet when inside a WiFi hotspotthat is when it is within the vicinity of an 
But most WiFi hotspots have a small coverage area of between and meters in diameter
What do we do then when we have a desperate need for wireless Internet access and we cannot access a WiFi hotspot Given that cellular telephony is now ubiquitous in many areas throughout the world a natural strategy is to extend cellular networks so that they support not only voice telephony but wireless Internet access as well
Ideally this Internet access would be at a reasonably high speed and would provide for seamless mobility allowing users to maintain their TCP sessions while traveling for example on a bus or a train
With sufficiently high upstream and downstream bit rates the user could even maintain video conferencing sessions while roaming about
This scenario is not that farfetched
Data rates of several megabits per second are becoming available as broadband data services such as those we will cover here become more widely deployed
In this section we provide a brief overview of current and emerging cellular Internet access technologies
Our focus here will be on both the wireless first hop as well as the network that connects the wireless first hop into the larger telephone network andor the Internet in Section 
well consider how calls are routed to a user moving between base stations
Our brief discussion will necessarily provide only a simplified and highlevel description of cellular technologies
Modern cellular communications of course has great breadth and depth with many universities offering several courses on the topic
Readers seeking a deeper understanding are encouraged to see Goodman Kaaranen Lin Korhonen Schiller Palat Scourias Turner Akyildiz as well as the particularly excellent and exhaustive references Mouly Sauter 
An Overview of Cellular Network Architecture In our description of cellular network architecture in this section well adopt the terminology of the Global System for Mobile Communications GSM standards
For history buffs the GSM acronym was originally derived from Groupe SpÃ©cial Mobile until the more anglicized name was adopted preserving the original acronym letters
In the s Europeans recognized the need for a panEuropean digital cellular telephony system that would replace the numerous incompatible analog cellular telephony systems leading to the GSM standard Mouly 
Europeans deployed GSM technology with great success in the early s and since then GSM has grown to be the pound gorilla of the cellular telephone world with more than of all cellular subscribers worldwide using GSM
CASE HISTORY G Cellular Mobile Versus Wireless LANs Many cellular mobile phone operators are deploying G cellular mobile systems
In some countries e.g
Korea and Japan G LTE coverage is higher than nearly ubiquitous
In average download rates over deployed LTE systems range from Mbps in the US and India to close to Mbps in New Zealand
These G systems are being deployed in licensed radiofrequency bands with some operators paying considerable sums to governments for spectrumuse licenses
G systems allow users to access the Internet from remote outdoor locations while on the move in a manner similar to todays cellular phoneonly access
In many cases a user may have simultaneous access to both wireless LANs and G
With the capacity of G systems being both more constrained and more expensive many mobile devices default to the use of WiFi rather than G when both are avilable
The question of whether wireless edge network access will be primarily over wireless LANs or cellular systems remains an open question The emerging wireless LAN infrastructure may become nearly ubiquitous
wireless LANs operating at Mbps and higher are enjoying widespread deployment
Essentially all laptops tablets and smartphones are factoryequipped with 
Furthermore emerging Internet appliancessuch as wireless cameras and picture framesalso have lowpowered wireless LAN capabilities
Wireless LAN base stations can also handle mobile phone appliances
Many phones are already capable of connecting to the cellular phone network or to an IP network either natively or using a Skypelike VoiceoverIP service thus bypassing the operators cellular voice and G data services
Of course many other experts believe that G not only will be a major success but will also dramatically revolutionize the way we work and live
Most likely both WiFi and G will both become prevalent wireless technologies with roaming wireless devices automatically selecting the access technology that provides the best service at their current physical location
When people talk about cellular technology they often classify the technology as belonging to one of several generations
The earliest generations were designed primarily for voice traffic
First generation G systems were analog FDMA systems designed exclusively for voiceonly communication
These G systems are almost extinct now having been replaced by digital G systems
The original G systems were also designed for voice but later extended .G to support data i.e
Internet as well as voice service
G systems also support voice and data but with an emphasis on data capabilities and higherspeed radio access links
The G systems being deployed today are based on LTE technology feature an allIP core network and provide integrated voice and data at multiMegabit speeds
Cellular Network Architecture G Voice Connections to the Telephone Network The term cellular refers to the fact that the region covered by a cellular network is partitioned into a number of geographic coverage areas known as cells shown as hexagons on the left side of Figure 
As with the .WiFi standard we studied in Section 
GSM has its own particular nomenclature
Each cell Figure 
Components of the GSM G cellular network architecture contains a base transceiver station BTS that transmits signals to and receives signals from the mobile stations in its cell
The coverage area of a cell depends on many factors including the transmitting power of the BTS the transmitting power of the user devices obstructing buildings in the cell and the height of base station antennas
Although Figure 
shows each cell containing one base transceiver station residing in the middle of the cell many systems today place the BTS at corners where three cells intersect so that a single BTS with directional antennas can service three cells
The GSM standard for G cellular systems uses combined FDMTDM radio for the air interface
Recall from Chapter that with pure FDM the channel is partitioned into a number of frequency bands with each band devoted to a call
Also recall from Chapter that with pure TDM time is partitioned into frames with each frame further partitioned into slots and each call being assigned the use of a particular slot in the revolving frame
In combined FDMTDM systems the channel is partitioned into a number of frequency subbands within each subband time is partitioned into frames and slots
Thus for a combined FDMTDM system if the channel is partitioned into F subbands and time is partitioned into T slots then the channel will be able to support F.T simultaneous calls
Recall that we saw in Section 
that cable access networks also use a combined FDMTDM approach
GSM systems consist of kHz frequency bands with each band supporting eight TDM calls
GSM encodes speech at kbps and 
A GSM networks base station controller BSC will typically service several tens of base transceiver stations
The role of the BSC is to allocate BTS radio channels to mobile subscribers perform paging finding the cell in which a mobile user is resident and perform handoff of mobile usersa topic well cover shortly in Section 
The base station controller and its controlled base transceiver stations collectively constitute a GSM base station subsystem BSS
As well see in Section 
the mobile switching center MSC plays the central role in user authorization and accounting e.g
determining whether a mobile device is allowed to connect to the cellular network call establishment and teardown and handoff
A single MSC will typically contain up to five BSCs resulting in approximately K subscribers per MSC
A cellular providers network will have a number of MSCs with special MSCs known as gateway MSCs connecting the providers cellular network to the larger public telephone network
G Cellular Data Networks Extending the Internet to Cellular Subscribers Our discussion in Section 
focused on connecting cellular voice users to the public telephone network
But of course when were on the go wed also like to read email access the Web get locationdependent services e.g
maps and restaurant recommendations and perhaps even watch streaming video
To do this our smartphone will need to run a full TCPIP protocol stack including the physical link network transport and application layers and connect into the Internet via the cellular data network
The topic of cellular data networks is a rather bewildering collection of competing and everevolving standards as one generation and halfgeneration succeeds the former and introduces new technologies and services with new acronyms
To make matters worse theres no single official body that sets requirements for .G G .G or G technologies making it hard to sort out the differences among competing standards
In our discussion below well focus on the UMTS Universal Mobile Telecommunications Service G and G standards developed by the rd Generation Partnership project GPP GPP 
Lets first take a topdown look at G cellular data network architecture shown in Figure 
G system architecture G Core Network The G core cellular data network connects radio access networks to the public Internet
The core network interoperates with components of the existing cellular voice network in particular the MSC that we previously encountered in Figure 
Given the considerable amount of existing infrastructure and profitable services in the existing cellular voice network the approach taken by the designers of G data services is clear leave the existing core GSM cellular voice network untouched adding additional cellular data functionality in parallel to the existing cellular voice network
The alternative integrating new data services directly into the core of the existing cellular voice networkwould have raised the same challenges encountered in Section 
where we discussed integrating new IPv and legacy IPv technologies in the Internet
There are two types of nodes in the G core network Serving GPRS Support Nodes SGSNs and Gateway GPRS Support Nodes GGSNs
GPRS stands for Generalized Packet Radio Service an early cellular data service in G networks here we discuss the evolved version of GPRS in G networks
An SGSN is responsible for delivering datagrams tofrom the mobile nodes in the radio access network to which the SGSN is attached
The SGSN interacts with the cellular voice networks MSC for that area providing user authorization and handoff maintaining location cell information about active mobile nodes and performing datagram forwarding between mobile nodes in the radio access network and a GGSN
The GGSN acts as a gateway connecting multiple SGSNs into the larger Internet
A GGSN is thus the last piece of G infrastructure that a datagram originating at a mobile node encounters before entering the larger Internet
To the outside world the GGSN looks like any other gateway router the mobility of the G nodes within the GGSNs network is hidden from the outside world behind the GGSN
G Radio Access Network The Wireless Edge The G radio access network is the wireless firsthop network that we see as a G user
The Radio Network Controller RNC typically controls several cell base transceiver stations similar to the base stations that we encountered in G systems but officially known in G UMTS parlance as a Node Bsa rather nondescriptive name
Each cells wireless link operates between the mobile nodes and a base transceiver station just as in G networks
The RNC connects to both the circuitswitched cellular voice network via an MSC and to the packetswitched Internet via an SGSN
Thus while G cellular voice and cellular data services use different core networks they share a common firstlasthop radio access network
A significant change in G UMTS over G networks is that rather than using GSMs FDMATDMA scheme UMTS uses a CDMA technique known as Direct Sequence Wideband CDMA DSWCDMA Dahlman within TDMA slots TDMA slots in turn are available on multiple frequenciesan interesting use of all three dedicated channelsharing approaches that we earlier identified in Chapter and similar to the approach taken in wired cable access networks see Section 
This change requires a new G cellular wirelessaccess network operating in parallel with the G BSS radio network shown in Figure 
The data service associated with the WCDMA specification is known as HSPA High Speed Packet Access and promises downlink data rates of up to Mbps
Details regarding G networks can be found at the rd Generation Partnership Project GPP Web site GPP 
On to G LTE Fourth generation G cellular systems are becoming widely deployed
In more than countries had G coverage exceeding 
The G LongTerm Evolution LTE standard Sauter put forward by the GPP has two important innovations over G systems an allIP core network and an enhanced radio access network as discussed below
G System Architecture An AllIP Core Network Figure 
shows the overall G network architecture which unfortunately introduces yet another rather impenetrable new vocabulary and set of acronyms for Figure 
G network architecture network components
But lets not get lost in these acronyms There are two important highlevel observations about the G architecture A unified allIP network architecture
Unlike the G network shown in Figure 
which has separate network components and paths for voice and data traffic the G architecture shown in Figure 
is allIPboth voice and data are carried in IP datagrams tofrom the wireless device the User Equipment UE in G parlance to the gateway to the packet gateway PGW that connects the G edge network to the rest of the network
With G the last vestiges of cellular networks roots in the telephony have disappeared giving way to universal IP service A clear separation of the G data plane and G control plane
Mirroring our distinction between the data and control planes for IPs network layer in Chapters and respectively the G network architecture also clearly separates the data and control planes
Well discuss their functionality below
A clear separation between the radio access network and the allIPcore network
IP datagrams carrying user data are forwarded between the user UE and the gateway PGW in Figure 
over a Ginternal IP network to the external Internet
Control packets are exchanged over this same internal network among the Gs control services components whose roles are described below
The principal components of the G architecture are as follows
The eNodeB is the logical descendant of the G base station and the G Radio Network Controller a.k.a Node B and again plays a central role here
Its dataplane role is to forward datagrams between UE over the LTE radio access network and the PGW
UE datagrams are encapsulated at the eNodeB and tunneled to the PGW through the G networks allIP enhanced packet core EPC
This tunneling between the eNodeB and PGW is similar the tunneling we saw in Section 
of IPv datagrams between two IPv endpoints through a network of IPv routers
These tunnels may have associated quality of service QoS guarantees
For example a G network may guarantee that voice traffic experiences no more than a msec delay between UE and PGW and has a packet loss rate of less than TCP traffic might have a guarantee of msec and a packet loss rate of less than 
Well cover QoS in Chapter 
In the control plane the eNodeB handles registration and mobility signaling traffic on behalf of the UE
The Packet Data Network Gateway PGW allocates IP addresses to the UEs and performs QoS enforcement
As a tunnel endpoint it also performs datagram encapsulationdecapsulation when forwarding a datagram tofrom a UE
The Serving Gateway SGW is the dataplane mobility anchor pointall UE traffic will pass through the SGW
The SGW also performs chargingbilling functions and lawful traffic interception
The Mobility Management Entity MME performs connection and mobility management on behalf of the UEs resident in the cell it controls
It receives UE subscription information from the HHS
We cover mobility in cellular networks in detail in Section 
The Home Subscriber Server HSS contains UE information including roaming access capabilities quality of service profiles and authentication information
As well see in Section 
the HSS obtains this information from the UEs home cellular provider
Very readable introductions to G network architecture and its EPC are Motorola Palat Sauter 
LTE Radio Access Network LTE uses a combination of frequency division multiplexing and time division multiplexing on the downstream channel known as orthogonal frequency division multiplexing OFDM Rohde Ericsson 
The term orthogonal comes from the fact the signals being sent on different frequency channels are created so that they interfere very little with each other even when channel frequencies are tightly spaced
In LTE each active mobile node is allocated one or more 
ms time slots in one or more of the channel frequencies
shows an allocation of eight time slots over four frequencies
By being allocated increasingly more time slots whether on the same frequency or on different frequencies a mobile node is able to achieve increasingly higher transmission rates
Slot reallocation among mobile Figure 
ms slots organized into ms frames at each frequency
An eightslot allocation is shown shaded
nodes can be performed as often as once every millisecond
Different modulation schemes can also be used to change the transmission rate see our earlier discussion of Figure 
and dynamic selection of modulation schemes in WiFi networks
The particular allocation of time slots to mobile nodes is not mandated by the LTE standard
Instead the decision of which mobile nodes will be allowed to transmit in a given time slot on a given frequency is determined by the scheduling algorithms provided by the LTE equipment vendor andor the network operator
With opportunistic scheduling Bender Kolding Kulkarni matching the physicallayer protocol to the channel conditions between the sender and receiver and choosing the receivers to which packets will be sent based on channel conditions allow the radio network controller to make best use of the wireless medium
In addition user priorities and contracted levels of service e.g
silver gold or platinum can be used in scheduling downstream packet transmissions
In addition to the LTE capabilities described above LTEAdvanced allows for downstream bandwidths of hundreds of Mbps by allocating aggregated channels to a mobile node Akyildiz 
An additional G wireless technologyWiMAX World Interoperability for Microwave Accessis a family of IEEE 
standards that differ significantly from LTE
WiMAX has not yet been able to enjoy the widespread deployment of LTE
A detailed discussion of WiMAX can be found on this books Web site
Mobility Management Principles Having covered the wireless nature of the communication links in a wireless network its now time to turn our attention to the mobility that these wireless links enable
In the broadest sense a mobile node is one that changes its point of attachment into the network over time
Because the term mobility has taken on many meanings in both the computer and telephony worlds it will serve us well first to consider several dimensions of mobility in some detail
From the network layers standpoint how mobile is a user A physically mobile user will present a very different set of challenges to the network layer depending on how he or she moves between points of attachment to the network
At one end of the spectrum in Figure 
a user may carry a laptop with a wireless network interface card around in a building
As we saw in Section 
this user is not mobile from a networklayer perspective
Moreover if the user associates with the same access point regardless of location the user is not even mobile from the perspective of the link layer
At the other end of the spectrum consider the user zooming along the autobahn in a BMW or Tesla at kilometers per hour passing through multiple wireless access networks and wanting to maintain an uninterrupted TCP connection to a remote application throughout the trip
This user is definitely mobile In between Figure 
Various degrees of mobility from the network layers point of view these extremes is a user who takes a laptop from one location e.g
office or dormitory into another e.g
coffeeshop classroom and wants to connect into thenetwork in the new location
This user is also mobile although less so than the BMW driver but does not need to maintain an ongoing connection while moving between points of attachment to the network
illustrates this spectrum of user mobility from the network layers perspective
How important is it for the mobile nodes address to always remain the same With mobile telephony your phone numberessentially the networklayer address of your phoneremains the same as you travel from one providers mobile phone network to another
Must a laptop similarly maintain the same IP address while moving between IP networks The answer to this question will depend strongly on the applications being run
For the BMW or Tesla driver who wants to maintain an uninterrupted TCP connection to a remote application while zipping along the autobahn it would be convenient to maintain the same IP address
Recall from Chapter that an Internet application needs to know the IP address and port number of the remote entity with which it is communicating
If a mobile entity is able to maintain its IP address as it moves mobility becomes invisible from the application standpoint
There is great value to this transparency an application need not be concerned with a potentially changing IP address and the same application code serves mobile and nonmobile connections alike
Well see in the following section that mobile IP provides this transparency allowing a mobile node to maintain its permanent IP address while moving among networks
On the other hand a less glamorous mobile user might simply want to turn off an office laptop bring that laptop home power up and work from home
If the laptop functions primarily as a client in clientserver applications e.g
sendread email browse the Web Telnet to a remote host from home the particular IP address used by the laptop is not that important
In particular one could get by fine with an address that is temporarily allocated to the laptop by the ISP serving the home
We saw in Section 
that DHCP already provides this functionality
What supporting wired infrastructure is available In all of our scenarios above weve implicitly assumed that there is a fixed infrastructure to which the mobile user can connectfor example the homes ISP network the wireless access network in the office or the wireless access networks lining the autobahn
What if no such infrastructure exists If two users are within communication proximity of each other can they establish a network connection in the absence of any other networklayer infrastructure Ad hoc networking provides precisely these capabilities
This rapidly developing area is at the cutting edge of mobile networking research and is beyond the scope of this book
Perkins and the IETF Mobile Ad Hoc Network manet working group Web pages manet provide thorough treatments of the subject
In order to illustrate the issues involved in allowing a mobile user to maintain ongoing connections while moving between networks lets consider a human analogy
A twentysomething adult moving out of the family home becomes mobile living in a series of dormitories andor apartments and often changing addresses
If an old friend wants to get in touch how can that friend find the address of her mobile friend One common way is to contact the family since a mobile adult will often register his or her current address with the family if for no other reason than so that the parents can send money to help pay the rent
The family home with its permanent address becomes that one place that others can go as a first step in communicating with the mobile adult
Later communication from the friend may be either indirect for example with mail being sent first to the parents home and then forwarded to the mobile adult or direct for example with the friend using the address obtained from the parents to send mail directly to her mobile friend
In a network setting the permanent home of a mobile node such as a laptop or smartphone is known as the home network and the entity within the home network that performs the mobility management functions discussed below on behalf of the mobile node is known as the home agent
The network in which the mobile node is currently residing is known as the foreign or visited network and the entity within the foreign network that helps the mobile node with the mobility management functions discussed below is known as a foreign agent
For mobile professionals their home network might likely be their company network while the visited network might be the network of a colleague they are visiting
A correspondent is the entity wishing to communicate with the mobile node
illustrates these concepts as well as addressing concepts considered below
In Figure 
note that agents are shown as being collocated with routers e.g
as processes running on routers but alternatively they could be executing on other hosts or servers in the network
Addressing We noted above that in order for user mobility to be transparent to network applications it is desirable for a mobile node to keep its address as it moves from one network Figure 
Initial elements of a mobile network architecture to another
When a mobile node is resident in a foreign network all traffic addressed to the nodes permanent address now needs to be routed to the foreign network
How can this be done One option is for the foreign network to advertise to all other networks that the mobile node is resident in its network
This could be via the usual exchange of intradomain and interdomain routing information and would require few changes to the existing routing infrastructure
The foreign network could simply advertise to its neighbors that it has a highly specific route to the mobile nodes permanent address that is essentially inform other networks that it has the correct path for routing datagrams to the mobile nodes permanent address see Section 
These neighbors would then propagate this routing information throughout the network as part of the normal procedure of updating routing information and forwarding tables
When the mobile node leaves one foreign network and joins another the new foreign network would advertise a new highly specific route to the mobile node and the old foreign network would withdraw its routing information regarding the mobile node
This solves two problems at once and it does so without making significant changes to the network layer infrastructure
Other networks know the location of the mobile node and it is easy to route datagrams to the mobile node since the forwarding tables will direct datagrams to the foreign network
A significant drawback however is that of scalability
If mobility management were to be the responsibility of network routers the routers would have to maintain forwarding table entries for potentially millions of mobile nodes and update these entries as nodes move
Some additional drawbacks are explored in the problems at the end of this chapter
An alternative approach and one that has been adopted in practice is to push mobility functionality from the network core to the network edgea recurring theme in our study of Internet architecture
A natural way to do this is via the mobile nodes home network
In much the same way that parents of the mobile twentysomething track their childs location the home agent in the mobile nodes home network can track the foreign network in which the mobile node resides
A protocol between the mobile node or a foreign agent representing the mobile node and the home agent will certainly be needed to update the mobile nodes location
Lets now consider the foreign agent in more detail
The conceptually simplest approach shown in Figure 
is to locate foreign agents at the edge routers in the foreign network
One role of the foreign agent is to create a socalled careof address COA for the mobile node with the network portion of the COA matching that of the foreign network
There are thus two addresses associated with a mobile node its permanent address analogous to our mobile youths familys home address and its COA sometimes known as a foreign address analogous to the address of the house in which our mobile youth is currently residing
In the example in Figure 
the permanent address of the mobile node is 
When visiting network 
the mobile node has a COA of 
A second role of the foreign agent is to inform the home agent that the mobile node is resident in its the foreign agents network and has the given COA
Well see shortly that the COA will be used to reroute datagrams to the mobile node via its foreign agent
Although we have separated the functionality of the mobile node and the foreign agent it is worth noting that the mobile node can also assume the responsibilities of the foreign agent
For example the mobile node could obtain a COA in the foreign network for example using a protocol such as DHCP and itself inform the home agent of its COA
Routing to a Mobile Node We have now seen how a mobile node obtains a COA and how the home agent can be informed of that address
But having the home agent know the COA solves only part of the problem
How should datagrams be addressed and forwarded to the mobile node Since only the home agent and not networkwide routers knows the location of the mobile node it will no longer suffice to simply address a datagram to the mobile nodes permanent address and send it into the networklayer infrastructure
Something more must be done
Two approaches can be identified which we will refer to as indirect and direct routing
Indirect Routing to a Mobile Node Lets first consider a correspondent that wants to send a datagram to a mobile node
In the indirect routing approach the correspondent simply addresses the datagram to the mobile nodes permanent address and sends the datagram into the network blissfully unaware of whether the mobile node is resident in its home network or is visiting a foreign network mobility is thus completely transparent to the correspondent
Such datagrams are first routed as usual to the mobile nodes home network
This is illustrated in step in Figure 
Lets now turn our attention to the home agent
In addition to being responsible for interacting with a foreign agent to track the mobile nodes COA the home agent has another very important function
Its second job is to be on the lookout for arriving datagrams addressed to nodes whose home network is that of the home agent but that are currently resident in a foreign network
The home agent intercepts these datagrams and then forwards them to a mobile node in a twostep process
The datagram is first forwarded to the foreign agent using the mobile nodes COA step in Figure 
and then forwarded from the foreign agent to the mobile node step in Figure 
Indirect routing to a mobile node It is instructive to consider this rerouting in more detail
The home agent will need to address the datagram using the mobile nodes COA so that the network layer will route the datagram to the foreign network
On the other hand it is desirable to leave the correspondents datagram intact since the application receiving the datagram should be unaware that the datagram was forwarded via the home agent
Both goals can be satisfied by having the home agent encapsulate the correspondents original complete datagram within a new larger datagram
This larger datagram is addressed and delivered to the mobile nodes COA
The foreign agent who owns the COA will receive and decapsulate the datagramthat is remove the correspondents original datagram from within the larger encapsulating datagram and forward step in Figure 
the original datagram to the mobile node
shows a correspondents original datagram being sent to the home network an encapsulated datagram being sent to the foreign agent and the original datagram being delivered to the mobile node
The sharp reader will note that the encapsulationdecapsulation described here is identical to the notion of tunneling discussed in Section 
in the context of IP multicast and IPv
Lets next consider how a mobile node sends datagrams to a correspondent
This is quite simple as the mobile node can address its datagram directly to the correspondent using its own permanent address as the source address and the Figure 
Encapsulation and decapsulation correspondents address as the destination address
Since the mobile node knows the correspondents address there is no need to route the datagram back through the home agent
This is shown as step in Figure 
Lets summarize our discussion of indirect routing by listing the new networklayer functionality required to support mobility
A mobilenodetoforeignagent protocol
The mobile node will register with the foreign agent when attaching to the foreign network
Similarly a mobile node will deregister with the foreign agent when it leaves the foreign network
A foreignagenttohomeagent registration protocol
The foreign agent will register the mobile nodes COA with the home agent
A foreign agent need not explicitly deregister a COA when a mobile node leaves its network because the subsequent registration of a new COA when the mobile node moves to a new network will take care of this
A homeagent datagram encapsulation protocol
Encapsulation and forwarding of the correspondents original datagram within a datagram addressed to the COA
A foreignagent decapsulation protocol
Extraction of the correspondents original datagram from the encapsulating datagram and the forwarding of the original datagram to the mobile node
The previous discussion provides all the piecesforeign agents the home agent and indirect forwardingneeded for a mobile node to maintain an ongoing connection while moving among networks
As an example of how these pieces fit together assume the mobile node is attached to foreign network A has registered a COA in network A with its home agent and is receiving datagrams that are being indirectly routed through its home agent
The mobile node now moves to foreign network B and registers with the foreign agent in network B which informs the home agent of the mobile nodes new COA
From this point on the home agent will reroute datagrams to foreign network B
As far as a correspondent is concerned mobility is transparentdatagrams are routed via the same home agent both before and after the move
As far as the home agent is concerned there is no disruption in the flow of datagramsarriving datagrams are first forwarded to foreign network A after the change in COA datagrams are forwarded to foreign network B
But will the mobile node see an interrupted flow of datagrams as it moves between networks As long as the time between the mobile nodes disconnection from network A at which point it can no longer receive datagrams via A and its attachment to network B at which point it will register a new COA with its home agent is small few datagrams will be lost
Recall from Chapter that endtoend connections can suffer datagram loss due to network congestion
Hence occasional datagram loss within a connection when a node moves between networks is by no means a catastrophic problem
If lossfree communication is required upper layer mechanisms will recover from datagram loss whether such loss results from network congestion or from user mobility
An indirect routing approach is used in the mobile IP standard RFC as discussed in Section 
Direct Routing to a Mobile Node The indirect routing approach illustrated in Figure 
suffers from an inefficiency known as the triangle routing problemdatagrams addressed to the mobile node must be routed first to the home agent and then to the foreign network even when a much more efficient route exists between the correspondent and the mobile node
In the worst case imagine a mobile user who is visiting the foreign network of a colleague
The two are sitting side by side and exchanging data over the network
Datagrams from the correspondent in this case the colleague of the visitor are routed to the mobile users home agent and then back again to the foreign network Direct routing overcomes the inefficiency of triangle routing but does so at the cost of additional complexity
In the direct routing approach a correspondent agent in the correspondents network first learns the COA of the mobile node
This can be done by having the correspondent agent query the home agent assuming that as in the case of indirect routing the mobile node has an uptodate value for its COA registered with its home agent
It is also possible for the correspondent itself to perform the function of the correspondent agent just as a mobile node could perform the function of the foreign agent
This is shown as steps and in Figure 
The correspondent agent then tunnels datagrams directly to the mobile nodes COA in a manner analogous to the tunneling performed by the home agent steps and in Figure 
While direct routing overcomes the triangle routing problem it introduces two important additional challenges A mobileuser location protocol is needed for the correspondent agent to query the home agent to obtain the mobile nodes COA steps and in Figure 
When the mobile node moves from one foreign network to another how will data now be forwarded to the new foreign network In the case of indirect routing this problem was easily solved by updating the COA maintained by the home agent
However with direct routing the home agent is queried for the COA by the correspondent agent only once at the beginning of the session
Thus updating the COA at the home agent while necessary will not be enough to solve the problem of routing data to the mobile nodes new foreign network
One solution would be to create a new protocol to notify the correspondent of the changing COA
An alternate solution and one that well see adopted in practice Figure 
Direct routing to a mobile user in GSM networks works as follows
Suppose data is currently being forwarded to the mobile node in the foreign network where the mobile node was located when the session first started step in Figure 
Well identify the foreign agent in that foreign network where the mobile node was first found as the anchor foreign agent
When the mobile node moves to a new foreign network step in Figure 
the mobile node registers with the new foreign agent step and the new foreign agent provides the anchor foreign agent with the mobile nodes new COA step 
When the anchor foreign agent receives an encapsulated datagram for a departed mobile node it can then reencapsulate the datagram and forward it to the mobile node step using the new COA
If the mobile node later moves yet again to a new foreign network the foreign agent in that new visited network would then contact the anchor foreign agent in order to set up forwarding to this new foreign network
Mobile transfer between networks with direct routing 
Mobile IP The Internet architecture and protocols for supporting mobility collectively known as mobile IP are defined primarily in RFC for IPv
Mobile IP is a flexible standard supporting many different modes of operation for example operation with or without a foreign agent multiple ways for agents and mobile nodes to discover each other use of single or multiple COAs and multiple forms of encapsulation
As such mobile IP is a complex standard and would require an entire book to describe in detail indeed one such book is Perkins b
Our modest goal here is to provide an overview of the most important aspects of mobile IP and to illustrate its use in a few commoncase scenarios
The mobile IP architecture contains many of the elements we have considered above including the concepts of home agents foreign agents careof addresses and encapsulationdecapsulation
The current standard RFC specifies the use of indirect routing to the mobile node
The mobile IP standard consists of three main pieces Agent discovery
Mobile IP defines the protocols used by a home or foreign agent to advertise its services to mobile nodes and protocols for mobile nodes to solicit the services of a foreign or home agent
Registration with the home agent
Mobile IP defines the protocols used by the mobile node andor foreign agent to register and deregister COAs with a mobile nodes home agent
Indirect routing of datagrams
The standard also defines the manner in which datagrams are forwarded to mobile nodes by a home agent including rules for forwarding datagrams rules for handling error conditions and several forms of encapsulation RFC RFC 
Security considerations are prominent throughout the mobile IP standard
For example authentication of a mobile node is clearly needed to ensure that a malicious user does not register a bogus careof address with a home agent which could cause all datagrams addressed to an IP address to be redirected to the malicious user
Mobile IP achieves security using many of the mechanisms that we will examine in Chapter so we will not address security considerations in our discussion below
Agent Discovery A mobile IP node arriving to a new network whether attaching to a foreign network or returning to its home network must learn the identity of the corresponding foreign or home agent
Indeed it is the discovery of a new foreign agent with a new network address that allows the network layer in a mobile node to learn that it has moved into a new foreign network
This process is known as agent discovery
Agent discovery can be accomplished in one of two ways via agent advertisement or via agent solicitation
With agent advertisement a foreign or home agent advertises its services using an extension to the existing router discovery protocol RFC 
The agent periodically broadcasts an ICMP message with a type field of router discovery on all links to which it is connected
The router discovery message contains the IP address of the router that is the agent thus allowing a mobile node to learn the agents IP address
The router discovery message also contains a mobility agent advertisement extension that contains additional information needed by the mobile node
Among the more important fields in the extension are the following Home agent bit H
Indicates that the agent is a home agent for the network in which it resides
Foreign agent bit F
Indicates that the agent is a foreign agent for the network in which it resides
Registration required bit R
Indicates that a mobile user in this network must register with a foreign agent
In particular a mobile user cannot obtain a careof address in the foreign network for example using DHCP and assume the functionality of the foreign agent for itself without registering with the foreign agent
ICMP router discovery message with mobility agent advertisement extension M G encapsulation bits
Indicate whether a form of encapsulation other than IPinIP encapsulation will be used
Careof address COA fields
A list of one or more careof addresses provided by the foreign agent
In our example below the COA will be associated with the foreign agent who will receive datagrams sent to the COA and then forward them to the appropriate mobile node
The mobile user will select one of these addresses as its COA when registering with its home agent
illustrates some of the key fields in the agent advertisement message
With agent solicitation a mobile node wanting to learn about agents without waiting to receive an agent advertisement can broadcast an agent solicitation message which is simply an ICMP message with type value 
An agent receiving the solicitation will unicast an agent advertisement directly to the mobile node which can then proceed as if it had received an unsolicited advertisement
Registration with the Home Agent Once a mobile IP node has received a COA that address must be registered with the home agent
This can be done either via the foreign agent who then registers the COA with the home agent or directly by the mobile IP node itself
We consider the former case below
Four steps are involved
Following the receipt of a foreign agent advertisement a mobile node sends a mobile IP registration message to the foreign agent
The registration message is carried within a UDP datagram and sent to port 
The registration message carries a COA advertised by the foreign agent the address of the home agent HA the permanent address of the mobile node MA the requested lifetime of the registration and a bit registration identification
The requested registration lifetime is the number of seconds that the registration is to be valid
If the registration is not renewed at the home agent within the specified lifetime the registration will become invalid
The registration identifier acts like a sequence number and serves to match a received registration reply with a registration request as discussed below
The foreign agent receives the registration message and records the mobile nodes permanent IP address
The foreign agent now knows that it should be looking for datagrams containing an encapsulated datagram whose destination address matches the permanent address of the mobile node
The foreign agent then sends a mobile IP registration message again within a UDP datagram to port of the home agent
The message contains the COA HA MA encapsulation format requested requested registration lifetime and registration identification
The home agent receives the registration request and checks for authenticity and correctness
The home agent binds the mobile nodes permanent IP address with the COA in the future datagrams arriving at the home agent and addressed to the mobile node will now be encapsulated and tunneled to the COA
The home agent sends a mobile IP registration reply containing the HA MA actual registration lifetime and the registration identification of the request that is being satisfied with this reply
The foreign agent receives the registration reply and then forwards it to the mobile node
At this point registration is complete and the mobile node can receive datagrams sent to its permanent address
illustrates these steps
Note that the home agent specifies a lifetime that is smaller than the lifetime requested by the mobile node
A foreign agent need not explicitly deregister a COA when a mobile node leaves its network
This will occur automatically when the mobile node moves to a new network whether another foreign network or its home network and registers a new COA
The mobile IP standard allows many additional scenarios and capabilities in addition to those described previously
The interested reader should consult Perkins b RFC 
Agent advertisement and mobile IP registration 
Managing Mobility in Cellular Networks Having examined how mobility is managed in IP networks lets now turn our attention to networks with an even longer history of supporting mobilitycellular telephony networks
Whereas we focused on the firsthop wireless link in cellular networks in Section 
well focus here on mobility using the GSM cellular network Goodman Mouly Scourias Kaaranen Korhonen Turner as our case study since it is a mature and widely deployed technology
Mobility in G and G networks is similar in principle to that used in GSM
As in the case of mobile IP well see that a number of the fundamental principles we identified in Section 
are embodied in GSMs network architecture
Like mobile IP GSM adopts an indirect routing approach see Section 
first routing the correspondents call to the mobile users home network and from there to the visited network
In GSM terminology the mobile userss home network is referred to as the mobile users home public land mobile network home PLMN
Since the PLMN acronym is a bit of a mouthful and mindful of our quest to avoid an alphabet soup of acronyms well refer to the GSM home PLMN simply as the home network
The home network is the cellular provider with which the mobile user has a subscription i.e
the provider that bills the user for monthly cellular service
The visited PLMN which well refer to simply as the visited network is the network in which the mobile user is currently residing
As in the case of mobile IP the responsibilities of the home and visited networks are quite different
The home network maintains a database known as the home location register HLR which contains the permanent cell phone number and subscriber profile information for each of its subscribers
Importantly the HLR also contains information about the current locations of these subscribers
That is if a mobile user is currently roaming in another providers cellular network the HLR contains enough information to obtain via a process well describe shortly an address in the visited network to which a call to the mobile user should be routed
As well see a special switch in the home network known as the Gateway Mobile services Switching Center GMSC is contacted by a correspondent when a call is placed to a mobile user
Again in our quest to avoid an alphabet soup of acronyms well refer to the GMSC here by a more descriptive term home MSC
The visited network maintains a database known as the visitor location register VLR
The VLR contains an entry for each mobile user that is currently in the portion of the network served by the VLR
VLR entries thus come and go as mobile users enter and leave the network
A VLR is usually colocated with the mobile switching center MSC that coordinates the setup of a call to and from the visited network
In practice a providers cellular network will serve as a home network for its subscribers and as a visited network for mobile users whose subscription is with a different cellular provider
Placing a call to a mobile user Indirect routing 
Routing Calls to a Mobile User Were now in a position to describe how a call is placed to a mobile GSM user in a visited network
Well consider a simple example below more complex scenarios are described in Mouly 
The steps as illustrated in Figure 
are as follows 
The correspondent dials the mobile users phone number
This number itself does not refer to a particular telephone line or location after all the phone number is fixed and the user is mobile
The leading digits in the number are sufficient to globally identify the mobiles home network
The call is routed from the correspondent through the PSTN to the home MSC in the mobiles home network
This is the first leg of the call
The home MSC receives the call and interrogates the HLR to determine the location of the mobile user
In the simplest case the HLR returns the mobile station roaming number MSRN which we will refer to as the roaming number
Note that this number is different from the mobiles permanent phone number which is associated with the mobiles home network
The roaming number is ephemeral It is temporarily assigned to a mobile when it enters a visited network
The roaming number serves a role similar to that of the careof address in mobile IP and like the COA is invisible to the correspondent and the mobile
If HLR does not have the roaming number it returns the address of the VLR in the visited network
In this case not shown in Figure 
the home MSC will need to query the VLR to obtain the roaming number of the mobile node
But how does the HLR get the roaming number or the VLR address in the first place What happens to these values when the mobile user moves to another visited network Well consider these important questions shortly
Given the roaming number the home MSC sets up the second leg of the call through the network to the MSC in the visited network
The call is completed being routed from the correspondent to the home MSC and from there to the visited MSC and from there to the base station serving the mobile user
An unresolved question in step is how the HLR obtains information about the location of the mobile user
When a mobile telephone is switched on or enters a part of a visited network that is covered by a new VLR the mobile must register with the visited network
This is done through the exchange of signaling messages between the mobile and the VLR
The visited VLR in turn sends a location update request message to the mobiles HLR
This message informs the HLR of either the roaming number at which the mobile can be contacted or the address of the VLR which can then later be queried to obtain the mobile number
As part of this exchange the VLR also obtains subscriber information from the HLR about the mobile and determines what services if any should be accorded the mobile user by the visited network
Handoffs in GSM A handoff occurs when a mobile station changes its association from one base station to another during a call
As shown in Figure 
a mobiles call is initially before handoff routed to the mobile through one base station which well refer to as the old base station and after handoff is routed to the mobile through another base Figure 
Handoff scenario between base stations with a common MSC station which well refer to as the new base station
Note that a handoff between base stations results not only in the mobile transmittingreceiving tofrom a new base station but also in the rerouting of the ongoing call from a switching point within the network to the new base station
Lets initially assume that the old and new base stations share the same MSC and that the rerouting occurs at this MSC
There may be several reasons for handoff to occur including the signal between the current base station and the mobile may have deteriorated to such an extent that the call is in danger of being dropped and a cell may have become overloaded handling a large number of calls
This congestion may be alleviated by handing off mobiles to less congested nearby cells
While it is associated with a base station a mobile periodically measures the strength of a beacon signal from its current base station as well as beacon signals from nearby base stations that it can hear
These measurements are reported once or twice a second to the mobiles current base station
Handoff in GSM is initiated by the old base station based on these measurements the current loads of mobiles in nearby cells and other factors Mouly 
The GSM standard does not specify the specific algorithm to be used by a base station to determine whether or not to perform handoff
illustrates the steps involved when a base station does decide to hand off a mobile user 
The old base station BS informs the visited MSC that a handoff is to be performed and the BS or possible set of BSs to which the mobile is to be handed off
The visited MSC initiates path setup to the new BS allocating the resources needed to carry the rerouted call and signaling the new BS that a handoff is about to occur
The new BS allocates and activates a radio channel for use by the mobile
The new BS signals back to the visited MSC and the old BS that the visitedMSCtonewBS path has been established and that the mobile should be Figure 
Steps in accomplishing a handoff between base stations with a common MSC informed of the impending handoff
The new BS provides all of the information that the mobile will need to associate with the new BS
The mobile is informed that it should perform a handoff
Note that up until this point the mobile has been blissfully unaware that the network has been laying the groundwork e.g
allocating a channel in the new BS and allocating a path from the visited MSC to the new BS for a handoff
The mobile and the new BS exchange one or more messages to fully activate the new channel in the new BS
The mobile sends a handoff complete message to the new BS which is forwarded up to the visited MSC
The visited MSC then reroutes the ongoing call to the mobile via the new BS
The resources allocated along the path to the old BS are then released
Lets conclude our discussion of handoff by considering what happens when the mobile moves to a BS that is associated with a different MSC than the old BS and what happens when this interMSC handoff occurs more than once
As shown in Figure 
GSM defines the notion of an anchor MSC
The anchor MSC is the MSC visited by the mobile when a call first begins the anchor MSC thus remains unchanged during the call
Throughout the calls duration and regardless of the number of interMSC Figure 
Rerouting via the anchor MSC Table 
Commonalities between mobile IP and GSM mobility GSM element Comment on GSM element Mobile IP element Home system Network to which the mobile users permanent phone number Home belongs
network Gateway mobile Home MSC point of contact to obtain routable address of Home switching center or mobile user
HLR database in home system containing agent simply home MSC permanent phone number profile information current location Home location register of mobile user subscription information
HLR Visited system Network other than home system where mobile user is Visited currently residing
network Visited mobile services Visited MSC responsible for setting up calls tofrom mobile Foreign switching center nodes in cells associated with MSC
VLR temporary database agent Visitor location register entry in visited system containing subscription information for VLR each visiting mobile user
Mobile station roaming Routable address for telephone call segment between home Careof number MSRN or MSC and visited MSC visible to neither the mobile nor the address simply roaming number correspondent
transfers performed by the mobile the call is routed from the home MSC to the anchor MSC and then from the anchor MSC to the visited MSC where the mobile is currently located
When a mobile moves from the coverage area of one MSC to another the ongoing call is rerouted from the anchor MSC to the new visited MSC containing the new base station
Thus at all times there are at most three MSCs the home MSC the anchor MSC and the visited MSC between the correspondent and the mobile
illustrates the routing of a call among the MSCs visited by a mobile user
Rather than maintaining a single MSC hop from the anchor MSC to the current MSC an alternative approach would have been to simply chain the MSCs visited by the mobile having an old MSC forward the ongoing call to the new MSC each time the mobile moves to a new MSC
Such MSC chaining can in fact occur in IS cellular networks with an optional path minimization step to remove MSCs between the anchor MSC and the current visited MSC Lin 
Lets wrap up our discussion of GSM mobility management with a comparison of mobility management in GSM and Mobile IP
The comparison in Table 
indicates that although IP and cellular networks are fundamentally different in many ways they share a surprising number of common functional elements and overall approaches in handling mobility
Wireless and Mobility Impact on HigherLayer Protocols In this chapter weve seen that wireless networks differ significantly from their wired counterparts at both the link layer as a result of wireless channel characteristics such as fading multipath and hidden terminals and at the network layer as a result of mobile users who change their points of attachment to the network
But are there important differences at the transport and application layers Its tempting to think that these differences will be minor since the network layer provides the same besteffort delivery service model to upper layers in both wired and wireless networks
Similarly if protocols such as TCP or UDP are used to provide transportlayer services to applications in both wired and wireless networks then the application layer should remain unchanged as well
In one sense our intuition is rightTCP and UDP can and do operate in networks with wireless links
On the other hand transport protocols in general and TCP in particular can sometimes have very different performance in wired and wireless networks and it is here in terms of performance that differences are manifested
Lets see why
Recall that TCP retransmits a segment that is either lost or corrupted on the path between sender and receiver
In the case of mobile users loss can result from either network congestion router buffer overflow or from handoff e.g
from delays in rerouting segments to a mobiles new point of attachment to the network
In all cases TCPs receivertosender ACK indicates only that a segment was not received intact the sender is unaware of whether the segment was lost due to congestion during handoff or due to detected bit errors
In all cases the senders response is the sameto retransmit the segment
TCPs congestioncontrol response is also the same in all casesTCP decreases its congestion window as discussed in Section 
By unconditionally decreasing its congestion window TCP implicitly assumes that segment loss results from congestion rather than corruption or handoff
We saw in Section 
that bit errors are much more common in wireless networks than in wired networks
When such bit errors occur or when handoff loss occurs theres really no reason for the TCP sender to decrease its congestion window and thus decrease its sending rate
Indeed it may well be the case that router buffers are empty and packets are flowing along the endtoend path unimpeded by congestion
Researchers realized in the early to mid s that given high bit error rates on wireless links and the possibility of handoff loss TCPs congestioncontrol response could be problematic in a wireless setting
Three broad classes of approaches are possible for dealing with this problem Local recovery
Local recovery protocols recover from bit errors when and where e.g
at the wireless link they occur e.g
ARQ protocol we studied in Section 
or more sophisticated approaches that use both ARQ and FEC Ayanoglu 
TCP sender awareness of wireless links
In the local recovery approaches the TCP sender is blissfully unaware that its segments are traversing a wireless link
An alternative approach is for the TCP sender and receiver to be aware of the existence of a wireless link to distinguish between congestive losses occurring in the wired network and corruptionloss occurring at the wireless link and to invoke congestion control only in response to congestive wirednetwork losses
Balakrishnan investigates various types of TCP assuming that end systems can make this distinction
Liu investigates techniques for distinguishing between losses on the wired and wireless segments of an endtoend path
In a splitconnection approach Bakre the endtoend connection between the mobile user and the other end point is broken into two transportlayer connections one from the mobile host to the wireless access point and one from the wireless access point to the other communication end point which well assume here is a wired host
The endtoend connection is thus formed by the concatenation of a wireless part and a wired part
The transport layer over the wireless segment can be a standard TCP connection Bakre or a specially tailored error recovery protocol on top of UDP
Yavatkar investigates the use of a transportlayer selective repeat protocol over the wireless connection
Measurements reported in Wei indicate that split TCP connections are widely used in cellular data networks and that significant improvements can indeed be made through the use of split TCP connections
Our treatment of TCP over wireless links has been necessarily brief here
Indepth surveys of TCP challenges and solutions in wireless networks can be found in Hanabali Leung 
We encourage you to consult the references for details of this ongoing area of research
Having considered transportlayer protocols let us next consider the effect of wireless and mobility on applicationlayer protocols
Here an important consideration is that wireless links often have relatively low bandwidths as we saw in Figure 
As a result applications that operate over wireless links particularly over cellular wireless links must treat bandwidth as a scarce commodity
For example a Web server serving content to a Web browser executing on a G phone will likely not be able to provide the same imagerich content that it gives to a browser operating over a wired connection
Although wireless links do provide challenges at the application layer the mobility they enable also makes possible a rich set of locationaware and contextaware applications Chen Baldauf 
More generally wireless and mobile networks will play a key role in realizing the ubiquitous computing environments of the future Weiser 
Its fair to say that weve only seen the tip of the iceberg when it comes to the impact of wireless and mobile networks on networked applications and their protocols 
Summary Wireless and mobile networks have revolutionized telephony and are having an increasingly profound impact in the world of computer networks as well
With their anytime anywhere untethered access into the global network infrastructure they are not only making network access more ubiquitous they are also enabling an exciting new set of locationdependent services
Given the growing importance of wireless and mobile networks this chapter has focused on the principles common link technologies and network architectures for supporting wireless and mobile communication
We began this chapter with an introduction to wireless and mobile networks drawing an important distinction between the challenges posed by the wireless nature of the communication links in such networks and by the mobility that these wireless links enable
This allowed us to better isolate identify and master the key concepts in each area
We focused first on wireless communication considering the characteristics of a wireless link in Section 
In Sections 
we examined the linklevel aspects of the IEEE 
WiFi wireless LAN standard two IEEE 
personal area networks Bluetooth and Zigbee and G and G cellular Internet access
We then turned our attention to the issue of mobility
In Section 
we identified several forms of mobility with points along this spectrum posing different challenges and admitting different solutions
We considered the problems of locating and routing to a mobile user as well as approaches for handing off the mobile user who dynamically moves from one point of attachment to the network to another
We examined how these issues were addressed in the mobile IP standard and in GSM in Sections 
Finally we considered the impact of wireless links and mobility on transportlayer protocols and networked applications in Section 
Although we have devoted an entire chapter to the study of wireless and mobile networks an entire book or more would be required to fully explore this exciting and rapidly expanding field
We encourage you to delve more deeply into this field by consulting the many references provided in this chapter
Homework Problems and Questions Chapter Review Questions Section 
What does it mean for a wireless network to be operating in infrastructure mode If the network is not in infrastructure mode what mode of operation is it in and what is the difference between that mode of operation and infrastructure mode R
What are the four types of wireless networks identified in our taxonomy in Section 
Which of these types of wireless networks have you used Section 
What are the differences between the following types of wireless channel impairments path loss multipath propagation interference from other sources R
As a mobile node gets farther and farther away from a base station what are two actions that a base station could take to ensure that the loss probability of a transmitted frame does not increase Sections 
Describe the role of the beacon frames in 
True or false Before an 
station transmits a data frame it must first send an RTS frame and receive a corresponding CTS frame
Why are acknowledgments used in 
but not in wired Ethernet R
True or false Ethernet and 
use the same frame structure
Describe how the RTS threshold works
Suppose the IEEE 
RTS and CTS frames were as long as the standard DATA and ACK frames
Would there be any advantage to using the CTS and RTS frames Why or why not R
mobility in which a wireless station moves from one BSS to another within the same subnet
When the APs are interconnected with a switch an AP may need to send a frame with a spoofed MAC address to get the switch to forward the frame properly
What are the differences between a master device in a Bluetooth network and a base station in an 
What is meant by a super frame in the 
Zigbee standard R
What is the role of the core network in the G cellular data architecture R
What is the role of the RNC in the G cellular data network architecture What role does the RNC play in the cellular voice network R
What is the role of the eNodeB MME PGW and SGW in G architecture R
What are three important differences between the G and G cellular architectures Sections 
If a node has a wireless connection to the Internet does that node have to be mobile Explain
Suppose that a user with a laptop walks around her house with her laptop and always accesses the Internet through the same access point
Is this user mobile from a network standpoint Explain
What is the difference between a permanent address and a careof address Who assigns a careof address R
Consider a TCP connection going over Mobile IP
True or false The TCP connection phase between the correspondent and the mobile host goes through the mobiles home network but the data transfer phase is directly between the correspondent and the mobile host bypassing the home network
What are the purposes of the HLR and VLR in GSM networks What elements of mobile IP are similar to the HLR and VLR R
What is the role of the anchor MSC in GSM networks Section 
What are three approaches that can be taken to avoid having a single wireless link degrade the performance of an endtoend transportlayer TCP connection Problems P
Consider the singlesender CDMA example in Figure 
What would be the senders output for the data bits shown if the senders CDMA code were P
Consider sender in Figure 
What is the senders output to the channel before it is added to the signal from sender Zim P
Suppose that the receiver in Figure 
wanted to receive the data being sent by sender 
Show by calculation that the receiver is indeed able to recover sender s data from the aggregate channel signal by using sender s code
For the twosender tworeceiver example give an example of two CDMA codes containing and values that do not allow the two receivers to extract the original transmitted bits from the two CDMA senders
Suppose there are two ISPs providing WiFi access in a particular cafÃ© with each ISP operating its own AP and having its own IP address block
Further suppose that by accident each ISP has configured its AP to operate over channel 
Will the 
protocol completely break down in this situation Discuss what happens when two stations each associated with a different ISP attempt to transmit at the same time
Now suppose that one AP operates over channel and the other over channel 
How do your answers change P
In step of the CSMACA protocol a station that successfully transmits a frame begins the CSMACA protocol for a second frame at step rather than at step 
What rationale might the designers of CSMACA have had in mind by having such a station not transmit the second frame immediately if the channel is sensed idle P
Suppose an .b station is configured to always reserve the channel with the RTSCTS sequence
Suppose this station suddenly wants to transmit bytes of data and all other stations are idle at this time
As a function of SIFS and DIFS and ignoring propagation delay and assuming no bit errors calculate the time required to transmit the frame and receive the acknowledgment
Consider the scenario shown in Figure 
in which there are four wireless nodes A B C and D
The radio coverage of the four nodes is shown via the shaded ovals all nodes share the same frequency
When A transmits it Figure 
Scenario for problem P can only be heardreceived by B when B transmits both A and C can hearreceive from B when C transmits both B and D can hearreceive from C when D transmits only C can hearreceive from D
Suppose now that each node has an infinite supply of messages that it wants to send to each of the other nodes
If a messages destination is not an immediate neighbor then the message must be relayed
For example if A wants to send to D a message from A must first be sent to B which then sends the message to C which then sends the message to D
Time is slotted with a message transmission time taking exactly one time slot e.g
as in slotted Aloha
During a slot a node can do one of the following i send a message ii receive a message if exactly one message is being sent to it iii remain silent
As always if a node hears two or more simultaneous transmissions a collision occurs and none of the transmitted messages are received successfully
You can assume here that there are no bitlevel errors and thus if exactly one message is sent it will be received correctly by those within the transmission radius of the sender
Suppose now that an omniscient controller i.e
a controller that knows the state of every node in the network can command each node to do whatever it the omniscient controller wishes i.e
to send a message to receive a message or to remain silent
Given this omniscient controller what is the maximum rate at which a data message can be transferred from C to A given that there are no other messages between any other sourcedestination pairs b
Suppose now that A sends messages to B and D sends messages to C
What is the combined maximum rate at which data messages can flow from A to B and from D to C c
Suppose now that A sends messages to B and C sends messages to D
What is the combined maximum rate at which data messages can flow from A to B and from C to D d
Suppose now that the wireless links are replaced by wired links
Repeat questions a through c again in this wired scenario
Now suppose we are again in the wireless scenario and that for every data message sent from source to destination the destination will send an ACK message back to the source e.g
as in TCP
Also suppose that each ACK message takes up one slot
Repeat questions ac above for this scenario
Describe the format of the 
You will have to do some reading outside of the text to find this information
Is there anything in the frame format that inherently limits the number of active nodes in an 
network to eight active nodes Explain
Consider the following idealized LTE scenario
The downstream channel see Figure 
is slotted in time across F frequencies
There are four nodes A B C and D reachable from the base station at rates of Mbps Mbps 
Mbps and Mbps respectively on the downstream channel
These rates assume that the base station utilizes all time slots available on all F frequencies to send to just one station
The base station has an infinite amount of data to send to each of the nodes and can send to any one of these four nodes using any of the F frequencies during any time slot in the downstream subframe
What is the maximum rate at which the base station can send to the nodes assuming it can send to any node it chooses during each time slot Is your solution fair Explain and define what you mean by fair
If there is a fairness requirement that each node must receive an equal amount of data during each one second interval what is the average transmission rate by the base station to all nodes during the downstream subframe Explain how you arrived at your answer
Suppose that the fairness criterion is that any node can receive at most twice as much data as any other node during the subframe
What is the average transmission rate by the base station to all nodes during the subframe Explain how you arrived at your answer
In Section 
one proposed solution that allowed mobile users to maintain their IP addresses as they moved among foreign networks was to have a foreign network advertise a highly specific route to the mobile user and use the existing routing infrastructure to propagate this information throughout the network
We identified scalability as one concern
Suppose that when a mobile user moves from one network to another the new foreign network advertises a specific route to the mobile user and the old foreign network withdraws its route
Consider how routing information propagates in a distancevector algorithm particularly for the case of interdomain routing among networks that span the globe
Will other routers be able to route datagrams immediately to the new foreign network as soon as the foreign network begins advertising its route b
Is it possible for different routers to believe that different foreign networks contain the mobile user c
Discuss the timescale over which other routers in the network will eventually learn the path to the mobile users
Suppose the correspondent in Figure 
Sketch the additional network layer infrastructure that would be needed to route the datagram from the original mobile user to the now mobile correspondent
Show the structure of the datagrams between the original mobile user and the now mobile correspondent as in Figure 
In mobile IP what effect will mobility have on endtoend delays of datagrams between the source and destination P
Consider the chaining example discussed at the end of Section 
Suppose a mobile user visits foreign networks A B and C and that a correspondent begins a connection to the mobile user when it is resident in foreign network A
List the sequence of messages between foreign agents and between foreign agents and the home agent as the mobile user moves from network A to network B to network C
Next suppose chaining is not performed and the correspondent as well as the home agent must be explicitly notified of the changes in the mobile users careof address
List the sequence of messages that would need to be exchanged in this second scenario
Consider two mobile nodes in a foreign network having a foreign agent
Is it possible for the two mobile nodes to use the same careof address in mobile IP Explain your answer
In our discussion of how the VLR updated the HLR with information about the mobiles current location what are the advantages and disadvantages of providing the MSRN as opposed to the address of the VLR to the HLR Wireshark Lab At the Web site for this textbook www.pearsonhighered.comcsresources youll find a Wireshark lab for this chapter that captures and studies the 
frames exchanged between a wireless laptop and an access point
AN INTERVIEW WITH Deborah Estrin Deborah Estrin is a Professor of Computer Science at Cornell Tech in New York City and a Professor of Public Health at Weill Cornell Medical College
She is founder of the Health Tech Hub at Cornell Tech and cofounder of the nonprofit startup Open mHealth
She received her Ph.D
in Computer Science from M.I.T
and her B.S
from UC Berkeley
Estrins early research focused on the design of network protocols including multicast and interdomain routing
In Estrin founded the NSFfunded Science and Technology Center at UCLA Center for Embedded Networked Sensing CENS httpcens.ucla.edu
CENS launched new areas of multidisciplinary computer systems research from sensor networks for environmental monitoring to participatory sensing for citizen science
Her current focus is on mobile health and small data leveraging the pervasiveness of mobile devices and digital interactions for health and life management as described in her TEDMED talk
Professor Estrin is an elected member of the American Academy of Arts and Sciences and the National Academy of Engineering 
She is a fellow of the IEEE ACM and AAAS
She was selected as the first ACMW Athena Lecturer awarded the Anita Borg Institutes Women of Vision Award for Innovation inducted into the WITI hall of fame and awarded Doctor Honoris Causa from EPFL and Uppsala University 
Please describe a few of the most exciting projects you have worked on during your career
What were the biggest challenges In the mids at USC and ISI I had the great fortune to work with the likes of Steve Deering Mark Handley and Van Jacobson on the design of multicast routing protocols in particular PIM
I tried to carry many of the architectural design lessons from multicast into the design of ecological monitoring arrays where for the first time I really began to take applications and multidisciplinary research seriously
That interest in jointly innovating in the social and technological space is what interests me so much about my latest area of research mobile health
The challenges in these projects were as diverse as the problem domains but what they all had in common was the need to keep our eyes open to whether we had the problem definition right as we iterated between design and deployment prototype and pilot
None of them were problems that could be solved analytically with simulation or even in constructed laboratory experiments
They all challenged our ability to retain clean architectures in the presence of messy problems and contexts and they all called for extensive collaboration
What changes and innovations do you see happening in wireless networks and mobility in the future In a prior edition of this interview I said that I have never put much faith into predicting the future but I did go on to speculate that we might see the end of feature phones i.e
those that are not programmable and are used only for voice and text messaging as smart phones become more and more powerful and the primary point of Internet access for manyand now not so many years later that is clearly the case
I also predicted that we would see the continued proliferation of embedded SIMs by which all sorts of devices have the ability to communicate via the cellular network at low data rates
While that has occurred we see many devices and Internet of Things that use embedded WiFi and other lower power shorter range forms of connectivity to local hubs
I did not anticipate at that time the emergence of a large consumer wearables market
By the time the next edition is published I expect broad proliferation of personal applications that leverage data from IoT and other digital traces
Where do you see the future of networking and the Internet Again I think its useful to look both back and forward
Previously I observed that the efforts in named data and softwaredefined networking would emerge to create a more manageable evolvable and richer infrastructure and more generally represent moving the role of architecture higher up in the stack
In the beginnings of the Internet architecture was layer and below with applications being more siloedmonolithic sitting on top
Now data and analytics dominate transport
The adoption of SDN which Im really happy to see is featured in this th edition of this book has been well beyond what I ever anticipated
However looking up the stack our dominant applications increasingly live in walled gardens whether mobile apps or large consumer platforms such as Facebook
As Data Science and Big Data techniques develop they might help to lure these applications out of their silos because of the value in connecting with other apps and platforms
What people inspired you professionally There are three people who come to mind
First Dave Clark the secret sauce and undersung hero of the Internet community
I was lucky to be around in the early days to see him act as the organizing principle of the IAB and Internet governance the priest of rough consensus and running code
Second Scott Shenker for his intellectual brilliance integrity and persistence
I strive for but rarely attain his clarity in defining problems and solutions
He is always the first person I email for advice on matters large and small
Third my sister Judy Estrin who had the creativity and courage to spend her career bringing ideas and concepts to market
Without the Judys of the world the Internet technologies would never have transformed our lives
What are your recommendations for students who want careers in computer science and networking First build a strong foundation in your academic work balanced with any and every realworld work experience you can get
As you look for a working environment seek opportunities in problem areas you really care about and with smart teams that you can learn from
Chapter Security in Computer Networks Way back in Section 
we described some of the more prevalent and damaging classes of Internet attacks including malware attacks denial of service sniffing source masquerading and message modification and deletion
Although we have since learned a tremendous amount about computer networks we still havent examined how to secure networks from those attacks
Equipped with our newly acquired expertise in computer networking and Internet protocols well now study indepth secure communication and in particular how computer networks can be defended from those nasty bad guys
Let us introduce Alice and Bob two people who want to communicate and wish to do so securely
This being a networking text we should remark that Alice and Bob could be two routers that want to exchange routing tables securely a client and server that want to establish a secure transport connection or two email applications that want to exchange secure emailall case studies that we will consider later in this chapter
Alice and Bob are wellknown fixtures in the security community perhaps because their names are more fun than a generic entity named A that wants to communicate securely with a generic entity named B
Love affairs wartime communication and business transactions are the commonly cited human needs for secure communications preferring the first to the latter two were happy to use Alice and Bob as our sender and receiver and imagine them in this first scenario
We said that Alice and Bob want to communicate and wish to do so securely but what precisely does this mean As we will see security like love is a manysplendored thing that is there are many facets to security
Certainly Alice and Bob would like for the contents of their communication to remain secret from an eavesdropper
They probably would also like to make sure that when they are communicating they are indeed communicating with each other and that if their communication is tampered with by an eavesdropper that this tampering is detected
In the first part of this chapter well cover the fundamental cryptography techniques that allow for encrypting communication authenticating the party with whom one is communicating and ensuring message integrity
In the second part of this chapter well examine how the fundamental cryptography principles can be used to create secure networking protocols
Once again taking a topdown approach well examine secure protocols in each of the top four layers beginning with the application layer
Well examine how to secure email how to secure a TCP connection how to provide blanket security at the network layer and how to secure a wireless LAN
In the third part of this chapter well consider operational security which is about protecting organizational networks from attacks
In particular well take a careful look at how firewalls and intrusion detection systems can enhance the security of an organizational network
What Is Network Security Lets begin our study of network security by returning to our lovers Alice and Bob who want to communicate securely
What precisely does this mean Certainly Alice wants only Bob to be able to understand a message that she has sent even though they are communicating over an insecure medium where an intruder Trudy the intruder may intercept whatever is transmitted from Alice to Bob
Bob also wants to be sure that the message he receives from Alice was indeed sent by Alice and Alice wants to make sure that the person with whom she is communicating is indeed Bob
Alice and Bob also want to make sure that the contents of their messages have not been altered in transit
They also want to be assured that they can communicate in the first place i.e
that no one denies them access to the resources needed to communicate
Given these considerations we can identify the following desirable properties of secure communication
Only the sender and intended receiver should be able to understand the contents of the transmitted message
Because eavesdroppers may intercept the message this necessarily requires that the message be somehow encrypted so that an intercepted message cannot be understood by an interceptor
This aspect of confidentiality is probably the most commonly perceived meaning of the term secure communication
Well study cryptographic techniques for encrypting and decrypting data in Section 
Alice and Bob want to ensure that the content of their communication is not altered either maliciously or by accident in transit
Extensions to the checksumming techniques that we encountered in reliable transport and data link protocols can be used to provide such message integrity
We will study message integrity in Section 
Both the sender and receiver should be able to confirm the identity of the other party involved in the communicationto confirm that the other party is indeed who or what they claim to be
Facetoface human communication solves this problem easily by visual recognition
When communicating entities exchange messages over a medium where they cannot see the other party authentication is not so simple
When a user wants to access an inbox how does the mail server verify that the user is the person he or she claims to be We study endpoint authentication in Section 
Almost all organizations companies universities and so on today have networks that are attached to the public Internet
These networks therefore can potentially be compromised
Attackers can attempt to deposit worms into the hosts in the network obtain corporate secrets map the internal network configurations and launch DoS attacks
Well see in Section 
that operational devices such as firewalls and intrusion detection systems are used to counter attacks against an organizations network
A firewall sits between the organizations network and the public network controlling packet access to and from the network
An intrusion detection system performs deep packet inspection alerting the network administrators about suspicious activity
Having established what we mean by network security lets next consider exactly what information an intruder may have access to and what actions can be taken by the intruder
illustrates the scenario
Alice the sender wants to send data to Bob the receiver
In order to exchange data securely while meeting the requirements of confidentiality endpoint authentication and message integrity Alice and Bob will exchange control messages and data messages in much the same way that TCP senders and receivers exchange control segments and data segments
Sender receiver and intruder Alice Bob and Trudy All or some of these messages will typically be encrypted
As discussed in Section 
an intruder can potentially perform eavesdroppingsniffing and recording control and data messages on the channel
modification insertion or deletion of messages or message content
As well see unless appropriate countermeasures are taken these capabilities allow an intruder to mount a wide variety of security attacks snooping on communication possibly stealing passwords and data impersonating another entity hijacking an ongoing session denying service to legitimate network users by overloading system resources and so on
A summary of reported attacks is maintained at the CERT Coordination Center CERT 
Having established that there are indeed real threats loose in the Internet what are the Internet equivalents of Alice and Bob our friends who need to communicate securely Certainly Bob and Alice might be human users at two end systems for example a real Alice and a real Bob who really do want to exchange secure email
They might also be participants in an electronic commerce transaction
For example a real Bob might want to transfer his credit card number securely to a Web server to purchase an item online
Similarly a real Alice might want to interact with her bank online
The parties needing secure communication might themselves also be part of the network infrastructure
Recall that the domain name system DNS see Section 
or routing daemons that exchange routing information see Chapter require secure communication between two parties
The same is true for network management applications a topic we examined in Chapter 
An intruder that could actively interfere with DNS lookups as discussed in Section 
routing computations RFC or network management functions RFC could wreak havoc in the Internet
Having now established the framework a few of the most important definitions and the need for network security let us next delve into cryptography
While the use of cryptography in providing confidentiality is selfevident well see shortly that it is also central to providing endpoint authentication and message integritymaking cryptography a cornerstone of network security
Principles of Cryptography Although cryptography has a long history dating back at least as far as Julius Caesar modern cryptographic techniques including many of those used in the Internet are based on advances made in the past years
Kahns book The Codebreakers Kahn and Singhs book The Code Book The Science of Secrecy from Ancient Egypt to Quantum Cryptography Singh provide a fascinating look at the Figure 
Cryptographic components long history of cryptography
A complete discussion of cryptography itself requires a complete book Kaufman Schneier and so we only touch on the essential aspects of cryptography particularly as they are practiced on the Internet
We also note that while our focus in this section will be on the use of cryptography for confidentiality well see shortly that cryptographic techniques are inextricably woven into authentication message integrity nonrepudiation and more
Cryptographic techniques allow a sender to disguise data so that an intruder can gain no information from the intercepted data
The receiver of course must be able to recover the original data from the disguised data
illustrates some of the important terminology
Suppose now that Alice wants to send a message to Bob
Alices message in its original form for example Bob I love you
Alice is known as plaintext or cleartext
Alice encrypts her plaintext message using an encryption algorithm so that the encrypted message known as ciphertext looks unintelligible to any intruder
Interestingly in many modern cryptographic systems including those used in the Internet the encryption technique itself is knownpublished standardized and available to everyone for example RFC RFC RFC NIST even a potential intruder Clearly if everyone knows the method for encoding data then there must be some secret information that prevents an intruder from decrypting the transmitted data
This is where keys come in
In Figure 
Alice provides a key KA a string of numbers or characters as input to the encryption algorithm
The encryption algorithm takes the key and the plaintext message m as input and produces ciphertext as output
The notation KAm refers to the ciphertext form encrypted using the key KA of the plaintext message m
The actual encryption algorithm that uses key KA will be evident from the context
Similarly Bob will provide a key KB to the decryption algorithm that takes the ciphertext and Bobs key as input and produces the original plaintext as output
That is if Bob receives an encrypted message KAm he decrypts it by computing KBKAmm
In symmetric key systems Alices and Bobs keys are identical and are secret
In public key systems a pair of keys is used
One of the keys is known to both Bob and Alice indeed it is known to the whole world
The other key is known only by either Bob or Alice but not both
In the following two subsections we consider symmetric key and public key systems in more detail
Symmetric Key Cryptography All cryptographic algorithms involve substituting one thing for another for example taking a piece of plaintext and then computing and substituting the appropriate ciphertext to create the encrypted message
Before studying a modern keybased cryptographic system let us first get our feet wet by studying a very old very simple symmetric key algorithm attributed to Julius Caesar known as the Caesar cipher a cipher is a method for encrypting data
For English text the Caesar cipher would work by taking each letter in the plaintext message and substituting the letter that is k letters later allowing wraparound that is having the letter z followed by the letter a in the alphabet
For example if k then the letter a in plaintext becomes d in ciphertext b in plaintext becomes e in ciphertext and so on
Here the value of k serves as the key
As an example the plaintext message bob i love you
Alice becomes ere l oryh brx
dolfh in ciphertext
While the ciphertext does indeed look like gibberish it wouldnt take long to break the code if you knew that the Caesar cipher was being used as there are only possible key values
An improvement on the Caesar cipher is the monoalphabetic cipher which also substitutes one letter of the alphabet with another letter of the alphabet
However rather than substituting according to a regular pattern for example substitution with an offset of k for all letters any letter can be substituted for any other letter as long as each letter has a unique substitute letter and vice versa
The substitution rule in Figure 
shows one possible rule for encoding plaintext
The plaintext message bob i love you
Alice becomes nkn s gktc wky
Thus as in the case of the Caesar cipher this looks like gibberish
A monoalphabetic cipher would also appear to be better than the Caesar cipher in that there are on the order of possible pairings of letters rather than possible pairings
A bruteforce approach of trying all possible pairings Figure 
A monoalphabetic cipher would require far too much work to be a feasible way of breaking the encryption algorithm and decoding the message
However by statistical analysis of the plaintext language for example knowing that the letters e and t are the most frequently occurring letters in typical English text accounting for percent and percent of letter occurrences and knowing that particular twoand threeletter occurrences of letters appear quite often together for example in it the ion ing and so forth make it relatively easy to break this code
If the intruder has some knowledge about the possible contents of the message then it is even easier to break the code
For example if Trudy the intruder is Bobs wife and suspects Bob of having an affair with Alice then she might suspect that the names bob and alice appear in the text
If Trudy knew for certain that those two names appeared in the ciphertext and had a copy of the example ciphertext message above then she could immediately determine seven of the letter pairings requiring fewer possibilities to be checked by a bruteforce method
Indeed if Trudy suspected Bob of having an affair she might well expect to find some other choice words in the message as well
When considering how easy it might be for Trudy to break Bob and Alices encryption scheme one can distinguish three different scenarios depending on what information the intruder has
In some cases the intruder may have access only to the intercepted ciphertext with no certain information about the contents of the plaintext message
We have seen how statistical analysis can help in a ciphertextonly attack on an encryption scheme
We saw above that if Trudy somehow knew for sure that bob and alice appeared in the ciphertext message then she could have determined the plaintext ciphertext pairings for the letters a l i c e b and o
Trudy might also have been fortunate enough to have recorded all of the ciphertext transmissions and then found Bobs own decrypted version of one of the transmissions scribbled on a piece of paper
When an intruder knows some of the plaintext ciphertext pairings we refer to this as a knownplaintext attack on the encryption scheme
In a chosenplaintext attack the intruder is able to choose the plaintext message and obtain its corresponding ciphertext form
For the simple encryption algorithms weve seen so far if Trudy could get Alice to send the message The quick brown fox jumps over the lazy dog she could completely break the encryption scheme
Well see shortly that for more sophisticated encryption techniques a chosenplaintext attack does not necessarily mean that the encryption technique can be broken
Five hundred years ago techniques improving on monoalphabetic encryption known as polyalphabetic encryption were invented
The idea behind polyalphabetic encryption is to use multiple monoalphabetic ciphers with a specific Figure 
A polyalphabetic cipher using two Caesar ciphers monoalphabetic cipher to encode a letter in a specific position in the plaintext message
Thus the same letter appearing in different positions in the plaintext message might be encoded differently
An example of a polyalphabetic encryption scheme is shown in Figure 
It has two Caesar ciphers with k and k shown as rows
We might choose to use these two Caesar ciphers C and C in the repeating pattern C C C C C
That is the first letter of plaintext is to be encoded using C the second and third using C the fourth using C and the fifth using C
The pattern then repeats with the sixth letter being encoded using C the seventh with C and so on
The plaintext message bob i love you
is thus encrypted ghu n etox dhz
Note that the first b in the plaintext message is encrypted using C while the second b is encrypted using C
In this example the encryption and decryption key is the knowledge of the two Caesar keys k k and the pattern C C C C C
Block Ciphers Let us now move forward to modern times and examine how symmetric key encryption is done today
There are two broad classes of symmetric encryption techniques stream ciphers and block ciphers
Well briefly examine stream ciphers in Section 
when we investigate security for wireless LANs
In this section we focus on block ciphers which are used in many secure Internet protocols including PGP for secure email SSL for securing TCP connections and IPsec for securing the networklayer transport
In a block cipher the message to be encrypted is processed in blocks of k bits
For example if k then the message is broken into bit blocks and each block is encrypted independently
To encode a block the cipher uses a onetoone mapping to map the kbit block of cleartext to a kbit block of ciphertext
Lets look at an example
Suppose that k so that the block cipher maps bit inputs cleartext to bit outputs ciphertext
One possible mapping is given in Table 
Notice that this is a onetoone mapping that is there is a different output for each input
This block cipher breaks the message up into bit blocks and encrypts each block according to the above mapping
You should verify that the message gets encrypted into 
Continuing with this bit block example note that the mapping in Table 
is just one mapping of many possible mappings
How many possible mappings are Table 
A specific bit block cipher input output input output there To answer this question observe that a mapping is nothing more than a permutation of all the possible inputs
There are possible inputs listed under the input columns
These eight inputs can be permuted in different ways
Since each of these permutations specifies a mapping there are possible mappings
We can view each of these mappings as a keyif Alice and Bob both know the mapping the key they can encrypt and decrypt the messages sent between them
The bruteforce attack for this cipher is to try to decrypt ciphtertext by using all mappings
With only mappings when k this can quickly be accomplished on a desktop PC
To thwart bruteforce attacks block ciphers typically use much larger blocks consisting of k bits or even larger
Note that the number of possible mappings for a general kblock cipher is k which is astronomical for even moderate values of k such as k
Although fulltable block ciphers as just described with moderate values of k can produce robust symmetric key encryption schemes they are unfortunately difficult to implement
For k and for a given mapping Alice and Bob would need to maintain a table with input values which is an infeasible task
Moreover if Alice and Bob were to change keys they would have to each regenerate the table
Thus a fulltable block cipher providing predetermined mappings between all inputs and outputs as in the example above is simply out of the question
Instead block ciphers typically use functions that simulate randomly permuted tables
An example adapted from Kaufman of such a function for k bits is shown in Figure 
The function first breaks a bit block into chunks with each chunk consisting of bits
Each bit chunk is processed by an bit to bit table which is of manageable size
For example the first chunk is processed by the table denoted by T
Next the output chunks are reassembled into a bit block
The positions of the bits in the block are then scrambled permuted to produce a bit output
This output is fed back to the bit input where another cycle begins
After n such cycles the function provides a bit block of ciphertext
The purpose of the rounds is to make each input bit affect most if not all of the final output bits
If only one round were used a given input bit would affect only of the output bits
The key for this block cipher algorithm would be the eight permutation tables assuming the scramble function is publicly known
An example of a block cipher Today there are a number of popular block ciphers including DES standing for Data Encryption Standard DES and AES standing for Advanced Encryption Standard
Each of these standards uses functions rather than predetermined tables along the lines of Figure 
albeit more complicated and specific to each cipher
Each of these algorithms also uses a string of bits for a key
For example DES uses bit blocks with a bit key
AES uses bit blocks and can operate with keys that are and bits long
An algorithms key determines the specific minitable mappings and permutations within the algorithms internals
The bruteforce attack for each of these ciphers is to cycle through all the keys applying the decryption algorithm with each key
Observe that with a key length of n there are n possible keys
NIST NIST estimates that a machine that could crack bit DES in one second that is try all keys in one second would take approximately trillion years to crack a bit AES key
CipherBlock Chaining In computer networking applications we typically need to encrypt long messages or long streams of data
If we apply a block cipher as described by simply chopping up the message into kbit blocks and independently encrypting each block a subtle but important problem occurs
To see this observe that two or more of the cleartext blocks can be identical
For example the cleartext in two or more blocks could be HTTP
For these identical blocks a block cipher would of course produce the same ciphertext
An attacker could potentially guess the cleartext when it sees identical ciphertext blocks and may even be able to decrypt the entire message by identifying identical ciphtertext blocks and using knowledge about the underlying protocol structure Kaufman 
To address this problem we can mix some randomness into the ciphertext so that identical plaintext blocks produce different ciphertext blocks
To explain this idea let mi denote the ith plaintext block ci denote the ith ciphertext block and ab denote the exclusiveor XOR of two bit strings a and b
Recall that the and and the XOR of two bit strings is done on a bitbybit basis
So for example 
Also denote the blockcipher encryption algorithm with key S as KS
The basic idea is as follows
The sender creates a random kbit number ri for the ith block and calculates ciKSmiri
Note that a new kbit random number is chosen for each block
The sender then sends c r c r c r and so on
Since the receiver receives ci and ri it can recover each block of the plaintext by computing miKSciri
It is important to note that although ri is sent in the clear and thus can be sniffed by Trudy she cannot obtain the plaintext mi since she does not know the key KS
Also note that if two plaintext blocks mi and mj are the same the corresponding ciphertext blocks ci and cj will be different as long as the random numbers ri and rj are different which occurs with very high probability
As an example consider the bit block cipher in Table 
Suppose the plaintext is 
If Alice encrypts this directly without including the randomness the resulting ciphertext becomes 
If Trudy sniffs this ciphertext because each of the three cipher blocks is the same she can correctly surmise that each of the three plaintext blocks are the same
Now suppose instead Alice generates the random blocks r r and r and uses the above technique to generate the ciphertext c c and c
Note that the three ciphertext blocks are different even though the plaintext blocks are the same
Alice then sends c r c and r
You should verify that Bob can obtain the original plaintext using the shared key KS
The astute reader will note that introducing randomness solves one problem but creates another namely Alice must transmit twice as many bits as before
Indeed for each cipher bit she must now also send a random bit doubling the required bandwidth
In order to have our cake and eat it too block ciphers typically use a technique called Cipher Block Chaining CBC
The basic idea is to send only one random value along with the very first message and then have the sender and receiver use the computed coded blocks in place of the subsequent random number
Specifically CBC operates as follows 
Before encrypting the message or the stream of data the sender generates a random kbit string called the Initialization Vector IV
Denote this initialization vector by c
The sender sends the IV to the receiver in cleartext
For the first block the sender calculates mc that is calculates the exclusiveor of the first block of cleartext with the IV
It then runs the result through the blockcipher algorithm to get the corresponding ciphertext block that is cKSmc
The sender sends the encrypted block c to the receiver
For the ith block the sender generates the ith ciphertext block from ci KSmici
Lets now examine some of the consequences of this approach
First the receiver will still be able to recover the original message
Indeed when the receiver receives ci it decrypts it with KS to obtain simici since the receiver also knows ci it then obtains the cleartext block from misici
Second even if two cleartext blocks are identical the corresponding ciphtertexts almost always will be different
Third although the sender sends the IV in the clear an intruder will still not be able to decrypt the ciphertext blocks since the intruder does not know the secret key S
Finally the sender only sends one overhead block the IV thereby negligibly increasing the bandwidth usage for long messages consisting of hundreds of blocks
As an example lets now determine the ciphertext for the bit block cipher in Table 
with plaintext and IVc
The sender first uses the IV to calculate cKSmc
The sender then calculates c KSmcKS and CKSmcKS 
The reader should verify that the receiver knowing the IV and KS can recover the original plaintext
CBC has an important consequence when designing secure network protocols well need to provide a mechanism within the protocol to distribute the IV from sender to receiver
Well see how this is done for several protocols later in this chapter
Public Key Encryption For more than years since the time of the Caesar cipher and up to the s encrypted communication required that the two communicating parties share a common secretthe symmetric key used for encryption and decryption
One difficulty with this approach is that the two parties must somehow agree on the shared key but to do so requires presumably secure communication Perhaps the parties could first meet and agree on the key in person for example two of Caesars centurions might meet at the Roman baths and thereafter communicate with encryption
In a networked world however communicating parties may never meet and may never converse except over the network
Is it possible for two parties to communicate with encryption without having a shared secret key that is known in advance In Diffie and Hellman Diffie demonstrated an algorithm known now as DiffieHellman Key Exchange to do just thata radically different and marvelously elegant approach toward secure communication that has led to the development of todays public key cryptography systems
Well see shortly that public key cryptography systems also have several wonderful properties that make them useful not only Figure 
Public key cryptography for encryption but for authentication and digital signatures as well
Interestingly it has recently come to light that ideas similar to those in Diffie and RSA had been independently developed in the early s in a series of secret reports by researchers at the CommunicationsElectronics Security Group in the United Kingdom Ellis 
As is often the case great ideas can spring up independently in many places fortunately public key advances took place not only in private but also in the public view as well
The use of public key cryptography is conceptually quite simple
Suppose Alice wants to communicate with Bob
As shown in Figure 
rather than Bob and Alice sharing a single secret key as in the case of symmetric key systems Bob the recipient of Alices messages instead has two keysa public key that is available to everyone in the world including Trudy the intruder and a private key that is known only to Bob
We will use the notation KB and KB to refer to Bobs public and private keys respectively
In order to communicate with Bob Alice first fetches Bobs public key
Alice then encrypts her message m to Bob using Bobs public key and a known for example standardized encryption algorithm that is Alice computes KBm
Bob receives Alices encrypted message and uses his private key and a known for example standardized decryption algorithm to decrypt Alices encrypted message
That is Bob computes KBKBm
We will see below that there are encryptiondecryption algorithms and techniques for choosing public and private keys such that KBKBmm that is applying Bobs public key KB to a message m to get KBm and then applying Bobs private key KB to the encrypted version of m that is computing KBKBm gives back m
This is a remarkable result In this manner Alice can use Bobs publicly available key to send a secret message to Bob without either of them having to distribute any secret keys We will see shortly that we can interchange the public key and private key encryption and get the same remarkable resultthat is KBBmKBKBmm
The use of public key cryptography is thus conceptually simple
But two immediate worries may spring to mind
A first concern is that although an intruder intercepting Alices encrypted message will see only gibberish the intruder knows both the key Bobs public key which is available for all the world to see and the algorithm that Alice used for encryption
Trudy can thus mount a chosenplaintext attack using the known standardized encryption algorithm and Bobs publicly available encryption key to encode any message she chooses Trudy might well try for example to encode messages or parts of messages that she suspects that Alice might send
Clearly if public key cryptography is to work key selection and encryptiondecryption must be done in such a way that it is impossible or at least so hard as to be nearly impossible for an intruder to either determine Bobs private key or somehow otherwise decrypt or guess Alices message to Bob
A second concern is that since Bobs encryption key is public anyone can send an encrypted message to Bob including Alice or someone claiming to be Alice
In the case of a single shared secret key the fact that the sender knows the secret key implicitly identifies the sender to the receiver
In the case of public key cryptography however this is no longer the case since anyone can send an encrypted message to Bob using Bobs publicly available key
A digital signature a topic we will study in Section 
is needed to bind a sender to a message
RSA While there may be many algorithms that address these concerns the RSA algorithm named after its founders Ron Rivest Adi Shamir and Leonard Adleman has become almost synonymous with public key cryptography
Lets first see how RSA works and then examine why it works
RSA makes extensive use of arithmetic operations using modulon arithmetic
So lets briefly review modular arithmetic
Recall that x mod n simply means the remainder of x when divided by n so for example mod 
In modular arithmetic one performs the usual operations of addition multiplication and exponentiation
However the result of each operation is replaced by the integer remainder that is left when the result is divided by n
Adding and multiplying with modular arithmetic is facilitated with the following handy facts a mod nb mod nmod nabmod n a mod nb mod nmod nabmod n a mod nb mod nmod nabmod n It follows from the third fact that a mod nd nad mod n which is an identity that we will soon find very useful
Now suppose that Alice wants to send to Bob an RSAencrypted message as shown in Figure 
In our discussion of RSA lets always keep in mind that a message is nothing but a bit pattern and every bit pattern can be uniquely represented by an integer number along with the length of the bit pattern
For example suppose a message is the bit pattern this message can be represented by the decimal integer 
Thus when encrypting a message with RSA it is equivalent to encrypting the unique integer number that represents the message
There are two interrelated components of RSA The choice of the public key and the private key The encryption and decryption algorithm To generate the public and private RSA keys Bob performs the following steps 
Choose two large prime numbers p and q
How large should p and q be The larger the values the more difficult it is to break RSA but the longer it takes to perform the encoding and decoding
RSA Laboratories recommends that the product of p and q be on the order of bits
For a discussion of how to find large prime numbers see Caldwell 
Compute npq and zpq
Choose a number e less than n that has no common factors other than with z
In this case e and z are said to be relatively prime
The letter e is used since this value will be used in encryption
Find a number d such that ed is exactly divisible that is with no remainder by z
The letter d is used because this value will be used in decryption
Put another way given e we choose d such that ed modz 
The public key that Bob makes available to the world KB is the pair of numbers n e his private key KB is the pair of numbers n d
The encryption by Alice and the decryption by Bob are done as follows Suppose Alice wants to send Bob a bit pattern represented by the integer number m with mn
To encode Alice performs the exponentiation me and then computes the integer remainder when me is divided by n
In other words the encrypted value c of Alices plaintext message m is cmemod n The bit pattern corresponding to this ciphertext c is sent to Bob
To decrypt the received ciphertext message c Bob computes mcdmod n which requires the use of his private key n d
Alices RSA encryption e n Plaintext Letter m numeric representation me Ciphertext cme mod n l o v e As a simple example of RSA suppose Bob chooses p and q
Admittedly these values are far too small to be secure
Then n and z
Bob chooses e since and have no common factors
Finally Bob chooses d since that is ed is exactly divisible by 
Bob makes the two values n and e public and keeps the value d secret
Observing these two public values suppose Alice now wants to send the letters l o v and e to Bob
Interpreting each letter as a number between and with a being and z being Alice and Bob perform the encryption and decryption shown in Tables 
Note that in this example we consider each of the four letters as a distinct message
A more realistic example would be to convert the four letters into their bit ASCII representations and then encrypt the integer corresponding to the resulting bit bit pattern
Such a realistic example generates numbers that are much too long to print in a textbook Given that the toy example in Tables 
has already produced some extremely large numbers and given that we saw earlier that p and q should each be several hundred bits long several practical issues regarding RSA come to mind
How does one choose large prime numbers How does one then choose e and d How does one perform exponentiation with large numbers A discussion of these important issues is beyond the scope of this book see Kaufman and the references therein for details
Bobs RSA decryption d n Ciphertext c cd m cd mod n Plaintext Letter l o v e Session Keys We note here that the exponentiation required by RSA is a rather timeconsuming process
By contrast DES is at least times faster in software and between and times faster in hardware RSA Fast 
As a result RSA is often used in practice in combination with symmetric key cryptography
For example if Alice wants to send Bob a large amount of encrypted data she could do the following
First Alice chooses a key that will be used to encode the data itself this key is referred to as a session key and is denoted by KS
Alice must inform Bob of the session key since this is the shared symmetric key they will use with a symmetric key cipher e.g
with DES or AES
Alice encrypts the session key using Bobs public key that is computes cKSe mod n
Bob receives the RSAencrypted session key c and decrypts it to obtain the session key KS
Bob now knows the session key that Alice will use for her encrypted data transfer
Why Does RSA Work RSA encryptiondecryption appears rather magical
Why should it be that by applying the encryption algorithm and then the decryption algorithm one recovers the original message In order to understand why RSA works again denote npq where p and q are the large prime numbers used in the RSA algorithm
Recall that under RSA encryption a message uniquely represented by an integer m is exponentiated to the power e using modulon arithmetic that is cmemod n Decryption is performed by raising this value to the power d again using modulon arithmetic
The result of an encryption step followed by a decryption step is thus me mod nd mod n
Lets now see what we can say about this quantity
As mentioned earlier one important property of modulo arithmetic is a mod nd mod nad mod n for any values a n and d
Thus using ame in this property we have memod ndmod nmedmod n It therefore remains to show that medmod nm
Although were trying to remove some of the magic about why RSA works to establish this well need to use a rather magical result from number theory here
Specifically well need the result that says if p and q are prime npq and zpq then xy mod n is the same as xy mod z mod n Kaufman 
Applying this result with xm and yed we have medmod nmedmod zmod n But remember that we have chosen e and d such that edmod z
This gives us medmod nmmod nm which is exactly the result we are looking for By first exponentiating to the power of e that is encrypting and then exponentiating to the power of d that is decrypting we obtain the original value m
Even more wonderful is the fact that if we first exponentiate to the power of d and then exponentiate to the power of ethat is we reverse the order of encryption and decryption performing the decryption operation first and then applying the encryption operationwe also obtain the original value m
This wonderful result follows immediately from the modular arithmetic mdmod nemod nmdemod nmedmod nmemod ndmod n The security of RSA relies on the fact that there are no known algorithms for quickly factoring a number in this case the public value n into the primes p and q
If one knew p and q then given the public value e one could easily compute the secret key d
On the other hand it is not known whether or not there exist fast algorithms for factoring a number and in this sense the security of RSA is not guaranteed
Another popular publickey encryption algorithm is the DiffieHellman algorithm which we will briefly explore in the homework problems
DiffieHellman is not as versatile as RSA in that it cannot be used to encrypt messages of arbitrary length it can be used however to establish a symmetric session key which is in turn used to encrypt messages
Message Integrity and Digital Signatures In the previous section we saw how encryption can be used to provide confidentiality to two communicating entities
In this section we turn to the equally important cryptography topic of providing message integrity also known as message authentication
Along with message integrity we will discuss two related topics in this section digital signatures and endpoint authentication
We define the message integrity problem using once again Alice and Bob
Suppose Bob receives a message which may be encrypted or may be in plaintext and he believes this message was sent by Alice
To authenticate this message Bob needs to verify 
The message indeed originated from Alice
The message was not tampered with on its way to Bob
Well see in Sections 
that this problem of message integrity is a critical concern in just about all secure networking protocols
As a specific example consider a computer network using a linkstate routing algorithm such as OSPF for determining routes between each pair of routers in the network see Chapter 
In a linkstate algorithm each router needs to broadcast a linkstate message to all other routers in the network
A routers linkstate message includes a list of its directly connected neighbors and the direct costs to these neighbors
Once a router receives linkstate messages from all of the other routers it can create a complete map of the network run its leastcost routing algorithm and configure its forwarding table
One relatively easy attack on the routing algorithm is for Trudy to distribute bogus linkstate messages with incorrect linkstate information
Thus the need for message integritywhen router B receives a link state message from router A router B should verify that router A actually created the message and further that no one tampered with the message in transit
In this section we describe a popular message integrity technique that is used by many secure networking protocols
But before doing so we need to cover another important topic in cryptography cryptographic hash functions
Cryptographic Hash Functions As shown in Figure 
a hash function takes an input m and computes a fixedsize string Hm known as a hash
The Internet checksum Chapter and CRCs Chapter meet this definition
A cryptographic hash function is required to have the following additional property It is computationally infeasible to find any two different messages x and y such that HxHy
Informally this property means that it is computationally infeasible for an intruder to substitute one message for another message that is protected by the hash Figure 
Hash functions Figure 
Initial message and fraudulent message have the same checksum function
That is if m Hm are the message and the hash of the message created by the sender then an intruder cannot forge the contents of another message y that has the same hash value as the original message
Lets convince ourselves that a simple checksum such as the Internet checksum would make a poor cryptographic hash function
Rather than performing s complement arithmetic as in the Internet checksum let us compute a checksum by treating each character as a byte and adding the bytes together using byte chunks at a time
Suppose Bob owes Alice 
and sends an IOU to Alice consisting of the text string IOU.BOB
The ASCII representation in hexadecimal notation for these letters is F E F 
top shows that the byte checksum for this message is B C D AC
A slightly different message and a much more costly one for Bob is shown in the bottom half of Figure 
The messages IOU.BOB and IOU.BOB have the same checksum
Thus this simple checksum algorithm violates the requirement above
Given the original data it is simple to find another set of data with the same checksum
Clearly for security purposes we are going to need a more powerful hash function than a checksum
The MD hash algorithm of Ron Rivest RFC is in wide use today
It computes a bit hash in a fourstep process consisting of a padding step adding a one followed by enough zeros so that the length of the message satisfies certain conditions an append step appending a bit representation of the message length before padding an initialization of an accumulator and a final looping step in which the messages word blocks are processed mangled in four rounds
For a description of MD including a C source code implementation see RFC 
The second major hash algorithm in use today is the Secure Hash Algorithm SHA FIPS 
This algorithm is based on principles similar to those used in the design of MD RFC the predecessor to MD
SHA a US federal standard is required for use whenever a cryptographic hash algorithm is needed for federal applications
It produces a bit message digest
The longer output length makes SHA more secure
Message Authentication Code Lets now return to the problem of message integrity
Now that we understand hash functions lets take a first stab at how we might perform message integrity 
Alice creates message m and calculates the hash Hm for example with SHA
Alice then appends Hm to the message m creating an extended message m Hm and sends the extended message to Bob
Bob receives an extended message m h and calculates Hm
If Hmh Bob concludes that everything is fine
This approach is obviously flawed
Trudy can create a bogus message m in which she says she is Alice calculate Hm and send Bob m Hm
When Bob receives the message everything checks out in step so Bob doesnt suspect any funny business
To perform message integrity in addition to using cryptographic hash functions Alice and Bob will need a shared secret s
This shared secret which is nothing more than a string of bits is called the authentication key
Using this shared secret message integrity can be performed as follows 
Alice creates message m concatenates s with m to create ms and calculates the hash Hms for example with SHA
Hms is called the message authentication code MAC
Alice then appends the MAC to the message m creating an extended message m Hms and sends the extended message to Bob
Bob receives an extended message m h and knowing s calculates the MAC Hms
If Hmsh Bob concludes that everything is fine
A summary of the procedure is shown in Figure 
Readers should note that the MAC here standing for message authentication code is not the same MAC used in linklayer protocols standing for medium access control One nice feature of a MAC is that it does not require an encryption algorithm
Indeed in many applications including the linkstate routing algorithm described earlier communicating entities are only concerned with message integrity and are not concerned with message confidentiality
Using a MAC the entities can authenticate Figure 
Message authentication code MAC the messages they send to each other without having to integrate complex encryption algorithms into the integrity process
As you might expect a number of different standards for MACs have been proposed over the years
The most popular standard today is HMAC which can be used either with MD or SHA
HMAC actually runs data and the authentication key through the hash function twice Kaufman RFC 
There still remains an important issue
How do we distribute the shared authentication key to the communicating entities For example in the linkstate routing algorithm we would somehow need to distribute the secret authentication key to each of the routers in the autonomous system
Note that the routers can all use the same authentication key
A network administrator could actually accomplish this by physically visiting each of the routers
Or if the network administrator is a lazy guy and if each router has its own public key the network administrator could distribute the authentication key to any one of the routers by encrypting it with the routers public key and then sending the encrypted key over the network to the router
Digital Signatures Think of the number of the times youve signed your name to a piece of paper during the last week
You sign checks credit card receipts legal documents and letters
Your signature attests to the fact that you as opposed to someone else have acknowledged andor agreed with the documents contents
In a digital world one often wants to indicate the owner or creator of a document or to signify ones agreement with a documents content
A digital signature is a cryptographic technique for achieving these goals in a digital world
Just as with handwritten signatures digital signing should be done in a way that is verifiable and nonforgeable
That is it must be possible to prove that a document signed by an individual was indeed signed by that individual the signature must be verifiable and that only that individual could have signed the document the signature cannot be forged
Lets now consider how we might design a digital signature scheme
Observe that when Bob signs a message Bob must put something on the message that is unique to him
Bob could consider attaching a MAC for the signature where the MAC is created by appending his key unique to him to the message and then taking the hash
But for Alice to verify the signature she must also have a copy of the key in which case the key would not be unique to Bob
Thus MACs are not going to get the job done here
Recall that with publickey cryptography Bob has both a public and private key with both of these keys being unique to Bob
Thus publickey cryptography is an excellent candidate for providing digital signatures
Let us now examine how it is done
Suppose that Bob wants to digitally sign a document m
We can think of the document as a file or a message that Bob is going to sign and send
As shown in Figure 
to sign this document Bob simply uses his private key KB to compute KBm
At first it might seem odd that Bob is using his private key which as we saw in Section 
was used to decrypt a message that had been encrypted with his public key to sign a document
But recall that encryption and decryption are nothing more than mathematical operations exponentiation to the power of e or d in RSA see Section 
and recall that Bobs goal is not to scramble or obscure the contents of the document but rather to sign the document in a manner that is verifiable and nonforgeable
Bobs digital signature of the document is KBm
Does the digital signature KBm meet our requirements of being verifiable and nonforgeable Suppose Alice has m and KBm
She wants to prove in court being Figure 
Creating a digital signature for a document litigious that Bob had indeed signed the document and was the only person who could have possibly signed the document
Alice takes Bobs public key KB and applies it to the digital signature KBm associated with the document m
That is she computes KBKBm and voilÃ  with a dramatic flurry she produces m which exactly matches the original document Alice then argues that only Bob could have signed the document for the following reasons Whoever signed the message must have used the private key KB in computing the signature KBm such that KBKBmm
The only person who could have known the private key KB is Bob
Recall from our discussion of RSA in Section 
that knowing the public key KB is of no help in learning the private key KB
Therefore the only person who could know KB is the person who generated the pair of keys KB KB in the first place Bob
Note that this assumes though that Bob has not given KB to anyone nor has anyone stolen KB from Bob
It is also important to note that if the original document m is ever modified to some alternate form m the signature that Bob created for m will not be valid for m since KBKBm does not equal m
Thus we see that digital signatures also provide message integrity allowing the receiver to verify that the message was unaltered as well as the source of the message
One concern with signing data by encryption is that encryption and decryption are computationally expensive
Given the overheads of encryption and decryption signing data via complete encryptiondecryption can be overkill
A more efficient approach is to introduce hash functions into the digital signature
Recall from Section 
that a hash algorithm takes a message m of arbitrary length and computes a fixedlength fingerprint of the message denoted by Hm
Using a hash function Bob signs the hash of a message rather than the message itself that is Bob calculates KBHm
Since Hm is generally much smaller than the original message m the computational effort required to create the digital signature is substantially reduced
In the context of Bob sending a message to Alice Figure 
provides a summary of the operational procedure of creating a digital signature
Bob puts his original long message through a hash function
He then digitally signs the resulting hash with his private key
The original message in cleartext along with the digitally signed message digest henceforth referred to as the digital signature is then sent to Alice
provides a summary of the operational procedure of the signature
Alice applies the senders public key to the message to obtain a hash result
Alice also applies the hash function to the cleartext message to obtain a second hash result
If the two hashes match then Alice can be sure about the integrity and author of the message
Before moving on lets briefly compare digital signatures with MACs since they have parallels but also have important subtle differences
Both digital signatures and Figure 
Sending a digitally signed message MACs start with a message or a document
To create a MAC out of the message we append an authentication key to the message and then take the hash of the result
Note that neither public key nor symmetric key encryption is involved in creating the MAC
To create a digital signature we first take the hash of the message and then encrypt the message with our private key using public key cryptography
Thus a digital signature is a heavier technique since it requires an underlying Public Key Infrastructure PKI with certification authorities as described below
Well see in Section 
that PGPa popular secure email systemuses digital signatures for message integrity
Weve seen already that OSPF uses MACs for message integrity
Well see in Sections 
that MACs are also used for popular transportlayer and networklayer security protocols
Public Key Certification An important application of digital signatures is public key certification that is certifying that a public key belongs to a specific entity
Public key certification is used in many popular secure networking protocols including IPsec and SSL
To gain insight into this problem lets consider an Internetcommerce version of the classic pizza prank
Alice is in the pizza delivery business and accepts orders Figure 
Verifying a signed message over the Internet
Bob a pizza lover sends Alice a plaintext message that includes his home address and the type of pizza he wants
In this message Bob also includes a digital signature that is a signed hash of the original plaintext message to prove to Alice that he is the true source of the message
To verify the signature Alice obtains Bobs public key perhaps from a public key server or from the email message and checks the digital signature
In this manner she makes sure that Bob rather than some adolescent prankster placed the order
This all sounds fine until clever Trudy comes along
As shown in Figure 
Trudy is indulging in a prank
She sends a message to Alice in which she says she is Bob gives Bobs home address and orders a pizza
In this message she also includes her Trudys public key although Alice naturally assumes it is Bobs public key
Trudy also attaches a digital signature which was created with her own Trudys private key
After receiving the message Alice applies Trudys public key thinking that it is Bobs to the digital signature and concludes that the plaintext message was Figure 
Trudy masquerades as Bob using public key cryptography indeed created by Bob
Bob will be very surprised when the delivery person brings a pizza with pepperoni and anchovies to his home We see from this example that for public key cryptography to be useful you need to be able to verify that you have the actual public key of the entity person router browser and so on with whom you want to communicate
For example when Alice wants to communicate with Bob using public key cryptography she needs to verify that the public key that is supposed to be Bobs is indeed Bobs
Binding a public key to a particular entity is typically done by a Certification Authority CA whose job is to validate identities and issue certificates
A CA has the following roles 
A CA verifies that an entity a person a router and so on is who it says it is
There are no mandated procedures for how certification is done
When dealing with a CA one must trust the CA to have performed a suitably rigorous identity verification
For example if Trudy were able to walk into the FlybyNight Figure 
Bob has his public key certified by the CA CA and simply announce I am Alice and receive certificates associated with the identity of Alice then one shouldnt put much faith in public keys certified by the FlybyNight CA
On the other hand one might or might not be more willing to trust a CA that is part of a federal or state program
You can trust the identity associated with a public key only to the extent to which you can trust a CA and its identity verification techniques
What a tangled web of trust we spin 
Once the CA verifies the identity of the entity the CA creates a certificate that binds the public key of the entity to the identity
The certificate contains the public key and globally unique identifying information about the owner of the public key for example a human name or an IP address
The certificate is digitally signed by the CA
These steps are shown in Figure 
Let us now see how certificates can be used to combat pizzaordering pranksters like Trudy and other undesirables
When Bob places his order he also sends his CAsigned certificate
Alice uses the CAs public key to check the validity of Bobs certificate and extract Bobs public key
Both the International Telecommunication Union ITU and the IETF have developed standards for CAs
ITU a specifies an authentication service as well as a specific syntax for certificates
RFC describes CAbased key management for use with secure Internet email
It is compatible with X
but goes beyond X
by establishing procedures and conventions for a key management architecture
describes some of the important fields in a certificate
Selected fields in an X
and RFC public key Field Name Description Version Version number of X
specification Serial CAissued unique identifier for a certificate number Signature Specifies the algorithm used by CA to sign this certificate Issuer Identity of CA issuing this certificate in distinguished name DN RFC format name Validity Start and end of period of validity for certificate period Subject Identity of entity whose public key is associated with this certificate in DN format name Subject The subjects public key as well indication of the public key algorithm and algorithm public key parameters to be used with this key 
EndPoint Authentication Endpoint authentication is the process of one entity proving its identity to another entity over a computer network for example a user proving its identity to an email server
As humans we authenticate each other in many ways We recognize each others faces when we meet we recognize each others voices on the telephone we are authenticated by the customs official who checks us against the picture on our passport
In this section we consider how one party can authenticate another party when the two are communicating over a network
We focus here on authenticating a live party at the point in time when communication is actually occurring
A concrete example is a user authenticating him or herself to an e mail server
This is a subtly different problem from proving that a message received at some point in the past did indeed come from that claimed sender as studied in Section 
When performing authentication over the network the communicating parties cannot rely on biometric information such as a visual appearance or a voiceprint
Indeed we will see in our later case studies that it is often network elements such as routers and clientserver processes that must authenticate each other
Here authentication must be done solely on the basis of messages and data exchanged as part of an authentication protocol
Typically an authentication protocol would run before the two communicating parties run some other protocol for example a reliable data transfer protocol a routing information exchange protocol or an email protocol
The authentication protocol first establishes the identities of the parties to each others satisfaction only after authentication do the parties get down to the work at hand
As in the case of our development of a reliable data transfer rdt protocol in Chapter we will find it instructive here to develop various versions of an authentication protocol which we will call ap authentication protocol and poke holes in each version Figure 
and a failure scenario as we proceed
If you enjoy this stepwise evolution of a design you might also enjoy Bryant which recounts a fictitious narrative between designers of an opennetwork authentication system and their discovery of the many subtle issues involved
Lets assume that Alice needs to authenticate herself to Bob
Authentication Protocol ap
Perhaps the simplest authentication protocol we can imagine is one where Alice simply sends a message to Bob saying she is Alice
This protocol is shown in Figure 
The flaw here is obvious there is no way for Bob actually to know that the person sending the message I am Alice is indeed Alice
For example Trudy the intruder could just as well send such a message
Authentication Protocol ap
If Alice has a wellknown network address e.g
an IP address from which she always communicates Bob could attempt to authenticate Alice by verifying that the source address on the IP datagram carrying the authentication message matches Alices wellknown address
In this case Alice would be authenticated
This might stop a very networknaive intruder from impersonating Alice but it wouldnt stop the determined student studying this book or many others From our study of the network and data link layers we know that it is not that hard for example if one had access to the operating system code and could build ones own operating system kernel as is the case with Linux and several other freely available operating systems to create an IP datagram put whatever IP source address we want for example Alices wellknown IP address into the IP datagram and send the datagram over the linklayer protocol to the firsthop router
From then Figure 
and a failure scenario on the incorrectly sourceaddressed datagram would be dutifully forwarded to Bob
This approach shown in Figure 
is a form of IP spoofing
IP spoofing can be avoided if Trudys firsthop router is configured to forward only datagrams containing Trudys IP source address RFC 
However this capability is not universally deployed or enforced
Bob would thus be foolish to assume that Trudys network manager who might be Trudy herself had configured Trudys firsthop router to forward only appropriately addressed datagrams
Authentication Protocol ap
One classic approach to authentication is to use a secret password
The password is a shared secret between the authenticator and the person being authenticated
Gmail Facebook telnet FTP and many other services use password authentication
In protocol ap
Alice thus sends her secret password to Bob as shown in Figure 
Since passwords are so widely used we might suspect that protocol ap
is fairly secure
If so wed be wrong The security flaw here is clear
If Trudy eavesdrops on Alices communication then she can learn Alices password
Lest you think this is unlikely consider the fact that when you Telnet to another machine and log in the login password is sent unencrypted to the Telnet server
Someone connected to the Telnet client or servers LAN can possibly sniff read and store all packets transmitted on the LAN and thus steal the login password
In fact this is a wellknown approach for stealing passwords see for example Jimenez 
Such a threat is obviously very real so ap
clearly wont do
Authentication Protocol ap
Our next idea for fixing ap
is naturally to encrypt the password
By encrypting the password we can prevent Trudy from learning Alices password
If we assume Figure 
and a failure scenario that Alice and Bob share a symmetric secret key KAB then Alice can encrypt the password and send her identification message I am Alice and her encrypted password to Bob
Bob then decrypts the password and assuming the password is correct authenticates Alice
Bob feels comfortable in authenticating Alice since Alice not only knows the password but also knows the shared secret key value needed to encrypt the password
Lets call this protocol ap
While it is true that ap
prevents Trudy from learning Alices password the use of cryptography here does not solve the authentication problem
Bob is subject to a playback attack Trudy need only eavesdrop on Alices communication record the encrypted version of the password and play back the encrypted version of the password to Bob to pretend that she is Alice
The use of an encrypted password in ap
doesnt make the situation manifestly different from that of protocol ap
in Figure 
Authentication Protocol ap
The failure scenario in Figure 
resulted from the fact that Bob could not distinguish between the original authentication of Alice and the later playback of Alices original authentication
That is Bob could not tell if Alice was live that is was currently really on the other end of the connection or whether the messages he was receiving were a recorded playback of a previous authentication of Alice
The very very observant reader will recall that the threeway TCP handshake protocol needed to address the same problemthe server side of a TCP connection did not want to accept a connection if the received SYN segment was an old copy retransmission of a SYN segment from an earlier connection
How did the TCP server side solve the problem of determining whether the client was really live It chose an initial sequence number that had not been used in a very long time sent that number to the client and then waited for the client to respond with an ACK segment containing that number
We can adopt the same idea here for authentication purposes
A nonce is a number that a protocol will use only once in a lifetime
That is once a protocol uses a nonce it will never use that number again
protocol uses a nonce as follows 
Alice sends the message I am Alice to Bob
Bob chooses a nonce R and sends it to Alice
Alice encrypts the nonce using Alice and Bobs symmetric secret key KAB and sends the encrypted nonce KAB R back to Bob
As in protocol ap
it is the fact that Alice knows KAB and uses it to encrypt a value that lets Bob know that the message he receives was generated by Alice
The nonce is used to ensure that Alice is live
Bob decrypts the received message
If the decrypted nonce equals the nonce he sent Alice then Alice is authenticated
is illustrated in Figure 
By using the onceinalifetime value R and then checking the returned value KAB R Bob can be sure that Alice is both who she says she is since she knows the secret key value needed to encrypt R and live since she has encrypted the nonce R that Bob just created
The use of a nonce and symmetric key cryptography forms the basis of ap
A natural question is whether we can use a nonce and public key cryptography rather than symmetric key cryptography to solve the authentication problem
This issue is explored in the problems at the end of the chapter
and a failure scenario 
Securing EMail In previous sections we examined fundamental issues in network security including symmetric key and public key cryptography endpoint authentication key distribution message integrity and digital signatures
We are now going to examine how these tools are being used to provide security in the Internet
Interestingly it is possible to provide security services in any of the top four layers of the Internet protocol stack
When security is provided for a specific applicationlayer protocol the application using the protocol will enjoy one or more security services such as confidentiality authentication or integrity
When security is provided for a transportlayer protocol all applications that use that protocol enjoy the security services of the transport protocol
When security is provided at the network layer on a hostto host basis all transportlayer segments and hence all applicationlayer data enjoy the security services of the network layer
When security is provided on a link basis then the data in all frames traveling over the link receive the security services of the link
In Sections 
we examine how security tools are being used in the application transport network and link layers
Being consistent with the general structure of this book we begin at the top of the protocol stack and discuss security at the application layer
Our approach is to use a specific application email as a case study for applicationlayer security
We then move down the protocol stack
Well examine the SSL protocol which provides security at the transport layer IPsec which provides security at the network layer and the security of the IEEE 
wireless LAN protocol
You might be wondering why security functionality is being provided at more than one layer in the Internet
Wouldnt it suffice simply to provide the security functionality at the network layer and be done with it There are two answers to this question
First although security at the network layer can offer blanket coverage by encrypting all the data in the datagrams that is all the transportlayer segments and by authenticating all the source IP addresses it cant provide userlevel security
For example a commerce site cannot rely on IPlayer security to authenticate a customer who is purchasing goods at the commerce site
Thus there is a need for security functionality at higher layers as well as blanket coverage at lower layers
Second it is generally easier to deploy new Internet services including security services at the higher layers of the protocol stack
While waiting for security to be broadly deployed at the network layer which is probably still many years in the future many application developers just do it and introduce security functionality into their favorite applications
A classic example is Pretty Good Privacy PGP which provides secure email discussed later in this section
Requiring only client and server application code PGP was one of the first security technologies to be broadly used in the Internet
Secure EMail We now use the cryptographic principles of Sections 
to create a secure email system
We create this highlevel design in an incremental manner at each step introducing new security services
When designing a secure email system let us keep in mind the racy example introduced in Section .the love affair between Alice and Bob
Imagine that Alice wants to send an email message to Bob and Trudy wants to intrude
Before plowing ahead and designing a secure email system for Alice and Bob we should consider which security features would be most desirable for them
First and foremost is confidentiality
As discussed in Section 
neither Alice nor Bob wants Trudy to read Alices email message
The second feature that Alice and Bob would most likely want to see in the secure email system is sender authentication
In particular when Bob receives the message I dont love you anymore
I never want to see you again
Formerly yours Alice he would naturally want to be sure that the message came from Alice and not from Trudy
Another feature that the two lovers would appreciate is message integrity that is assurance that the message Alice sends is not modified while en route to Bob
Finally the email system should provide receiver authentication that is Alice wants to make sure that she is indeed sending the letter to Bob and not to someone else for example Trudy who is impersonating Bob
So lets begin by addressing the foremost concern confidentiality
The most straightforward way to provide confidentiality is for Alice to encrypt the message with symmetric key technology such as DES or AES and for Bob to decrypt the message on receipt
As discussed in Section 
if the symmetric key is long enough and if only Alice and Bob have the key then it is extremely difficult for anyone else including Trudy to read the message
Although this approach is straightforward it has the fundamental difficulty that we discussed in Section .distributing a symmetric key so that only Alice and Bob have copies of it
So we naturally consider an alternative approachpublic key cryptography using for example RSA
In the public key approach Bob makes his public key publicly available e.g
in a public key server or on his personal Web page Alice encrypts her message with Bobs public key and she sends the encrypted message to Bobs email address
When Bob receives the message he simply decrypts it with his private key
Assuming that Alice knows for sure that the public key is Bobs public key this approach is an excellent means to provide the desired confidentiality
One problem however is that public key encryption is relatively inefficient particularly for long messages
To overcome the efficiency problem lets make use of a session key discussed in Section 
In particular Alice selects a random symmetric session key KS encrypts her message m with the symmetric key encrypts the symmetric key with Bobs public key KB concatenates the encrypted message and the encrypted symmetric key to form a package and sends the package to Bobs Figure 
Alice used a symmetric session key KS to send a secret email to Bob email address
The steps are illustrated in Figure 
In this and the subsequent figures the circled represents concatenation and the circled represents deconcatenation
When Bob receives the package he uses his private key KB to obtain the symmetric key KS and uses the symmetric key KS to decrypt the message m
Having designed a secure email system that provides confidentiality lets now design another system that provides both sender authentication and message integrity
Well suppose for the moment that Alice and Bob are no longer concerned with confidentiality they want to share their feelings with everyone and are concerned only about sender authentication and message integrity
To accomplish this task we use digital signatures and message digests as described in Section 
Specifically Alice applies a hash function H for example MD to her message m to obtain a message digest signs the result of the hash function with her private key KA to create a digital signature concatenates the original unencrypted message with the signature to create a package and sends the package to Bobs email address
When Bob receives the package he applies Alices public key KA to the signed message digest and compares the result of this operation with his own hash H of the message
The steps are illustrated in Figure 
As discussed in Section 
if the two results are the same Bob can be pretty confident that the message came from Alice and is unaltered
Now lets consider designing an email system that provides confidentiality sender authentication and message integrity
This can be done by combining the procedures in Figures 
Alice first creates a preliminary package exactly as in Figure 
that consists of her original message along with a digitally signed hash of the message
She then treats this preliminary package as a message in itself and sends this new message through the sender steps in Figure 
creating a new package that is sent to Bob
The steps applied by Alice are shown in Figure 
When Bob receives the package he first applies his side of Figure 
and then his Figure 
Using hash functions and digital signatures to provide sender authentication and message integrity side of Figure 
It should be clear that this design achieves the goal of providing confidentiality sender authentication and message integrity
Note that in this scheme Alice uses public key cryptography twice once with her own private key and once with Bobs public key
Similarly Bob also uses public key cryptography twiceonce with his private key and once with Alices public key
The secure email design outlined in Figure 
probably provides satisfactory security for most email users for most occasions
But there is still one important issue that remains to be addressed
The design in Figure 
requires Alice to obtain Bobs public key and requires Bob to obtain Alices public key
The distribution of these public keys is a nontrivial problem
For example Trudy might masquerade as Bob and give Alice her own public key while saying that it is Bobs public key Figure 
Alice uses symmetric key cyptography public key cryptography a hash function and a digital signature to provide secrecy sender authentication and message integrity enabling her to receive the message meant for Bob
As we learned in Section 
a popular approach for securely distributing public keys is to certify the public keys using a CA
PGP Written by Phil Zimmermann in Pretty Good Privacy PGP is a nice example of an email encryption scheme PGPI 
Versions of PGP are available in the public domain for example you can find the PGP software for your favorite platform as well as lots of interesting reading at the International PGP Home Page PGPI 
The PGP design is in essence the same as the design shown in Figure 
Depending on the version the PGP software uses MD or SHA for calculating the message digest CAST tripleDES or IDEA for symmetric key encryption and RSA for the public key encryption
When PGP is installed the software creates a public key pair for the user
The public key can be posted on the users Web site or placed in a public key server
The private key is protected by the use of a password
The password has to be entered every time the user accesses the private key
PGP gives the user the option of digitally signing the message encrypting the message or both digitally signing and encrypting
shows a PGP signed message
This message appears after the MIME header
The encoded data in the message is KAHm that is the digitally signed message digest
As we discussed above in order for Bob to verify the integrity of the message he needs to have access to Alices public key
shows a secret PGP message
This message also appears after the MIME header
Of course the plaintext message is not included within the secret email message
When a sender such as Alice wants both confidentiality and integrity PGP contains a message like that of Figure 
within the message of Figure 
PGP also provides a mechanism for public key certification but the mechanism is quite different from the more conventional CA
PGP public keys are certified by Figure 
A PGP signed message Figure 
A secret PGP message a web of trust
Alice herself can certify any keyusername pair when she believes the pair really belong together
In addition PGP permits Alice to say that she trusts another user to vouch for the authenticity of more keys
Some PGP users sign each others keys by holding keysigning parties
Users physically gather exchange public keys and certify each others keys by signing them with their private keys
Securing TCP Connections SSL In the previous section we saw how cryptographic techniques can provide confidentiality data integrity and endpoint authentication to a specific application namely email
In this section well drop down a layer in the protocol stack and examine how cryptography can enhance TCP with security services including confidentiality data integrity and endpoint authentication
This enhanced version of TCP is commonly known as Secure Sockets Layer SSL
A slightly modified version of SSL version called Transport Layer Security TLS has been standardized by the IETF RFC 
The SSL protocol was originally designed by Netscape but the basic ideas behind securing TCP had predated Netscapes work for example see Woo Woo 
Since its inception SSL has enjoyed broad deployment
SSL is supported by all popular Web browsers and Web servers and it is used by Gmail and essentially all Internet commerce sites including Amazon eBay and TaoBao
Hundreds of billions of dollars are spent over SSL every year
In fact if you have ever purchased anything over the Internet with your credit card the communication between your browser and the server for this purchase almost certainly went over SSL
You can identify that SSL is being used by your browser when the URL begins with https rather than http
To understand the need for SSL lets walk through a typical Internet commerce scenario
Bob is surfing the Web and arrives at the Alice Incorporated site which is selling perfume
The Alice Incorporated site displays a form in which Bob is supposed to enter the type of perfume and quantity desired his address and his payment card number
Bob enters this information clicks on Submit and expects to receive via ordinary postal mail the purchased perfumes he also expects to receive a charge for his order in his next payment card statement
This all sounds good but if no security measures are taken Bob could be in for a few surprises
If no confidentiality encryption is used an intruder could intercept Bobs order and obtain his payment card information
The intruder could then make purchases at Bobs expense
If no data integrity is used an intruder could modify Bobs order having him purchase ten times more bottles of perfume than desired
Finally if no server authentication is used a server could display Alice Incorporateds famous logo when in actuality the site maintained by Trudy who is masquerading as Alice Incorporated
After receiving Bobs order Trudy could take Bobs money and run
Or Trudy could carry out an identity theft by collecting Bobs name address and credit card number
SSL addresses these issues by enhancing TCP with confidentiality data integrity server authentication and client authentication
SSL is often used to provide security to transactions that take place over HTTP
However because SSL secures TCP it can be employed by any application that runs over TCP
SSL provides a simple Application Programmer Interface API with sockets which is similar and analogous to TCPs API
When an application wants to employ SSL the application includes SSL classeslibraries
As shown in Figure 
although SSL technically resides in the application layer from the developers perspective it is a transport protocol that provides TCPs services enhanced with security services
The Big Picture We begin by describing a simplified version of SSL one that will allow us to get a bigpicture understanding of the why and how of SSL
We will refer to this simplified Figure 
Although SSL technically resides in the application layer from the developers perspective it is a transportlayer protocol version of SSL as almostSSL
After describing almostSSL in the next subsection well then describe the real SSL filling in the details
AlmostSSL and SSL has three phases handshake key derivation and data transfer
We now describe these three phases for a communication session between a client Bob and a server Alice with Alice having a privatepublic key pair and a certificate that binds her identity to her public key
Handshake During the handshake phase Bob needs to a establish a TCP connection with Alice b verify that Alice is really Alice and c send Alice a master secret key which will be used by both Alice and Bob to generate all the symmetric keys they need for the SSL session
These three steps are shown in Figure 
Note that once the TCP connection is established Bob sends Alice a hello message
Alice then responds with her certificate which contains her public key
As discussed in Section 
because the certificate has been certified by a CA Bob knows for sure that the public key in the certificate belongs to Alice
Bob then generates a Master Secret MS which will only be used for this SSL session encrypts the MS with Alices public key to create the Encrypted Master Secret EMS and sends the EMS to Alice
Alice decrypts the EMS with her private key to get the MS
After this phase both Bob and Alice and no one else know the master secret for this SSL session
The almostSSL handshake beginning with a TCP connection Key Derivation In principle the MS now shared by Bob and Alice could be used as the symmetric session key for all subsequent encryption and data integrity checking
It is however generally considered safer for Alice and Bob to each use different cryptographic keys and also to use different keys for encryption and integrity checking
Thus both Alice and Bob use the MS to generate four keys EB session encryption key for data sent from Bob to Alice MB session MAC key for data sent from Bob to Alice EA session encryption key for data sent from Alice to Bob MA session MAC key for data sent from Alice to Bob Alice and Bob each generate the four keys from the MS
This could be done by simply slicing the MS into four keys
But in real SSL it is a little more complicated as well see
At the end of the key derivation phase both Alice and Bob have all four keys
The two encryption keys will be used to encrypt data the two MAC keys will be used to verify the integrity of the data
Data Transfer Now that Alice and Bob share the same four session keys EB MB EA and MA they can start to send secured data to each other over the TCP connection
Since TCP is a bytestream protocol a natural approach would be for SSL to encrypt application data on the fly and then pass the encrypted data on the fly to TCP
But if we were to do this where would we put the MAC for the integrity check We certainly do not want to wait until the end of the TCP session to verify the integrity of all of Bobs data that was sent over the entire session To address this issue SSL breaks the data stream into records appends a MAC to each record for integrity checking and then encrypts the record MAC
To create the MAC Bob inputs the record data along with the key MB into a hash function as discussed in Section 
To encrypt the package record MAC Bob uses his session encryption key EB
This encrypted package is then passed to TCP for transport over the Internet
Although this approach goes a long way it still isnt bulletproof when it comes to providing data integrity for the entire message stream
In particular suppose Trudy is a womaninthemiddle and has the ability to insert delete and replace segments in the stream of TCP segments sent between Alice and Bob
Trudy for example could capture two segments sent by Bob reverse the order of the segments adjust the TCP sequence numbers which are not encrypted and then send the two reverseordered segments to Alice
Assuming that each TCP segment encapsulates exactly one record lets now take a look at how Alice would process these segments
TCP running in Alice would think everything is fine and pass the two records to the SSL sublayer
SSL in Alice would decrypt the two records
SSL in Alice would use the MAC in each record to verify the data integrity of the two records
SSL would then pass the decrypted byte streams of the two records to the application layer but the complete byte stream received by Alice would not be in the correct order due to reversal of the records You are encouraged to walk through similar scenarios for when Trudy removes segments or when Trudy replays segments
The solution to this problem as you probably guessed is to use sequence numbers
SSL does this as follows
Bob maintains a sequence number counter which begins at zero and is incremented for each SSL record he sends
Bob doesnt actually include a sequence number in the record itself but when he calculates the MAC he includes the sequence number in the MAC calculation
Thus the MAC is now a hash of the data plus the MAC key MB plus the current sequence number
Alice tracks Bobs sequence numbers allowing her to verify the data integrity of a record by including the appropriate sequence number in the MAC calculation
This use of SSL sequence numbers prevents Trudy from carrying out a womaninthemiddle attack such as reordering or replaying segments
Why SSL Record The SSL record as well as the almostSSL record is shown in Figure 
The record consists of a type field version field length field data field and MAC field
Note that the first three fields are not encrypted
The type field indicates whether the record is a handshake message or a message that contains application data
It is also used to close the SSL connection as discussed below
SSL at the receiving end uses the length field to extract the SSL records out of the incoming TCP byte stream
The version field is selfexplanatory
A More Complete Picture The previous subsection covered the almostSSL protocol it served to give us a basic understanding of the why and how of SSL
Now that we have a basic understanding of SSL we can dig a little deeper and examine the essentials of the actual SSL protocol
In parallel to reading this description of the SSL protocol you are encouraged to complete the Wireshark SSL lab available at the textbooks Web site
Record format for SSL SSL Handshake SSL does not mandate that Alice and Bob use a specific symmetric key algorithm a specific publickey algorithm or a specific MAC
Instead SSL allows Alice and Bob to agree on the cryptographic algorithms at the beginning of the SSL session during the handshake phase
Additionally during the handshake phase Alice and Bob send nonces to each other which are used in the creation of the session keys EB MB EA and MA
The steps of the real SSL handshake are as follows 
The client sends a list of cryptographic algorithms it supports along with a client nonce
From the list the server chooses a symmetric algorithm for example AES a public key algorithm for example RSA with a specific key length and a MAC algorithm
It sends back to the client its choices as well as a certificate and a server nonce
The client verifies the certificate extracts the servers public key generates a PreMaster Secret PMS encrypts the PMS with the servers public key and sends the encrypted PMS to the server
Using the same key derivation function as specified by the SSL standard the client and server independently compute the Master Secret MS from the PMS and nonces
The MS is then sliced up to generate the two encryption and two MAC keys
Furthermore when the chosen symmetric cipher employs CBC such as DES or AES then two Initialization Vectors IVs one for each side of the connectionare also obtained from the MS
Henceforth all messages sent between client and server are encrypted and authenticated with the MAC
The client sends a MAC of all the handshake messages
The server sends a MAC of all the handshake messages
The last two steps protect the handshake from tampering
To see this observe that in step the client typically offers a list of algorithmssome strong some weak
This list of algorithms is sent in cleartext since the encryption algorithms and keys have not yet been agreed upon
Trudy as a womaninthe middle could delete the stronger algorithms from the list forcing the client to select a weak algorithm
To prevent such a tampering attack in step the client sends a MAC of the concatenation of all the handshake messages it sent and received
The server can compare this MAC with the MAC of the handshake messages it received and sent
If there is an inconsistency the server can terminate the connection
Similarly the server sends a MAC of the handshake messages it has seen allowing the client to check for inconsistencies
You may be wondering why there are nonces in steps and 
Dont sequence numbers suffice for preventing the segment replay attack The answer is yes but they dont alone prevent the connection replay attack
Consider the following connection replay attack
Suppose Trudy sniffs all messages between Alice and Bob
The next day Trudy masquerades as Bob and sends to Alice exactly the same sequence of messages that Bob sent to Alice on the previous day
If Alice doesnt use nonces she will respond with exactly the same sequence of messages she sent the previous day
Alice will not suspect any funny business as each message she receives will pass the integrity check
If Alice is an e commerce server she will think that Bob is placing a second order for exactly the same thing
On the other hand by including a nonce in the protocol Alice will send different nonces for each TCP session causing the encryption keys to be different on the two days
Therefore when Alice receives playedback SSL records from Trudy the records will fail the integrity checks and the bogus ecommerce transaction will not succeed
In summary in SSL nonces are used to defend against the connection replay attack and sequence numbers are used to defend against replaying individual packets during an ongoing session
Connection Closure At some point either Bob or Alice will want to end the SSL session
One approach would be to let Bob end the SSL session by simply terminating the underlying TCP connectionthat is by having Bob send a TCP FIN segment to Alice
But such a naive design sets the stage for the truncation attack whereby Trudy once again gets in the middle of an ongoing SSL session and ends the session early with a TCP FIN
If Trudy were to do this Alice would think she received all of Bobs data when actuality she only received a portion of it
The solution to this problem is to indicate in the type field whether the record serves to terminate the SSL session
Although the SSL type is sent in the clear it is authenticated at the receiver using the records MAC
By including such a field if Alice were to receive a TCP FIN before receiving a closure SSL record she would know that something funny was going on
This completes our introduction to SSL
Weve seen that it uses many of the cryptography principles discussed in Sections 
Readers who want to explore SSL on yet a deeper level can read Rescorlas highly readable book on SSL Rescorla 
NetworkLayer Security IPsec and Virtual Private Networks The IP security protocol more commonly known as IPsec provides security at the network layer
IPsec secures IP datagrams between any two networklayer entities including hosts and routers
As we will soon describe many institutions corporations government branches nonprofit organizations and so on use IPsec to create virtual private networks VPNs that run over the public Internet
Before getting into the specifics of IPsec lets step back and consider what it means to provide confidentiality at the network layer
With networklayer confidentiality between a pair of network entities for example between two routers between two hosts or between a router and a host the sending entity encrypts the payloads of all the datagrams it sends to the receiving entity
The encrypted payload could be a TCP segment a UDP segment an ICMP message and so on
If such a networklayer service were in place all data sent from one entity to the otherincluding email Web pages TCP handshake messages and management messages such as ICMP and SNMPwould be hidden from any third party that might be sniffing the network
For this reason networklayer security is said to provide blanket coverage
In addition to confidentiality a networklayer security protocol could potentially provide other security services
For example it could provide source authentication so that the receiving entity can verify the source of the secured datagram
A networklayer security protocol could provide data integrity so that the receiving entity can check for any tampering of the datagram that may have occurred while the datagram was in transit
A networklayer security service could also provide replayattack prevention meaning that Bob could detect any duplicate datagrams that an attacker might insert
We will soon see that IPsec indeed provides mechanisms for all these security services that is for confidentiality source authentication data integrity and replayattack prevention
IPsec and Virtual Private Networks VPNs An institution that extends over multiple geographical regions often desires its own IP network so that its hosts and servers can send data to each other in a secure and confidential manner
To achieve this goal the institution could actually deploy a standalone physical networkincluding routers links and a DNS infrastructurethat is completely separate from the public Internet
Such a disjoint network dedicated to a particular institution is called a private network
Not surprisingly a private network can be very costly as the institution needs to purchase install and maintain its own physical network infrastructure
Instead of deploying and maintaining a private network many institutions today create VPNs over the existing public Internet
With a VPN the institutions interoffice traffic is sent over the public Internet rather than over a physically independent network
But to provide confidentiality the interoffice traffic is encrypted before it enters the public Internet
A simple example of a VPN is shown in Figure 
Here the institution consists of a headquarters a branch office and traveling salespersons that typically access the Internet from their hotel rooms
There is only one salesperson shown in the figure
In this VPN whenever two hosts within headquarters send IP datagrams to each other or whenever two hosts within the branch office want to communicate they use goodold vanilla IPv that is without IPsec services
However when two of the institutions hosts Figure 
Virtual private network VPN communicate over a path that traverses the public Internet the traffic is encrypted before it enters the Internet
To get a feel for how a VPN works lets walk through a simple example in the context of Figure 
When a host in headquarters sends an IP datagram to a salesperson in a hotel the gateway router in headquarters converts the vanilla IPv datagram into an IPsec datagram and then forwards this IPsec datagram into the Internet
This IPsec datagram actually has a traditional IPv header so that the routers in the public Internet process the datagram as if it were an ordinary IPv datagramto them the datagram is a perfectly ordinary datagram
But as shown Figure 
the payload of the IPsec datagram includes an IPsec header which is used for IPsec processing furthermore the payload of the IPsec datagram is encrypted
When the IPsec datagram arrives at the salespersons laptop the OS in the laptop decrypts the payload and provides other security services such as verifying data integrity and passes the unencrypted payload to the upperlayer protocol for example to TCP or UDP
We have just given a highlevel overview of how an institution can employ IPsec to create a VPN
To see the forest through the trees we have brushed aside many important details
Lets now take a closer look
The AH and ESP Protocols IPsec is a rather complex animalit is defined in more than a dozen RFCs
Two important RFCs are RFC which describes the overall IP security architecture and RFC which provides an overview of the IPsec protocol suite
Our goal in this textbook as usual is not simply to rehash the dry and arcane RFCs but instead take a more operational and pedagogic approach to describing the protocols
In the IPsec protocol suite there are two principal protocols the Authentication Header AH protocol and the Encapsulation Security Payload ESP protocol
When a source IPsec entity typically a host or a router sends secure datagrams to a destination entity also a host or a router it does so with either the AH protocol or the ESP protocol
The AH protocol provides source authentication and data integrity but does not provide confidentiality
The ESP protocol provides source authentication data integrity and confidentiality
Because confidentiality is often critical for VPNs and other IPsec applications the ESP protocol is much more widely used than the AH protocol
In order to demystify IPsec and avoid much of its complication we will henceforth focus exclusively on the ESP protocol
Readers wanting to learn also about the AH protocol are encouraged to explore the RFCs and other online resources
Security Associations IPsec datagrams are sent between pairs of network entities such as between two hosts between two routers or between a host and router
Before sending IPsec datagrams from source entity to destination entity the source and destination entities create a networklayer logical connection
This logical connection is called a security association SA
An SA is a simplex logical connection that is it is unidirectional from source to destination
If both entities want to send secure datagrams to each other then two SAs that is two logical connections need to be established one in each direction
For example consider once again the institutional VPN in Figure 
This institution consists of a headquarters office a branch office and say n traveling salespersons
For the sake of example lets suppose that there is bidirectional IPsec traffic between headquarters and the branch office and bi directional IPsec traffic between headquarters and the salespersons
In this VPN how many SAs are there To answer this question note that there are two SAs between the headquarters gateway router and the branchoffice gateway router one in each direction for each salespersons laptop there are two SAs between the headquarters gateway router and the laptop again one in each direction
So in total there are n SAs
Keep in mind however that not all traffic sent into the Internet by the gateway routers or by the laptops will be IPsec secured
For example a host in headquarters may want to access a Web server such as Amazon or Google in the public Internet
Thus the gateway router and the laptops will emit into the Internet both vanilla IPv datagrams and secured IPsec datagrams
Security association SA from R to R Lets now take a look inside an SA
To make the discussion tangible and concrete lets do this in the context of an SA from router R to router R in Figure 
You can think of Router R as the headquarters gateway router and Router R as the branch office gateway router from Figure 
Router R will maintain state information about this SA which will include A bit identifier for the SA called the Security Parameter Index SPI The origin interface of the SA in this case 
and the destination interface of the SA in this case 
The type of encryption to be used for example DES with CBC The encryption key The type of integrity check for example HMAC with MD The authentication key Whenever router R needs to construct an IPsec datagram for forwarding over this SA it accesses this state information to determine how it should authenticate and encrypt the datagram
Similarly router R will maintain the same state information for this SA and will use this information to authenticate and decrypt any IPsec datagram that arrives from the SA
An IPsec entity router or host often maintains state information for many SAs
For example in the VPN example in Figure 
with n salespersons the headquarters gateway router maintains state information for n SAs
An IPsec entity stores the state information for all of its SAs in its Security Association Database SAD which is a data structure in the entitys OS kernel
The IPsec Datagram Having now described SAs we can now describe the actual IPsec datagram
IPsec has two different packet forms one for the socalled tunnel mode and the other for the socalled transport mode
The tunnel mode being more appropriate for VPNs Figure 
IPsec datagram format is more widely deployed than the transport mode
In order to further demystify IPsec and avoid much of its complication we henceforth focus exclusively on the tunnel mode
Once you have a solid grip on the tunnel mode you should be able to easily learn about the transport mode on your own
The packet format of the IPsec datagram is shown in Figure 
You might think that packet formats are boring and insipid but we will soon see that the IPsec datagram actually looks and tastes like a popular TexMex delicacy Lets examine the IPsec fields in the context of Figure 
Suppose router R receives an ordinary IPv datagram from host 
in the headquarters network which is destined to host 
in the branchoffice network
Router R uses the following recipe to convert this original IPv datagram into an IPsec datagram Appends to the back of the original IPv datagram which includes the original header fields an ESP trailer field Encrypts the result using the algorithm and key specified by the SA Appends to the front of this encrypted quantity a field called ESP header the resulting package is called the enchilada Creates an authentication MAC over the whole enchilada using the algorithm and key specified in the SA Appends the MAC to the back of the enchilada forming the payload Finally creates a brand new IP header with all the classic IPv header fields together normally bytes long which it appends before the payload Note that the resulting IPsec datagram is a bona fide IPv datagram with the traditional IPv header fields followed by a payload
But in this case the payload contains an ESP header the original IP datagram an ESP trailer and an ESP authentication field with the original datagram and ESP trailer encrypted
The original IP datagram has 
for the source IP address and 
for the destination IP address
Because the IPsec datagram includes the original IP datagram these addresses are included and encrypted as part of the payload of the IPsec packet
But what about the source and destination IP addresses that are in the new IP header that is in the leftmost header of the IPsec datagram As you might expect they are set to the source and destination router interfaces at the two ends of the tunnels namely 
Also the protocol number in this new IPv header field is not set to that of TCP UDP or SMTP but instead to designating that this is an IPsec datagram using the ESP protocol
After R sends the IPsec datagram into the public Internet it will pass through many routers before reaching R
Each of these routers will process the datagram as if it were an ordinary datagramthey are completely oblivious to the fact that the datagram is carrying IPsecencrypted data
For these public Internet routers because the destination IP address in the outer header is R the ultimate destination of the datagram is R
Having walked through an example of how an IPsec datagram is constructed lets now take a closer look at the ingredients in the enchilada
We see in Figure 
that the ESP trailer consists of three fields padding pad length and next header
Recall that block ciphers require the message to be encrypted to be an integer multiple of the block length
Padding consisting of meaningless bytes is used so that when added to the original datagram along with the pad length and next header fields the resulting message is an integer number of blocks
The padlength field indicates to the receiving entity how much padding was inserted and thus needs to be removed
The next header identifies the type e.g
UDP of data contained in the payloaddata field
The payload data typically the original IP datagram and the ESP trailer are concatenated and then encrypted
Appended to the front of this encrypted unit is the ESP header which is sent in the clear and consists of two fields the SPI and the sequence number field
The SPI indicates to the receiving entity the SA to which the datagram belongs the receiving entity can then index its SAD with the SPI to determine the appropriate authenticationdecryption algorithms and keys
The sequence number field is used to defend against replay attacks
The sending entity also appends an authentication MAC
As stated earlier the sending entity calculates a MAC over the whole enchilada consisting of the ESP header the original IP datagram and the ESP trailerwith the datagram and trailer being encrypted
Recall that to calculate a MAC the sender appends a secret MAC key to the enchilada and then calculates a fixedlength hash of the result
When R receives the IPsec datagram R observes that the destination IP address of the datagram is R itself
R therefore processes the datagram
Because the protocol field in the leftmost IP header is R sees that it should apply IPsec ESP processing to the datagram
First peering into the enchilada R uses the SPI to determine to which SA the datagram belongs
Second it calculates the MAC of the enchilada and verifies that the MAC is consistent with the value in the ESP MAC field
If it is it knows that the enchilada comes from R and has not been tampered with
Third it checks the sequencenumber field to verify that the datagram is fresh and not a replayed datagram
Fourth it decrypts the encrypted unit using the decryption algorithm and key associated with the SA
Fifth it removes padding and extracts the original vanilla IP datagram
And finally sixth it forwards the original datagram into the branch office network toward its ultimate destination
Whew what a complicated recipe huh Well no one ever said that preparing and unraveling an enchilada was easy There is actually another important subtlety that needs to be addressed
It centers on the following question When R receives an unsecured datagram from a host in the headquarters network and that datagram is destined to some destination IP address outside of headquarters how does R know whether it should be converted to an IPsec datagram And if it is to be processed by IPsec how does R know which SA of many SAs in its SAD should be used to construct the IPsec datagram The problem is solved as follows
Along with a SAD the IPsec entity also maintains another data structure called the Security Policy Database SPD
The SPD indicates what types of datagrams as a function of source IP address destination IP address and protocol type are to be IPsec processed and for those that are to be IPsec processed which SA should be used
In a sense the information in a SPD indicates what to do with an arriving datagram the information in the SAD indicates how to do it
Summary of IPsec Services So what services does IPsec provide exactly Let us examine these services from the perspective of an attacker say Trudy who is a womaninthemiddle sitting somewhere on the path between R and R in Figure 
Assume throughout this discussion that Trudy does not know the authentication and encryption keys used by the SA
What can and cannot Trudy do First Trudy cannot see the original datagram
If fact not only is the data in the original datagram hidden from Trudy but so is the protocol number the source IP address and the destination IP address
For datagrams sent over the SA Trudy only knows that the datagram originated from some host in 
and is destined to some host in 
She does not know if it is carrying TCP UDP or ICMP data she does not know if it is carrying HTTP SMTP or some other type of application data
This confidentiality thus goes a lot farther than SSL
Second suppose Trudy tries to tamper with a datagram in the SA by flipping some of its bits
When this tampered datagram arrives at R it will fail the integrity check using the MAC thwarting Trudys vicious attempts once again
Third suppose Trudy tries to masquerade as R creating a IPsec datagram with source 
and destination 
Trudys attack will be futile as this datagram will again fail the integrity check at R
Finally because IPsec includes sequence numbers Trudy will not be able create a successful replay attack
In summary as claimed at the beginning of this section IPsec providesbetween any pair of devices that process packets through the network layer confidentiality source authentication data integrity and replayattack prevention
IKE Key Management in IPsec When a VPN has a small number of end points for example just two routers as in Figure 
the network administrator can manually enter the SA information encryptionauthentication algorithms and keys and the SPIs into the SADs of the endpoints
Such manual keying is clearly impractical for a large VPN which may consist of hundreds or even thousands of IPsec routers and hosts
Large geographically distributed deployments require an automated mechanism for creating the SAs
IPsec does this with the Internet Key Exchange IKE protocol specified in RFC 
IKE has some similarities with the handshake in SSL see Section 
Each IPsec entity has a certificate which includes the entitys public key
As with SSL the IKE protocol has the two entities exchange certificates negotiate authentication and encryption algorithms and securely exchange key material for creating session keys in the IPsec SAs
Unlike SSL IKE employs two phases to carry out these tasks
Lets investigate these two phases in the context of two routers R and R in Figure 
The first phase consists of two exchanges of message pairs between R and R During the first exchange of messages the two sides use DiffieHellman see Homework Problems to create a bidirectional IKE SA between the routers
To keep us all confused this bidirectional IKE SA is entirely different from the IPsec SAs discussed in Sections 
The IKE SA provides an authenticated and encrypted channel between the two routers
During this first messagepair exchange keys are established for encryption and authentication for the IKE SA
Also established is a master secret that will be used to compute IPSec SA keys later in phase 
Observe that during this first step RSA public and private keys are not used
In particular neither R nor R reveals its identity by signing a message with its private key
During the second exchange of messages both sides reveal their identity to each other by signing their messages
However the identities are not revealed to a passive sniffer since the messages are sent over the secured IKE SA channel
Also during this phase the two sides negotiate the IPsec encryption and authentication algorithms to be employed by the IPsec SAs
In phase of IKE the two sides create an SA in each direction
At the end of phase the encryption and authentication session keys are established on both sides for the two SAs
The two sides can then use the SAs to send secured datagrams as described in Sections 
The primary motivation for having two phases in IKE is computational costsince the second phase doesnt involve any publickey cryptography IKE can generate a large number of SAs between the two IPsec entities with relatively little computational cost
Securing Wireless LANs Security is a particularly important concern in wireless networks where radio waves carrying frames can propagate far beyond the building containing the wireless base station and hosts
In this section we present a brief introduction to wireless security
For a more indepth treatment see the highly readable book by Edney and Arbaugh Edney 
The issue of security in 
has attracted considerable attention in both technical circles and in the media
While there has been considerable discussion there has been little debatethere seems to be universal agreement that the original 
specification contains a number of serious security flaws
Indeed public domain software can now be downloaded that exploits these holes making those who use the vanilla 
security mechanisms as open to security attacks as users who use no security features at all
In the following section we discuss the security mechanisms initially standardized in the 
specification known collectively as Wired Equivalent Privacy WEP
As the name suggests WEP is meant to provide a level of security similar to that found in wired networks
Well then discuss a few of the security holes in WEP and discuss the .i standard a fundamentally more secure version of 
adopted in 
Wired Equivalent Privacy WEP The IEEE 
WEP protocol was designed in to provide authentication and data encryption between a host and a wireless access point that is base station using a symmetric shared key approach
WEP does not specify a key management algorithm so it is assumed that the host and wireless access point have somehow agreed on the key via an outofband method
Authentication is carried out as follows 
A wireless host requests authentication by an access point
The access point responds to the authentication request with a byte nonce value
The wireless host encrypts the nonce using the symmetric key that it shares with the access point
The access point decrypts the hostencrypted nonce
If the decrypted nonce matches the nonce value originally sent to the host then the host is authenticated by the access point
The WEP data encryption algorithm is illustrated in Figure 
A secret bit symmetric key KS is assumed to be known by both a host and the access point
In addition a bit Initialization Vector IV is appended to the bit key to create a bit key that will be used to encrypt a single frame
The IV will Figure 
WEP protocol change from one frame to another and hence each frame will be encrypted with a different bit key
Encryption is performed as follows
First a byte CRC value see Section 
is computed for the data payload
The payload and the four CRC bytes are then encrypted using the RC stream cipher
We will not cover the details of RC here see Schneier and Edney for details
For our purposes it is enough to know that when presented with a key value in this case the bit KS IV key the RC algorithm produces a stream of key values kIVkIVkIV that are used to encrypt the data and CRC value in a frame
For practical purposes we can think of these operations being performed a byte at a time
Encryption is performed by XORing the ith byte of data di with the ith key kiIV in the stream of key values generated by the KS IV pair to produce the ith byte of ciphertext ci cidikiIV The IV value changes from one frame to the next and is included in plaintext in the header of each WEPencrypted 
frame as shown in Figure 
The receiver takes the secret bit symmetric key that it shares with the sender appends the IV and uses the resulting bit key which is identical to the key used by the sender to perform encryption to decrypt the frame dicikiIV Proper use of the RC algorithm requires that the same bit key value never be used more than once
Recall that the WEP key changes on a framebyframe basis
For a given KS which changes rarely if ever this means that there are only unique keys
If these keys are chosen randomly we can show Edney that the probability of having chosen the same IV value and hence used the same bit key is more than percent after only frames
With Kbyte frame sizes and a data transmission rate of Mbps only a few seconds are needed before frames are transmitted
Furthermore since the IV is transmitted in plaintext in the frame an eavesdropper will know whenever a duplicate IV value is used
To see one of the several problems that occur when a duplicate key is used consider the following chosenplaintext attack taken by Trudy against Alice
Suppose that Trudy possibly using IP spoofing sends a request for example an HTTP or FTP request to Alice to transmit a file with known content d d d d
Trudy also observes the encrypted data c c c c
Since dicikiIV if we XOR ci with each side of this equality we have dicikiIV With this relationship Trudy can use the known values of di and ci to compute kiIV
The next time Trudy sees the same value of IV being used she will know the key sequence kIVkIVkIV and will thus be able to decrypt the encrypted message
There are several additional security concerns with WEP as well
Fluhrer described an attack exploiting a known weakness in RC when certain weak keys are chosen
Stubblefield discusses efficient ways to implement and exploit this attack
Another concern with WEP involves the CRC bits shown in Figure 
and transmitted in the 
frame to detect altered bits in the payload
However an attacker who changes the encrypted content e.g
substituting gibberish for the original encrypted data computes a CRC over the substituted gibberish and places the CRC into a WEP frame can produce an 
frame that will be accepted by the receiver
What is needed here are message integrity techniques such as those we studied in Section 
to detect content tampering or substitution
For more details of WEP security see Edney Wright and the references therein
IEEE .i Soon after the release of IEEE 
work began on developing a new and improved version of 
with stronger security mechanisms
The new standard known as .i underwent final ratification in 
As well see while WEP provided relatively weak encryption only a single way to perform authentication and no key distribution mechanisms IEEE .i provides for much stronger forms of encryption an extensible set of authentication mechanisms and a key distribution mechanism
In the following we present an overview of .i an excellent streaming audio technical overview of .i is TechOnline 
overviews the .i framework
In addition to the wireless client and access point .i defines an authentication server with which the AP can communicate
Separating the authentication server from the AP allows one authentication server to serve many APs centralizing the often sensitive decisions Figure 
.i Four phases of operation regarding authentication and access within the single server and keeping AP costs and complexity low
.i operates in four phases 
In the discovery phase the AP advertises its presence and the forms of authentication and encryption that can be provided to the wireless client node
The client then requests the specific forms of authentication and encryption that it desires
Although the client and AP are already exchanging messages the client has not yet been authenticated nor does it have an encryption key and so several more steps will be required before the client can communicate with an arbitrary remote host over the wireless channel
Mutual authentication and Master Key MK generation
Authentication takes place between the wireless client and the authentication server
In this phase the access point acts essentially as a relay forwarding messages between the client and the authentication server
The Extensible Authentication Protocol EAP RFC defines the endtoend message formats used in a simple requestresponse mode of interaction between the client and authentication server
As shown in Figure 
EAP messages are encapsulated using EAPoL EAP over LAN IEEE .X and sent over the 
These EAP messages are then decapsulated at the access point and then reencapsulated using the RADIUS protocol for transmission over UDPIP to the authentication server
While Figure 
EAP is an endtoend protocol
EAP messages are encapsulated using EAPoL over the wireless link between the client and the access point and using RADIUS over UDPIP between the access point and the authentication server the RADIUS server and protocol RFC are not required by the .i protocol they are de facto standard components for .i
The recently standardized DIAMETER protocol RFC is likely to replace RADIUS in the near future
With EAP the authentication server can choose one of a number of ways to perform authentication
While .i does not mandate a particular authentication method the EAP TLS authentication scheme RFC is often used
EAPTLS uses public key techniques including nonce encryption and message digests similar to those we studied in Section 
to allow the client and the authentication server to mutually authenticate each other and to derive a Master Key MK that is known to both parties
Pairwise Master Key PMK generation
The MK is a shared secret known only to the client and the authentication server which they each use to generate a second key the Pairwise Master Key PMK
The authentication server then sends the PMK to the AP
This is where we wanted to be The client and AP now have a shared key recall that in WEP the problem of key distribution was not addressed at all and have mutually authenticated each other
Theyre just about ready to get down to business
Temporal Key TK generation
With the PMK the wireless client and AP can now generate additional keys that will be used for communication
Of particular interest is the Temporal Key TK which will be used to perform the linklevel encryption of data sent over the wireless link and to an arbitrary remote host
.i provides several forms of encryption including an AESbased encryption scheme and a strengthened version of WEP encryption
Operational Security Firewalls and Intrusion Detection Systems Weve seen throughout this chapter that the Internet is not a very safe placebad guys are out there wreaking all sorts of havoc
Given the hostile nature of the Internet lets now consider an organizations network and the network administrator who administers it
From a network administrators point of view the world divides quite neatly into two campsthe good guys who belong to the organizations network and who should be able to access resources inside the organizations network in a relatively unconstrained manner and the bad guys everyone else whose access to network resources must be carefully scrutinized
In many organizations ranging from medieval castles to modern corporate office buildings there is a single point of entryexit where both good guys and bad guys entering and leaving the organization are securitychecked
In a castle this was done at a gate at one end of the drawbridge in a corporate building this is done at the security desk
In a computer network when traffic enteringleaving a network is securitychecked logged dropped or forwarded it is done by operational devices known as firewalls intrusion detection systems IDSs and intrusion prevention systems IPSs
Firewalls A firewall is a combination of hardware and software that isolates an organizations internal network from the Internet at large allowing some packets to pass and blocking others
A firewall allows a network administrator to control access between the outside world and resources within the administered network by managing the traffic flow to and from these resources
A firewall has three goals All traffic from outside to inside and vice versa passes through the firewall
shows a firewall sitting squarely at the boundary between the administered network and the rest of the Internet
While large organizations may use multiple levels of firewalls or distributed firewalls Skoudis locating a firewall at a single access point to the network as shown in Figure 
makes it easier to manage and enforce a securityaccess policy
Only authorized traffic as defined by the local security policy will be allowed to pass
With all traffic entering and leaving the institutional network passing through the firewall the firewall can restrict access to authorized traffic
The firewall itself is immune to penetration
The firewall itself is a device connected to the network
If not designed or installed properly it can be compromised in which case it provides only a false sense of security which is worse than no firewall at all
Firewall placement between the administered network and the outside world Cisco and Check Point are two of the leading firewall vendors today
You can also easily create a firewall packet filter from a Linux box using iptables publicdomain software that is normally shipped with Linux
Furthermore as discussed in Chapters and firewalls are now frequently implemented in routers and controlled remotely using SDNs
Firewalls can be classified in three categories traditional packet filters stateful filters and application gateways
Well cover each of these in turn in the following subsections
Traditional Packet Filters As shown in Figure 
an organization typically has a gateway router connecting its internal network to its ISP and hence to the larger public Internet
All traffic leaving and entering the internal network passes through this router and it is at this router where packet filtering occurs
A packet filter examines each datagram in isolation determining whether the datagram should be allowed to pass or should be dropped based on administratorspecific rules
Filtering decisions are typically based on IP source or destination address Protocol type in IP datagram field TCP UDP ICMP OSPF and so on TCP or UDP source and destination port Table 
Policies and corresponding filtering rules for an organizations network 
with Web server at 
Policy Firewall Setting No outside Web access
Drop all outgoing packets to any IP address port 
No incoming TCP connections except those for Drop all incoming TCP SYN packets to any organizations public Web server only
IP except 
Prevent Webradios from eating up the Drop all incoming UDP packetsexcept DNS available bandwidth
Prevent your network from being used for a Drop all ICMP ping packets going to a smurf DoS attack
broadcast address eg 
Prevent your network from being tracerouted
Drop all outgoing ICMP TTL expired traffic
TCP flag bits SYN ACK and so on ICMP message type Different rules for datagrams leaving and entering the network Different rules for the different router interfaces A network administrator configures the firewall based on the policy of the organization
The policy may take user productivity and bandwidth usage into account as well as the security concerns of an organization
lists a number of possible polices an organization may have and how they would be addressed with a packet filter
For example if the organization doesnt want any incoming TCP connections except those for its public Web server it can block all incoming TCP SYN segments except TCP SYN segments with destination port and the destination IP address corresponding to the Web server
If the organization doesnt want its users to monopolize access bandwidth with Internet radio applications it can block all notcritical UDP traffic since Internet radio is often sent over UDP
If the organization doesnt want its internal network to be mapped tracerouted by an outsider it can block all ICMP TTL expired messages leaving the organizations network
A filtering policy can be based on a combination of addresses and port numbers
For example a filtering router could forward all Telnet datagrams those with a port number of except those going to and coming from a list of specific IP addresses
This policy permits Telnet connections to and from hosts on the allowed list
Unfortunately basing the policy on external addresses provides no protection against datagrams that have had their source addresses spoofed
Filtering can also be based on whether or not the TCP ACK bit is set
This trick is quite useful if an organization wants to let its internal clients connect to external servers but wants to prevent external clients from connecting to internal servers
An access control list for a router interface action source address dest address protocol source dest flag port port bit allow 
outside of TCP any 
allow outside of 
TCP ACK 
outside of UDP 
allow outside of 
deny all all all all all all Recall from Section 
that the first segment in every TCP connection has the ACK bit set to whereas all the other segments in the connection have the ACK bit set to 
Thus if an organization wants to prevent external clients from initiating connections to internal servers it simply filters all incoming segments with the ACK bit set to 
This policy kills all TCP connections originating from the outside but permits connections originating internally
Firewall rules are implemented in routers with access control lists with each router interface having its own list
An example of an access control list for an organization 
is shown in Table 
This access control list is for an interface that connects the router to the organizations external ISPs
Rules are applied to each datagram that passes through the interface from top to bottom
The first two rules together allow internal users to surf the Web The first rule allows any TCP packet with destination port to leave the organizations network the second rule allows any TCP packet with source port and the ACK bit set to enter the organizations network
Note that if an external source attempts to establish a TCP connection with an internal host the connection will be blocked even if the source or destination port is 
The second two rules together allow DNS packets to enter and leave the organizations network
In summary this rather restrictive access control list blocks all traffic except Web traffic initiated from within the organization and DNS traffic
CERT Filtering provides a list of recommended portprotocol packet filterings to avoid a number of wellknown security holes in existing network applications
Stateful Packet Filters In a traditional packet filter filtering decisions are made on each packet in isolation
Stateful filters actually track TCP connections and use this knowledge to make filtering decisions
Connection table for stateful filter source address dest address source port dest port 
To understand stateful filters lets reexamine the access control list in Table 
Although rather restrictive the access control list in Table 
nevertheless allows any packet arriving from the outside with ACK and source port to get through the filter
Such packets could be used by attackers in attempts to crash internal systems with malformed packets carry out denialofservice attacks or map the internal network
The naive solution is to block TCP ACK packets as well but such an approach would prevent the organizations internal users from surfing the Web
Stateful filters solve this problem by tracking all ongoing TCP connections in a connection table
This is possible because the firewall can observe the beginning of a new connection by observing a threeway handshake SYN SYNACK and ACK and it can observe the end of a connection when it sees a FIN packet for the connection
The firewall can also conservatively assume that the connection is over when it hasnt seen any activity over the connection for say seconds
An example connection table for a firewall is shown in Table 
This connection table indicates that there are currently three ongoing TCP connections all of which have been initiated from within the organization
Additionally the stateful filter includes a new column check connection in its access control list as shown in Table 
Note that Table 
is identical to the access control list in Table 
except now it indicates that the connection should be checked for two of the rules
Lets walk through some examples to see how the connection table and the extended access control list work handinhand
Suppose an attacker attempts to send a malformed packet into the organizations network by sending a datagram with TCP source port and with the ACK flag set
Further suppose that this packet has source port number and source IP address 
When this packet reaches the firewall the firewall checks the access control list in Table 
which indicates that the connection table must also be checked before permitting this packet to enter the organizations network
The firewall duly checks the connection table sees that this packet is not part of an ongoing TCP connection and rejects the packet
As a second example suppose that an internal user wants to surf an external Web site
Because this user first sends a TCP SYN segment the users TCP connection gets recorded in the connection table
When Table 
Access control list for stateful filter action source address dest address protocol source dest flag check port port bit conxion allow 
outside of TCP any 
allow outside of 
TCP ACK X 
outside of UDP 
allow outside of 
UDP X 
deny all all all all all all the Web server sends back packets with the ACK bit necessarily set the firewall checks the table and sees that a corresponding connection is in progress
The firewall will thus let these packets pass thereby not interfering with the internal users Web surfing activity
Application Gateway In the examples above we have seen that packetlevel filtering allows an organization to perform coarsegrain filtering on the basis of the contents of IP and TCPUDP headers including IP addresses port numbers and acknowledgment bits
But what if an organization wants to provide a Telnet service to a restricted set of internal users as opposed to IP addresses And what if the organization wants such privileged users to authenticate themselves first before being allowed to create Telnet sessions to the outside world Such tasks are beyond the capabilities of traditional and stateful filters
Indeed information about the identity of the internal users is applicationlayer data and is not included in the IPTCPUDP headers
To have finerlevel security firewalls must combine packet filters with application gateways
Application gateways look beyond the IPTCPUDP headers and make policy decisions based on application data
An application gateway is an applicationspecific server through which all application data inbound and outbound must pass
Multiple application gateways can run on the same host but each gateway is a separate server with its own processes
To get some insight into application gateways lets design a firewall that allows only a restricted set of internal users to Telnet outside and prevents all external clients from Telneting inside
Such a policy can be accomplished by implementing Figure 
Firewall consisting of an application gateway and a filter a combination of a packet filter in a router and a Telnet application gateway as shown in Figure 
The routers filter is configured to block all Telnet connections except those that originate from the IP address of the application gateway
Such a filter configuration forces all outbound Telnet connections to pass through the application gateway
Consider now an internal user who wants to Telnet to the outside world
The user must first set up a Telnet session with the application gateway
An application running in the gateway which listens for incoming Telnet sessions prompts the user for a user ID and password
When the user supplies this information the application gateway checks to see if the user has permission to Telnet to the outside world
If not the Telnet connection from the internal user to the gateway is terminated by the gateway
If the user has permission then the gateway prompts the user for the host name of the external host to which the user wants to connect sets up a Telnet session between the gateway and the external host and relays to the external host all data arriving from the user and relays to the user all data arriving from the external host
Thus the Telnet application gateway not only performs user authorization but also acts as a Telnet server and a Telnet client relaying information between the user and the remote Telnet server
Note that the filter will permit step because the gateway initiates the Telnet connection to the outside world
CASE HISTORY ANONYMITY AND PRIVACY Suppose you want to visit a controversial Web site for example a political activist site and you dont want to reveal your IP address to the Web site dont want your local ISP which may be your home or office ISP to know that you are visiting the site and dont want your local ISP to see the data you are exchanging with the site
If you use the traditional approach of connecting directly to the Web site without any encryption you fail on all three counts
Even if you use SSL you fail on the first two counts Your source IP address is presented to the Web site in every datagram you send and the destination address of every packet you send can easily be sniffed by your local ISP
To obtain privacy and anonymity you can instead use a combination of a trusted proxy server and SSL as shown in Figure 
With this approach you first make an SSL connection to the trusted proxy
You then send into this SSL connection an HTTP request for a page at the desired site
When the proxy receives the SSLencrypted HTTP request it decrypts the request and forwards the cleartext HTTP request to the Web site
The Web site then responds to the proxy which in turn forwards the response to you over SSL
Because the Web site only sees the IP address of the proxy and not of your clients address you are indeed obtaining anonymous access to the Web site
And because all traffic between you and the proxy is encrypted your local ISP cannot invade your privacy by logging the site you visited or recording the data you are exchanging
Many companies today such as proxify .com make available such proxy services
Of course in this solution your proxy knows everything It knows your IP address and the IP address of the site youre surfing and it can see all the traffic in cleartext exchanged between you and the Web site
Such a solution therefore is only as good as the trustworthiness of the proxy
A more robust approach taken by the TOR anonymizing and privacy service is to route your traffic through a series of noncolluding proxy servers TOR 
In particular TOR allows independent individuals to contribute proxies to its proxy pool
When a user connects to a server using TOR TOR randomly chooses from its proxy pool a chain of three proxies and routes all traffic between client and server over the chain
In this manner assuming the proxies do not collude no one knows that communication took place between your IP address and the target Web site
Furthermore although cleartext is sent between the last proxy and the server the last proxy doesnt know what IP address is sending and receiving the cleartext
Providing anonymity and privacy with a proxy Internal networks often have multiple application gateways for example gateways for Telnet HTTP FTP and email
In fact an organizations mail server see Section 
and Web cache are application gateways
Application gateways do not come without their disadvantages
First a different application gateway is needed for each application
Second there is a performance penalty to be paid since all data will be relayed via the gateway
This becomes a concern particularly when multiple users or applications are using the same gateway machine
Finally the client software must know how to contact the gateway when the user makes a request and must know how to tell the application gateway what external server to connect to
Intrusion Detection Systems Weve just seen that a packet filter traditional and stateful inspects IP TCP UDP and ICMP header fields when deciding which packets to let pass through the firewall
However to detect many attack types we need to perform deep packet inspection that is look beyond the header fields and into the actual application data that the packets carry
As we saw in Section 
application gateways often do deep packet inspection
But an application gateway only does this for a specific application
Clearly there is a niche for yet another devicea device that not only examines the headers of all packets passing through it like a packet filter but also performs deep packet inspection unlike a packet filter
When such a device observes a suspicious packet or a suspicious series of packets it could prevent those packets from entering the organizational network
Or because the activity is only deemed as suspicious the device could let the packets pass but send alerts to a network administrator who can then take a closer look at the traffic and take appropriate actions
A device that generates alerts when it observes potentially malicious traffic is called an intrusion detection system IDS
A device that filters out suspicious traffic is called an intrusion prevention system IPS
In this section we study both systemsIDS and IPStogether since the most interesting technical aspect of these systems is how they detect suspicious traffic and not whether they send alerts or drop packets
We will henceforth collectively refer to IDS systems and IPS systems as IDS systems
An IDS can be used to detect a wide range of attacks including network mapping emanating for example from nmap port scans TCP stack scans DoS bandwidthflooding attacks worms and viruses OS vulnerability attacks and application vulnerability attacks
See Section 
for a survey of network attacks
Today thousands of organizations employ IDS systems
Many of these deployed systems are proprietary marketed by Cisco Check Point and other security equipment vendors
But many of the deployed IDS systems are publicdomain systems such as the immensely popular Snort IDS system which well discuss shortly
An organization may deploy one or more IDS sensors in its organizational network
shows an organization that has three IDS sensors
When multiple sensors are deployed they typically work in concert sending information about Figure 
An organization deploying a filter an application gateway and IDS sensors suspicious traffic activity to a central IDS processor which collects and integrates the information and sends alarms to network administrators when deemed appropriate
In Figure 
the organization has partitioned its network into two regions a highsecurity region protected by a packet filter and an application gateway and monitored by IDS sensors and a lowersecurity regionreferred to as the demilitarized zone DMZwhich is protected only by the packet filter but also monitored by IDS sensors
Note that the DMZ includes the organizations servers that need to communicate with the outside world such as its public Web server and its authoritative DNS server
You may be wondering at this stage why multiple IDS sensors Why not just place one IDS sensor just behind the packet filter or even integrated with the packet filter in Figure 
We will soon see that an IDS not only needs to do deep packet inspection but must also compare each passing packet with tens of thousands of signatures this can be a significant amount of processing particularly if the organization receives gigabitssec of traffic from the Internet
By placing the IDS sensors further downstream each sensor sees only a fraction of the organizations traffic and can more easily keep up
Nevertheless highperformance IDS and IPS systems are available today and many organizations can actually get by with just one sensor located near its access router
IDS systems are broadly classified as either signaturebased systems or anomalybased systems
A signaturebased IDS maintains an extensive database of attack signatures
Each signature is a set of rules pertaining to an intrusion activity
A signature may simply be a list of characteristics about a single packet e.g
source and destination port numbers protocol type and a specific string of bits in the packet payload or may relate to a series of packets
The signatures are normally created by skilled network security engineers who research known attacks
An organizations network administrator can customize the signatures or add its own to the database
Operationally a signaturebased IDS sniffs every packet passing by it comparing each sniffed packet with the signatures in its database
If a packet or series of packets matches a signature in the database the IDS generates an alert
The alert could be sent to the network administrator in an email message could be sent to the network management system or could simply be logged for future inspection
Signaturebased IDS systems although widely deployed have a number of limitations
Most importantly they require previous knowledge of the attack to generate an accurate signature
In other words a signaturebased IDS is completely blind to new attacks that have yet to be recorded
Another disadvantage is that even if a signature is matched it may not be the result of an attack so that a false alarm is generated
Finally because every packet must be compared with an extensive collection of signatures the IDS can become overwhelmed with processing and actually fail to detect many malicious packets
An anomalybased IDS creates a traffic profile as it observes traffic in normal operation
It then looks for packet streams that are statistically unusual for example an inordinate percentage of ICMP packets or a sudden exponential growth in port scans and ping sweeps
The great thing about anomalybased IDS systems is that they dont rely on previous knowledge about existing attacksthat is they can potentially detect new undocumented attacks
On the other hand it is an extremely challenging problem to distinguish between normal traffic and statistically unusual traffic
To date most IDS deployments are primarily signaturebased although some include some anomalybased features
Snort Snort is a publicdomain open source IDS with hundreds of thousands of existing deployments Snort Koziol 
It can run on Linux UNIX and Windows platforms
It uses the generic sniffing interface libpcap which is also used by Wireshark and many other packet sniffers
It can easily handle Mbps of traffic for installations with gibabitsec traffic rates multiple Snort sensors may be needed
To gain some insight into Snort lets take a look at an example of a Snort signature alert icmp EXTERNAL_NET any HOME_NET any msgICMP PING NMAP dsize itype This signature is matched by any ICMP packet that enters the organizations network HOME_NET from the outside EXTERNAL_NET is of type ICMP ping and has an empty payload dsize 
Since nmap see Section 
generates ping packets with these specific characteristics this signature is designed to detect nmap ping sweeps
When a packet matches this signature Snort generates an alert that includes the message ICMP PING NMAP 
Perhaps what is most impressive about Snort is the vast community of users and security experts that maintain its signature database
Typically within a few hours of a new attack the Snort community writes and releases an attack signature which is then downloaded by the hundreds of thousands of Snort deployments distributed around the world
Moreover using the Snort signature syntax network administrators can tailor the signatures to their own organizations needs by either modifying existing signatures or creating entirely new ones
Summary In this chapter weve examined the various mechanisms that our secret lovers Bob and Alice can use to communicate securely
Weve seen that Bob and Alice are interested in confidentiality so they alone are able to understand the contents of a transmitted message endpoint authentication so they are sure that they are talking with each other and message integrity so they are sure that their messages are not altered in transit
Of course the need for secure communication is not confined to secret lovers
Indeed we saw in Sections 
that security can be used in various layers in a network architecture to protect against bad guys who have a large arsenal of possible attacks at hand
The first part of this chapter presented various principles underlying secure communication
In Section 
we covered cryptographic techniques for encrypting and decrypting data including symmetric key cryptography and public key cryptography
DES and RSA were examined as specific case studies of these two major classes of cryptographic techniques in use in todays networks
In Section 
we examined two approaches for providing message integrity message authentication codes MACs and digital signatures
The two approaches have a number of parallels
Both use cryptographic hash functions and both techniques enable us to verify the source of the message as well as the integrity of the message itself
One important difference is that MACs do not rely on encryption whereas digital signatures require a public key infrastructure
Both techniques are extensively used in practice as we saw in Sections 
Furthermore digital signatures are used to create digital certificates which are important for verifying the validity of public keys
In Section 
we examined endpoint authentication and introduced nonces to defend against the replay attack
In Sections 
we examined several security networking protocols that enjoy extensive use in practice
We saw that symmetric key cryptography is at the core of PGP SSL IPsec and wireless security
We saw that public key cryptography is crucial for both PGP and SSL
We saw that PGP uses digital signatures for message integrity whereas SSL and IPsec use MACs
Having now an understanding of the basic principles of cryptography and having studied how these principles are actually used you are now in position to design your own secure network protocols Armed with the techniques covered in Sections 
Bob and Alice can communicate securely
One can only hope that they are networking students who have learned this material and can thus avoid having their tryst uncovered by Trudy But confidentiality is only a small part of the network security picture
As we learned in Section 
increasingly the focus in network security has been on securing the network infrastructure against a potential onslaught by the bad guys
In the latter part of this chapter we thus covered firewalls and IDS systems which inspect packets entering and leaving an organizations network
This chapter has covered a lot of ground while focusing on the most important topics in modern network security
Readers who desire to dig deeper are encouraged to investigate the references cited in this chapter
In particular we recommend Skoudis for attacks and operational security Kaufman for cryptography and how it applies to network security Rescorla for an indepth but readable treatment of SSL and Edney for a thorough discussion of 
security including an insightful investigation into WEP and its flaws
Homework Problems and Questions Chapter Review Problems SECTION 
What are the differences between message confidentiality and message integrity Can you have confidentiality without integrity Can you have integrity without confidentiality Justify your answer
Internet entities routers switches DNS servers Web servers user end systems and so on often need to communicate securely
Give three specific example pairs of Internet entities that may want secure communication
From a service perspective what is an important difference between a symmetrickey system and a publickey system R
Suppose that an intruder has an encrypted message as well as the decrypted version of that message
Can the intruder mount a ciphertextonly attack a knownplaintext attack or a chosen plaintext attack R
Consider an block cipher
How many possible input blocks does this cipher have How many possible mappings are there If we view each mapping as a key then how many possible keys does this cipher have R
Suppose N people want to communicate with each of N other people using symmetric key encryption
All communication between any two people i and j is visible to all other people in this group of N and no other person in this group should be able to decode their communication
How many keys are required in the system as a whole Now suppose that public key encryption is used
How many keys are required in this case R
Suppose n a and b
Use an identity of modular arithmetic to calculate in your head abmod n
Suppose you want to encrypt the message by encrypting the decimal number that corresponds to the message
What is the decimal number SECTIONS 
In what way does a hash provide a better message integrity check than a checksum such as the Internet checksum R
Can you decrypt a hash of a message to get the original message Explain your answer
Consider a variation of the MAC algorithm Figure 
where the sender sends m Hms where Hms is the concatenation of Hm and s
Is this variation flawed Why or why not R
What does it mean for a signed document to be verifiable and nonforgeable R
In what way does the publickey encrypted message hash provide a better digital signature than the publickey encrypted message R
Suppose certifier.com creates a certificate for foo.com
Typically the entire certificate would be encrypted with certifier.coms public key
True or false R
Suppose Alice has a message that she is ready to send to anyone who asks
Thousands of people want to obtain Alices message but each wants to be sure of the integrity of the message
In this context do you think a MACbased or a digitalsignaturebased integrity scheme is more suitable Why R
What is the purpose of a nonce in an endpoint authentication protocol R
What does it mean to say that a nonce is a onceinalifetime value In whose lifetime R
Is the message integrity scheme based on HMAC susceptible to playback attacks If so how can a nonce be incorporated into the scheme to remove this susceptibility SECTIONS 
Suppose that Bob receives a PGP message from Alice
How does Bob know for sure that Alice created the message rather than say Trudy Does PGP use a MAC for message integrity R
In the SSL record there is a field for SSL sequence numbers
True or false R
What is the purpose of the random nonces in the SSL handshake R
Suppose an SSL session employs a block cipher with CBC
True or false The server sends to the client the IV in the clear
Suppose Bob initiates a TCP connection to Trudy who is pretending to be Alice
During the handshake Trudy sends Bob Alices certificate
In what step of the SSL handshake algorithm will Bob discover that he is not communicating with Alice R
Consider sending a stream of packets from Host A to Host B using IPsec
Typically a new SA will be established for each packet sent in the stream
True or false R
Suppose that TCP is being run over IPsec between headquarters and the branch office in Figure 
If TCP retransmits the same packet then the two corresponding packets sent by R packets will have the same sequence number in the ESP header
True or false R
An IKE SA and an IPsec SA are the same thing
True or false R
Consider WEP for 
Suppose that the data is and the keystream is 
What is the resulting ciphertext R
In WEP an IV is sent in the clear in every frame
True or false SECTION 
Stateful packet filters maintain two data structures
Name them and briefly describe what they do
Consider a traditional stateless packet filter
This packet filter may filter packets based on TCP flag bits as well as other header fields
True or false R
In a traditional packet filter each interface can have its own access control list
True or false R
Why must an application gateway work in conjunction with a router filter to be effective R
Signaturebased IDSs and IPSs inspect into the payloads of TCP and UDP segments
True or false Problems P
Using the monoalphabetic cipher in Figure 
encode the message This is an easy problem
Decode the message rmiju uamu xyj
Show that Trudys knownplaintext attack in which she knows the ciphertext plaintext translation pairs for seven letters reduces the number of possible substitutions to be checked in the example in Section 
by approximately 
Consider the polyalphabetic system shown in Figure 
Will a chosenplaintext attack that is able to get the plaintext encoding of the message The quick brown fox jumps over the lazy dog
be sufficient to decode all messages Why or why not P
Consider the block cipher in Figure 
Suppose that each block cipher Ti simply reverses the order of the eight input bits so that for example becomes 
Further suppose that the bit scrambler does not modify any bits so that the output value of the mth bit is equal to the input value of the mth bit
a With n and the original bit input equal to repeated eight times what is the value of the output b Repeat part a but now change the last bit of the original bit input from a to a 
c Repeat parts a and b but now suppose that the bit scrambler inverses the order of the bits
Consider the block cipher in Figure 
For a given key Alice and Bob would need to keep eight tables each bits by bits
For Alice or Bob to store all eight tables how many bits of storage are necessary How does this number compare with the number of bits required for a fulltable bit block cipher P
Consider the bit block cipher in Table 
Suppose the plaintext is 
a Initially assume that CBC is not used
What is the resulting ciphertext b Suppose Trudy sniffs the ciphertext
Assuming she knows that a bit block cipher without CBC is being employed but doesnt know the specific cipher what can she surmise c Now suppose that CBC is used with IV
What is the resulting ciphertext P
a Using RSA choose p and q and encode the word dog by encrypting each letter separately
Apply the decryption algorithm to the encrypted version to recover the original plaintext message
b Repeat part a but now encrypt dog as one message m
Consider RSA with p and q
What are n and z b
Let e be 
Why is this an acceptable choice for e c
Find d such that de mod z and d
Encrypt the message m using the key n e
Let c denote the corresponding ciphertext
Show all work
Hint To simplify the calculations use the fact a mod nb mod nmod nabmodn P
In this problem we explore the DiffieHellman DH publickey encryption algorithm which allows two entities to agree on a shared key
The DH algorithm makes use of a large prime number p and another large number g less than p
Both p and g are made public so that an attacker would know them
In DH Alice and Bob each independently choose secret keys SA and SB respectively
Alice then computes her public key TA by raising g to SA and then taking mod p
Bob similarly computes his own public key TB by raising g to SB and then taking mod p
Alice and Bob then exchange their public keys over the Internet
Alice then calculates the shared secret key S by raising TB to SA and then taking mod p
Similarly Bob calculates the shared key S by raising TA to SB and then taking mod p
Prove that in general Alice and Bob obtain the same symmetric key that is prove SS
With p and g suppose Alice and Bob choose private keys SA and SB respectively
Calculate Alices and Bobs public keys TA and TB
Show all work
Following up on part b now calculate S as the shared symmetric key
Show all work
Provide a timing diagram that shows how DiffieHellman can be attacked by a manin themiddle
The timing diagram should have three vertical lines one for Alice one for Bob and one for the attacker Trudy
Suppose Alice wants to communicate with Bob using symmetric key cryptography using a session key KS
In Section 
we learned how publickey cryptography can be used to distribute the session key from Alice to Bob
In this problem we explore how the session key can be distributedwithout public key cryptographyusing a key distribution center KDC
The KDC is a server that shares a unique secret symmetric key with each registered user
For Alice and Bob denote these keys by KAKDC and KBKDC
Design a scheme that uses the KDC to distribute KS to Alice and Bob
Your scheme should use three messages to distribute the session key a message from Alice to the KDC a message from the KDC to Alice and finally a message from Alice to Bob
The first message is KAKDC A B
Using the notation KAKDC KBKDC S A and B answer the following questions
What is the second message b
What is the third message P
Compute a third message different from the two messages in Figure 
that has the same checksum as the messages in Figure 
Suppose Alice and Bob share two secret keys an authentication key S and a symmetric encryption key S
Augment Figure 
so that both integrity and confidentiality are provided
In the BitTorrent PP file distribution protocol see Chapter the seed breaks the file into blocks and the peers redistribute the blocks to each other
Without any protection an attacker can easily wreak havoc in a torrent by masquerading as a benevolent peer and sending bogus blocks to a small subset of peers in the torrent
These unsuspecting peers then redistribute the bogus blocks to other peers which in turn redistribute the bogus blocks to even more peers
Thus it is critical for BitTorrent to have a mechanism that allows a peer to verify the integrity of a block so that it doesnt redistribute bogus blocks
Assume that when a peer joins a torrent it initially gets a .torrent file from a fully trusted source
Describe a simple scheme that allows peers to verify the integrity of blocks
The OSPF routing protocol uses a MAC rather than digital signatures to provide message integrity
Why do you think a MAC was chosen over digital signatures P
Consider our authentication protocol in Figure 
in which Alice authenticates herself to Bob which we saw works well i.e
we found no flaws in it
Now suppose that while Alice is authenticating herself to Bob Bob must authenticate himself to Alice
Give a scenario by which Trudy pretending to be Alice can now authenticate herself to Bob as Alice
Hint Consider that the sequence of operations of the protocol one with Trudy initiating and one with Bob initiating can be arbitrarily interleaved
Pay particular attention to the fact that both Bob and Alice will use a nonce and that if care is not taken the same nonce can be used maliciously
A natural question is whether we can use a nonce and public key cryptography to solve the endpoint authentication problem in Section 
Consider the following natural protocol Alice sends the message I am Alice to Bob
Bob chooses a nonce R and sends it to Alice
Alice uses her private key to encrypt the nonce and sends the resulting value to Bob
Bob applies Alices public key to the received message
Thus Bob computes R and authenticates Alice
Diagram this protocol using the notation for public and private keys employed in the textbook
Suppose that certificates are not used
Describe how Trudy can become a womanin themiddle by intercepting Alices messages and then pretending to be Alice to Bob
shows the operations that Alice must perform with PGP to provide confidentiality authentication and integrity
Diagram the corresponding operations that Bob must perform on the package received from Alice
Suppose Alice wants to send an email to Bob
Bob has a publicprivate key pair KBKB and Alice has Bobs certificate
But Alice does not have a public private key pair
Alice and Bob and the entire world share the same hash function H
In this situation is it possible to design a scheme so that Bob can verify that Alice created the message If so show how with a block diagram for Alice and Bob
Is it possible to design a scheme that provides confidentiality for sending the message from Alice to Bob If so show how with a block diagram for Alice and Bob
Consider the Wireshark output below for a portion of an SSL session
Is Wireshark packet sent by the client or server b
What is the servers IP address and port number c
Assuming no loss and no retransmissions what will be the sequence number of the next TCP segment sent by the client d
How many SSL records does Wireshark packet contain e
Does packet contain a Master Secret or an Encrypted Master Secret or neither f
Assuming that the handshake type field is byte and each length field is bytes what are the values of the first and last bytes of the Master Secret or Encrypted Master Secret g
The client encrypted handshake message takes into account how many SSL records h
The server encrypted handshake message takes into account how many SSL records P
In Section 
it is shown that without sequence numbers Trudy a womaninthe middle can wreak havoc in an SSL session by interchanging TCP segments
Can Trudy do something similar by deleting a TCP segment What does she need to do to succeed at the deletion attack What effect will it have Wireshark screenshot reprinted by permission of the Wireshark Foundation
Suppose Alice and Bob are communicating over an SSL session
Suppose an attacker who does not have any of the shared keys inserts a bogus TCP segment into a packet stream with correct TCP checksum and sequence numbers and correct IP addresses and port numbers
Will SSL at the receiving side accept the bogus packet and pass the payload to the receiving application Why or why not P
The following truefalse questions pertain to Figure 
When a host in 
sends a datagram to an Amazon.com server the router R will encrypt the datagram using IPsec
When a host in 
sends a datagram to a host in 
the router R will change the source and destination address of the IP datagram
Suppose a host in 
initiates a TCP connection to a Web server in 
As part of this connection all datagrams sent by R will have protocol number in the leftmost IPv header field
Consider sending a TCP segment from a host in 
to a host in 
Suppose the acknowledgment for this segment gets lost so that TCP resends the segment
Because IPsec uses sequence numbers R will not resend the TCP segment
Consider the example in Figure 
Suppose Trudy is a womaninthemiddle who can insert datagrams into the stream of datagrams going from R and R
As part of a replay attack Trudy sends a duplicate copy of one of the datagrams sent from R to R
Will R decrypt the duplicate datagram and forward it into the branchoffice network If not describe in detail how R detects the duplicate datagram
Consider the following pseudoWEP protocol
The key is bits and the IV is bits
The IV is appended to the end of the key when generating the keystream
Suppose that the shared secret key is 
The keystreams for the four possible inputs are as follows 
Suppose all messages are bits long
Suppose the ICV integrity check is bits long and is calculated by XORing the first bits of data with the last bits of data
Suppose the pseudo WEP packet consists of three fields first the IV field then the message field and last the ICV field with some of these fields encrypted
We want to send the message m using the IV and using WEP
What will be the values in the three WEP fields b
Show that when the receiver decrypts the WEP packet it recovers the message and the ICV
Suppose Trudy intercepts a WEP packet not necessarily with the IV and wants to modify it before forwarding it to the receiver
Suppose Trudy flips the first ICV bit
Assuming that Trudy does not know the keystreams for any of the IVs what other bits must Trudy also flip so that the received packet passes the ICV check d
Justify your answer by modifying the bits in the WEP packet in part a decrypting the resulting packet and verifying the integrity check
Provide a filter table and a connection table for a stateful firewall that is as restrictive as possible but accomplishes the following a
Allows all internal users to establish Telnet sessions with external hosts
Allows external users to surf the company Web site at 
But otherwise blocks all inbound and outbound traffic
The internal network is 
In your solution suppose that the connection table is currently caching three connections all from inside to outside
Youll need to invent appropriate IP addresses and port numbers
Suppose Alice wants to visit the Web site activist.com using a TORlike service
This service uses two noncolluding proxy servers Proxy and Proxy
Alice first obtains the certificates each containing a public key for Proxy and Proxy from some central server
Denote KKK and K for the encryptiondecryption with public and private RSA keys
Using a timing diagram provide a protocol as simple as possible that enables Alice to establish a shared session key S with Proxy
Denote Sm for encryptiondecryption of data m with the shared key S
Using a timing diagram provide a protocol as simple as possible that allows Alice to establish a shared session key S with Proxy without revealing her IP address to Proxy
Assume now that shared keys S and S are now established
Using a timing diagram provide a protocol as simple as possible and not using publickey cryptography that allows Alice to request an html page from activist.com without revealing her IP address to Proxy and without revealing to Proxy which site she is visiting
Your diagram should end with an HTTP request arriving at activist.com
Wireshark Lab In this lab available from the book Web site we investigate the Secure Sockets Layer SSL protocol
Recall from Section 
that SSL is used for securing a TCP connection and that it is extensively used in practice for secure Internet transactions
In this lab we will focus on the SSL records sent over the TCP connection
We will attempt to delineate and classify each of the records with a goal of understanding the why and how for each record
We investigate the various SSL record types as well as the fields in the SSL messages
We do so by analyzing a trace of the SSL records sent between your host and an ecommerce server
IPsec Lab In this lab available from the book Web site we will explore how to create IPsec SAs between linux boxes
You can do the first part of the lab with two ordinary linux boxes each with one Ethernet adapter
But for the second part of the lab you will need four linux boxes two of which having two Ethernet adapters
In the second half of the lab you will create IPsec SAs using the ESP protocol in the tunnel mode
You will do this by first manually creating the SAs and then by having IKE create the SAs
AN INTERVIEW WITH Steven M
Bellovin Steven M
Bellovin joined the faculty at Columbia University after many years at the Network Services Research Lab at ATT Labs Research in Florham Park New Jersey
His focus is on networks security and why the two are incompatible
In he was awarded the Usenix Lifetime Achievement Award for his work in the creation of Usenet the first newsgroup exchange network that linked two or more computers and allowed users to share information and join in discussions
Steve is also an elected member of the National Academy of Engineering
He received his BA from Columbia University and his PhD from the University of North Carolina at Chapel Hill
What led you to specialize in the networking security area This is going to sound odd but the answer is simple It was fun
My background was in systems programming and systems administration which leads fairly naturally to security
And Ive always been interested in communications ranging back to parttime systems programming jobs when I was in college
My work on security continues to be motivated by two thingsa desire to keep computers useful which means that their function cant be corrupted by attackers and a desire to protect privacy
What was your vision for Usenet at the time that you were developing it And now We originally viewed it as a way to talk about computer science and computer programming around the country with a lot of local use for administrative matters forsale ads and so on
In fact my original prediction was one to two messages per day from sites at the most ever
But the real growth was in peoplerelated topics includingbut not limited tohuman interactions with computers
My favorite newsgroups over the years have been things like rec.woodworking as well as sci.crypt
To some extent netnews has been displaced by the Web
Were I to start designing it today it would look very different
But it still excels as a way to reach a very broad audience that is interested in the topic without having to rely on particular Web sites
Has anyone inspired you professionally In what ways Professor Fred Brooksthe founder and original chair of the computer science department at the University of North Carolina at Chapel Hill the manager of the team that developed the IBM S and OS and the author of The Mythical ManMonthwas a tremendous influence on my career
More than anything else he taught outlook and tradeoffshow to look at problems in the context of the real world and how much messier the real world is than a theorist would like and how to balance competing interests in designing a solution
Most computer work is engineeringthe art of making the right tradeoffs to satisfy many contradictory objectives
What is your vision for the future of networking and security Thus far much of the security we have has come from isolation
A firewall for example works by cutting off access to certain machines and services
But were in an era of increasing connectivityits gotten harder to isolate things
Worse yet our production systems require far more separate pieces interconnected by networks
Securing all that is one of our biggest challenges
What would you say have been the greatest advances in security How much further do we have to go At least scientifically we know how to do cryptography
Thats been a big help
But most security problems are due to buggy code and thats a much harder problem
In fact its the oldest unsolved problem in computer science and I think it will remain that way
The challenge is figuring out how to secure systems when we have to build them out of insecure components
We can already do that for reliability in the face of hardware failures can we do the same for security Do you have any advice for students about the Internet and networking security Learning the mechanisms is the easy part
Learning how to think paranoid is harder
You have to remember that probability distributions dont applythe attackers can and will find improbable conditions
And the details mattera lot
Chapter Multimedia Networking While lounging in bed or riding buses and subways people in all corners of the world are currently using the Internet to watch movies and television shows on demand
Internet movie and television distribution companies such as Netflix and Amazon in North America and Youku and Kankan in China have practically become household names
But people are not only watching Internet videos they are using sites like YouTube to upload and distribute their own usergenerated content becoming Internet video producers as well as consumers
Moreover network applications such as Skype Google Talk and WeChat enormously popular in China allow people to not only make telephone calls over the Internet but to also enhance those calls with video and multiperson conferencing
In fact we predict that by the end of the current decade most of the video consumption and voice conversations will take place endtoend over the Internet more typically to wireless devices connected to the Internet via cellular and WiFi access networks
Traditional telephony and broadcast television are quickly becoming obsolete
We begin this chapter with a taxonomy of multimedia applications in Section 
Well see that a multimedia application can be classified as either streaming stored audiovideo conversational voicevideooverIP or streaming live audiovideo
Well see that each of these classes of applications has its own unique service requirements that differ significantly from those of traditional elastic applications such as email Web browsing and remote login
In Section 
well examine video streaming in some detail
Well explore many of the underlying principles behind video streaming including client buffering prefetching and adapting video quality to available bandwidth
In Section 
we investigate conversational voice and video which unlike elastic applications are highly sensitive to endtoend delay but can tolerate occasional loss of data
Here well examine how techniques such as adaptive playout forward error correction and error concealment can mitigate against networkinduced packet loss and delay
Well also examine Skype as a case study
In Section 
well study RTP and SIP two popular protocols for realtime conversational voice and video applications
In Section 
well investigate mechanisms within the network that can be used to distinguish one class of traffic e.g
delaysensitive applications such as conversational voice from another e.g
elastic applications such as browsing Web pages and provide differentiated service among multiple classes of traffic
Multimedia Networking Applications We define a multimedia network application as any network application that employs audio or video
In this section we provide a taxonomy of multimedia applications
Well see that each class of applications in the taxonomy has its own unique set of service requirements and design issues
But before diving into an indepth discussion of Internet multimedia applications it is useful to consider the intrinsic characteristics of the audio and video media themselves
Properties of Video Perhaps the most salient characteristic of video is its high bit rate
Video distributed over the Internet typically ranges from kbps for lowquality video conferencing to over Mbps for streaming high definition movies
To get a sense of how video bandwidth demands compare with those of other Internet applications lets briefly consider three different users each using a different Internet application
Our first user Frank is going quickly through photos posted on his friends Facebook pages
Lets assume that Frank is looking at a new photo every seconds and that photos are on average Kbytes in size
As usual throughout this discussion we make the simplifying assumption that Kbyte bits
Our second user Martha is streaming music from the Internet the cloud to her smartphone
Lets assume Martha is using a service such as Spotify to listen to many MP songs one after the other each encoded at a rate of kbps
Our third user Victor is watching a video that has been encoded at Mbps
Finally lets suppose that the session length for all three users is seconds approximately minutes
compares the bit rates and the total bytes transferred for these three users
We see that video streaming consumes by far the most bandwidth having a bit rate of more than ten times greater than that of the Facebook and musicstreaming applications
Therefore when design Table 
Comparison of bitrate requirements of three Internet applications Bit rate Bytes transferred in min Facebook Frank kbps Mbytes Martha Music kbps Mbytes Victor Video Mbps Gbyte ing networked video applications the first thing we must keep in mind is the high bitrate requirements of video
Given the popularity of video and its high bit rate it is perhaps not surprising that Cisco predicts Cisco that streaming and stored video will be approximately percent of global consumer Internet traffic by 
Another important characteristic of video is that it can be compressed thereby trading off video quality with bit rate
A video is a sequence of images typically being displayed at a constant rate for example at or images per second
An uncompressed digitally encoded image consists of an array of pixels with each pixel encoded into a number of bits to represent luminance and color
There are two types of redundancy in video both of which can be exploited by video compression
Spatial redundancy is the redundancy within a given image
Intuitively an image that consists of mostly white space has a high degree of redundancy and can be efficiently compressed without significantly sacrificing image quality
Temporal redundancy reflects repetition from image to subsequent image
If for example an image and the subsequent image are exactly the same there is no reason to reencode the subsequent image it is instead more efficient simply to indicate during encoding that the subsequent image is exactly the same
Todays offtheshelf compression algorithms can compress a video to essentially any bit rate desired
Of course the higher the bit rate the better the image quality and the better the overall user viewing experience
We can also use compression to create multiple versions of the same video each at a different quality level
For example we can use compression to create say three versions of the same video at rates of kbps Mbps and Mbps
Users can then decide which version they want to watch as a function of their current available bandwidth
Users with highspeed Internet connections might choose the Mbps version users watching the video over G with a smartphone might choose the kbps version
Similarly the video in a video conference application can be compressed onthefly to provide the best video quality given the available endtoend bandwidth between conversing users
Properties of Audio Digital audio including digitized speech and music has significantly lower bandwidth requirements than video
Digital audio however has its own unique properties that must be considered when designing multimedia network applications
To understand these properties lets first consider how analog audio which humans and musical instruments generate is converted to a digital signal The analog audio signal is sampled at some fixed rate for example at samples per second
The value of each sample will be some real number
Each of the samples is then rounded to one of a finite number of values
This operation is referred to as quantization
The number of such finite valuescalled quantization valuesis typically a power of two for example quantization values
Each of the quantization values is represented by a fixed number of bits
For example if there are quantization values then each valueand hence each audio sampleis represented by one byte
The bit representations of all the samples are then concatenated together to form the digital representation of the signal
As an example if an analog audio signal is sampled at samples per second and each sample is quantized and represented by bits then the resulting digital signal will have a rate of bits per second
For playback through audio speakers the digital signal can then be converted backthat is decodedto an analog signal
However the decoded analog signal is only an approximation of the original signal and the sound quality may be noticeably degraded for example highfrequency sounds may be missing in the decoded signal
By increasing the sampling rate and the number of quantization values the decoded signal can better approximate the original analog signal
Thus as with video there is a tradeoff between the quality of the decoded signal and the bitrate and storage requirements of the digital signal
The basic encoding technique that we just described is called pulse code modulation PCM
Speech encoding often uses PCM with a sampling rate of samples per second and bits per sample resulting in a rate of kbps
The audio compact disk CD also uses PCM with a sampling rate of samples per second with bits per sample this gives a rate of 
kbps for mono and 
Mbps for stereo
PCMencoded speech and music however are rarely used in the Internet
Instead as with video compression techniques are used to reduce the bit rates of the stream
Human speech can be compressed to less than kbps and still be intelligible
A popular compression technique for near CD quality stereo music is MPEG layer more commonly known as MP
MP encoders can compress to many different rates kbps is the most common encoding rate and produces very little sound degradation
A related standard is Advanced Audio Coding AAC which has been popularized by Apple
As with video multiple versions of a prerecorded audio stream can be created each at a different bit rate
Although audio bit rates are generally much less than those of video users are generally much more sensitive to audio glitches than video glitches
Consider for example a video conference taking place over the Internet
If from time to time the video signal is lost for a few seconds the video conference can likely proceed without too much user frustration
If however the audio signal is frequently lost the users may have to terminate the session
Types of Multimedia Network Applications The Internet supports a large variety of useful and entertaining multimedia applications
In this subsection we classify multimedia applications into three broad categories i streaming stored audiovideo ii conversational voicevideooverIP and iii streaming live audiovideo
As we will soon see each of these application categories has its own set of service requirements and design issues
Streaming Stored Audio and Video To keep the discussion concrete we focus here on streaming stored video which typically combines video and audio components
Streaming stored audio such as Spotifys streaming music service is very similar to streaming stored video although the bit rates are typically much lower
In this class of applications the underlying medium is prerecorded video such as a movie a television show a prerecorded sporting event or a prerecorded usergenerated video such as those commonly seen on YouTube
These prerecorded videos are placed on servers and users send requests to the servers to view the videos on demand
Many Internet companies today provide streaming video including YouTube Google Netflix Amazon and Hulu
Streaming stored video has three key distinguishing features
In a streaming stored video application the client typically begins video playout within a few seconds after it begins receiving the video from the server
This means that the client will be playing out from one location in the video while at the same time receiving later parts of the video from the server
This technique known as streaming avoids having to download the entire video file and incurring a potentially long delay before playout begins
Because the media is prerecorded the user may pause reposition forward reposition backward fastforward and so on through the video content
The time from when the user makes such a request until the action manifests itself at the client should be less than a few seconds for acceptable responsiveness
Once playout of the video begins it should proceed according to the original timing of the recording
Therefore data must be received from the server in time for its playout at the client otherwise users experience video frame freezing when the client waits for the delayed frames or frame skipping when the client skips over delayed frames
By far the most important performance measure for streaming video is average throughput
In order to provide continuous playout the network must provide an average throughput to the streaming application that is at least as large the bit rate of the video itself
As we will see in Section 
by using buffering and prefetching it is possible to provide continuous playout even when the throughput fluctuates as long as the average throughput averaged over seconds remains above the video rate Wang 
For many streaming video applications prerecorded video is stored on and streamed from a CDN rather than from a single data center
There are also many PP video streaming applications for which the video is stored on users hosts peers with different chunks of video arriving from different peers that may spread around the globe
Given the prominence of Internet video streaming we will explore video streaming in some depth in Section 
paying particular attention to client buffering prefetching adapting quality to bandwidth availability and CDN distribution
Conversational Voice and VideooverIP Realtime conversational voice over the Internet is often referred to as Internet telephony since from the users perspective it is similar to the traditional circuitswitched telephone service
It is also commonly called VoiceoverIP VoIP
Conversational video is similar except that it includes the video of the participants as well as their voices
Most of todays voice and video conversational systems allow users to create conferences with three or more participants
Conversational voice and video are widely used in the Internet today with the Internet companies Skype QQ and Google Talk boasting hundreds of millions of daily users
In our discussion of application service requirements in Chapter Figure 
we identified a number of axes along which application requirements can be classified
Two of these axestiming considerations and tolerance of data lossare particularly important for conversational voice and video applications
Timing considerations are important because audio and video conversational applications are highly delaysensitive
For a conversation with two or more interacting speakers the delay from when a user speaks or moves until the action is manifested at the other end should be less than a few hundred milliseconds
For voice delays smaller than milliseconds are not perceived by a human listener delays between and milliseconds can be acceptable and delays exceeding milliseconds can result in frustrating if not completely unintelligible voice conversations
On the other hand conversational multimedia applications are losstolerantoccasional loss only causes occasional glitches in audiovideo playback and these losses can often be partially or fully concealed
These delaysensitive but losstolerant characteristics are clearly different from those of elastic data applications such as Web browsing email social networks and remote login
For elastic applications long delays are annoying but not particularly harmful the completeness and integrity of the transferred data however are of paramount importance
We will explore conversational voice and video in more depth in Section 
paying particular attention to how adaptive playout forward error correction and error concealment can mitigate against networkinduced packet loss and delay
Streaming Live Audio and Video This third class of applications is similar to traditional broadcast radio and television except that transmission takes place over the Internet
These applications allow a user to receive a live radio or television transmissionsuch as a live sporting event or an ongoing news eventtransmitted from any corner of the world
Today thousands of radio and television stations around the world are broadcasting content over the Internet
Live broadcastlike applications often have many users who receive the same audiovideo program at the same time
In the Internet today this is typically done with CDNs Section 
As with streaming stored multimedia the network must provide each live multimedia flow with an average throughput that is larger than the video consumption rate
Because the event is live delay can also be an issue although the timing constraints are much less stringent than those for conversational voice
Delays of up to ten seconds or so from when the user chooses to view a live transmission to when playout begins can be tolerated
We will not cover streaming live media in this book because many of the techniques used for streaming live mediainitial buffering delay adaptive bandwidth use and CDN distributionare similar to those for streaming stored media
Streaming Stored Video For streaming video applications prerecorded videos are placed on servers and users send requests to these servers to view the videos on demand
The user may watch the video from beginning to end without interruption may stop watching the video well before it ends or interact with the video by pausing or repositioning to a future or past scene
Streaming video systems can be classified into three categories UDP streaming HTTP streaming and adaptive HTTP streaming see Section 
Although all three types of systems are used in practice the majority of todays systems employ HTTP streaming and adaptive HTTP streaming
A common characteristic of all three forms of video streaming is the extensive use of clientside application buffering to mitigate the effects of varying endtoend delays and varying amounts of available bandwidth between server and client
For streaming video both stored and live users generally can tolerate a small severalsecond initial delay between when the client requests a video and when video playout begins at the client
Consequently when the video starts to arrive at the client the client need not immediately begin playout but can instead build up a reserve of video in an application buffer
Once the client has built up a reserve of several seconds of bufferedbutnotyetplayed video the client can then begin video playout
There are two important advantages provided by such client buffering
First clientside buffering can absorb variations in servertoclient delay
If a particular piece of video data is delayed as long as it arrives before the reserve of receivedbutnotyetplayed video is exhausted this long delay will not be noticed
Second if the servertoclient bandwidth briefly drops below the video consumption rate a user can continue to enjoy continuous playback again as long as the client application buffer does not become completely drained
illustrates clientside buffering
In this simple example suppose that video is encoded at a fixed bit rate and thus each video block contains video frames that are to be played out over the same fixed amount of time Î
The server transmits the first video block at t the second block at tÎ the third block at tÎ and so on
Once the client begins playout each block should be played out Î time units after the previous block in order to reproduce the timing of the original recorded video
Because of the variable endtoend network delays different video blocks experience different delays
The first video block arrives at the client at t and the second block arrives at t
The network delay for the ith block is the horizontal distance between the time the block was transmitted by the server and the time it is received at the client note that the network delay varies from one video block to another
In this example if the client were to begin playout as soon as the first block arrived at t then the second block would not have arrived in time to be played out at out at tÎ
In this case video playout would either have to stall waiting for block to arrive or block could be skippedboth resulting in undesirable playout impairments
Instead if the client were to delay the start of playout until t when blocks through have all arrived periodic playout can proceed with all blocks having been received before their playout time
Client playout delay in video streaming 
UDP Streaming We only briefly discuss UDP streaming here referring the reader to more indepth discussions of the protocols behind these systems where appropriate
With UDP streaming the server transmits video at a rate that matches the clients video consumption rate by clocking out the video chunks over UDP at a steady rate
For example if the video consumption rate is Mbps and each UDP packet carries bits of video then the server would transmit one UDP packet into its socket every bits Mbps msec
As we learned in Chapter because UDP does not employ a congestioncontrol mechanism the server can push packets into the network at the consumption rate of the video without the ratecontrol restrictions of TCP
UDP streaming typically uses a small clientside buffer big enough to hold less than a second of video
Before passing the video chunks to UDP the server will encapsulate the video chunks within transport packets specially designed for transporting audio and video using the RealTime Transport Protocol RTP RFC or a similar possibly proprietary scheme
We delay our coverage of RTP until Section 
where we discuss RTP in the context of conversational voice and video systems
Another distinguishing property of UDP streaming is that in addition to the servertoclient video stream the client and server also maintain in parallel a separate control connection over which the client sends commands regarding session state changes such as pause resume reposition and so on
The Real Time Streaming Protocol RTSP RFC explained in some detail in the Web site for this textbook is a popular open protocol for such a control connection
Although UDP streaming has been employed in many opensource systems and proprietary products it suffers from three significant drawbacks
First due to the unpredictable and varying amount of available bandwidth between server and client constantrate UDP streaming can fail to provide continuous playout
For example consider the scenario where the video consumption rate is Mbps and the servertoclient available bandwidth is usually more than Mbps but every few minutes the available bandwidth drops below Mbps for several seconds
In such a scenario a UDP streaming system that transmits video at a constant rate of Mbps over RTPUDP would likely provide a poor user experience with freezing or skipped frames soon after the available bandwidth falls below Mbps
The second drawback of UDP streaming is that it requires a media control server such as an RTSP server to process clienttoserver interactivity requests and to track client state e.g
the clients playout point in the video whether the video is being paused or played and so on for each ongoing client session
This increases the overall cost and complexity of deploying a largescale videoondemand system
The third drawback is that many firewalls are configured to block UDP traffic preventing the users behind these firewalls from receiving UDP video
HTTP Streaming In HTTP streaming the video is simply stored in an HTTP server as an ordinary file with a specific URL
When a user wants to see the video the client establishes a TCP connection with the server and issues an HTTP GET request for that URL
The server then sends the video file within an HTTP response message as quickly as possible that is as quickly as TCP congestion control and flow control will allow
On the client side the bytes are collected in a client application buffer
Once the number of bytes in this buffer exceeds a predetermined threshold the client application begins playbackspecifically it periodically grabs video frames from the client application buffer decompresses the frames and displays them on the users screen
We learned in Chapter that when transferring a file over TCP the servertoclient transmission rate can vary significantly due to TCPs congestion control mechanism
In particular it is not uncommon for the transmission rate to vary in a sawtooth manner associated with TCP congestion control
Furthermore packets can also be significantly delayed due to TCPs retransmission mechanism
Because of these characteristics of TCP the conventional wisdom in the s was that video streaming would never work well over TCP
Over time however designers of streaming video systems learned that TCPs congestion control and reliabledata transfer mechanisms do not necessarily preclude continuous playout when client buffering and prefetching discussed in the next section are used
The use of HTTP over TCP also allows the video to traverse firewalls and NATs more easily which are often configured to block most UDP traffic but to allow most HTTP traffic
Streaming over HTTP also obviates the need for a media control server such as an RTSP server reducing the cost of a large scale deployment over the Internet
Due to all of these advantages most video streaming applications todayincluding YouTube and Netflixuse HTTP streaming over TCP as its underlying streaming protocol
Prefetching Video As we just learned clientside buffering can be used to mitigate the effects of varying endtoend delays and varying available bandwidth
In our earlier example in Figure 
the server transmits video at the rate at which the video is to be played out
However for streaming stored video the client can attempt to download the video at a rate higher than the consumption rate thereby prefetching video frames that are to be consumed in the future
This prefetched video is naturally stored in the client application buffer
Such prefetching occurs naturally with TCP streaming since TCPs congestion avoidance mechanism will attempt to use all of the available bandwidth between server and client
To gain some insight into prefetching lets take a look at a simple example
Suppose the video consumption rate is Mbps but the network is capable of delivering the video from server to client at a constant rate of 
Then the client will not only be able to play out the video with a very small playout delay but will also be able to increase the amount of buffered video data by Kbits every second
In this manner if in the future the client receives data at a rate of less than Mbps for a brief period of time the client will be able to continue to provide continuous playback due to the reserve in its buffer
Wang shows that when the average TCP throughput is roughly twice the media bit rate streaming over TCP results in minimal starvation and low buffering delays
Client Application Buffer and TCP Buffers Figure 
illustrates the interaction between client and server for HTTP streaming
At the server side the portion of the video file in white has already been sent into the servers socket while the darkened portion is what remains to be sent
After passing through the socket door the bytes are placed in the TCP send buffer before being transmitted into the Internet as described in Chapter 
In Figure 
because the TCP send buffer at the server side is shown to be full the server is momentarily prevented from sending more bytes from the video file into the socket
On the client side the client application media player reads bytes from the TCP receive buffer through its client socket and places the bytes into the client application buffer
At the same time the client application periodically grabs video frames from the client application buffer decompresses the frames and displays them on the users screen
Note that if the client application buffer is larger than the video file then the whole process of moving bytes from the servers storage to the clients application buffer is equivalent to an ordinary file download over HTTPthe client simply pulls the video off the server as fast as TCP will allow Figure 
Streaming stored video over HTTPTCP Consider now what happens when the user pauses the video during the streaming process
During the pause period bits are not removed from the client application buffer even though bits continue to enter the buffer from the server
If the client application buffer is finite it may eventually become full which will cause back pressure all the way back to the server
Specifically once the client application buffer becomes full bytes can no longer be removed from the client TCP receive buffer so it too becomes full
Once the client receive TCP buffer becomes full bytes can no longer be removed from the server TCP send buffer so it also becomes full
Once the TCP becomes full the server cannot send any more bytes into the socket
Thus if the user pauses the video the server may be forced to stop transmitting in which case the server will be blocked until the user resumes the video
In fact even during regular playback that is without pausing if the client application buffer becomes full back pressure will cause the TCP buffers to become full which will force the server to reduce its rate
To determine the resulting rate note that when the client application removes f bits it creates room for f bits in the client application buffer which in turn allows the server to send f additional bits
Thus the server send rate can be no higher than the video consumption rate at the client
Therefore a full client application buffer indirectly imposes a limit on the rate that video can be sent from server to client when streaming over HTTP
Analysis of Video Streaming Some simple modeling will provide more insight into initial playout delay and freezing due to application buffer depletion
As shown in Figure 
let B denote the size Figure 
Analysis of clientside buffering for video streaming in bits of the clients application buffer and let Q denote the number of bits that must be buffered before the client application begins playout
Of course QB
Let r denote the video consumption rate the rate at which the client draws bits out of the client application buffer during playback
So for example if the videos frame rate is framessec and each compressed frame is bits then r Mbps
To see the forest through the trees well ignore TCPs send and receive buffers
Lets assume that the server sends bits at a constant rate x whenever the client buffer is not full
This is a gross simplification since TCPs send rate varies due to congestion control well examine more realistic timedependent rates xt in the problems at the end of this chapter
Suppose at time t the application buffer is empty and video begins arriving to the client application buffer
We now ask at what time ttp does playout begin And while we are at it at what time ttf does the client application buffer become full First lets determine tp the time when Q bits have entered the application buffer and playout begins
Recall that bits arrive to the client application buffer at rate x and no bits are removed from this buffer before playout begins
Thus the amount of time required to build up Q bits the initial buffering delay is tpQx
Now lets determine tf the point in time when the client application buffer becomes full
We first observe that if xr that is if the server send rate is less than the video consumption rate then the client buffer will never become full Indeed starting at time tp the buffer will be depleted at rate r and will only be filled at rate xr
Eventually the client buffer will empty out entirely at which time the video will freeze on the screen while the client buffer waits another tp seconds to build up Q bits of video
Thus when the available rate in the network is less than the video rate playout will alternate between periods of continuous playout and periods of freezing
In a homework problem you will be asked to determine the length of each continuous playout and freezing period as a function of Q r and x
Now lets determine tf for when xr
In this case starting at time tp the buffer increases from Q to B at rate xr since bits are being depleted at rate r but are arriving at rate x as shown in Figure 
Given these hints you will be asked in a homework problem to determine tf the time the client buffer becomes full
Note that when the available rate in the network is more than the video rate after the initial buffering delay the user will enjoy continuous playout until the video ends
Early Termination and Repositioning the Video HTTP streaming systems often make use of the HTTP byterange header in the HTTP GET request message which specifies the specific range of bytes the client currently wants to retrieve from the desired video
This is particularly useful when the user wants to reposition that is jump to a future point in time in the video
When the user repositions to a new position the client sends a new HTTP request indicating with the byterange header from which byte in the file should the server send data
When the server receives the new HTTP request it can forget about any earlier request and instead send bytes beginning with the byte indicated in the byterange request
While we are on the subject of repositioning we briefly mention that when a user repositions to a future point in the video or terminates the video early some prefetchedbutnotyetviewed data transmitted by the server will go unwatcheda waste of network bandwidth and server resources
For example suppose that the client buffer is full with B bits at some time t into the video and at this time the user repositions to some instant ttBr into the video and then watches the video to completion from that point on
In this case all B bits in the buffer will be unwatched and the bandwidth and server resources that were used to transmit those B bits have been completely wasted
There is significant wasted bandwidth in the Internet due to early termination which can be quite costly particularly for wireless links Ihm 
For this reason many streaming systems use only a moderatesize client application buffer or will limit the amount of prefetched video using the byterange header in HTTP requests Rao 
Repositioning and early termination are analogous to cooking a large meal eating only a portion of it and throwing the rest away thereby wasting food
So the next time your parents criticize you for wasting food by not eating all your dinner you can quickly retort by saying they are wasting bandwidth and server resources when they reposition while watching movies over the Internet But of course two wrongs do not make a rightboth food and bandwidth are not to be wasted In Sections 
we covered UDP streaming and HTTP streaming respectively
A third type of streaming is Dynamic Adaptive Streaming over HTTP DASH which uses multiple versions of the video each compressed at a different rate
DASH is discussed in detail in Section 
CDNs are often used to distribute stored and live video
CDNs are discussed in detail in Section 
VoiceoverIP Realtime conversational voice over the Internet is often referred to as Internet telephony since from the users perspective it is similar to the traditional circuitswitched telephone service
It is also commonly called VoiceoverIP VoIP
In this section we describe the principles and protocols underlying VoIP
Conversational video is similar in many respects to VoIP except that it includes the video of the participants as well as their voices
To keep the discussion focused and concrete we focus here only on voice in this section rather than combined voice and video
Limitations of the BestEffort IP Service The Internets networklayer protocol IP provides besteffort service
That is to say the service makes its best effort to move each datagram from source to destination as quickly as possible but makes no promises whatsoever about getting the packet to the destination within some delay bound or about a limit on the percentage of packets lost
The lack of such guarantees poses significant challenges to the design of realtime conversational applications which are acutely sensitive to packet delay jitter and loss
In this section well cover several ways in which the performance of VoIP over a besteffort network can be enhanced
Our focus will be on applicationlayer techniques that is approaches that do not require any changes in the network core or even in the transport layer at the end hosts
To keep the discussion concrete well discuss the limitations of besteffort IP service in the context of a specific VoIP example
The sender generates bytes at a rate of bytes per second every msecs the sender gathers these bytes into a chunk
A chunk and a special header discussed below are encapsulated in a UDP segment via a call to the socket interface
Thus the number of bytes in a chunk is msecs bytessec bytes and a UDP segment is sent every msecs
If each packet makes it to the receiver with a constant endtoend delay then packets arrive at the receiver periodically every msecs
In these ideal conditions the receiver can simply play back each chunk as soon as it arrives
But unfortunately some packets can be lost and most packets will not have the same endtoend delay even in a lightly congested Internet
For this reason the receiver must take more care in determining when to play back a chunk and what to do with a missing chunk
Packet Loss Consider one of the UDP segments generated by our VoIP application
The UDP segment is encapsulated in an IP datagram
As the datagram wanders through the network it passes through router buffers that is queues while waiting for transmission on outbound links
It is possible that one or more of the buffers in the path from sender to receiver is full in which case the arriving IP datagram may be discarded never to arrive at the receiving application
Loss could be eliminated by sending the packets over TCP which provides for reliable data transfer rather than over UDP
However retransmission mechanisms are often considered unacceptable for conversational realtime audio applications such as VoIP because they increase endtoend delay Bolot 
Furthermore due to TCP congestion control packet loss may result in a reduction of the TCP senders transmission rate to a rate that is lower than the receivers drain rate possibly leading to buffer starvation
This can have a severe impact on voice intelligibility at the receiver
For these reasons most existing VoIP applications run over UDP by default
Baset reports that UDP is used by Skype unless a user is behind a NAT or firewall that blocks UDP segments in which case TCP is used
But losing packets is not necessarily as disastrous as one might think
Indeed packet loss rates between and percent can be tolerated depending on how voice is encoded and transmitted and on how the loss is concealed at the receiver
For example forward error correction FEC can help conceal packet loss
Well see below that with FEC redundant information is transmitted along with the original information so that some of the lost original data can be recovered from the redundant information
Nevertheless if one or more of the links between sender and receiver is severely congested and packet loss exceeds to percent for example on a wireless link then there is really nothing that can be done to achieve acceptable audio quality
Clearly besteffort service has its limitations
EndtoEnd Delay Endtoend delay is the accumulation of transmission processing and queuing delays in routers propagation delays in links and endsystem processing delays
For realtime conversational applications such as VoIP endtoend delays smaller than msecs are not perceived by a human listener delays between and msecs can be acceptable but are not ideal and delays exceeding msecs can seriously hinder the interactivity in voice conversations
The receiving side of a VoIP application will typically disregard any packets that are delayed more than a certain threshold for example more than msecs
Thus packets that are delayed by more than the threshold are effectively lost
Packet Jitter A crucial component of endtoend delay is the varying queuing delays that a packet experiences in the networks routers
Because of these varying delays the time from when a packet is generated at the source until it is received at the receiver can fluctuate from packet to packet as shown in Figure 
This phenomenon is called jitter
As an example consider two consecutive packets in our VoIP application
The sender sends the second packet msecs after sending the first packet
But at the receiver the spacing between these packets can become greater than msecs
To see this suppose the first packet arrives at a nearly empty queue at a router but just before the second packet arrives at the queue a large number of packets from other sources arrive at the same queue
Because the first packet experiences a small queuing delay and the second packet suffers a large queuing delay at this router the first and second packets become spaced by more than msecs
The spacing between consecutive packets can also become less than msecs
To see this again consider two consecutive packets
Suppose the first packet joins the end of a queue with a large number of packets and the second packet arrives at the queue before this first packet is transmitted and before any packets from other sources arrive at the queue
In this case our two packets find themselves one right after the other in the queue
If the time it takes to transmit a packet on the routers outbound link is less than msecs then the spacing between first and second packets becomes less than msecs
The situation is analogous to driving cars on roads
Suppose you and your friend are each driving in your own cars from San Diego to Phoenix
Suppose you and your friend have similar driving styles and that you both drive at kmhour traffic permitting
If your friend starts out one hour before you depending on intervening traffic you may arrive at Phoenix more or less than one hour after your friend
If the receiver ignores the presence of jitter and plays out chunks as soon as they arrive then the resulting audio quality can easily become unintelligible at the receiver
Fortunately jitter can often be removed by using sequence numbers timestamps and a playout delay as discussed below
Removing Jitter at the Receiver for Audio For our VoIP application where packets are being generated periodically the receiver should attempt to provide periodic playout of voice chunks in the presence of random network jitter
This is typically done by combining the following two mechanisms Prepending each chunk with a timestamp
The sender stamps each chunk with the time at which the chunk was generated
Delaying playout of chunks at the receiver
As we saw in our earlier discussion of Figure 
the playout delay of the received audio chunks must be long enough so that most of the packets are received before their scheduled playout times
This playout delay can either be fixed throughout the duration of the audio session or vary adaptively during the audio session lifetime
We now discuss how these three mechanisms when combined can alleviate or even eliminate the effects of jitter
We examine two playback strategies fixed playout delay and adaptive playout delay
Fixed Playout Delay With the fixeddelay strategy the receiver attempts to play out each chunk exactly q msecs after the chunk is generated
So if a chunk is timestamped at the sender at time t the receiver plays out the chunk at time tq assuming the chunk has arrived by that time
Packets that arrive after their scheduled playout times are discarded and considered lost
What is a good choice for q VoIP can support delays up to about msecs although a more satisfying conversational experience is achieved with smaller values of q
On the other hand if q is made much smaller than msecs then many packets may miss their scheduled playback times due to the networkinduced packet jitter
Roughly speaking if large variations in endtoend delay are typical it is preferable to use a large q on the other hand if delay is small and variations in delay are also small it is preferable to use a small q perhaps less than msecs
The tradeoff between the playback delay and packet loss is illustrated in Figure 
The figure shows the times at which packets are generated and played Figure 
Packet loss for different fixed playout delays out for a single talk spurt
Two distinct initial playout delays are considered
As shown by the leftmost staircase the sender generates packets at regular intervalssay every msecs
The first packet in this talk spurt is received at time r
As shown in the figure the arrivals of subsequent packets are not evenly spaced due to the network jitter
For the first playout schedule the fixed initial playout delay is set to pr
With this schedule the fourth packet does not arrive by its scheduled playout time and the receiver considers it lost
For the second playout schedule the fixed initial playout delay is set to pr
For this schedule all packets arrive before their scheduled playout times and there is therefore no loss
Adaptive Playout Delay The previous example demonstrates an important delayloss tradeoff that arises when designing a playout strategy with fixed playout delays
By making the initial playout delay large most packets will make their deadlines and there will therefore be negligible loss however for conversational services such as VoIP long delays can become bothersome if not intolerable
Ideally we would like the playout delay to be minimized subject to the constraint that the loss be below a few percent
The natural way to deal with this tradeoff is to estimate the network delay and the variance of the network delay and to adjust the playout delay accordingly at the beginning of each talk spurt
This adaptive adjustment of playout delays at the beginning of the talk spurts will cause the senders silent periods to be compressed and elongated however compression and elongation of silence by a small amount is not noticeable in speech
Following Ramjee we now describe a generic algorithm that the receiver can use to adaptively adjust its playout delays
To this end let ti the timestamp of the ith packet the time the packet was generated by the sender ri the time packet i is received by receiver pi the time packet i is played at receiver The endtoend network delay of the ith packet is riti
Due to network jitter this delay will vary from packet to packet
Let di denote an estimate of the average network delay upon reception of the ith packet
This estimate is constructed from the timestamps as follows diudiuriti where u is a fixed constant for example u
Thus di is a smoothed average of the observed network delays rtriti
The estimate places more weight on the recently observed network delays than on the observed network delays of the distant past
This form of estimate should not be completely unfamiliar a similar idea is used to estimate roundtrip times in TCP as discussed in Chapter 
Let vi denote an estimate of the average deviation of the delay from the estimated average delay
This estimate is also constructed from the timestamps viuviu ritidi The estimates di and vi are calculated for every packet received although they are used only to determine the playout point for the first packet in any talk spurt
Once having calculated these estimates the receiver employs the following algorithm for the playout of packets
If packet i is the first packet of a talk spurt its playout time pi is computed as pitidiKvi where K is a positive constant for example K
The purpose of the Kvi term is to set the playout time far enough into the future so that only a small fraction of the arriving packets in the talk spurt will be lost due to late arrivals
The playout point for any subsequent packet in a talk spurt is computed as an offset from the point in time when the first packet in the talk spurt was played out
In particular let qipiti be the length of time from when the first packet in the talk spurt is generated until it is played out
If packet j also belongs to this talk spurt it is played out at time pjtjqi The algorithm just described makes perfect sense assuming that the receiver can tell whether a packet is the first packet in the talk spurt
This can be done by examining the signal energy in each received packet
Recovering from Packet Loss We have discussed in some detail how a VoIP application can deal with packet jitter
We now briefly describe several schemes that attempt to preserve acceptable audio quality in the presence of packet loss
Such schemes are called loss recovery schemes
Here we define packet loss in a broad sense A packet is lost either if it never arrives at the receiver or if it arrives after its scheduled playout time
Our VoIP example will again serve as a context for describing loss recovery schemes
As mentioned at the beginning of this section retransmitting lost packets may not be feasible in a real time conversational application such as VoIP
Indeed retransmitting a packet that has missed its playout deadline serves absolutely no purpose
And retransmitting a packet that overflowed a router queue cannot normally be accomplished quickly enough
Because of these considerations VoIP applications often use some type of loss anticipation scheme
Two types of loss anticipation schemes are forward error correction FEC and interleaving
Forward Error Correction FEC The basic idea of FEC is to add redundant information to the original packet stream
For the cost of marginally increasing the transmission rate the redundant information can be used to reconstruct approximations or exact versions of some of the lost packets
Following Bolot and Perkins we now outline two simple FEC mechanisms
The first mechanism sends a redundant encoded chunk after every n chunks
The redundant chunk is obtained by exclusive ORing the n original chunks Shacham 
In this manner if any one packet of the group of n packets is lost the receiver can fully reconstruct the lost packet
But if two or more packets in a group are lost the receiver cannot reconstruct the lost packets
By keeping n the group size small a large fraction of the lost packets can be recovered when loss is not excessive
However the smaller the group size the greater the relative increase of the transmission rate
In particular the transmission rate increases by a factor of n so that if n then the transmission rate increases by percent
Furthermore this simple scheme increases the playout delay as the receiver must wait to receive the entire group of packets before it can begin playout
For more practical details about how FEC works for multimedia transport see RFC 
The second FEC mechanism is to send a lowerresolution audio stream as the redundant information
For example the sender might create a nominal audio stream and a corresponding lowresolution low bit rate audio stream
The nominal stream could be a PCM encoding at kbps and the lowerquality stream could be a GSM encoding at kbps
The lowbit rate stream is referred to as the redundant stream
As shown in Figure 
the sender constructs the nth packet by taking the nth chunk from the nominal stream and appending to it the nst chunk from the redundant stream
In this manner whenever there is nonconsecutive packet loss the receiver can conceal the loss by playing out the low bit rate encoded chunk that arrives with the subsequent packet
Of course lowbit rate chunks give lower quality than the nominal chunks
However a stream of mostly highquality chunks occasional low quality chunks and no missing chunks gives good overall audio quality
Note that in this scheme the receiver only has to receive two packets before playback so that the increased playout delay is small
Furthermore if the lowbit rate encoding is much less than the nominal encoding then the marginal increase in the transmission rate will be small
In order to cope with consecutive loss we can use a simple variation
Instead of appending just the nst lowbit rate chunk to the nth nominal chunk the sender can append the nst and nnd low bit rate chunk or append the nst and nrd lowbit rate chunk and so on
By appending more low bit rate chunks to each nominal chunk the audio quality at the receiver becomes acceptable for a wider variety of harsh besteffort environments
On the other hand the additional chunks increase the transmission bandwidth and the playout delay
Piggybacking lowerquality redundant information Interleaving As an alternative to redundant transmission a VoIP application can send interleaved audio
As shown in Figure 
the sender resequences units of audio data before transmission so that originally adjacent units are separated by a certain distance in the transmitted stream
Interleaving can mitigate the effect of packet losses
If for example units are msecs in length and chunks are msecs that is four units per chunk then the first chunk could contain units and the second chunk could contain units and and so on
shows that the loss of a single packet from an interleaved stream results in multiple small gaps in the reconstructed stream as opposed to the single large gap that would occur in a noninterleaved stream
Interleaving can significantly improve the perceived quality of an audio stream Perkins 
It also has low overhead
The obvious disadvantage of interleaving is that it increases latency
This limits its use for conversational applications such as VoIP although it can perform well for streaming stored audio
A major advantage of interleaving is that it does not increase the bandwidth requirements of a stream
Error Concealment Error concealment schemes attempt to produce a replacement for a lost packet that is similar to the original
As discussed in Perkins this is possible since audio Figure 
Sending interleaved audio signals and in particular speech exhibit large amounts of shortterm selfsimilarity
As such these techniques work for relatively small loss rates less than percent and for small packets msecs
When the loss length approaches the length of a phoneme msecs these techniques break down since whole phonemes may be missed by the listener
Perhaps the simplest form of receiverbased recovery is packet repetition
Packet repetition replaces lost packets with copies of the packets that arrived immediately before the loss
It has low computational complexity and performs reasonably well
Another form of receiverbased recovery is interpolation which uses audio before and after the loss to interpolate a suitable packet to cover the loss
Interpolation performs somewhat better than packet repetition but is significantly more computationally intensive Perkins 
Case Study VoIP with Skype Skype is an immensely popular VoIP application with over million accounts active on a daily basis
In addition to providing hosttohost VoIP service Skype offers hosttophone services phonetohost services and multiparty hosttohost video conferencing services
Here a host is again any Internet connected IP device including PCs tablets and smartphones
Skype was acquired by Microsoft in 
Because the Skype protocol is proprietary and because all Skypes control and media packets are encrypted it is difficult to precisely determine how Skype operates
Nevertheless from the Skype Web site and several measurement studies researchers have learned how Skype generally works Baset Guha Chen Suh Ren Zhang X 
For both voice and video the Skype clients have at their disposal many different codecs which are capable of encoding the media at a wide range of rates and qualities
For example video rates for Skype have been measured to be as low as kbps for a lowquality session up to almost Mbps for a high quality session Zhang X 
Typically Skypes audio quality is better than the POTS Plain Old Telephone Service quality provided by the wireline phone system
Skype codecs typically sample voice at samplessec or higher which provides richer tones than POTS which samples at sec
By default Skype sends audio and video packets over UDP
However control packets are sent over TCP and media packets are also sent over TCP when firewalls block UDP streams
Skype uses FEC for loss recovery for both voice and video streams sent over UDP
The Skype client also adapts the audio and video streams it sends to current network conditions by changing video quality and FEC overhead Zhang X 
Skype uses PP techniques in a number of innovative ways nicely illustrating how PP can be used in applications that go beyond content distribution and file sharing
As with instant messaging hosttohost Internet telephony is inherently PP since at the heart of the application pairs of users that is peers communicate with each other in real time
But Skype also employs PP techniques for two other important functions namely for user location and for NAT traversal
Skype peers As shown in Figure 
the peers hosts in Skype are organized into a hierarchical overlay network with each peer classified as a super peer or an ordinary peer
Skype maintains an index that maps Skype usernames to current IP addresses and port numbers
This index is distributed over the super peers
When Alice wants to call Bob her Skype client searches the distributed index to determine Bobs current IP address
Because the Skype protocol is proprietary it is currently not known how the index mappings are organized across the super peers although some form of DHT organization is very possible
PP techniques are also used in Skype relays which are useful for establishing calls between hosts in home networks
Many home network configurations provide access to the Internet through NATs as discussed in Chapter 
Recall that a NAT prevents a host from outside the home network from initiating a connection to a host within the home network
If both Skype callers have NATs then there is a problemneither can accept a call initiated by the other making a call seemingly impossible
The clever use of super peers and relays nicely solves this problem
Suppose that when Alice signs in she is assigned to a nonNATed super peer and initiates a session to that super peer
Since Alice is initiating the session her NAT permits this session
This session allows Alice and her super peer to exchange control messages
The same happens for Bob when he signs in
Now when Alice wants to call Bob she informs her super peer who in turn informs Bobs super peer who in turn informs Bob of Alices incoming call
If Bob accepts the call the two super peers select a third nonNATed super peerthe relay peerwhose job will be to relay data between Alice and Bob
Alices and Bobs super peers then instruct Alice and Bob respectively to initiate a session with the relay
As shown in Figure 
Alice then sends voice packets to the relay over the Alicetorelay connection which was initiated by Alice and the relay then forwards these packets over the relaytoBob connection which was initiated by Bob packets from Bob to Alice flow over these same two relay connections in reverse
And voilaBob and Alice have an endtoend connection even though neither can accept a session originating from outside
Up to now our discussion on Skype has focused on calls involving two persons
Now lets examine multiparty audio conference calls
With N participants if each user were to send a copy of its audio stream to each of the N other users then a total of NN audio streams would need to be sent into the network to support the audio conference
To reduce this bandwidth usage Skype employs a clever distribution technique
Specifically each user sends its audio stream to the conference initiator
The conference initiator combines the audio streams into one stream basically by adding all the audio signals together and then sends a copy of each combined stream to each of the other N participants
In this manner the number of streams is reduced to N
For ordinary twoperson video conversations Skype routes the call peertopeer unless NAT traversal is required in which case the call is relayed through a nonNATed peer as described earlier
For a video conference call involving N participants due to the nature of the video medium Skype does not combine the call into one stream at one location and then redistribute the stream to all the participants as it does for voice calls
Instead each participants video stream is routed to a server cluster located in Estonia as of which in turn relays to each participant the N streams of the N other participants Zhang X 
You may be wondering why each participant sends a copy to a server rather than directly sending a copy of its video stream to each of the other N participants Indeed for both approaches NN video streams are being collectively received by the N participants in the conference
The reason is because upstream link bandwidths are significantly lower than downstream link bandwidths in most access links the upstream links may not be able to support the N streams with the PP approach
VoIP systems such as Skype WeChat and Google Talk introduce new privacy concerns
Specifically when Alice and Bob communicate over VoIP Alice can sniff Bobs IP address and then use geolocation services MaxMind Quova to determine Bobs current location and ISP for example his work or home ISP
In fact with Skype it is possible for Alice to block the transmission of certain packets during call establishment so that she obtains Bobs current IP address say every hour without Bob knowing that he is being tracked and without being on Bobs contact list
Furthermore the IP address discovered from Skype can be correlated with IP addresses found in BitTorrent so that Alice can determine the files that Bob is downloading LeBlond 
Moreover it is possible to partially decrypt a Skype call by doing a traffic analysis of the packet sizes in a stream White 
Protocols for RealTime Conversational Applications Realtime conversational applications including VoIP and video conferencing are compelling and very popular
It is therefore not surprising that standards bodies such as the IETF and ITU have been busy for many years and continue to be busy at hammering out standards for this class of applications
With the appropriate standards in place for realtime conversational applications independent companies are creating new products that interoperate with each other
In this section we examine RTP and SIP for realtime conversational applications
Both standards are enjoying widespread implementation in industry products
RTP In the previous section we learned that the sender side of a VoIP application appends header fields to the audio chunks before passing them to the transport layer
These header fields include sequence numbers and timestamps
Since most multimedia networking applications can make use of sequence numbers and timestamps it is convenient to have a standardized packet structure that includes fields for audiovideo data sequence number and timestamp as well as other potentially useful fields
RTP defined in RFC is such a standard
RTP can be used for transporting common formats such as PCM ACC and MP for sound and MPEG and H
It can also be used for transporting proprietary sound and video formats
Today RTP enjoys widespread implementation in many products and research prototypes
It is also complementary to other important realtime interactive protocols such as SIP
In this section we provide an introduction to RTP
We also encourage you to visit Henning Schulzrinnes RTP site SchulzrinneRTP which provides a wealth of information on the subject
Also you may want to visit the RAT site RAT which documents VoIP application that uses RTP
RTP Basics RTP typically runs on top of UDP
The sending side encapsulates a media chunk within an RTP packet then encapsulates the packet in a UDP segment and then hands the segment to IP
The receiving side extracts the RTP packet from the UDP segment then extracts the media chunk from the RTP packet and then passes the chunk to the media player for decoding and rendering
As an example consider the use of RTP to transport voice
Suppose the voice source is PCMencoded that is sampled quantized and digitized at kbps
Further suppose that the application collects the encoded data in msec chunks that is bytes in a chunk
The sending side precedes each chunk of the audio data with an RTP header that includes the type of audio encoding a sequence number and a timestamp
The RTP header is normally bytes
The audio chunk along with the RTP header form the RTP packet
The RTP packet is then sent into the UDP socket interface
At the receiver side the application receives the RTP packet from its socket interface
The application extracts the audio chunk from the RTP packet and uses the header fields of the RTP packet to properly decode and play back the audio chunk
If an application incorporates RTPinstead of a proprietary scheme to provide payload type sequence numbers or timestampsthen the application will more easily interoperate with other networked multimedia applications
For example if two different companies develop VoIP software and they both incorporate RTP into their product there may be some hope that a user using one of the VoIP products will be able to communicate with a user using the other VoIP product
In Section 
well see that RTP is often used in conjunction with SIP an important standard for Internet telephony
It should be emphasized that RTP does not provide any mechanism to ensure timely delivery of data or provide other qualityofservice QoS guarantees it does not even guarantee delivery of packets or prevent outoforder delivery of packets
Indeed RTP encapsulation is seen only at the end systems
Routers do not distinguish between IP datagrams that carry RTP packets and IP datagrams that dont
RTP allows each source for example a camera or a microphone to be assigned its own independent RTP stream of packets
For example for a video conference between two participants four RTP streams could be openedtwo streams for transmitting the audio one in each direction and two streams for transmitting the video again one in each direction
However many popular encoding techniquesincluding MPEG and MPEG bundle the audio and video into a single stream during the encoding process
When the audio and video are bundled by the encoder then only one RTP stream is generated in each direction
RTP packets are not limited to unicast applications
They can also be sent over onetomany and many tomany multicast trees
For a manytomany multicast session all of the sessions senders and sources typically use the same multicast group for sending their RTP streams
RTP multicast streams belonging together such as audio and video streams emanating from multiple senders in a video conference application belong to an RTP session
RTP header fields RTP Packet Header Fields As shown in Figure 
the four main RTP packet header fields are the payload type sequence number timestamp and source identifier fields
The payload type field in the RTP packet is bits long
For an audio stream the payload type field is used to indicate the type of audio encoding for example PCM adaptive delta modulation linear predictive encoding that is being used
If a sender decides to change the encoding in the middle of a session the sender can inform the receiver of the change through this payload type field
The sender may want to change the encoding in order to increase the audio quality or to decrease the RTP stream bit rate
lists some of the audio payload types currently supported by RTP
For a video stream the payload type is used to indicate the type of video encoding for example motion JPEG MPEG MPEG H
Again the sender can change video encoding on the fly during a session
lists some of the video payload types currently supported by RTP
The other important fields are the following Sequence number field
The sequence number field is bits long
The sequence number increments by one for each RTP packet sent and may be used by the receiver to detect packet loss and to restore packet sequence
For example if the receiver side of the application receives a stream of RTP packets with a gap between sequence numbers and then the receiver knows that packets and are missing
The receiver can then attempt to conceal the lost data
The timestamp field is bits long
It reflects the sampling instant of the first byte in the RTP data packet
As we saw in the preceding section the receiver can use timestamps to remove packet jitter introduced in the network and to provide synchronous playout at the receiver
The timestamp is derived from a sampling clock at the sender
As an example for audio the timestamp clock increments by one for each sampling period for example each Î¼sec for an kHz sampling clock if the audio application generates chunks consisting of encoded samples then the timestamp increases by for each RTP packet when the source is active
The timestamp clock continues to increase at a constant rate even if the source is inactive
Synchronization source identifier SSRC
The SSRC field is bits long
It identifies the source of the RTP stream
Typically each stream in an RTP session has a distinct SSRC
The SSRC is not the IP address of the sender but instead is a number that the source assigns randomly when the new stream is started
The probability that two streams get assigned the same SSRC is very small
Should this happen the two sources pick a new SSRC value
Audio payload types supported by RTP PayloadType Number Audio Format Sampling Rate Rate PCM Î¼law kHz kbps kHz 
kbps GSM kHz kbps LPC kHz 
kHz kbps MPEG Audio kHz G
kHz kbps Table 
Some video payload types supported by RTP PayloadType Number Video Format Motion JPEG H
MPEG video MPEG video 
SIP The Session Initiation Protocol SIP defined in RFC RFC is an open and lightweight protocol that does the following It provides mechanisms for establishing calls between a caller and a callee over an IP network
It allows the caller to notify the callee that it wants to start a call
It allows the participants to agree on media encodings
It also allows participants to end calls
It provides mechanisms for the caller to determine the current IP address of the callee
Users do not have a single fixed IP address because they may be assigned addresses dynamically using DHCP and because they may have multiple IP devices each with a different IP address
It provides mechanisms for call management such as adding new media streams during the call changing the encoding during the call inviting new participants during the call call transfer and call holding
Setting Up a Call to a Known IP Address To understand the essence of SIP it is best to take a look at a concrete example
In this example Alice is at her PC and she wants to call Bob who is also working at his PC
Alices and Bobs PCs are both equipped with SIPbased software for making and receiving phone calls
In this initial example well assume that Alice knows the IP address of Bobs PC
illustrates the SIP callestablishment process
In Figure 
we see that an SIP session begins when Alice sends Bob an INVITE message which resembles an HTTP request message
This INVITE message is sent over UDP to the wellknown port for SIP
SIP messages can also be sent over TCP
The INVITE message includes an identifier for Bob bob
an indication of Alices current IP address an indication that Alice desires to receive audio which is to be encoded in format AVP PCM encoded Î¼law and Figure 
SIP call establishment when Alice knows Bobs IP address encapsulated in RTP and an indication that she wants to receive the RTP packets on port 
After receiving Alices INVITE message Bob sends an SIP response message which resembles an HTTP response message
This response SIP message is also sent to the SIP port 
Bobs response includes a OK as well as an indication of his IP address his desired encoding and packetization for reception and his port number to which the audio packets should be sent
Note that in this example Alice and Bob are going to use different audioencoding mechanisms Alice is asked to encode her audio with GSM whereas Bob is asked to encode his audio with PCM Î¼law
After receiving Bobs response Alice sends Bob an SIP acknowledgment message
After this SIP transaction Bob and Alice can talk
For visual convenience Figure 
shows Alice talking after Bob but in truth they would normally talk at the same time
Bob will encode and packetize the audio as requested and send the audio packets to port number at IP address 
Alice will also encode and packetize the audio as requested and send the audio packets to port number at IP address 
From this simple example we have learned a number of key characteristics of SIP
First SIP is an out ofband protocol The SIP messages are sent and received in sockets that are different from those used for sending and receiving the media data
Second the SIP messages themselves are ASCIIreadable and resemble HTTP messages
Third SIP requires all messages to be acknowledged so it can run over UDP or TCP
In this example lets consider what would happen if Bob does not have a PCM Î¼law codec for encoding audio
In this case instead of responding with OK Bob would likely respond with a Not Acceptable and list in the message all the codecs he can use
Alice would then choose one of the listed codecs and send another INVITE message this time advertising the chosen codec
Bob could also simply reject the call by sending one of many possible rejection reply codes
There are many such codes including busy gone payment required and forbidden
SIP Addresses In the previous example Bobs SIP address is sipbob
However we expect manyif not mostSIP addresses to resemble email addresses
For example Bobs address might be sipbobdomain.com
When Alices SIP device sends an INVITE message the message would include this emaillike address the SIP infrastructure would then route the message to the IP device that Bob is currently using as well discuss below
Other possible forms for the SIP address could be Bobs legacy phone number or simply Bobs firstmiddlelast name assuming it is unique
An interesting feature of SIP addresses is that they can be included in Web pages just as peoples e mail addresses are included in Web pages with the mailto URL
For example suppose Bob has a personal homepage and he wants to provide a means for visitors to the homepage to call him
He could then simply include the URL sipbobdomain.com
When the visitor clicks on the URL the SIP application in the visitors device is launched and an INVITE message is sent to Bob
SIP Messages In this short introduction to SIP well not cover all SIP message types and headers
Instead well take a brief look at the SIP INVITE message along with a few common header lines
Let us again suppose that Alice wants to initiate a VoIP call to Bob and this time Alice knows only Bobs SIP address bobdomain.com and does not know the IP address of the device that Bob is currently using
Then her message might look something like this INVITE sipbobdomain.com SIP
Via SIP.UDP 
From sipalicehereway.com To sipbobdomain.com CallID aeapigeon.hereway.com ContentType applicationsdp ContentLength cIN IP 
maudio RTPAVP The INVITE line includes the SIP version as does an HTTP request message
Whenever an SIP message passes through an SIP device including the device that originates the message it attaches a Via header which indicates the IP address of the device
Well see soon that the typical INVITE message passes through many SIP devices before reaching the callees SIP application
Similar to an email message the SIP message includes a From header line and a To header line
The message includes a CallID which uniquely identifies the call similar to the messageID in email
It includes a ContentType header line which defines the format used to describe the content contained in the SIP message
It also includes a ContentLength header line which provides the length in bytes of the content in the message
Finally after a carriage return and line feed the message contains the content
In this case the content provides information about Alices IP address and how Alice wants to receive the audio
Name Translation and User Location In the example in Figure 
we assumed that Alices SIP device knew the IP address where Bob could be contacted
But this assumption is quite unrealistic not only because IP addresses are often dynamically assigned with DHCP but also because Bob may have multiple IP devices for example different devices for his home work and car
So now let us suppose that Alice knows only Bobs email address bobdomain.com and that this same address is used for SIPbased calls
In this case Alice needs to obtain the IP address of the device that the user bobdomain.com is currently using
To find this out Alice creates an INVITE message that begins with INVITE bobdomain.com SIP
and sends this message to an SIP proxy
The proxy will respond with an SIP reply that might include the IP address of the device that bobdomain.com is currently using
Alternatively the reply might include the IP address of Bobs voicemail box or it might include a URL of a Web page that says Bob is sleeping
Leave me alone
Also the result returned by the proxy might depend on the caller If the call is from Bobs wife he might accept the call and supply his IP address if the call is from Bobs motherin law he might respond with the URL that points to the Iamsleeping Web page Now you are probably wondering how can the proxy server determine the current IP address for bobdomain.com To answer this question we need to say a few words about another SIP device the SIP registrar
Every SIP user has an associated registrar
Whenever a user launches an SIP application on a device the application sends an SIP register message to the registrar informing the registrar of its current IP address
For example when Bob launches his SIP application on his PDA the application would send a message along the lines of REGISTER sipdomain.com SIP
Via SIP.UDP 
From sipbobdomain.com To sipbobdomain.com Expires Bobs registrar keeps track of Bobs current IP address
Whenever Bob switches to a new SIP device the new device sends a new register message indicating the new IP address
Also if Bob remains at the same device for an extended period of time the device will send refresh register messages indicating that the most recently sent IP address is still valid
In the example above refresh messages need to be sent every seconds to maintain the address at the registrar server
It is worth noting that the registrar is analogous to a DNS authoritative name server The DNS server translates fixed host names to fixed IP addresses the SIP registrar translates fixed human identifiers for example bobdomain.com to dynamic IP addresses
Often SIP registrars and SIP proxies are run on the same host
Now lets examine how Alices SIP proxy server obtains Bobs current IP address
From the preceding discussion we see that the proxy server simply needs to forward Alices INVITE message to Bobs registrarproxy
The registrarproxy could then forward the message to Bobs current SIP device
Finally Bob having now received Alices INVITE message could send an SIP response to Alice
As an example consider Figure 
in which jimumass.edu currently working on 
wants to initiate a VoiceoverIP VoIP session with keithupenn.edu currently working on 
The following steps are taken Figure 
Session initiation involving SIP proxies and registrars Jim sends an INVITE message to the umass SIP proxy
The proxy does a DNS lookup on the SIP registrar upenn.edu not shown in diagram and then forwards the message to the registrar server
Because keithupenn.edu is no longer registered at the upenn registrar the upenn registrar sends a redirect response indicating that it should try keithnyu.edu
The umass proxy sends an INVITE message to the NYU SIP registrar
The NYU registrar knows the IP address of keithupenn.edu and forwards the INVITE message to the host 
which is running Keiths SIP client
An SIP response is sent back through registrarsproxies to the SIP client on 
Media is sent directly between the two clients
There is also an SIP acknowledgment message which is not shown
Our discussion of SIP has focused on call initiation for voice calls
SIP being a signaling protocol for initiating and ending calls in general can be used for video conference calls as well as for textbased sessions
In fact SIP has become a fundamental component in many instant messaging applications
Readers desiring to learn more about SIP are encouraged to visit Henning Schulzrinnes SIP Web site SchulzrinneSIP 
In particular on this site you will find open source software for SIP clients and servers SIP Software 
Network Support for Multimedia In Sections 
we learned how applicationlevel mechanisms such as client buffering prefetching adapting media quality to available bandwidth adaptive playout and loss mitigation techniques can be used by multimedia applications to improve a multimedia applications performance
We also learned how content distribution networks and PP overlay networks can be used to provide a systemlevel approach for delivering multimedia content
These techniques and approaches are all designed to be used in todays besteffort Internet
Indeed they are in use today precisely because the Internet provides only a single besteffort class of service
But as designers of computer networks we cant help but ask whether the network rather than the applications or applicationlevel infrastructure alone might provide mechanisms to support multimedia content delivery
As well see shortly the answer is of course yes But well also see that a number of these new networklevel mechanisms have yet to be widely deployed
This may be due to their complexity and to the fact that applicationlevel techniques together with besteffort service and properly dimensioned network resources for example bandwidth can indeed provide a goodenough even if notalwaysperfect endtoend multimedia delivery service
summarizes three broad approaches towards providing networklevel support for multimedia applications
Making the best of besteffort service
The applicationlevel mechanisms and infrastructure that we studied in Sections 
can be successfully used in a welldimensioned network where packet loss and excessive endtoend delay rarely occur
When demand increases are forecasted the ISPs deploy additional bandwidth and switching capacity to continue to ensure satisfactory delay and packetloss performance Huang 
Well discuss such network dimensioning further in Section 
Since the early days of the Internet its been envisioned that different types of traffic for example as indicated in the TypeofService field in the IPv packet header could be provided with different classes of service rather than a single onesizefitsall besteffort service
With differentiated service one type of traffic might be given strict priority over another class of traffic when both types of traffic are queued at a router
For example packets belonging to a real time conversational application might be given priority over other packets due to their stringent delay constraints
Introducing differentiated service into the network will require new mechanisms for packet marking indicating a packets class of service packet scheduling and more
Well cover differentiated service and new network mechanisms needed to implement this service in Sections 
Three networklevel approaches to supporting multimedia applications Approach Granularity Guarantee Mechanisms Complexity Deployment to date Making the all traffic none or applicationlayer minimal everywhere best of best treated soft support CDNs effort service equally overlays network level resource provisioning Differentiated different none or packet marking medium some service classes of soft policing traffic scheduling treated differently Per each soft or packet marking light little connection source hard once policing Qualityof destination flow is scheduling call Service QoS flows treated admitted admission and Guarantees differently signaling Perconnection QualityofService QoS Guarantees
With perconnection QoS guarantees each instance of an application explicitly reserves endtoend bandwidth and thus has a guaranteed endtoend performance
A hard guarantee means the application will receive its requested quality of service QoS with certainty
A soft guarantee means the application will receive its requested quality of service with high probability
For example if a user wants to make a VoIP call from Host A to Host B the users VoIP application reserves bandwidth explicitly in each link along a route between the two hosts
But permitting applications to make reservations and requiring the network to honor the reservations requires some big changes
First we need a protocol that on behalf of the applications reserves link bandwidth on the paths from the senders to their receivers
Second well need new scheduling policies in the router queues so that perconnection bandwidth reservations can be honored
Finally in order to make a reservation the applications must give the network a description of the traffic that they intend to send into the network and the network will need to police each applications traffic to make sure that it abides by that description
These mechanisms when combined require new and complex software in hosts and routers
Because perconnection QoS guaranteed service has not seen significant deployment well cover these mechanisms only briefly in Section 
Dimensioning BestEffort Networks Fundamentally the difficulty in supporting multimedia applications arises from their stringent performance requirementslow endtoend packet delay delay jitter and lossand the fact that packet delay delay jitter and loss occur whenever the network becomes congested
A first approach to improving the quality of multimedia applicationsan approach that can often be used to solve just about any problem where resources are constrainedis simply to throw money at the problem and thus simply avoid resource contention
In the case of networked multimedia this means providing enough link capacity throughout the network so that network congestion and its consequent packet delay and loss never or only very rarely occurs
With enough link capacity packets could zip through todays Internet without queuing delay or loss
From many perspectives this is an ideal situationmultimedia applications would perform perfectly users would be happy and this could all be achieved with no changes to Internets besteffort architecture
The question of course is how much capacity is enough to achieve this nirvana and whether the costs of providing enough bandwidth are practical from a business standpoint to the ISPs
The question of how much capacity to provide at network links in a given topology to achieve a given level of performance is often known as bandwidth provisioning
The even more complicated problem of how to design a network topology where to place routers how to interconnect routers with links and what capacity to assign to links to achieve a given level of endtoend performance is a network design problem often referred to as network dimensioning
Both bandwidth provisioning and network dimensioning are complex topics well beyond the scope of this textbook
We note here however that the following issues must be addressed in order to predict applicationlevel performance between two network end points and thus provision enough capacity to meet an applications performance requirements
Models of traffic demand between network end points
Models may need to be specified at both the call level for example users arriving to the network and starting up endtoend applications and at the packet level for example packets being generated by ongoing applications
Note that workload may change over time
Welldefined performance requirements
For example a performance requirement for supporting delaysensitive traffic such as a conversational multimedia application might be that the probability that the endtoend delay of the packet is greater than a maximum tolerable delay be less than some small value Fraleigh 
Models to predict endtoend performance for a given workload model and techniques to find a minimal cost bandwidth allocation that will result in all user requirements being met
Here researchers are busy developing performance models that can quantify performance for a given workload and optimization techniques to find minimalcost bandwidth allocations meeting performance requirements
Given that todays besteffort Internet could from a technology standpoint support multimedia traffic at an appropriate performance level if it were dimensioned to do so the natural question is why todays Internet doesnt do so
The answers are primarily economic and organizational
From an economic standpoint would users be willing to pay their ISPs enough for the ISPs to install sufficient bandwidth to support multimedia applications over a besteffort Internet The organizational issues are perhaps even more daunting
Note that an endtoend path between two multimedia end points will pass through the networks of multiple ISPs
From an organizational standpoint would these ISPs be willing to cooperate perhaps with revenue sharing to ensure that the endtoend path is properly dimensioned to support multimedia applications For a perspective on these economic and organizational issues see Davies 
For a perspective on provisioning tier backbone networks to support delaysensitive traffic see Fraleigh 
Providing Multiple Classes of Service Perhaps the simplest enhancement to the onesizefitsall besteffort service in todays Internet is to divide traffic into classes and provide different levels of service to these different classes of traffic
For example an ISP might well want to provide a higher class of service to delaysensitive VoiceoverIP or teleconferencing traffic and charge more for this service than to elastic traffic such as email or HTTP
Alternatively an ISP may simply want to provide a higher quality of service to customers willing to pay more for this improved service
A number of residential wiredaccess ISPs and cellular wirelessaccess ISPs have adopted such tiered levels of servicewith platinumservice subscribers receiving better performance than gold or silverservice subscribers
Were all familiar with different classes of service from our everyday livesfirstclass airline passengers get better service than businessclass passengers who in turn get better service than those of us who fly economy class VIPs are provided immediate entry to events while everyone else waits in line elders are revered in some countries and provided seats of honor and the finest food at a table
Its important to note that such differential service is provided among aggregates of traffic that is among classes of traffic not among individual connections
For example all firstclass passengers are handled the same with no firstclass passenger receiving any better treatment than any other firstclass passenger just as all VoIP packets would receive the same treatment within the network independent of the particular endtoend connection to which they belong
As we will see by dealing with a small number of traffic aggregates rather than a large number of individual connections the new network mechanisms required to provide betterthanbest service can be kept relatively simple
The early Internet designers clearly had this notion of multiple classes of service in mind
Recall the typeofservice ToS field in the IPv header discussed in Chapter 
IEN ISI describes the ToS field also present in an ancestor of the IPv datagram as follows The Type of Service field provides an indication of the abstract parameters of the quality of service desired
These parameters are to be used to guide the selection of the actual service parameters when transmitting a datagram through a particular network
Several networks offer service precedence which somehow treats high precedence traffic as more important that other traffic
More than four decades ago the vision of providing different levels of service to different classes of traffic was clear However its taken us an equally long period of time to realize this vision
Motivating Scenarios Lets begin our discussion of network mechanisms for providing multiple classes of service with a few motivating scenarios
shows a simple network scenario in which two application packet flows originate on Hosts H and H on one LAN and are destined for Hosts H and H on another LAN
The routers on the two LANs are connected by a 
Lets assume the LAN speeds are significantly higher than 
Mbps and focus on the output queue of router R it is here that packet delay and packet loss will occur if the aggregate sending rate of H and H exceeds 
Lets further suppose that a Mbps audio application for example a CDquality audio call shares the Figure 
Competing audio and HTTP applications 
Mbps link between R and R with an HTTP Webbrowsing application that is downloading a Web page from H to H
In the besteffort Internet the audio and HTTP packets are mixed in the output queue at R and typically transmitted in a firstinfirstout FIFO order
In this scenario a burst of packets from the Web server could potentially fill up the queue causing IP audio packets to be excessively delayed or lost due to buffer overflow at R
How should we solve this potential problem Given that the HTTP Web browsing application does not have time constraints our intuition might be to give strict priority to audio packets at R
Under a strict priority scheduling discipline an audio packet in the R output buffer would always be transmitted before any HTTP packet in the R output buffer
The link from R to R would look like a dedicated link of 
Mbps to the audio traffic with HTTP traffic using the RtoR link only when no audio traffic is queued
In order for R to distinguish between the audio and HTTP packets in its queue each packet must be marked as belonging to one of these two classes of traffic
This was the original goal of the typeofservice ToS field in IPv
As obvious as this might seem this then is our first insight into mechanisms needed to provide multiple classes of traffic Insight Packet marking allows a router to distinguish among packets belonging to different classes of traffic
Note that although our example considers a competing multimedia and elastic flow the same insight applies to the case that platinum gold and silver classes of service are implementeda packet marking mechanism is still needed to indicate that class of service to which a packet belongs
Now suppose that the router is configured to give priority to packets marked as belonging to the Mbps audio application
Since the outgoing link speed is 
Mbps even though the HTTP packets receive lower priority they can still on average receive 
Mbps of transmission service
But what happens if the audio application starts sending packets at a rate of 
Mbps or higher either maliciously or due to an error in the application In this case the HTTP packets will starve that is they will not receive any service on the RtoR link
Similar problems would occur if multiple applications for example multiple audio calls all with the same class of service as the audio application were sharing the links bandwidth they too could collectively starve the FTP session
Ideally one wants a degree of isolation among classes of traffic so that one class of traffic can be protected from the other
This protection could be implemented at different places in the networkat each and every router at first entry to the network or at interdomain network boundaries
This then is our second insight Insight It is desirable to provide a degree of traffic isolation among classes so that one class is not adversely affected by another class of traffic that misbehaves
Well examine several specific mechanisms for providing such isolation among traffic classes
We note here that two broad approaches can be taken
First it is possible to perform traffic policing as shown in Figure 
If a traffic class or flow must meet certain criteria for example that the audio flow not exceed a peak rate of Mbps then a policing mechanism can be put into place to ensure that these criteria are indeed observed
If the policed application misbehaves the policing mechanism will take some action for example drop or delay packets that are in violation of the criteria so that the traffic actually entering the network conforms to the criteria
The leaky bucket mechanism that well examine shortly is perhaps the most widely used policing mechanism
In Figure 
the packet classification and marking mechanism Insight and the policing mechanism Insight are both implemented together at the networks edge either in the end system or at an edge router
A complementary approach for providing isolation among traffic classes is for the linklevel packet scheduling mechanism to explicitly allocate a fixed amount of link bandwidth to each class
For example the audio class could be allocated Mbps at R and the HTTP class could be allocated 
In this case the audio and Figure 
Policing and marking the audio and HTTP traffic classes Figure 
Logical isolation of audio and HTTP traffic classes HTTP flows see a logical link with capacity 
Mbps respectively as shown in Figure 
With strict enforcement of the linklevel allocation of bandwidth a class can use only the amount of bandwidth that has been allocated in particular it cannot utilize bandwidth that is not currently being used by others
For example if the audio flow goes silent for example if the speaker pauses and generates no audio packets the HTTP flow would still not be able to transmit more than 
Mbps over the RtoR link even though the audio flows Mbps bandwidth allocation is not being used at that moment
Since bandwidth is a useitorloseit resource there is no reason to prevent HTTP traffic from using bandwidth not used by the audio traffic
Wed like to use bandwidth as efficiently as possible never wasting it when it could be otherwise used
This gives rise to our third insight Insight While providing isolation among classes or flows it is desirable to use resources for example link bandwidth and buffers as efficiently as possible
Recall from our discussion in Sections 
that packets belonging to various network flows are multiplexed and queued for transmission at the output buffers associated with a link
The manner in which queued packets are selected for transmission on the link is known as the linkscheduling discipline and was discussed in detail in Section 
Recall that in Section 
three linkscheduling disciplines were discussed namely FIFO priority queuing and Weighted Fair Queuing WFQ
Well see soon see that WFQ will play a particularly important role for isolating the traffic classes
The Leaky Bucket One of our earlier insights was that policing the regulation of the rate at which a class or flow we will assume the unit of policing is a flow in our discussion below is allowed to inject packets into the network is an important QoS mechanism
But what aspects of a flows packet rate should be policed We can identify three important policing criteria each differing from the other according to the time scale over which the packet flow is policed Average rate
The network may wish to limit the longterm average rate packets per time interval at which a flows packets can be sent into the network
A crucial issue here is the interval of time over which the average rate will be policed
A flow whose average rate is limited to packets per second is more constrained than a source that is limited to packets per minute even though both have the same average rate over a long enough interval of time
For example the latter constraint would allow a flow to send packets in a given secondlong interval of time while the former constraint would disallow this sending behavior
While the averagerate constraint limits the amount of traffic that can be sent into the network over a relatively long period of time a peakrate constraint limits the maximum number of packets that can be sent over a shorter period of time
Using our example above the network may police a flow at an average rate of packets per minute while limiting the flows peak rate to packets per second
The network may also wish to limit the maximum number of packets the burst of packets that can be sent into the network over an extremely short interval of time
In the limit as the interval length approaches zero the burst size limits the number of packets that can be instantaneously sent into the network
Even though it is physically impossible to instantaneously send multiple packets into the network after all every link has a physical transmission rate that cannot be exceeded the abstraction of a maximum burst size is a useful one
The leaky bucket mechanism is an abstraction that can be used to characterize these policing limits
As shown in Figure 
a leaky bucket consists of a bucket that can hold up to b tokens
Tokens are added to this bucket as follows
New tokens which may potentially be added to the bucket are always being generated at a rate of r tokens per second
We assume here for simplicity that the unit of time is a second
If the bucket is filled with less than b tokens when a token is generated the newly generated token is added to the bucket otherwise the newly generated token is ignored and the token bucket remains full with b tokens
Let us now consider how the leaky bucket can be used to police a packet flow
Suppose that before a packet is transmitted into the network it must first remove a token from the token bucket
If the token bucket is empty the packet must wait for Figure 
The leaky bucket policer a token
An alternative is for the packet to be dropped although we will not consider that option here
Let us now consider how this behavior polices a traffic flow
Because there can be at most b tokens in the bucket the maximum burst size for a leakybucketpoliced flow is b packets
Furthermore because the token generation rate is r the maximum number of packets that can enter the network of any interval of time of length t is rtb
Thus the tokengeneration rate r serves to limit the longterm average rate at which packets can enter the network
It is also possible to use leaky buckets specifically two leaky buckets in series to police a flows peak rate in addition to the longterm average rate see the homework problems at the end of this chapter
Leaky Bucket Weighted Fair Queuing Provable Maximum Delay in a Queue Lets close our discussion on policing by showing how the leaky bucket and WFQ can be combined to provide a bound on the delay through a routers queue
Readers who have forgotten about WFQ are encouraged to review WFQ which is covered in Section 
Lets consider a routers output link that multiplexes n flows each policed by a leaky bucket with parameters bi and riin using WFQ scheduling
We use the term flow here loosely to refer to the set of packets that are not distinguished from each other by the scheduler
In practice a flow might be comprised of traffic from a single endto end connection or a collection of many such connections see Figure 
Recall from our discussion of WFQ that each flow i is guaranteed to receive a share of the link bandwidth equal to at least Rwi wj where R is the transmission Figure 
n multiplexed leaky bucket flows with WFQ scheduling rate of the link in packetssec
What then is the maximum delay that a packet will experience while waiting for service in the WFQ that is after passing through the leaky bucket Let us focus on flow 
Suppose that flow s token bucket is initially full
A burst of b packets then arrives to the leaky bucket policer for flow 
These packets remove all of the tokens without wait from the leaky bucket and then join the WFQ waiting area for flow 
Since these b packets are served at a rate of at least Rwi wj packetsec the last of these packets will then have a maximum delay dmax until its transmission is completed where dmaxbRw wj The rationale behind this formula is that if there are b packets in the queue and packets are being serviced removed from the queue at a rate of at least Rw wj packets per second then the amount of time until the last bit of the last packet is transmitted cannot be more than bRw wj
A homework problem asks you to prove that as long as rRw wj then dmax is indeed the maximum delay that any packet in flow will ever experience in the WFQ queue
Diffserv Having seen the motivation insights and specific mechanisms for providing multiple classes of service lets wrap up our study of approaches toward proving multiple classes of service with an examplethe Internet Diffserv architecture RFC Kilkki 
Diffserv provides service differentiationthat is the ability to handle different classes of traffic in different ways within the Internet in a scalable manner
The need for scalability arises from the fact that millions of simultaneous sourcedestination traffic flows may be present at a backbone router
Well see shortly that this need is met by placing only simple functionality within the network core with more complex control operations being implemented at the networks edge
Lets begin with the simple network shown in Figure 
Well describe one possible use of Diffserv here other variations are possible as described in RFC 
The Diffserv architecture consists of two sets of functional elements Edge functions Packet classification and traffic conditioning
At the incoming edge of the network that is at either a Diffservcapable host that generates traffic or at the first Diffservcapable router that the traffic passes through arriving packets are marked
More specifically the differentiated service DS field in the IPv or IPv packet header is set to some value RFC 
The definition of the DS field is intended to supersede the earlier definitions of the IPv typeof service field and the IPv traffic class fields that we discussed in Chapter 
For example in Figure 
packets being sent from H to H might be marked at R while packets being sent from H to H might be marked at R
The mark that a packet receives identifies the class of traffic to which it belongs
Different classes of traffic will then receive different service within the core network
A simple Diffserv network example Core function Forwarding
When a DSmarked packet arrives at a Diffservcapable router the packet is forwarded onto its next hop according to the socalled perhop behavior PHB associated with that packets class
The perhop behavior influences how a routers buffers and link bandwidth are shared among the competing classes of traffic
A crucial tenet of the Diffserv architecture is that a routers perhop behavior will be based only on packet markings that is the class of traffic to which a packet belongs
Thus if packets being sent from H to H in Figure 
receive the same marking as packets being sent from H to H then the network routers treat these packets as an aggregate without distinguishing whether the packets originated at H or H
For example R would not distinguish between packets from H and H when forwarding these packets on to R
Thus the Diffserv architecture obviates the need to keep router state for individual source destination pairsa critical consideration in making Diffserv scalable
An analogy might prove useful here
At many largescale social events for example a large public reception a large dance club or discothÃ¨que a concert or a football game people entering the event receive a pass of one type or another VIP passes for Very Important People over passes for people who are years old or older for example if alcoholic drinks are to be served backstage passes at concerts press passes for reporters even an ordinary pass for the Ordinary Person
These passes are typically distributed upon entry to the event that is at the edge of the event
It is here at the edge where computationally intensive operations such as paying for entry checking for the appropriate type of invitation and matching an invitation against a piece of identification are performed
Furthermore there may be a limit on the number of people of a given type that are allowed into an event
If there is such a limit people may have to wait before entering the event
Once inside the event ones pass allows one to receive differentiated service at many locations around the eventa VIP is provided with free drinks a better table free food entry to exclusive rooms and fawning service
Conversely an ordinary person is excluded from certain areas pays for drinks and receives only basic service
In both cases the service received within the event depends solely on the type of ones pass
Moreover all people within a class are treated alike
provides a logical view of the classification and marking functions within the edge router
Packets arriving to the edge router are first classified
The classifier selects packets based on the values of one or more packet header fields for example source address destination address source port destination port and protocol ID and steers the packet to the appropriate marking function
As noted above a packets marking is carried in the DS field in the packet header
In some cases an end user may have agreed to limit its packetsending rate to conform to a declared traffic profile
The traffic profile might contain a limit on the peak rate as well as the burstiness of the packet flow as we saw previously with the leaky bucket mechanism
As long as the user sends packets into the network in a way that conforms to the negotiated traffic profile the packets receive their priority Figure 
A simple Diffserv network example marking and are forwarded along their route to the destination
On the other hand if the traffic profile is violated outofprofile packets might be marked differently might be shaped for example delayed so that a maximum rate constraint would be observed or might be dropped at the network edge
The role of the metering function shown in Figure 
is to compare the incoming packet flow with the negotiated traffic profile and to determine whether a packet is within the negotiated traffic profile
The actual decision about whether to immediately remark forward delay or drop a packet is a policy issue determined by the network administrator and is not specified in the Diffserv architecture
So far we have focused on the marking and policing functions in the Diffserv architecture
The second key component of the Diffserv architecture involves the perhop behavior PHB performed by Diffserv capable routers
PHB is rather cryptically but carefully defined as a description of the externally observable forwarding behavior of a Diffserv node applied to a particular Diffserv behavior aggregate RFC 
Digging a little deeper into this definition we can see several important considerations embedded within A PHB can result in different classes of traffic receiving different performance that is different externally observable forwarding behaviors
While a PHB defines differences in performance behavior among classes it does not mandate any particular mechanism for achieving these behaviors
As long as the externally observable performance criteria are met any implementation mechanism and any bufferbandwidth allocation policy can be used
For example a PHB would not require that a particular packetqueuing discipline for example a priority queue versus a WFQ queue versus a FCFS queue be used to achieve a particular behavior
The PHB is the end to which resource allocation and implementation mechanisms are the means
Differences in performance must be observable and hence measurable
Two PHBs have been defined an expedited forwarding EF PHB RFC and an assured forwarding AF PHB RFC 
The expedited forwarding PHB specifies that the departure rate of a class of traffic from a router must equal or exceed a configured rate
The assured forwarding PHB divides traffic into four classes where each AF class is guaranteed to be provided with some minimum amount of bandwidth and buffering
Lets close our discussion of Diffserv with a few observations regarding its service model
First we have implicitly assumed that Diffserv is deployed within a single administrative domain but typically an end toend service must be fashioned from multiple ISPs sitting between communicating end systems
In order to provide endtoend Diffserv service all the ISPs between the end systems must not only provide this service but most also cooperate and make settlements in order to offer end customers true endtoend service
Without this kind of cooperation ISPs directly selling Diffserv service to customers will find themselves repeatedly saying Yes we know you paid extra but we dont have a service agreement with the ISP that dropped and delayed your traffic
Im sorry that there were so many gaps in your VoIP call Second if Diffserv were actually in place and the network ran at only moderate load most of the time there would be no perceived difference between a besteffort service and a Diffserv service
Indeed endtoend delay is usually dominated by access rates and router hops rather than by queuing delays in the routers
Imagine the unhappy Diffserv customer who has paid more for premium service but finds that the besteffort service being provided to others almost always has the same performance as premium service 
PerConnection QualityofService QoS Guarantees Resource Reservation and Call Admission In the previous section we have seen that packet marking and policing traffic isolation and linklevel scheduling can provide one class of service with better performance than another
Under certain scheduling disciplines such as priority scheduling the lower classes of traffic are essentially invisible to the highestpriority class of traffic
With proper network dimensioning the highest class of service can indeed achieve extremely low packet loss and delayessentially circuitlike performance
But can the network guarantee that an ongoing flow in a highpriority traffic class will continue to receive such service throughout the flows duration using only the mechanisms that we have described so far It cannot
In this section well see why yet additional network mechanisms and protocols are required when a hard service guarantee is provided to individual connections
Lets return to our scenario from Section 
and consider two Mbps audio applications transmitting their packets over the 
Mbps link as shown in Figure 
The combined data rate of the two flows Mbps exceeds the link capacity
Even with classification and marking isolation of flows and sharing of unused bandwidth of which there is none this is clearly a losing proposition
There is simply not enough bandwidth to accommodate the needs of both applications at Figure 
Two competing audio applications overloading the RtoR link the same time
If the two applications equally share the bandwidth each application would lose percent of its transmitted packets
This is such an unacceptably low QoS that both audio applications are completely unusable theres no need even to transmit any audio packets in the first place
Given that the two applications in Figure 
cannot both be satisfied simultaneously what should the network do Allowing both to proceed with an unusable QoS wastes network resources on application flows that ultimately provide no utility to the end user
The answer is hopefully clearone of the application flows should be blocked that is denied access to the network while the other should be allowed to proceed on using the full Mbps needed by the application
The telephone network is an example of a network that performs such call blockingif the required resources an endtoend circuit in the case of the telephone network cannot be allocated to the call the call is blocked prevented from entering the network and a busy signal is returned to the user
In our example there is no gain in allowing a flow into the network if it will not receive a sufficient QoS to be considered usable
Indeed there is a cost to admitting a flow that does not receive its needed QoS as network resources are being used to support a flow that provides no utility to the end user
By explicitly admitting or blocking flows based on their resource requirements and the source requirements of alreadyadmitted flows the network can guarantee that admitted flows will be able to receive their requested QoS
Implicit in the need to provide a guaranteed QoS to a flow is the need for the flow to declare its QoS requirements
This process of having a flow declare its QoS requirement and then having the network either accept the flow at the required QoS or block the flow is referred to as the call admission process
This then is our fourth insight in addition to the three earlier insights from Section 
into the mechanisms needed to provide QoS
Insight If sufficient resources will not always be available and QoS is to be guaranteed a call admission process is needed in which flows declare their QoS requirements and are then either admitted to the network at the required QoS or blocked from the network if the required QoS cannot be provided by the network
Our motivating example in Figure 
highlights the need for several new network mechanisms and protocols if a call an endtoend flow is to be guaranteed a given quality of service once it begins Resource reservation
The only way to guarantee that a call will have the resources link bandwidth buffers needed to meet its desired QoS is to explicitly allocate those resources to the calla process known in networking parlance as resource reservation
Once resources are reserved the call has ondemand access to these resources throughout its duration regardless of the demands of all other calls
If a call reserves and receives a guarantee of x Mbps of link bandwidth and never transmits at a rate greater than x the call will see loss and delayfree performance
If resources are to be reserved then the network must have a mechanism for calls to request and reserve resources
Since resources are not infinite a call making a call admission request will be denied admission that is be blocked if the requested resources are not available
Such a call admission is performed by the telephone networkwe request resources when we dial a number
If the circuits TDMA slots needed to complete the call are available the circuits are allocated and the call is completed
If the circuits are not available then the call is blocked and we receive a busy signal
A blocked call can try again to gain admission to the network but it is not allowed to send traffic into the network until it has successfully completed the call admission process
Of course a router that allocates link bandwidth should not allocate more than is available at that link
Typically a call may reserve only a fraction of the links bandwidth and so a router may allocate link bandwidth to more than one call
However the sum of the allocated bandwidth to all calls should be less than the link capacity if hard quality of service guarantees are to be provided
Call setup signaling
The call admission process described above requires that a call be able to reserve sufficient resources at each and every network router on its sourcetodestination path to ensure that its endtoend QoS requirement is met
Each router must determine the local resources required by the session consider the amounts of its resources that are already committed to other ongoing sessions and determine whether it has sufficient resources to satisfy the perhop QoS requirement of the session at this router without violating local QoS guarantees made to an already admitted session
A signaling protocol is needed to coordinate these various activitiesthe perhop allocation of local resources as well as the overall endtoend decision of whether or not the call has been able to reserve suf Figure 
The call setup process ficient resources at each and every router on the endtoend path
This is the job of the call setup protocol as shown in Figure 
The RSVP protocol Zhang RFC was proposed for this purpose within an Internet architecture for providing qualityofservice guarantees
In ATM networks the Qb protocol Black carries this information among the ATM networks switches and end point
Despite a tremendous amount of research and development and even products that provide for per connection quality of service guarantees there has been almost no extended deployment of such services
There are many possible reasons
First and foremost it may well be the case that the simple applicationlevel mechanisms that we studied in Sections 
combined with proper network dimensioning Section 
provide good enough besteffort network service for multimedia applications
In addition the added complexity and cost of deploying and managing a network that provides perconnection quality of service guarantees may be judged by ISPs to be simply too high given predicted customer revenues for that service
Summary Multimedia networking is one of the most exciting developments in the Internet today
People throughout the world less and less time in front of their televisions and are instead use their smartphones and devices to receive audio and video transmissions both live and prerecorded
Moreover with sites like YouTube users have become producers as well as consumers of multimedia Internet content
In addition to video distribution the Internet is also being used to transport phone calls
In fact over the next years the Internet along with wireless Internet access may make the traditional circuit switched telephone system a thing of the past
VoIP not only provides phone service inexpensively but also provides numerous valueadded services such as video conferencing online directory services voice messaging and integration into social networks such as Facebook and WeChat
In Section 
we described the intrinsic characteristics of video and voice and then classified multimedia applications into three categories i streaming stored audiovideo ii conversational voicevideooverIP and iii streaming live audiovideo
In Section 
we studied streaming stored video in some depth
For streaming video applications prerecorded videos are placed on servers and users send requests to these servers to view the videos on demand
We saw that streaming video systems can be classified into two categories UDP streaming and HTTP
We observed that the most important performance measure for streaming video is average throughput
In Section 
we examined how conversational multimedia applications such as VoIP can be designed to run over a besteffort network
For conversational multimedia timing considerations are important because conversational applications are highly delaysensitive
On the other hand conversational multimedia applications are losstolerantoccasional loss only causes occasional glitches in audiovideo playback and these losses can often be partially or fully concealed
We saw how a combination of client buffers packet sequence numbers and timestamps can greatly alleviate the effects of networkinduced jitter
We also surveyed the technology behind Skype one of the leading voice and videooverIP companies
In Section 
we examined two of the most important standardized protocols for VoIP namely RTP and SIP
In Section 
we introduced how several network mechanisms linklevel scheduling disciplines and traffic policing can be used to provide differentiated service among several classes of traffic
Homework Problems and Questions Chapter Review Questions SECTION 
Reconstruct Table 
for when Victor Video is watching a Mbps video Facebook Frank is looking at a new Kbyte image every seconds and Martha Music is listening to kbps audio stream
There are two types of redundancy in video
Describe them and discuss how they can be exploited for efficient compression
Suppose an analog audio signal is sampled times per second and each sample is quantized into one of levels
What would be the resulting bit rate of the PCM digital audio signal R
Multimedia applications can be classified into three categories
Name and describe each category
Streaming video systems can be classified into three categories
Name and briefly describe each of these categories
List three disadvantages of UDP streaming
With HTTP streaming are the TCP receive buffer and the clients application buffer the same thing If not how do they interact R
Consider the simple model for HTTP streaming
Suppose the server sends bits at a constant rate of Mbps and playback begins when million bits have been received
What is the initial buffering delay tp SECTION 
What is the difference between endtoend delay and packet jitter What are the causes of packet jitter R
Why is a packet that is received after its scheduled playout time considered lost R
describes two FEC schemes
Briefly summarize them
Both schemes increase the transmission rate of the stream by adding overhead
Does interleaving also increase the transmission rate SECTION 
How are different RTP streams in different sessions identified by a receiver How are different streams from within the same session identified R
What is the role of a SIP registrar How is the role of an SIP registrar different from that of a home agent in Mobile IP Problems P
Consider the figure below
Similar to our discussion of Figure 
suppose that video is encoded at a fixed bit rate and thus each video block contains video frames that are to be played out over the same fixed amount of time Î
The server transmits the first video block at t the second block at tÎ the third block at tÎ and so on
Once the client begins playout each block should be played out Î time units after the previous block
Suppose that the client begins playout as soon as the first block arrives at t
In the figure below how many blocks of video including the first block will have arrived at the client in time for their playout Explain how you arrived at your answer
Suppose that the client begins playout now at tÎ
How many blocks of video including the first block will have arrived at the client in time for their playout Explain how you arrived at your answer
In the same scenario at b above what is the largest number of blocks that is ever stored in the client buffer awaiting playout Explain how you arrived at your answer
What is the smallest playout delay at the client such that every video block has arrived in time for its playout Explain how you arrived at your answer
Recall the simple model for HTTP streaming shown in Figure 
Recall that B denotes the size of the clients application buffer and Q denotes the number of bits that must be buffered before the client application begins playout
Also r denotes the video consumption rate
Assume that the server sends bits at a constant rate x whenever the client buffer is not full
Suppose that xr
As discussed in the text in this case playout will alternate between periods of continuous playout and periods of freezing
Determine the length of each continuous playout and freezing period as a function of Q r and x
Now suppose that xr
At what time ttf does the client application buffer become full P
Recall the simple model for HTTP streaming shown in Figure 
Suppose the buffer size is infinite but the server sends bits at variable rate xt
Specifically suppose xt has the following sawtooth shape
The rate is initially zero at time t and linearly climbs to H at time tT
It then repeats this pattern again and again as shown in the figure below
What is the servers average send rate b
Suppose that Q so that the client starts playback as soon as it receives a video frame
What will happen c
Now suppose Q and HTQ
Determine as a function of Q H and T the time at which playback first begins
Suppose Hr and QHT
Prove there will be no freezing after the initial playout delay
Find the smallest value of Q such that there will be no freezing after the initial playback delay
Now suppose that the buffer size B is finite
As a function of Q B T and H determine the time ttf when the client application buffer first becomes full
Recall the simple model for HTTP streaming shown in Figure 
Suppose the client application buffer is infinite the server sends at the constant rate x and the video consumption rx
rate is r with Also suppose playback begins immediately
Suppose that the user terminates the video early at time tE
At the time of termination the server stops sending bits if it hasnt already sent all the bits in the video
Suppose the video is infinitely long
How many bits are wasted that is sent but not viewed b
Suppose the video is T seconds long with TE
How many bits are wasted that is sent but not viewed P
Consider a DASH system as discussed in Section 
for which there are N video versions at N different rates and qualities and N audio versions at N different rates and qualities
Suppose we want to allow the player to choose at any time any of the N video versions and any of the N audio versions
If we create files so that the audio is mixed in with the video so server sends only one media stream at given time how many files will the server need to store each a different URL b
If the server instead sends the audio and video streams separately and has the client synchronize the streams how many files will the server need to store P
In the VoIP example in Section 
let h be the total number of header bytes added to each chunk including UDP and IP header
Assuming an IP datagram is emitted every msecs find the transmission rate in bits per second for the datagrams generated by one side of this application
What is a typical value of h when RTP is used P
Consider the procedure described in Section 
for estimating average delay di
Suppose that u
Let rt be the most recent sample delay let rt be the next most recent sample delay and so on
For a given audio application suppose four packets have arrived at the receiver with sample delays rt rt rt and rt
Express the estimate of delay d in terms of the four samples
Generalize your formula for n sample delays
For the formula in part b let n approach infinity and give the resulting formula
Comment on why this averaging procedure is called an exponential moving average
Repeat parts a and b in Question P for the estimate of average delay deviation
For the VoIP example in Section 
we introduced an online procedure exponential moving average for estimating delay
In this problem we will examine an alternative procedure
Let ti be the timestamp of the ith packet received let ri be the time at which the ith packet is received
Let dn be our estimate of average delay after receiving the nth packet
After the first packet is received we set the delay estimate equal to drt
Suppose that we would like dnrtrtrntnn for all n
Give a recursive formula for dn in terms of dn rn and tn
Describe why for Internet telephony the delay estimate described in Section 
is more appropriate than the delay estimate outlined in part a
Compare the procedure described in Section 
for estimating average delay with the procedure in Section 
for estimating roundtrip time
What do the procedures have in common How are they different P
Consider the figure below which is similar to Figure 
A sender begins sending packetized audio periodically at t
The first packet arrives at the receiver at t
What are the delays from sender to receiver ignoring any playout delays of packets through Note that each vertical and horizontal line segment in the figure has a length of or time units
If audio playout begins as soon as the first packet arrives at the receiver at t which of the first eight packets sent will not arrive in time for playout c
If audio playout begins at t which of the first eight packets sent will not arrive in time for playout d
What is the minimum playout delay at the receiver that results in all of the first eight packets arriving in time for their playout P
Consider again the figure in P showing packet audio transmission and reception times
Compute the estimated delay for packets through using the formula for di from Section 
Use a value of u
Compute the estimated deviation of the delay from the estimated average for packets through using the formula for vi from Section 
Use a value of u
Recall the two FEC schemes for VoIP described in Section 
Suppose the first scheme generates a redundant chunk for every four original chunks
Suppose the second scheme uses a lowbit rate encoding whose transmission rate is percent of the transmission rate of the nominal stream
How much additional bandwidth does each scheme require How much playback delay does each scheme add b
How do the two schemes perform if the first packet is lost in every group of five packets Which scheme will have better audio quality c
How do the two schemes perform if the first packet is lost in every group of two packets Which scheme will have better audio quality P
Consider an audio conference call in Skype with N participants
Suppose each participant generates a constant stream of rate r bps
How many bits per second will the call initiator need to send How many bits per second will each of the other N participants need to send What is the total send rate aggregated over all participants b
Repeat part a for a Skype video conference call using a central server
Repeat part b but now for when each peer sends a copy of its video stream to each of the N other peers
Suppose we send into the Internet two IP datagrams each carrying a different UDP segment
The first datagram has source IP address A destination IP address B source port P and destination port T
The second datagram has source IP address A destination IP address B source port P and destination port T
Suppose that A is different from A and that P is different from P
Assuming that both datagrams reach their final destination will the two UDP datagrams be received by the same socket Why or why not b
Suppose Alice Bob and Claire want to have an audio conference call using SIP and RTP
For Alice to send and receive RTP packets to and from Bob and Claire is only one UDP socket sufficient in addition to the socket needed for the SIP messages If yes then how does Alices SIP client distinguish between the RTP packets received from Bob and Claire P
True or false a
If stored video is streamed directly from a Web server to a media player then the application is using TCP as the underlying transport protocol
When using RTP it is possible for a sender to change encoding in the middle of a session
All applications that use RTP must use port 
If an RTP session has a separate audio and video stream for each sender then the audio and video streams use the same SSRC
In differentiated services while perhop behavior defines differences in performance among classes it does not mandate any particular mechanism for achieving these performances
Suppose Alice wants to establish an SIP session with Bob
In her INVITE message she includes the line maudio RTPAVP AVP denotes GSM audio
Alice has therefore indicated in this message that she wishes to send GSM audio
Referring to the preceding statement Alice has indicated in her INVITE message that she will send audio to port 
SIP messages are typically sent between SIP entities using a default SIP port number
In order to maintain registration SIP clients must periodically send REGISTER messages
SIP mandates that all SIP clients support G
Consider the figure below which shows a leaky bucket policer being fed by a stream of packets
The token buffer can hold at most two tokens and is initially full at t
New tokens arrive at a rate of one token per slot
The output link speed is such that if two packets obtain tokens at the beginning of a time slot they can both go to the output link in the same slot
The timing details of the system are as follows A
Packets if any arrive at the beginning of the slot
Thus in the figure packets and arrive in slot 
If there are already packets in the queue then the arriving packets join the end of the queue
Packets proceed towards the front of the queue in a FIFO manner
After the arrivals have been added to the queue if there are any queued packets one or two of those packets depending on the number of available tokens will each remove a token from the token buffer and go to the output link during that slot
Thus packets and each remove a token from the buffer since there are initially two tokens and go to the output link during slot 
A new token is added to the token buffer if it is not full since the token generation rate is r tokenslot
Time then advances to the next time slot and these steps repeat
Answer the following questions a
For each time slot identify the packets that are in the queue and the number of tokens in the bucket immediately after the arrivals have been processed step above but before any of the packets have passed through the queue and removed a token
Thus for the t time slot in the example above packets and are in the queue and there are two tokens in the buffer
For each time slot indicate which packets appear on the output after the tokens have been removed from the queue
Thus for the t time slot in the example above packets and appear on the output link from the leaky buffer during slot 
Repeat P but assume that r
Assume again that the bucket is initially full
Consider P and suppose now that r and that b as before
Will your answer to the question above change P
Consider the leaky bucket policer that polices the average rate and burst size of a packet flow
We now want to police the peak rate p as well
Show how the output of this leaky bucket policer can be fed into a second leaky bucket policer so that the two leaky buckets in series police the average rate peak rate and burst size
Be sure to give the bucket size and token generation rate for the second policer
A packet flow is said to conform to a leaky bucket specification r b with burst size b and average rate r if the number of packets that arrive to the leaky bucket is less than rtb packets in every interval of time of length t for all t
Will a packet flow that conforms to a leaky bucket specification r b ever have to wait at a leaky bucket policer with parameters r and b Justify your answer
Show that as long as rRw wj then dmax is indeed the maximum delay that any packet in flow will ever experience in the WFQ queue
Programming Assignment In this lab you will implement a streaming video server and client
The client will use the realtime streaming protocol RTSP to control the actions of the server
The server will use the realtime protocol RTP to packetize the video for transport over UDP
You will be given Python code that partially implements RTSP and RTP at the client and server
Your job will be to complete both the client and server code
When you are finished you will have created a clientserver application that does the following The client sends SETUP PLAY PAUSE and TEARDOWN RTSP commands and the server responds to the commands
When the server is in the playing state it periodically grabs a stored JPEG frame packetizes the frame with RTP and sends the RTP packet into a UDP socket
The client receives the RTP packets removes the JPEG frames decompresses the frames and renders the frames on the clients monitor
The code you will be given implements the RTSP protocol in the server and the RTP depacketization in the client
The code also takes care of displaying the transmitted video
You will need to implement RTSP in the client and RTP server
This programming assignment will significantly enhance the students understanding of RTP RTSP and streaming video
It is highly recommended
The assignment also suggests a number of optional exercises including implementing the RTSP DESCRIBE command at both client and server
You can find full details of the assignment as well as an overview of the RTSP protocol at the Web site www.pearsonhighered.comcsresources
AN INTERVIEW WITH 
Henning Schulzrinne Henning Schulzrinne is a professor chair of the Department of Computer Science and head of the Internet RealTime Laboratory at Columbia University
He is the coauthor of RTP RTSP SIP and GISTkey protocols for audio and video communications over the Internet
Henning received his BS in electrical and industrial engineering at TU Darmstadt in Germany his MS in electrical and computer engineering at the University of Cincinnati and his PhD in electrical engineering at the University of Massachusetts Amherst
What made you decide to specialize in multimedia networking This happened almost by accident
As a PhD student I got involved with DARTnet an experimental network spanning the United States with T lines
DARTnet was used as a proving ground for multicast and Internet realtime tools
That led me to write my first audio tool NeVoT
Through some of the DARTnet participants I became involved in the IETF in the thennascent Audio Video Transport working group
This group later ended up standardizing RTP
What was your first job in the computer industry What did it entail My first job in the computer industry was soldering together an Altair computer kit when I was a high school student in Livermore California
Back in Germany I started a little consulting company that devised an address management program for a travel agencystoring data on cassette tapes for our TRS and using an IBM Selectric typewriter with a homebrew hardware interface as a printer
My first real job was with ATT Bell Laboratories developing a network emulator for constructing experimental networks in a lab environment
What are the goals of the Internet RealTime Lab Our goal is to provide components and building blocks for the Internet as the single future communications infrastructure
This includes developing new protocols such as GIST for networklayer signaling and LoST for finding resources by location or enhancing protocols that we have worked on earlier such as SIP through work on rich presence peertopeer systems nextgeneration emergency calling and service creation tools
Recently we have also looked extensively at wireless systems for VoIP as .b and .n networks and maybe WiMax networks are likely to become important lastmile technologies for telephony
We are also trying to greatly improve the ability of users to diagnose faults in the complicated tangle of providers and equipment using a peertopeer fault diagnosis system called DYSWIS Do You See What I See
We try to do practically relevant work by building prototypes and open source systems by measuring performance of real systems and by contributing to IETF standards
What is your vision for the future of multimedia networking We are now in a transition phase just a few years shy of when IP will be the universal platform for multimedia services from IPTV to VoIP
We expect radio telephone and TV to be available even during snowstorms and earthquakes so when the Internet takes over the role of these dedicated networks users will expect the same level of reliability
We will have to learn to design network technologies for an ecosystem of competing carriers service and content providers serving lots of technically untrained users and defending them against a small but destructive set of malicious and criminal users
Changing protocols is becoming increasingly hard
They are also becoming more complex as they need to take into account competing business interests security privacy and the lack of transparency of networks caused by firewalls and network address translators
Since multimedia networking is becoming the foundation for almost all of consumer entertainment there will be an emphasis on managing very large networks at low cost
Users will expect ease of use such as finding the same content on all of their devices
Why does SIP have a promising future As the current wireless network upgrade to G networks proceeds there is the hope of a single multimedia signaling mechanism spanning all types of networks from cable modems to corporate telephone networks and public wireless networks
Together with software radios this will make it possible in the future that a single device can be used on a home network as a cordless BlueTooth phone in a corporate network via 
and in the wide area via G networks
Even before we have such a single universal wireless device the personal mobility mechanisms make it possible to hide the differences between networks
One identifier becomes the universal means of reaching a person rather than remembering or passing around half a dozen technology or locationspecific telephone numbers
SIP also breaks apart the provision of voice bit transport from voice services
It now becomes technically possible to break apart the local telephone monopoly where one company provides neutral bit transport while others provide IP dial tone and the classical telephone services such as gateways call forwarding and caller ID
Beyond multimedia signaling SIP offers a new service that has been missing in the Internet event notification
We have approximated such services with HTTP kludges and email but this was never very satisfactory
Since events are a common abstraction for distributed systems this may simplify the construction of new services
Do you have any advice for students entering the networking field Networking bridges disciplines
It draws from electrical engineering all aspects of computer science operations research statistics economics and other disciplines
Thus networking researchers have to be familiar with subjects well beyond protocols and routing algorithms
Given that networks are becoming such an important part of everyday life students wanting to make a difference in the field should think of the new resource constraints in networks human time and effort rather than just bandwidth or storage
Work in networking research can be immensely satisfying since it is about allowing people to communicate and exchange ideas one of the essentials of being human
The Internet has become the third major global infrastructure next to the transportation system and energy distribution
Almost no part of the economy can work without highperformance networks so there should be plenty of opportunities for the foreseeable future

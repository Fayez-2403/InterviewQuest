Databases and Database Users Databases and database systems are an essential component of life in modern society most of us encounter several activities every day that involve some interaction with a database. For example if we go to the bank to deposit or withdraw funds if we make a hotel or airline reservation if we access a computerized library catalog to search for a bibliographic item or if we purchase something online such as a book toy or computer chances are that our activities will involve someone or some computer program accessing a database. Even purchasing items at a supermarket often automatically updates the database that holds the inventory of grocery items. These interactions are examples of what we may call traditional database applications in which most of the information that is stored and accessed is either textual or numeric. In the past few years advances in technology have led to exciting new applications of database systems. New media technology has made it possible to store images audio clips and video streams digitally. These types of files are becoming an important component of multimedia databases. Geographic information systems can store and analyze maps weather data and satellite images. Data warehouses and online analytical processing systems are used in many companies to extract and analyze useful business information from very large databases to support decision making. Real time and active database technology is used to control industrial and manufacturing processes. And database search techniques are being applied to the World Wide Web to improve the search for information that is needed by users browsing the Internet. To understand the fundamentals of database technology however we must start from the basics of traditional database applications. In Section we start by defining a database and then we explain other basic terms. In Section we provide a chapter Chapter Databases and Database Users simple UNIVERSITY database example to illustrate our discussion. Section describes some of the main characteristics of database systems and Sections and categorize the types of personnel whose jobs involve using and interacting with database systems. Sections and offer a more thorough discussion of the various capabilities provided by database systems and discuss some typical database applications. Section summarizes the chapter. The reader who desires a quick introduction to database systems can study Sections through then skip or browse through Sections through and go on to Chapter Introduction Databases and database technology have a major impact on the growing use of computers. It is fair to say that databases play a critical role in almost all areas where computers are used including business electronic commerce engineering medicine genetics law education and library science. The word database is so commonly used that we must begin by defining what a database is. Our initial definition is quite general. A database is a collection of related By data we mean known facts that can be recorded and that have implicit meaning. For example consider the names telephone numbers and addresses of the people you know. You may have recorded this data in an indexed address book or you may have stored it on a hard drive using a personal computer and software such as Microsoft Access or Excel. This collection of related data with an implicit meaning is a database. The preceding definition of database is quite general for example we may consider the collection of words that make up this page of text to be related data and hence to constitute a database. However the common use of the term database is usually more restricted. A database has the following implicit properties A database represents some aspect of the real world sometimes called the miniworld or the universe of discourse . Changes to the miniworld are reflected in the database. A database is a logically coherent collection of data with some inherent meaning. A random assortment of data cannot correctly be referred to as a database. A database is designed built and populated with data for a specific purpose. It has an intended group of users and some preconceived applications in which these users are interested. In other words a database has some source from which data is derived some degree of interaction with events in the real world and an audience that is actively will use the word data as both singular and plural as is common in database literature the context will determine whether it is singular or plural. In standard English data is used for plural and datum for singular. Introduction ested in its contents. The end users of a database may perform business transactions or events may happen that cause the information in the database to change. In order for a database to be accurate and reliable at all times it must be a true reflection of the miniworld that it represents therefore changes must be reflected in the database as soon as possible. A database can be of any size and complexity. For example the list of names and addresses referred to earlier may consist of only a few hundred records each with a simple structure. On the other hand the computerized catalog of a large library may contain half a million entries organized under different categories by primary author’s last name by subject by book title with each category organized alphabetically. A database of even greater size and complexity is maintained by the Internal Revenue Service to monitor tax forms filed by . taxpayers. If we assume that there are million taxpayers and each taxpayer files an average of five forms with approximately characters of information per form we would have a database of × × × characters of information. If the IRS keeps the past three returns of each taxpayer in addition to the current return we would have a database of × bytes gigabytes . This huge amount of information must be organized and managed so that users can search for retrieve and update the data as needed. An example of a large commercial database is . It contains data for over million books CDs videos DVDs games electronics apparel and other items. The database occupies over terabytes and is stored on different computers . About million visitors access each day and use the database to make purchases. The database is continually updated as new books and other items are added to the inventory and stock quantities are updated as purchases are transacted. About people are responsible for keeping the Amazon database up to date. A database may be generated and maintained manually or it may be computerized. For example a library card catalog is a database that may be created and maintained manually. A computerized database may be created and maintained either by a group of application programs written specifically for that task or by a database management system. We are only concerned with computerized databases in this book. A database management system is a collection of programs that enables users to create and maintain a database. The DBMS is a general purpose software system that facilitates the processes of defining constructing manipulating and sharing databases among various users and applications. Defining a database involves specifying the data types structures and constraints of the data to be stored in the database. The database definition or descriptive information is also stored by the DBMS in the form of a database catalog or dictionary it is called meta data. Constructing the database is the process of storing the data on some storage medium that is controlled by the DBMS. Manipulating a database includes functions such as querying the database to retrieve specific data updating the database to reflect changes in the Chapter Databases and Database Users miniworld and generating reports from the data. Sharing a database allows multiple users and programs to access the database simultaneously. An application program accesses the database by sending queries or requests for data to the DBMS. A typically causes some data to be retrieved a transaction may cause some data to be read and some data to be written into the database. Other important functions provided by the DBMS include protecting the database and maintaining it over a long period of time. Protection includes system protection against hardware or software malfunction and security protection against unauthorized or malicious access. A typical large database may have a life cycle of many years so the DBMS must be able to maintain the database system by allowing the system to evolve as requirements change over time. It is not absolutely necessary to use general purpose DBMS software to implement a computerized database. We could write our own set of programs to create and maintain the database in effect creating our own special purpose DBMS software. In either case whether we use a general purpose DBMS or not we usually have to deploy a considerable amount of complex software. In fact most DBMSs are very complex software systems. To complete our initial definitions we will call the database and DBMS software together a database system. Figure illustrates some of the concepts we have discussed so far. An Example Let us consider a simple example that most readers may be familiar with a UNIVERSITY database for maintaining information concerning students courses and grades in a university environment. Figure shows the database structure and a few sample data for such a database. The database is organized as five files each of which stores data records of the same The STUDENT file stores data on each student the COURSE file stores data on each course the SECTION file stores data on each section of a course the GRADEREPORT file stores the grades that students receive in the various sections they have completed and the PREREQUISITE file stores the prerequisites of each course. To define this database we must specify the structure of the records of each file by specifying the different types of data elements to be stored in each record. In Figure each STUDENT record includes data to represent the student’s Name Studentnumber Class and term query originally meaning a question or an inquiry is loosely used for all types of interactions with databases including modifying the data. use the term file informally here. At a conceptual level a file is a collection of records that may or may not be ordered. An Example Database System Users Programmers Appl cat on Programs Quer es Software to Process Quer es Programs Software to Access Stored Data Stored Database Stored Database Def n on DBMS Software Figure A simplified database system environment. Major each COURSE record includes data to represent the Coursename Coursenumber Credithours and Department and so on. We must also specify a data type for each data element within a record. For example we can specify that Name of STUDENT is a string of alphabetic characters Studentnumber of STUDENT is an integer and Grade of GRADEREPORT is a single character from the set ‘A’ ‘B’ ‘C’ ‘D’ ‘F’ ‘I’ . We may also use a coding scheme to represent the values of a data item. For example in Figure we represent the Class of a STUDENT as for freshman for sophomore for junior for senior and for graduate student. To construct the UNIVERSITY database we store data to represent each student course section grade report and prerequisite as a record in the appropriate file. Notice that records in the various files may be related. For example the record for Smith in the STUDENT file is related to two records in the GRADEREPORT file that specify Smith’s grades in two sections. Similarly each record in the PREREQUISITE file relates two course records one representing the course and the other representing the prerequisite. Most medium size and large databases include many types of records and have many relationships among the records. Chapter Databases and Database Users Name Studentnumber Class Major Sm th C S Brown C S STUDENT Coursename Coursenumber Cred thours Department Intro to Computer Sc ence C S Data Structures C S D screte Mathemat cs MATH Database C S COURSE Sect on dent er Coursenumber Semester Year Instructor Fall K ng Fall Anderson Spr ng Knuth Fall Chang Fall Anderson Fall Stone SECTION Studentnumber Sect on dent er Grade B C A A B A GRADEREPORT Coursenumber Prerequ s tenumber PREREQUISITE Figure A database that stores student and course information. Characteristics of the Database Approach Database manipulation involves querying and updating. Examples of queries are as follows Retrieve the transcript a list of all courses and grades of ‘Smith’ List the names of students who took the section of the ‘Database’ course offered in fall and their grades in that section List the prerequisites of the ‘Database’ course Examples of updates include the following Change the class of ‘Smith’ to sophomore Create a new section for the ‘Database’ course for this semester Enter a grade of ‘A’ for ‘Smith’ in the ‘Database’ section of last semester These informal queries and updates must be specified precisely in the query language of the DBMS before they can be processed. At this stage it is useful to describe the database as a part of a larger undertaking known as an information system within any organization. The Information Technology department within a company designs and maintains an information system consisting of various computers storage systems application software and databases. Design of a new application for an existing database or design of a brand new database starts off with a phase called requirements specification and analysis. These requirements are documented in detail and transformed into a conceptual design that can be represented and manipulated using some computerized tools so that it can be easily maintained modified and transformed into a database implementation. The design is then translated to a logical design that can be expressed in a data model implemented in a commercial DBMS. The final stage is physical design during which further specifications are provided for storing and accessing the database. The database design is implemented populated with actual data and continuously maintained to reflect the state of the miniworld. Characteristics of the Database Approach A number of characteristics distinguish the database approach from the much older approach of programming with files. In traditional file processing each user defines and implements the files needed for a specific software application as part of programming the application. For example one user the grade reporting office may keep files on students and their grades. Programs to print a student’s transcript and to enter new grades are implemented as part of the application. A second user the accounting office may keep track of students’ fees and their payments. Although both users are interested in data about students each user maintains separate files and programs to manipulate these files because each requires some data not avail Chapter Databases and Database Users able from the other user’s files. This redundancy in defining and storing data results in wasted storage space and in redundant efforts to maintain common up to date data. In the database approach a single repository maintains data that is defined once and then accessed by various users. In file systems each application is free to name data elements independently. In contrast in a database the names or labels of data are defined once and used repeatedly by queries transactions and applications. The main characteristics of the database approach versus the file processing approach are the following Self describing nature of a database system Insulation between programs and data and data abstraction Support of multiple views of the data Sharing of data and multiuser transaction processing We describe each of these characteristics in a separate section. We will discuss additional characteristics of database systems in Sections through Self Describing Nature of a Database System A fundamental characteristic of the database approach is that the database system contains not only the database itself but also a complete definition or description of the database structure and constraints. This definition is stored in the DBMS catalog which contains information such as the structure of each file the type and storage format of each data item and various constraints on the data. The information stored in the catalog is called meta data and it describes the structure of the primary database is specified in two parts. The interface of an operation includes the operation name and the data types of its arguments . The implementation of the operation is specified separately and can be changed without affecting the interface. User application programs can operate on the data by invoking these operations through their names and arguments regardless of how the operations are implemented. This may be termed program operation independence. The characteristic that allows program data independence and program operation independence is called data abstraction. A DBMS provides users with a conceptual representation of data that does not include many of the details of how the data is stored or how the operations are implemented. Informally a data model is a type of data abstraction that is used to provide this conceptual representation. The data model uses logical concepts such as objects their properties and their interrelationships that may be easier for most users to understand than computer storage concepts. Hence the data model hides storage and implementation details that are not of interest to most database users. For example reconsider Figures and The internal implementation of a file may be defined by its record length the number of characters in each record and each data item may be specified by its starting byte within a record and its length in bytes. The STUDENT record would thus be represented as shown in Figure But a typical database user is not concerned with the location of each data item within a record or its length rather the user is concerned that when a reference is made to Name of STUDENT the correct value is returned. A conceptual representation of the STUDENT records is shown in Figure Many other details of file storage organization such as the access paths specified on a file can be hidden from database users by the DBMS we discuss storage details in Chapters and Data Item Name Start ng Pos on n Record Length n Characters Name Studentnumber Class Major Figure Internal storage format for a STUDENT record based on the database catalog in Figure Characteristics of the Database Approach In the database approach the detailed structure and organization of each file are stored in the catalog. Database users and application programs refer to the conceptual representation of the files and the DBMS extracts the details of file storage from the catalog when these are needed by the DBMS file access modules. Many data models can be used to provide this data abstraction to database users. A major part of this book is devoted to presenting various data models and the concepts they use to abstract the representation of data. In object oriented and object relational databases the abstraction process includes not only the data structure but also the operations on the data. These operations provide an abstraction of miniworld activities commonly understood by the users. For example an operation CALCULATEGPA can be applied to a STUDENT object to calculate the grade point average. Such operations can be invoked by the user queries or application programs without having to know the details of how the operations are implemented. In that sense an abstraction of the miniworld activity is made available to the user as an abstract operation. Support of Multiple Views of the Data A database typically has many users each of whom may require a different perspective or view of the database. A view may be a subset of the database or it may contain virtual data that is derived from the database files but is not explicitly stored. Some users may not need to be aware of whether the data they refer to is stored or derived. A multiuser DBMS whose users have a variety of distinct applications must provide facilities for defining multiple views. For example one user of the database of Figure may be interested only in accessing and printing the transcript of each student the view for this user is shown in Figure A second user who is interested only in checking that students have taken all the prerequisites of each course for which they register may require the view shown in Figure Sharing of Data and Multiuser Transaction Processing A multiuser DBMS as its name implies must allow multiple users to access the database at the same time. This is essential if data for multiple applications is to be integrated and maintained in a single database. The DBMS must include concurrency control software to ensure that several users trying to update the same data do so in a controlled manner so that the result of the updates is correct. For example when several reservation agents try to assign a seat on an airline flight the DBMS should ensure that each seat can be accessed by only one agent at a time for assignment to a passenger. These types of applications are generally called online transaction processing applications. A fundamental role of multiuser DBMS software is to ensure that concurrent transactions operate correctly and efficiently. The concept of a transaction has become central to many database applications. A transaction is an executing program or process that includes one or more database accesses such as reading or updating of database records. Each transaction is supposed to execute a logically correct database access if executed in its entirety without interference from other transactions. The DBMS must enforce several transaction Chapter Databases and Database Users properties. The isolation property ensures that each transaction appears to execute in isolation from other transactions even though hundreds of transactions may be executing concurrently. The atomicity property ensures that either all the database operations in a transaction are executed or none are. We discuss transactions in detail in Part The preceding characteristics are important in distinguishing a DBMS from traditional file processing software. In Section we discuss additional features that characterize a DBMS. First however we categorize the different types of people who work in a database system environment. Actors on the Scene For a small personal database such as the list of addresses discussed in Section one person typically defines constructs and manipulates the database and there is no sharing. However in large organizations many people are involved in the design use and maintenance of a large database with hundreds of users. In this section we identify the people whose jobs involve the day to day use of a large database we call them the actors on the scene. In Section we consider people who may be called workers behind the scene those who work to maintain the database system environment but who are not actively interested in the database contents as part of their daily job. Studentname Studenttranscr pt Coursenumber Grade Semester Year Sect on d Sm th C Fall B Fall Brown A Fall A Fall B Spr ng A Fall TRANSCRIPT Coursename Coursenumber Prerequ s tes Database Data Structures COURSEPREREQUISITES Figure Two views derived from the database in Figure The TRANSCRIPT view. The COURSEPREREQUISITES view. Actors on the Scene Database Administrators In any organization where many people use the same resources there is a need for a chief administrator to oversee and manage these resources. In a database environment the primary resource is the database itself and the secondary resource is the DBMS and related software. Administering these resources is the responsibility of the database administrator . The DBA is responsible for authorizing access to the database coordinating and monitoring its use and acquiring software and hardware resources as needed. The DBA is accountable for problems such as security breaches and poor system response time. In large organizations the DBA is assisted by a staff that carries out these functions. Database Designers Database designers are responsible for identifying the data to be stored in the database and for choosing appropriate structures to represent and store this data. These tasks are mostly undertaken before the database is actually implemented and populated with data. It is the responsibility of database designers to communicate with all prospective database users in order to understand their requirements and to create a design that meets these requirements. In many cases the designers are on the staff of the DBA and may be assigned other staff responsibilities after the database design is completed. Database designers typically interact with each potential group of users and develop views of the database that meet the data and processing requirements of these groups. Each view is then analyzed and integrated with the views of other user groups. The final database design must be capable of supporting the requirements of all user groups. End Users End users are the people whose jobs require access to the database for querying updating and generating reports the database primarily exists for their use. There are several categories of end users Casual end users occasionally access the database but they may need different information each time. They use a sophisticated database query language to specify their requests and are typically middle or high level managers or other occasional browsers. Naive or parametric end users make up a sizable portion of database end users. Their main job function revolves around constantly querying and updating the database using standard types of queries and updates called canned transactions that have been carefully programmed and tested. The tasks that such users perform are varied Bank tellers check account balances and post withdrawals and deposits. Reservation agents for airlines hotels and car rental companies check availability for a given request and make reservations. Chapter Databases and Database Users Employees at receiving stations for shipping companies enter package identifications via bar codes and descriptive information through buttons to update a central database of received and in transit packages. Sophisticated end users include engineers scientists business analysts and others who thoroughly familiarize themselves with the facilities of the DBMS in order to implement their own applications to meet their complex requirements. Standalone users maintain personal databases by using ready made program packages that provide easy to use menu based or graphics based interfaces. An example is the user of a tax package that stores a variety of personal financial data for tax purposes. A typical DBMS provides multiple facilities to access a database. Naive end users need to learn very little about the facilities provided by the DBMS they simply have to understand the user interfaces of the standard transactions designed and implemented for their use. Casual users learn only a few facilities that they may use repeatedly. Sophisticated users try to learn most of the DBMS facilities in order to achieve their complex requirements. Standalone users typically become very proficient in using a specific software package. System Analysts and Application Programmers System analysts determine the requirements of end users especially naive and parametric end users and develop specifications for standard canned transactions that meet these requirements. Application programmers implement these specifications as programs then they test debug document and maintain these canned transactions. Such analysts and programmers commonly referred to as software developers or software engineers should be familiar with the full range of capabilities provided by the DBMS to accomplish their tasks. Workers behind the Scene In addition to those who design use and administer a database others are associated with the design development and operation of the DBMS software and system environment. These persons are typically not interested in the database content itself. We call them the workers behind the scene and they include the following categories DBMS system designers and implementers design and implement the DBMS modules and interfaces as a software package. A DBMS is a very complex software system that consists of many components or modules including modules for implementing the catalog query language processing interface processing accessing and buffering data controlling concurrency and handling data recovery and security. The DBMS must interface with other system software such as the operating system and compilers for various programming languages. Advantages of Using the DBMS Approach Tool developers design and implement tools the software packages that facilitate database modeling and design database system design and improved performance. Tools are optional packages that are often purchased separately. They include packages for database design performance monitoring natural language or graphical interfaces prototyping simulation and test data generation. In many cases independent software vendors develop and market these tools. Operators and maintenance personnel are responsible for the actual running and maintenance of the hardware and software environment for the database system. Although these categories of workers behind the scene are instrumental in making the database system available to end users they typically do not use the database contents for their own purposes. Advantages of Using the DBMS Approach In this section we discuss some of the advantages of using a DBMS and the capabilities that a good DBMS should possess. These capabilities are in addition to the four main characteristics discussed in Section The DBA must utilize these capabilities to accomplish a variety of objectives related to the design administration and use of a large multiuser database. Controlling Redundancy In traditional software development utilizing file processing every user group maintains its own files for handling its data processing applications. For example consider the UNIVERSITY database example of Section here two groups of users might be the course registration personnel and the accounting office. In the traditional approach each group independently keeps files on students. The accounting office keeps data on registration and related billing information whereas the registration office keeps track of student courses and grades. Other groups may further duplicate some or all of the same data in their own files. This redundancy in storing the same data multiple times leads to several problems. First there is the need to perform a single logical update such as entering data on a new student multiple times once for each file where student data is recorded. This leads to duplication of effort. Second storage space is wasted when the same data is stored repeatedly and this problem may be serious for large databases. Third files that represent the same data may become inconsistent. This may happen because an update is applied to some of the files but not to others. Even if an update such as adding a new student is applied to all the appropriate files the data concerning the student may still be inconsistent because the updates are applied independently by each user group. For example one user group may enter a student’s birth date erroneously as whereas the other user groups may enter the correct value of Chapter Databases and Database Users Studentnumber Studentname Sect on dent er Coursenumber Grade Sm th B Sm th C Brown A Brown A Brown B Brown A GRADEREPORT Studentnumber Studentname Sect on dent er Coursenumber Grade Brown B GRADEREPORT Figure Redundant storage of Studentname and Coursename in GRADEREPORT. Consistent data. Inconsistent record. In the database approach the views of different user groups are integrated during database design. Ideally we should have a database design that stores each logical data item such as a student’s name or birth date in only one place in the database. This is known as data normalization and it ensures consistency and saves storage space . However in practice it is sometimes necessary to use controlled redundancy to improve the performance of queries. For example we may store Studentname and Coursenumber redundantly in a GRADEREPORT file . Many specialized languages and environments exist for specifying GUIs. Capabilities for providing Web GUI interfaces to a database or Webenabling a database are also quite common. Representing Complex Relationships among Data A database may include numerous varieties of data that are interrelated in many ways. Consider the example shown in Figure The record for ‘Brown’ in the STUDENT file is related to four records in the GRADEREPORT file. Similarly each section record is related to one course record and to a number of GRADEREPORT records one for each student who completed that section. A DBMS must have the capability to represent a variety of complex relationships among the data to define new relationships as they arise and to retrieve and update related data easily and efficiently. Enforcing Integrity Constraints Most database applications have certain integrity constraints that must hold for the data. A DBMS should provide capabilities for defining and enforcing these con Advantages of Using the DBMS Approach straints. The simplest type of integrity constraint involves specifying a data type for each data item. For example in Figure we specified that the value of the Class data item within each STUDENT record must be a one digit integer and that the value of Name must be a string of no more than alphabetic characters. To restrict the value of Class between and would be an additional constraint that is not shown in the current catalog. A more complex type of constraint that frequently occurs involves specifying that a record in one file must be related to records in other files. For example in Figure we can specify that every section record must be related to a course record. This is known as a referential integrity constraint. Another type of constraint specifies uniqueness on data item values such as every course record must have a unique value for Coursenumber. This is known as a key or uniqueness constraint. These constraints are derived from the meaning or semantics of the data and of the miniworld it represents. It is the responsibility of the database designers to identify integrity constraints during database design. Some constraints can be specified to the DBMS and automatically enforced. Other constraints may have to be checked by update programs or at the time of data entry. For typical large applications it is customary to call such constraints business rules. A data item may be entered erroneously and still satisfy the specified integrity constraints. For example if a student receives a grade of ‘A’ but a grade of ‘C’ is entered in the database the DBMS cannot discover this error automatically because ‘C’ is a valid value for the Grade data type. Such data entry errors can only be discovered manually and corrected later by updating the database. However a grade of ‘Z’ would be rejected automatically by the DBMS because ‘Z’ is not a valid value for the Grade data type. When we discuss each data model in subsequent chapters we will introduce rules that pertain to that model implicitly. For example in the Entity Relationship model in Chapter a relationship must involve at least two entities. Such rules are inherent rules of the data model and are automatically assumed to guarantee the validity of the model. Permitting Inferencing and Actions Using Rules Some database systems provide capabilities for defining deduction rules for inferencing new information from the stored database facts. Such systems are called deductive database systems. For example there may be complex rules in the miniworld application for determining when a student is on probation. These can be specified declaratively as rules which when compiled and maintained by the DBMS can determine all students on probation. In a traditional DBMS an explicit procedural program code would have to be written to support such applications. But if the miniworld rules change it is generally more convenient to change the declared deduction rules than to recode procedural programs. In today’s relational database systems it is possible to associate triggers with tables. A trigger is a form of a rule activated by updates to the table which results in performing some additional operations to some other tables sending messages and so on. More involved procedures to enforce rules are popularly called stored procedures they become a part of the overall database definition and are invoked appropriately when certain conditions are met. More powerful functionality is provided by active database systems which Chapter Databases and Database Users provide active rules that can automatically initiate actions when certain events and conditions occur. Additional Implications of Using the Database Approach This section discusses some additional implications of using the database approach that can benefit most organizations. Potential for Enforcing Standards. The database approach permits the DBA to define and enforce standards among database users in a large organization. This facilitates communication and cooperation among various departments projects and users within the organization. Standards can be defined for names and formats of data elements display formats report structures terminology and so on. The DBA can enforce standards in a centralized database environment more easily than in an environment where each user group has control of its own data files and software. Reduced Application Development Time. A prime selling feature of the database approach is that developing a new application such as the retrieval of certain data from the database for printing a new report takes very little time. Designing and implementing a large multiuser database from scratch may take more time than writing a single specialized file application. However once a database is up and running substantially less time is generally required to create new applications using DBMS facilities. Development time using a DBMS is estimated to be one sixth to one fourth of that for a traditional file system. Flexibility. It may be necessary to change the structure of a database as requirements change. For example a new user group may emerge that needs information not currently in the database. In response it may be necessary to add a file to the database or to extend the data elements in an existing file. Modern DBMSs allow certain types of evolutionary changes to the structure of the database without affecting the stored data and the existing application programs. Availability of Up to Date Information. A DBMS makes the database available to all users. As soon as one user’s update is applied to the database all other users can immediately see this update. This availability of up to date information is essential for many transaction processing applications such as reservation systems or banking databases and it is made possible by the concurrency control and recovery subsystems of a DBMS. Economies of Scale. The DBMS approach permits consolidation of data and applications thus reducing the amount of wasteful overlap between activities of data processing personnel in different projects or departments as well as redundancies among applications. This enables the whole organization to invest in more powerful processors storage devices or communication gear rather than having each department purchase its own equipment. This reduces overall costs of operation and management. A Brief History of Database Applications A Brief History of Database Applications We now give a brief historical overview of the applications that use DBMSs and how these applications provided the impetus for new types of database systems. Early Database Applications Using Hierarchical and Network Systems Many early database applications maintained records in large organizations such as corporations universities hospitals and banks. In many of these applications there were large numbers of records of similar structure. For example in a university application similar information would be kept for each student each course each grade record and so on. There were also many types of records and many interrelationships among them. One of the main problems with early database systems was the intermixing of conceptual relationships with the physical storage and placement of records on disk. Hence these systems did not provide sufficient data abstraction and program data independence capabilities. For example the grade records of a particular student could be physically stored next to the student record. Although this provided very efficient access for the original queries and transactions that the database was designed to handle it did not provide enough flexibility to access records efficiently when new queries and transactions were identified. In particular new queries that required a different storage organization for efficient processing were quite difficult to implement efficiently. It was also laborious to reorganize the database when changes were made to the application’s requirements. Another shortcoming of early systems was that they provided only programming language interfaces. This made it time consuming and expensive to implement new queries and transactions since new programs had to be written tested and debugged. Most of these database systems were implemented on large and expensive mainframe computers starting in the and continuing through the and The main types of early systems were based on three main paradigms hierarchical systems network model based systems and inverted file systems. Providing Data Abstraction and Application Flexibility with Relational Databases Relational databases were originally proposed to separate the physical storage of data from its conceptual representation and to provide a mathematical foundation for data representation and querying. The relational data model also introduced high level query languages that provided an alternative to programming language interfaces making it much faster to write new queries. Relational representation of data somewhat resembles the example we presented in Figure Relational systems were initially targeted to the same applications as earlier systems and provided flexibility to develop new queries quickly and to reorganize the database as requirements changed. Hence data abstraction and program data independence were much improved when compared to earlier systems. Chapter Databases and Database Users Early experimental relational systems developed in the late and the commercial relational database management systems introduced in the early were quite slow since they did not use physical storage pointers or record placement to access related data records. With the development of new storage and indexing techniques and better query processing and optimization their performance improved. Eventually relational databases became the dominant type of database system for traditional database applications. Relational databases now exist on almost all types of computers from small personal computers to large servers. Object Oriented Applications and the Need for More Complex Databases The emergence of object oriented programming languages in the and the need to store and share complex structured objects led to the development of object oriented databases . Initially OODBs were considered a competitor to relational databases since they provided more general data structures. They also incorporated many of the useful object oriented paradigms such as abstract data types encapsulation of operations inheritance and object identity. However the complexity of the model and the lack of an early standard contributed to their limited use. They are now mainly used in specialized applications such as engineering design multimedia publishing and manufacturing systems. Despite expectations that they will make a big impact their overall penetration into the database products market remains under today. In addition many object oriented concepts were incorporated into the newer versions of relational DBMSs leading to object relational database management systems known as ORDBMSs. Interchanging Data on the Web for E Commerce Using XML The World Wide Web provides a large network of interconnected computers. Users can create documents using a Web publishing language such as HyperText Markup Language and store these documents on Web servers where other users can access them. Documents can be linked through hyperlinks which are pointers to other documents. In the electronic commerce emerged as a major application on the Web. It quickly became apparent that parts of the information on e commerce Web pages were often dynamically extracted data from DBMSs. A variety of techniques were developed to allow the interchange of data on the Web. Currently eXtended Markup Language is considered to be the primary standard for interchanging data among various types of databases and Web pages. XML combines concepts from the models used in document systems with database modeling concepts. Chapter is devoted to the discussion of XML. Extending Database Capabilities for New Applications The success of database systems in traditional applications encouraged developers of other types of applications to attempt to use them. Such applications traditionally used their own specialized file and data structures. Database systems now offer A Brief History of Database Applications extensions to better support the specialized requirements for some of these applications. The following are some examples of these applications Scientific applications that store large amounts of data resulting from scientific experiments in areas such as high energy physics the mapping of the human genome and the discovery of protein structures. Storage and retrieval of images including scanned news or personal photographs satellite photographic images and images from medical procedures such as x rays and MRIs . Storage and retrieval of videos such as movies and video clips from news or personal digital cameras. Data mining applications that analyze large amounts of data searching for the occurrences of specific patterns or relationships and for identifying unusual patterns in areas such as credit card usage. Spatial applications that store spatial locations of data such as weather information maps used in geographical information systems and in automobile navigational systems. Time series applications that store information such as economic data at regular points in time such as daily sales and monthly gross national product figures. It was quickly apparent that basic relational systems were not very suitable for many of these applications usually for one or more of the following reasons More complex data structures were needed for modeling the application than the simple relational representation. New data types were needed in addition to the basic numeric and character string types. New operations and query language constructs were necessary to manipulate the new data types. New storage and indexing structures were needed for efficient searching on the new data types. This led DBMS developers to add functionality to their systems. Some functionality was general purpose such as incorporating concepts from object oriented databases into relational systems. Other functionality was special purpose in the form of optional modules that could be used for specific applications. For example users could buy a time series module to use with their relational DBMS for their time series application. Many large organizations use a variety of software application packages that work closely with database back ends. The database back end represents one or more databases possibly from different vendors and using different data models that maintain data that is manipulated by these packages for supporting transactions generating reports and answering ad hoc queries. One of the most commonly used systems includes Enterprise Resource Planning which is used to consolidate a variety of functional areas within an organization including production sales Chapter Databases and Database Users distribution marketing finance human resources and so on. Another popular type of system is Customer Relationship Management software that spans order processing as well as marketing and customer support functions. These applications are Web enabled in that internal and external users are given a variety of Webportal interfaces to interact with the back end databases. Databases versus Information Retrieval Traditionally database technology applies to structured and formatted data that arises in routine applications in government business and industry. Database technology is heavily used in manufacturing retail banking insurance finance and health care industries where structured data is collected through forms such as invoices or patient registration documents. An area related to database technology is Information Retrieval which deals with books manuscripts and various forms of library based articles. Data is indexed cataloged and annotated using keywords. IR is concerned with searching for material based on these keywords and with the many problems dealing with document processing and free form text processing. There has been a considerable amount of work done on searching for text based on keywords finding documents and ranking them based on relevance automatic text categorization classification of text documents by topics and so on. With the advent of the Web and the proliferation of HTML pages running into the billions there is a need to apply many of the IR techniques to processing data on the Web. Data on Web pages typically contains images text and objects that are active and change dynamically. Retrieval of information on the Web is a new problem that requires techniques from databases and IR to be applied in a variety of novel combinations. We discuss concepts related to information retrieval and Web search in Chapter When Not to Use a DBMS In spite of the advantages of using a DBMS there are a few situations in which a DBMS may involve unnecessary overhead costs that would not be incurred in traditional file processing. The overhead costs of using a DBMS are due to the following High initial investment in hardware software and training The generality that a DBMS provides for defining and processing data Overhead for providing security concurrency control recovery and integrity functions Therefore it may be more desirable to use regular files under the following circumstances Simple well defined database applications that are not expected to change at all Stringent real time requirements for some application programs that may not be met because of DBMS overhead Review Questions Embedded systems with limited storage capacity where a general purpose DBMS would not fit No multiple user access to data Certain industries and applications have elected not to use general purpose DBMSs. For example many computer aided design tools used by mechanical and civil engineers have proprietary file and data management software that is geared for the internal manipulations of drawings and objects. Similarly communication and switching systems designed by companies like AT&T were early manifestations of database software that was made to run very fast with hierarchically organized data for quick access and routing of calls. Similarly GIS implementations often implement their own data organization schemes for efficiently implementing functions related to processing maps physical contours lines polygons and so on. General purpose DBMSs are inadequate for their purpose. Summary In this chapter we defined a database as a collection of related data where data means recorded facts. A typical database represents some aspect of the real world and is used for specific purposes by one or more groups of users. A DBMS is a generalized software package for implementing and maintaining a computerized database. The database and software together form a database system. We identified several characteristics that distinguish the database approach from traditional fileprocessing applications and we discussed the main categories of database users or the actors on the scene. We noted that in addition to database users there are several categories of support personnel or workers behind the scene in a database environment. We presented a list of capabilities that should be provided by the DBMS software to the DBA database designers and end users to help them design administer and use a database. Then we gave a brief historical perspective on the evolution of database applications. We pointed out the marriage of database technology with information retrieval technology which will play an important role due to the popularity of the Web. Finally we discussed the overhead costs of using a DBMS and discussed some situations in which it may not be advantageous to use one. Review Questions Define the following terms data database DBMS database system database catalog program data independence user view DBA end user canned transaction deductive database system persistent object meta data and transaction processing application. What four main types of actions involve databases Briefly discuss each. Discuss the main characteristics of the database approach and how it differs from traditional file systems. Chapter Databases and Database Users What are the responsibilities of the DBA and the database designers What are the different types of database end users Discuss the main activities of each. Discuss the capabilities that should be provided by a DBMS. Discuss the differences between database systems and information retrieval systems. Exercises Identify some informal queries and update operations that you would expect to apply to the database shown in Figure What is the difference between controlled and uncontrolled redundancy Illustrate with examples. Specify all the relationships among the records of the database shown in Figure Give some additional views that may be needed by other user groups for the database shown in Figure Cite some examples of integrity constraints that you think can apply to the database shown in Figure Give examples of systems in which it may make sense to use traditional file processing instead of a database approach. Consider Figure a. If the name of the ‘CS’ Department changes to ‘CSSE’ Department and the corresponding prefix for the course number also changes identify the columns in the database that would need to be updated. b. Can you restructure the columns in the COURSE SECTION and PREREQUISITE tables so that only one column will need to be updated Selected Bibliography The October issue of Communications of the ACM and Kim include several articles describing next generation DBMSs many of the database features discussed in the former are now commercially available. The March issue of ACM Computing Surveys offers an early introduction to database systems and may provide a historical perspective for the interested reader. Database System Concepts and Architecture The architecture of DBMS packages has evolved from the early monolithic systems where the whole DBMS software package was one tightly integrated system to the modern DBMS packages that are modular in design with a client server system architecture. This evolution mirrors the trends in computing where large centralized mainframe computers are being replaced by hundreds of distributed workstations and personal computers connected via communications networks to various types of server machines Web servers database servers file servers application servers and so on. In a basic client server DBMS architecture the system functionality is distributed between two types of A client module is typically designed so that it will run on a user workstation or personal computer. Typically application programs and user interfaces that access the database run in the client module. Hence the client module handles user interaction and provides the user friendly interfaces such as forms or menu based GUIs . The other kind of module called a server module typically handles data storage access search and other functions. We discuss client server architectures in more detail in Section First we must study more basic concepts that will give us a better understanding of modern database architectures. In this chapter we present the terminology and basic concepts that will be used throughout the book. Section discusses data models and defines the concepts of schemas and instances which are fundamental to the study of database systems. Then we discuss the three schema DBMS architecture and data independence in Section this provides a user’s perspective on what a DBMS is supposed to do. In Section we describe the types of interfaces and languages that are typically provided by a DBMS. Section discusses the database system software environment. chapter we shall see in Section there are variations on this simple two tier client server architecture. Chapter Database System Concepts and Architecture Section gives an overview of various types of client server architectures. Finally Section presents a classification of the types of DBMS packages. Section summarizes the chapter. The material in Sections through provides more detailed concepts that may be considered as supplementary to the basic introductory material. Data Models Schemas and Instances One fundamental characteristic of the database approach is that it provides some level of data abstraction. Data abstraction generally refers to the suppression of details of data organization and storage and the highlighting of the essential features for an improved understanding of data. One of the main characteristics of the database approach is to support data abstraction so that different users can perceive data at their preferred level of detail. A data model a collection of concepts that can be used to describe the structure of a database provides the necessary means to achieve this By structure of a database we mean the data types relationships and constraints that apply to the data. Most data models also include a set of basic operations for specifying retrievals and updates on the database. In addition to the basic operations provided by the data model it is becoming more common to include concepts in the data model to specify the dynamic aspect or behavior of a database application. This allows the database designer to specify a set of valid user defined operations that are allowed on the database An example of a user defined operation could be COMPUTEGPA which can be applied to a STUDENT object. On the other hand generic operations to insert delete modify or retrieve any kind of object are often included in the basic data model operations. Concepts to specify behavior are fundamental to object oriented data models data models which provide concepts that may be easily understood by end users but that are not too far removed from the way data is organized in computer storage. Representational data models hide many details of data storage on disk but can be implemented on a computer system directly. Conceptual data models use concepts such as entities attributes and relationships. An entity represents a real world object or concept such as an employee or a project from the miniworld that is described in the database. An attribute represents some property of interest that further describes an entity such as the employee’s name or salary. A relationship among two or more entities represents an association among the entities for example a works on relationship between an employee and a project. Chapter presents the Entity Relationship model a popular high level conceptual data model. Chapter describes additional abstractions used for advanced modeling such as generalization specialization and categories . Representational or implementation data models are the models used most frequently in traditional commercial DBMSs. These include the widely used relational data model as well as the so called legacy data models the network and hierarchical models that have been widely used in the past. Part is devoted to the relational data model and its constraints operations and The SQL standard for relational databases is described in Chapters and Representational data models represent data by using record structures and hence are sometimes called record based data models. We can regard the object data model as an example of a new family of higher level implementation data models that are closer to conceptual data models. A standard for object databases called the ODMG object model has been proposed by the Object Data Management Group . We describe the general characteristics of object databases and the object model proposed standard in Chapter Object data models are also frequently utilized as high level conceptual models particularly in the software engineering domain. Physical data models describe how data is stored as files in the computer by representing information such as record formats record orderings and access paths. An access path is a structure that makes the search for particular database records efficient. We discuss physical storage techniques and access structures in Chapters and An index is an example of an access path that allows direct access to data using an index term or a keyword. It is similar to the index at the end of this book except that it may be organized in a linear hierarchical or some other fashion. term implementation data model is not a standard term we have introduced it to refer to the available data models in commercial database systems. summary of the hierarchical and network data models is included in Appendices D and E. They are accessible from the book’s Web site. Chapter Database System Concepts and Architecture Schemas Instances and Database State In any data model it is important to distinguish between the description of the database and the database itself. The description of a database is called the database schema which is specified during database design and is not expected to change Most data models have certain conventions for displaying schemas as A displayed schema is called a schema diagram. Figure shows a schema diagram for the database shown in Figure the diagram displays the structure of each record type but not the actual instances of records. We call each object in the schema such as STUDENT or COURSE a schema construct. A schema diagram displays only some aspects of a schema such as the names of record types and data items and some types of constraints. Other aspects are not specified in the schema diagram for example Figure shows neither the data type of each data item nor the relationships among the various files. Many types of constraints are not represented in schema diagrams. A constraint such as students majoring in computer science must take before the end of their sophomore year is quite difficult to represent diagrammatically. The actual data in a database may change quite frequently. For example the database shown in Figure changes every time we add a new student or enter a new grade. The data in the database at a particular moment in time is called a database state or snapshot. It is also called the current set of occurrences or instances in the Sectionidentifier Semester Coursenumber Instructor Year SECTION Coursename Coursenumber Credithours Department COURSE Name Studentnumber Class Major STUDENT Coursenumber Prerequisitenumber PREREQUISITE Studentnumber Grade Sectionidentifie GRADEREPORT Figure Schema diagram for the database in Figure changes are usually needed as the requirements of the database applications change. Newer database systems include operations for allowing schema changes although the schema change process is more involved than simple database updates. is customary in database parlance to use schemas as the plural for schema even though schemata is the proper plural form. The word scheme is also sometimes used to refer to a schema. Three Schema Architecture and Data Independence database. In a given database state each schema construct has its own current set of instances for example the STUDENT construct will contain the set of individual student entities as its instances. Many database states can be constructed to correspond to a particular database schema. Every time we insert or delete a record or change the value of a data item in a record we change one state of the database into another state. The distinction between database schema and database state is very important. When we define a new database we specify its database schema only to the DBMS. At this point the corresponding database state is the empty state with no data. We get the initial state of the database when the database is first populated or loaded with the initial data. From then on every time an update operation is applied to the database we get another database state. At any point in time the database has a current state. The DBMS is partly responsible for ensuring that every state of the database is a valid state that is a state that satisfies the structure and constraints specified in the schema. Hence specifying a correct schema to the DBMS is extremely important and the schema must be designed with utmost care. The DBMS stores the descriptions of the schema constructs and constraints also called the meta data in the DBMS catalog so that DBMS software can refer to the schema whenever it needs to. The schema is sometimes called the intension and a database state is called an extension of the schema. Although as mentioned earlier the schema is not supposed to change frequently it is not uncommon that changes occasionally need to be applied to the schema as the application requirements change. For example we may decide that another data item needs to be stored for each record in a file such as adding the Dateofbirth to the STUDENT schema in Figure This is known as schema evolution. Most modern DBMSs include some operations for schema evolution that can be applied while the database is operational. Three Schema Architecture and Data Independence Three of the four important characteristics of the database approach listed in Section are use of a catalog to store the database description so as to make it self describing insulation of programs and data and support of multiple user views. In this section we specify an architecture for database systems called the three schema architecture that was proposed to help achieve and visualize these characteristics. Then we discuss the concept of data independence further. current state is also called the current snapshot of the database. It has also been called a database instance but we prefer to use the term instance to refer to individual records. is also known as the ANSI SPARC architecture after the committee that proposed it . Some DBMSs allow different data models to be used at the conceptual and external levels. An example is Universal Data Base a DBMS from IBM which uses the relational model to describe the conceptual schema but may use an object oriented model to describe an external schema. Notice that the three schemas are only descriptions of data the stored data that actually exists is at the physical level only. In a DBMS based on the three schema architecture each user group refers to its own external schema. Hence the DBMS must transform a request specified on an external schema into a request against the conceptual schema and then into a request on the internal schema for processing over the stored database. If the request is a database retrieval the data extracted from the stored database must be reformatted to match the user’s external view. The processes of transforming requests and results between levels are called mappings. These mappings may be time consuming so some DBMSs especially those that are meant to support small databases do not support external views. Even in such systems however a certain amount of mapping is necessary to transform requests between the conceptual and internal levels. Data Independence The three schema architecture can be used to further explain the concept of data independence which can be defined as the capacity to change the schema at one level of a database system without having to change the schema at the next higher level. We can define two types of data independence Logical data independence is the capacity to change the conceptual schema without having to change external schemas or application programs. We may change the conceptual schema to expand the database to change constraints or to reduce the database . In the last case external schemas that refer only to the remaining data should not be affected. For example the external schema of Figure should not be affected by changing the GRADEREPORT file shown in Figure into the one shown in Figure Only the view definition and the mappings need to be changed in a DBMS that supports logical data independence. After the conceptual schema undergoes a logical reorganization application programs that reference the external schema constructs must work as before. Chapter Database System Concepts and Architecture Changes to constraints can be applied to the conceptual schema without affecting the external schemas or application programs. Physical data independence is the capacity to change the internal schema without having to change the conceptual schema. Hence the external schemas need not be changed as well. Changes to the internal schema may be needed because some physical files were reorganized for example by creating additional access structures to improve the performance of retrieval or update. If the same data as before remains in the database we should not have to change the conceptual schema. For example providing an access path to improve retrieval speed of section records is used by the DBA and by database designers to define both schemas. The DBMS will have a DDL compiler whose function is to process DDL statements in order to identify descriptions of the schema constructs and to store the schema description in the DBMS catalog. In DBMSs where a clear separation is maintained between the conceptual and internal levels the DDL is used to specify the conceptual schema only. Another language the storage definition language is used to specify the internal schema. The mappings between the two schemas may be specified in either one of these languages. In most relational DBMSs today there is no specific language that performs the role of SDL. Instead the internal schema is specified by a combination of functions parameters and specifications related to storage. These permit the DBA staff to control indexing choices and mapping of data to storage. For a true three schema architecture we would need a third language the view definition language to specify user views and their mappings to the conceptual schema but in most DBMSs the DDL is used to define both conceptual and external schemas. In relational DBMSs SQL is used in the role of VDL to define user or application views as results of predefined queries for these purposes. In current DBMSs the preceding types of languages are usually not considered distinct languages rather a comprehensive integrated language is used that includes constructs for conceptual schema definition view definition and data manipulation. Storage definition is typically kept separate since it is used for defining physical storage structures to fine tune the performance of the database system which is usually done by the DBA staff. A typical example of a comprehensive database language is the SQL relational database language that lead the user through the formulation of a request. Menus do away with the need to memorize the specific commands and syntax of a query language rather the query is composed step bystep by picking options from a menu that is displayed by the system. Pull down menus are a very popular technique in Web based user interfaces. They are also often used in browsing interfaces which allow a user to look through the contents of a database in an exploratory and unstructured manner. Forms Based Interfaces. A forms based interface displays a form to each user. Users can fill out all of the form entries to insert new data or they can fill out only certain entries in which case the DBMS will retrieve matching data for the remaining entries. Forms are usually designed and programmed for naive users as interfaces to canned transactions. Many DBMSs have forms specification languages object databases the host and data sublanguages typically form one integrated language for example C++ with some extensions to support database functionality. Some relational systems also provide integrated languages for example Oracle’s PL SQL. to the English meaning of the word query it should really be used to describe retrievals only not updates. Database Languages and Interfaces which are special languages that help programmers specify such forms. SQL Forms is a form based language that specifies queries using a form designed in conjunction with the relational database schema. Oracle Forms is a component of the Oracle product suite that provides an extensive set of features to design and build applications using forms. Some systems have utilities that define a form by letting the end user interactively construct a sample form on the screen. Graphical User Interfaces. A GUI typically displays a schema to the user in diagrammatic form. The user then can specify a query by manipulating the diagram. In many cases GUIs utilize both menus and forms. Most GUIs use a pointing device such as a mouse to select certain parts of the displayed schema diagram. Natural Language Interfaces. These interfaces accept requests written in English or some other language and attempt to understand them. A natural language interface usually has its own schema which is similar to the database conceptual schema as well as a dictionary of important words. The natural language interface refers to the words in its schema as well as to the set of standard words in its dictionary to interpret the request. If the interpretation is successful the interface generates a high level query corresponding to the natural language request and submits it to the DBMS for processing otherwise a dialogue is started with the user to clarify the request. The capabilities of natural language interfaces have not advanced rapidly. Today we see search engines that accept strings of natural language words and match them with documents at specific sites or Web pages on the Web at large . They use predefined indexes on words and use ranking functions to retrieve and present resulting documents in a decreasing degree of match. Such “free form” textual query interfaces are not yet common in structured relational or legacy model databases although a research area called keyword based querying has emerged recently for relational databases. Speech Input and Output. Limited use of speech as an input query and speech as an answer to a question or result of a request is becoming commonplace. Applications with limited vocabularies such as inquiries for telephone directory flight arrival departure and credit card account information are allowing speech for input and output to enable customers to access this information. The speech input is detected using a library of predefined words and used to set up the parameters that are supplied to the queries. For output a similar conversion from text or numbers into speech takes place. Interfaces for Parametric Users. Parametric users such as bank tellers often have a small set of operations that they must perform repeatedly. For example a teller is able to use single function keys to invoke routine and repetitive transactions such as account deposits or withdrawals or balance inquiries. Systems analysts and programmers design and implement a special interface for each known class of naive users. Usually a small set of abbreviated commands is included with the goal of minimizing the number of keystrokes required for each request. For example Chapter Database System Concepts and Architecture function keys in a terminal can be programmed to initiate various commands. This allows the parametric user to proceed with a minimal number of keystrokes. Interfaces for the DBA. Most database systems contain privileged commands that can be used only by the DBA staff. These include commands for creating accounts setting system parameters granting account authorization changing a schema and reorganizing the storage structures of a database. The Database System Environment A DBMS is a complex software system. In this section we discuss the types of software components that constitute a DBMS and the types of computer system software with which the DBMS interacts. DBMS Component Modules Figure illustrates in a simplified form the typical DBMS components. The figure is divided into two parts. The top part of the figure refers to the various users of the database environment and their interfaces. The lower part shows the internals of the DBMS responsible for storage of data and processing of transactions. The database and the DBMS catalog are usually stored on disk. Access to the disk is controlled primarily by the operating system which schedules disk read write. Many DBMSs have their own buffer management module to schedule disk read write because this has a considerable effect on performance. Reducing disk read write improves performance considerably. A higher level stored data manager module of the DBMS controls access to DBMS information that is stored on disk whether it is part of the database or the catalog. Let us consider the top part of Figure first. It shows interfaces for the DBA staff casual users who work with interactive interfaces to formulate queries application programmers who create programs using some host programming languages and parametric users who do data entry work by supplying parameters to predefined transactions. The DBA staff works on defining the database and tuning it by making changes to its definition using the DDL and other privileged commands. The DDL compiler processes schema definitions specified in the DDL and stores descriptions of the schemas in the DBMS catalog. The catalog includes information such as the names and sizes of files names and data types of data items storage details of each file mapping information among schemas and constraints. In addition the catalog stores many other types of information that are needed by the DBMS modules which can then look up the catalog information as needed. Casual users and persons with occasional need for information from the database interact using some form of interface which we call the interactive query interface in Figure We have not explicitly shown any menu based or form based interaction that may be used to generate the interactive query automatically. These queries are parsed and validated for correctness of the query syntax the names of files and The Database System Environment Query Compiler Runtime Database Processor Precompiler System Catalog Data Dictionary Quer y Optimizer DML Compiler Host Language Compiler Concurrency Control Backup Recovery Subsystems Stored Data Manager Compiled Transactions Stored Database DBA Commands Queries and Transactions Input Output Query and Transaction from Database Execution DDL Compiler DDL Statements Privileged Commands Interactive Query Application Programs DBA Staff Casual Users Application Programmers Users Parametric Users Figure Component modules of a DBMS and their interactions. data elements and so on by a query compiler that compiles them into an internal form. This internal query is subjected to query optimization operations between the disk and main memory. The runtime database processor handles other aspects of data transfer such as management of buffers in the main memory. Some DBMSs have their own buffer management module while others depend on the OS for buffer management. We have shown concurrency control and backup and recovery systems separately as a module in this figure. They are integrated into the working of the runtime database processor for purposes of transaction management. It is now common to have the client program that accesses the DBMS running on a separate computer from the computer on which the database resides. The former is called the client computer running a DBMS client software and the latter is called the database server. In some cases the client accesses a middle computer called the application server which in turn accesses the database server. We elaborate on this topic in Section Figure is not meant to describe a specific DBMS rather it illustrates typical DBMS modules. The DBMS interacts with the operating system when disk accesses to the database or to the catalog are needed. If the computer system is shared by many users the OS will schedule DBMS disk access requests and DBMS processing along with other processes. On the other hand if the computer system is mainly dedicated to running the database server the DBMS will control main memory buffering of disk pages. The DBMS also interfaces with compilers for generalpurpose host programming languages and with application servers and client programs running on separate machines through the system network interface. Database System Utilities In addition to possessing the software modules just described most DBMSs have database utilities that help the DBA manage the database system. Common utilities have the following types of functions Loading. A loading utility is used to load existing data files such as text files or sequential files into the database. Usually the current for The Database System Environment mat of the data file and the desired database file structure are specified to the utility which then automatically reformats the data and stores it in the database. With the proliferation of DBMSs transferring data from one DBMS to another is becoming common in many organizations. Some vendors are offering products that generate the appropriate loading programs given the existing source and target database storage descriptions . Such tools are also called conversion tools. For the hierarchical DBMS called IMS and for many network DBMSs including IDMS SUPRA and IMAGE the vendors or third party companies are making a variety of conversion tools available to transform data into the relational model. Backup. A backup utility creates a backup copy of the database usually by dumping the entire database onto tape or other mass storage medium. The backup copy can be used to restore the database in case of catastrophic disk failure. Incremental backups are also often used where only changes since the previous backup are recorded. Incremental backup is more complex but saves storage space. Database storage utility can be used to reorganize a set of database files into different file organizations and create new access paths to improve performance. Performance monitoring. Such a utility monitors database usage and provides statistics to the DBA. The DBA uses the statistics in making decisions such as whether or not to reorganize files or whether to add or drop indexes to improve performance. Other utilities may be available for sorting files handling data compression monitoring access by users interfacing with the network and performing other functions. Tools Application Environments and Communications Facilities Other tools are often available to database designers users and the DBMS. CASE are used in the design phase of database systems. Another tool that can be quite useful in large organizations is an expanded data dictionary system. In addition to storing catalog information about schemas and constraints the data dictionary stores other information such as design decisions usage standards application program descriptions and user information. Such a system is also called an information repository. This information can be accessed directly by users or the DBA when needed. A data dictionary utility is similar to the DBMS catalog but it includes a wider variety of information and is accessed mainly by users rather than by the DBMS software. CASE stands for computer aided software engineering many CASE tools are used primarily for database design. Chapter Database System Concepts and Architecture Application development environments such as PowerBuilder or JBuilder have been quite popular. These systems provide an environment for developing database applications and include facilities that help in many facets of database systems including database design GUI development querying and updating and application program development. The DBMS also needs to interface with communications software whose function is to allow users at locations remote from the database system site to access the database through computer terminals workstations or personal computers. These are connected to the database site through data communications hardware such as Internet routers phone lines long haul networks local networks or satellite communication devices. Many commercial database systems have communication packages that work with the DBMS. The integrated DBMS and data communications system is called a DB DC system. In addition some distributed DBMSs are physically distributed over multiple machines. In this case communications networks are needed to connect the machines. These are often local area networks but they can also be other types of networks. Centralized and Client Server Architectures for DBMSs Centralized DBMSs Architecture Architectures for DBMSs have followed trends similar to those for general computer system architectures. Earlier architectures used mainframe computers to provide the main processing for all system functions including user application programs and user interface programs as well as all the DBMS functionality. The reason was that most users accessed such systems via computer terminals that did not have processing power and only provided display capabilities. Therefore all processing was performed remotely on the computer system and only display information and controls were sent from the computer to the display terminals which were connected to the central computer via various types of communications networks. As prices of hardware declined most users replaced their terminals with PCs and workstations. At first database systems used these computers similarly to how they had used display terminals so that the DBMS itself was still a centralized DBMS in which all the DBMS functionality application program execution and user interface processing were carried out on one machine. Figure illustrates the physical components in a centralized architecture. Gradually DBMS systems started to exploit the available processing power at the user side which led to client server DBMS architectures. Basic Client Server Architectures First we discuss client server architecture in general then we see how it is applied to DBMSs. The client server architecture was developed to deal with computing environments in which a large number of PCs workstations file servers printers data Centralized and Client Server Architectures for DBMSs Display Monitor Display Monitor Network Software Hardware Firmware Operating System Display Monitor Application Programs DBMS Controller CPU Controller . . . . . . . . . Controller Memory Disk I O Devices Compilers Text Editors Terminal Display Control System Bus Terminals . . . . . . Figure A physical centralized architecture. base servers Web servers e mail servers and other software and equipment are connected via a network. The idea is to define specialized servers with specific functionalities. For example it is possible to connect a number of PCs or small workstations as clients to a file server that maintains the files of the client machines. Another machine can be designated as a printer server by being connected to various printers all print requests by the clients are forwarded to this machine. Web servers or e mail servers also fall into the specialized server category. The resources provided by specialized servers can be accessed by many client machines. The client machines provide the user with the appropriate interfaces to utilize these servers as well as with local processing power to run local applications. This concept can be carried over to other software packages with specialized programs such as a CAD package being stored on specific server machines and being made accessible to multiple clients. Figure illustrates client server architecture at the logical level Figure is a simplified diagram that shows the physical architecture. Some machines would be client sites only . Client Client Client Print Server DBMS Server File Server . . . . . . Network Figure Logical two tier client server architecture. Chapter Database System Concepts and Architecture Client CLIENT Site Client with Disk Client Site Diskless Client Server Site Server Communication Network Site n Server and Client . . . Client Server Figure Physical two tier client server architecture. Other machines would be dedicated servers and others would have both client and server functionality. The concept of client server architecture assumes an underlying framework that consists of many PCs and workstations as well as a smaller number of mainframe machines connected via LANs and other types of computer networks. A client in this framework is typically a user machine that provides user interface capabilities and local processing. When a client requires access to additional functionality such as database access that does not exist at that machine it connects to a server that provides the needed functionality. A server is a system containing both hardware and software that can provide services to the client machines such as file access printing archiving or database access. In general some machines install only client software others only server software and still others may include both client and server software as illustrated in Figure However it is more common that client and server software usually run on separate machines. Two main types of basic DBMS architectures were created on this underlying client server framework two tier and three tier. We discuss them next. Two Tier Client Server Architectures for DBMSs In relational database management systems many of which started as centralized systems the system components that were first moved to the client side were the user interface and application programs. Because SQL once the connection is created the client program can communicate with the DBMS. A standard called Open Database Connectivity provides an application programming interface which allows client side programs to call the DBMS as long as both client and server machines have the necessary software installed. Most DBMS vendors provide ODBC drivers for their systems. A client program can actually connect to several RDBMSs and send query and transaction requests using the ODBC API which are then processed at the server sites. Any query results are sent back to the client program which can process and display the results as needed. A related standard for the Java programming language called JDBC has also been defined. This allows Java client programs to access one or more DBMSs through a standard interface. The different approach to two tier client server architecture was taken by some object oriented DBMSs where the software modules of the DBMS were divided between client and server in a more integrated way. For example the server level may include the part of the DBMS software responsible for handling data storage on disk pages local concurrency control and recovery buffering and caching of disk pages and other such functions. Meanwhile the client level may handle the user interface data dictionary functions DBMS interactions with programming language compilers global query optimization concurrency control and recovery across multiple servers structuring of complex objects from the data in the buffers and other such functions. In this approach the client server interaction is more tightly coupled and is done internally by the DBMS modules some of which reside on the client and some on the server rather than by the users programmers. The exact division of functionality can vary from system to system. In such a client server architecture the server has been called a data server because it provides data in disk pages to the client. This data can then be structured into objects for the client programs by the client side DBMS software. The architectures described here are called two tier architectures because the software components are distributed over two systems client and server. The advantages of this architecture are its simplicity and seamless compatibility with existing systems. The emergence of the Web changed the roles of clients and servers leading to the three tier architecture. Three Tier and n Tier Architectures for Web Applications Many Web applications use an architecture called the three tier architecture which adds an intermediate layer between the client and the database server as illustrated in Figure Chapter Database System Concepts and Architecture GUI Web Interface Client Application Server or Web Server Database Server Application Programs Web Pages Database Management System Presentation Layer Business Logic Layer Database Services Layer Figure Logical three tier client server architecture with a couple of commonly used nomenclatures. This intermediate layer or middle tier is called the application server or the Web server depending on the application. This server plays an intermediary role by running application programs and storing business rules that are used to access data from the database server. It can also improve database security by checking a client’s credentials before forwarding a request to the database server. Clients contain GUI interfaces and some additional application specific business rules. The intermediate server accepts requests from the client processes the request and sends database queries and commands to the database server and then acts as a conduit for passing processed data from the database server to the clients where it may be processed further and filtered to be presented to users in GUI format. Thus the user interface application rules and data access act as the three tiers. Figure shows another architecture used by database and other application package vendors. The presentation layer displays information to the user and allows data entry. The business logic layer handles intermediate rules and constraints before data is passed up to the user or down to the DBMS. The bottom layer includes all data management services. The middle layer can also act as a Web server which retrieves query results from the database server and formats them into dynamic Web pages that are viewed by the Web browser at the client side. Other architectures have also been proposed. It is possible to divide the layers between the user and the stored data further into finer components thereby giving rise to n tier architectures where n may be four or five tiers. Typically the business logic layer is divided into multiple layers. Besides distributing programming and data throughout a network n tier applications afford the advantage that any one tier can run on an appropriate processor or operating system platform and can be handled independently. Vendors of ERP and CRM packages often use a middleware layer which accounts for the front end modules communicating with a number of back end databases . Classification of Database Management Systems Advances in encryption and decryption technology make it safer to transfer sensitive data from server to client in encrypted form where it will be decrypted. The latter can be done by the hardware or by advanced software. This technology gives higher levels of data security but the network security issues remain a major concern. Various technologies for data compression also help to transfer large amounts of data from servers to clients over wired and wireless networks. Classification of Database Management Systems Several criteria are normally used to classify DBMSs. The first is the data model on which the DBMS is based. The main data model used in many current commercial DBMSs is the relational data model. The object data model has been implemented in some commercial systems but has not had widespread use. Many legacy applications still run on database systems based on the hierarchical and network data models. Examples of hierarchical DBMSs include IMS and some other systems like System and TDMS. IMS is still used at governmental and industrial installations including hospitals and banks although many of its users have converted to relational systems. The network data model was used by many vendors and the resulting products like IDMS DMS IMAGE VAXDBMS and SUPRA still have a following and their user groups have their own active organizations. If we add IBM’s popular VSAM file system to these we can easily say that a reasonable percentage of worldwide computerized data is still in these so called legacy database systems. The relational DBMSs are evolving continuously and in particular have been incorporating many of the concepts that were developed in object databases. This has led to a new class of DBMSs called object relational DBMSs. We can categorize DBMSs based on the data model relational object object relational hierarchical network and other. More recently some experimental DBMSs are based on the XML model which is a tree structured data model. These have been called native XML DBMSs. Several commercial relational DBMSs have added XML interfaces and storage to their products. The second criterion used to classify DBMSs is the number of users supported by the system. Single user systems support only one user at a time and are mostly used with PCs. Multiuser systems which include the majority of DBMSs support concurrent multiple users. The third criterion is the number of sites over which the database is distributed. A DBMS is centralized if the data is stored at a single computer site. A centralized DBMS can support multiple users but the DBMS and the database reside totally at a single computer site. A distributed DBMS can have the actual database and DBMS software distributed over many sites connected by a computer network. Homogeneous DDBMSs use the same DBMS software at all the sites whereas Chapter Database System Concepts and Architecture heterogeneous DDBMSs can use different DBMS software at each site. It is also possible to develop middleware software to access several autonomous preexisting databases stored under heterogeneousDBMSs. This leads to a federated DBMS in which the participating DBMSs are loosely coupled and have a degree of local autonomy. Many DDBMSs use client server architecture as we described in Section The fourth criterion is cost. It is difficult to propose a classification of DBMSs based on cost. Today we have open source DBMS products like MySQL and PostgreSQL that are supported by third party vendors with additional services. The main RDBMS products are available as free examination copy versions as well as personal versions which may cost under and allow a fair amount of functionality. The giant systems are being sold in modular form with components to handle distribution replication parallel processing mobile capability and so on and with a large number of parameters that must be defined for the configuration. Furthermore they are sold in the form of licenses site licenses allow unlimited use of the database system with any number of copies running at the customer site. Another type of license limits the number of concurrent users or the number of user seats at a location. Standalone single user versions of some systems like Microsoft Access are sold per copy or included in the overall configuration of a desktop or laptop. In addition data warehousing and mining features as well as support for additional data types are made available at extra cost. It is possible to pay millions of dollars for the installation and maintenance of large database systems annually. We can also classify a DBMS on the basis of the types of access path options for storing files. One well known family of DBMSs is based on inverted file structures. Finally a DBMS can be general purpose or special purpose. When performance is a primary consideration a special purpose DBMS can be designed and built for a specific application such a system cannot be used for other applications without major changes. Many airline reservations and telephone directory systems developed in the past are special purpose DBMSs. These fall into the category of online transaction processing systems which must support a large number of concurrent transactions without imposing excessive delays. Let us briefly elaborate on the main criterion for classifying DBMSs the data model. The basic relational data model represents a database as a collection of tables where each table can be stored as a separate file. The database in Figure resembles a relational representation. Most relational databases use the high level query language called SQL and support a limited form of user views. We discuss the relational model and its languages and operations in Chapters through and techniques for programming relational applications in Chapters and The object data model defines a database in terms of objects their properties and their operations. Objects with the same structure and behavior belong to a class and classes are organized into hierarchies . The operations of each class are specified in terms of predefined procedures called methods. Relational DBMSs have been extending their models to incorporate object database Classification of Database Mangement Systems concepts and other capabilities these systems are referred to as object relational or extended relational systems. We discuss object databases and object relational systems in Chapter The XML model has emerged as a standard for exchanging data over the Web and has been used as a basis for implementing several prototype native XML systems. XML uses hierarchical tree structures. It combines database concepts with concepts from document representation models. Data is represented as elements with the use of tags data can be nested to create complex hierarchical structures. This model conceptually resembles the object model but uses different terminology. XML capabilities have been added to many commercial DBMS products. We present an overview of XML in Chapter Two older historically important data models now known as legacy data models are the network and hierarchical models. The network model represents data as record types and also represents a limited type of relationship called a set type. A or one to many relationship relates one instance of a record to many record instances using some pointer linking mechanism in these models. Figure shows a network schema diagram for the database of Figure where record types are shown as rectangles and set types are shown as labeled directed arrows. The network model also known as the CODASYL DBTG has an associated record at a time language that must be embedded in a host programming language. The network DML was proposed in the Database Task Group Report as an extension of the COBOL language. It provides commands for locating records directly . It has commands to support traversals within set types . It also has commands to store new data GRADEREPORT SECTION COURSEOFFERINGS STUDENTGRADES HASA ISA PREREQUISITE SECTIONGRADES STUDENT COURSE Figure The schema of Figure in network model notation. DBTG stands for Conference on Data Systems Languages Database Task Group which is the committee that specified the network model and its language. Chapter Database System Concepts and Architecture and to make it part of a set type . The language also handles many additional considerations such as the currency of record types and set types which are defined by the current position of the navigation process within the database. It is prominently used by IDMS IMAGE and SUPRA DBMSs today. The hierarchical model represents data as hierarchical tree structures. Each hierarchy represents a number of related records. There is no standard language for the hierarchical model. A popular hierarchical DML is of the IMS system. It dominated the DBMS market for over years between and and is still a widely used DBMS worldwide holding a large percentage of data in governmental health care and banking and insurance databases. Its DML called was a de facto industry standard for a long time. has commands to locate a record . It has navigational facilities to navigate within hierarchies . It has appropriate facilities to store and update records . Currency issues during navigation are also handled with additional features in the Summary In this chapter we introduced the main concepts used in database systems. We defined a data model and we distinguished three main categories High level or conceptual data models Low level or physical data models Representational or implementation data models We distinguished the schema or description of a database from the database itself. The schema does not change very often whereas the database state changes every time data is inserted deleted or modified. Then we described the three schema DBMS architecture which allows three schema levels An internal schema describes the physical storage structure of the database. A conceptual schema is a high level description of the whole database. External schemas describe the views of different user groups. A DBMS that cleanly separates the three levels must have mappings between the schemas to transform requests and query results from one level to the next. Most DBMSs do not separate the three levels completely. We used the three schema architecture to define the concepts of logical and physical data independence. full chapters on the network and hierarchical models from the second edition of this book are available from this book’s Companion Website at Review Questions Then we discussed the main types of languages and interfaces that DBMSs support. A data definition language is used to define the database conceptual schema. In most DBMSs the DDL also defines user views and sometimes storage structures in other DBMSs separate languages or functions exist for specifying storage structures. This distinction is fading away in today’s relational implementations with SQL serving as a catchall language to perform multiple roles including view definition. The storage definition part was included in SQL’s early versions but is now typically implemented as special commands for the DBA in relational DBMSs. The DBMS compiles all schema definitions and stores their descriptions in the DBMS catalog. A data manipulation language is used for specifying database retrievals and updates. DMLs can be high level or low level . A high level DML can be embedded in a host programming language or it can be used as a standalone language in the latter case it is often called a query language. We discussed different types of interfaces provided by DBMSs and the types of DBMS users with which each interface is associated. Then we discussed the database system environment typical DBMS software modules and DBMS utilities for helping users and the DBA staff perform their tasks. We continued with an overview of the two tier and three tier architectures for database applications progressively moving toward n tier which are now common in many applications particularly Web database applications. Finally we classified DBMSs according to several criteria data model number of users number of sites types of access paths and cost. We discussed the availability of DBMSs and additional modules from no cost in the form of open source software to configurations that annually cost millions to maintain. We also pointed out the variety of licensing arrangements for DBMS and related products. The main classification of DBMSs is based on the data model. We briefly discussed the main data models used in current commercial DBMSs. Review Questions Define the following terms data model database schema database state internal schema conceptual schema external schema data independence DDL DML SDL VDL query language host language data sublanguage database utility catalog client server architecture three tier architecture and n tier architecture. Discuss the main categories of data models. What are the basic differences between the relational model the object model and the XML model What is the difference between a database schema and a database state Describe the three schema architecture. Why do we need mappings between schema levels How do different schema definition languages support this architecture Chapter Database System Concepts and Architecture What is the difference between logical data independence and physical data independence Which one is harder to achieve Why What is the difference between procedural and nonprocedural DMLs Discuss the different types of user friendly interfaces and the types of users who typically use each. With what other computer system software does a DBMS interact What is the difference between the two tier and three tier client server architectures Discuss some types of database utilities and tools and their functions. What is the additional functionality incorporated in n tier architecture . Identify the column or the group of columns in the other tables that must be unique across all rows in the table. Selected Bibliography Selected Bibliography Many database textbooks including Date Silberschatz et al. Ramakrishnan and Gehrke Garcia Molina et al. and Abiteboul et al. provide a discussion of the various database concepts presented here. Tsichritzis and Lochovsky is an early textbook on data models. Tsichritzis and Klug and Jardine present the three schema architecture which was first suggested in the DBTG CODASYL report and later in an American National Standards Institute report An in depth analysis of the relational data model and some of its possible extensions is given in Codd The proposed standard for object oriented databases is described in Cattell et al. Many documents describing XML are available on the Web such as XML Examples of database utilities are the ETI Connect Analyze and Transform tools and the database administration tool DBArtisan from Embarcadero Technologies . This page intentionally left blank The Relational Data Model and SQL This page intentionally left blank The Relational Data Model and Relational Database Constraints This chapter opens Part of the book which covers relational databases. The relational data model was first introduced by Ted Codd of IBM Research in in a classic paper include and Informix Dynamic Server Oracle and Rdb Sybase DBMS and SQLServer and Access . In addition several open source systems such as MySQL and PostgreSQL are available. Because of the importance of the relational model all of Part is devoted to this model and some of the languages associated with it. In Chapters and we describe the SQL query language which is the standard for commercial relational DBMSs. Chapter covers the operations of the relational algebra and introduces the relational calculus these are two formal languages associated with the relational model. The relational calculus is considered to be the basis for the SQL language and the relational algebra is used in the internals of many database implementations for query processing and optimization . chapter Chapter The Relational Data Model and Relational Database Constraints Other aspects of the relational model are presented in subsequent parts of the book. Chapter relates the relational model data structures to the constructs of the ER and EER models Names The set of character strings that represent names of persons. Gradepointaverages. Possible values of computed grade point averages each must be a real number between and Employeeages. Possible ages of employees in a company each must be an integer value between and Academicdepartmentnames. The set of academic department names in a university such as Computer Science Economics and Physics. Academicdepartmentcodes. The set of academic department codes such as ‘CS’ ‘ECON’ and ‘PHYS’. The preceding are called logical definitions of domains. A data type or format is also specified for each domain. For example the data type for the domain Usaphonenumbers can be declared as a character string of the form ddddddd where each d is a numeric digit and the first three digits form a valid telephone area code. The data type for Employeeages is an integer number between and For Academicdepartmentnames the data type is the set of all character strings that represent valid department names. A domain is thus given a name data type and format. Additional information for interpreting the values of a domain can also be given for example a numeric domain such as Personweights should have the units of measurement such as pounds or kilograms. Chapter The Relational Data Model and Relational Database Constraints A relation R denoted by An is made up of a relation name R and a list of attributes An. Each attribute Ai is the name of a role played by some domain D in the relation schema R. D is called the domain of Ai and is denoted by dom. A relation schema is used to describe a relation R is called the name of this relation. The degree of a relation is the number of attributes n of its relation schema. A relation of degree seven which stores information about university students would contain seven attributes describing each student. as follows STUDENT Using the data type of each attribute the definition is sometimes written as STUDENT For this relation schema STUDENT is the name of the relation which has seven attributes. In the preceding definition we showed assignment of generic types such as string or integer to the attributes. More precisely we can specify the following previously defined domains for some of the attributes of the STUDENT relation dom Names dom Socialsecuritynumbers dom dom USAphonenumbers and dom Gradepointaverages. It is also possible to refer to attributes of a relation schema by their position within the relation thus the second attribute of the STUDENT relation is Ssn whereas the fourth attribute is Address. A relation also denoted by r is a set of n tuples r t t t m . Each n tuple t is an ordered list of n values t vn where each value vi ≤ i ≤ n is an element of dom or is a special NULL value. . The terms relation intension for the schema R and relation extension for a relation state r are also commonly used. Figure shows an example of a STUDENT relation which corresponds to the STUDENT schema just specified. Each tuple in the relation represents a particular student entity . We display the relation as a table where each tuple is shown as a row and each attribute corresponds to a column header indicating a role or interpretation of the values in that column. NULL values represent attributes whose values are unknown or do not exist for some individual STUDENT tuple. relation schema is sometimes called a relation scheme. the large increase in phone numbers caused by the proliferation of mobile phones most metropolitan areas in the . now have multiple area codes so seven digit local dialing has been discontinued in most areas. We changed this domain to Usaphonenumbers instead of Localphonenumbers which would be a more general choice. This illustrates how database requirements can change over time. has also been called a relation instance. We will not use this term because instance is also used to refer to a single tuple or row. Relational Model Concepts Relation Name Tuples STUDENT Name Benjamin Bayer Chung cha Kim Dick Davidson Rohan Panchal Barbara Benson Ssn Homephone NULL Address Bluebonnet Lane Kirby Road Elgin Road Lark Lane Fontana Lane Officephone NULL NULL NULL Age Gpa Attributes Figure The attributes and tuples of a relation STUDENT. The earlier definition of a relation can be restated more formally using set theory concepts as follows. A relation r is a mathematical relation of degree n on the domains dom which is a subset of the Cartesian product of the domains that define R r ⊆ × × × dom The Cartesian product specifies all possible combinations of values from the underlying domains. Hence if we denote the total number of values or cardinality in a domain D by |D| the total number of tuples in the Cartesian product is × × × |dom| This product of cardinalities of all domains represents the total number of possible instances or tuples that can ever exist in any relation state r. Of all these possible combinations a relation state at a given time the current relation state reflects only the valid tuples that represent a particular state of the real world. In general as the state of the real world changes so does the relation state by being transformed into another relation state. However the schema R is relatively static and changes very infrequently for example as a result of adding an attribute to represent new information that was not originally stored in the relation. It is possible for several attributes to have the same domain. The attribute names indicate different roles or interpretations for the domain. For example in the STUDENT relation the same domain USAphonenumbers plays the role of Homephone referring to the home phone of a student and the role of Officephone referring to the office phone of the student. A third possible attribute with the same domain could be Mobilephone. Characteristics of Relations The earlier definition of relations implies certain characteristics that make a relation different from a file or a table. We now discuss some of these characteristics. Chapter The Relational Data Model and Relational Database Constraints Dick Davidson Barbara Benson Rohan Panchal Chung cha Kim NULL Elgin Road Fontana Lane Lark Lane Kirby Road NULL NULL Benjamin Bayer Bluebonnet Lane NULL STUDENT Name Ssn Homephone Address Officephone Age Gpa Figure The relation STUDENT from Figure with a different order of tuples. Ordering of Tuples in a Relation. A relation is defined as a set of tuples. Mathematically elements of a set have no order among them hence tuples in a relation do not have any particular order. In other words a relation is not sensitive to the ordering of tuples. However in a file records are physically stored on disk so there always is an order among the records. This ordering indicates first second ith and last records in the file. Similarly when we display a relation as a table the rows are displayed in a certain order. Tuple ordering is not part of a relation definition because a relation attempts to represent facts at a logical or abstract level. Many tuple orders can be specified on the same relation. For example tuples in the STUDENT relation in Figure could be ordered by values of Name Ssn Age or some other attribute. The definition of a relation does not specify any order There is no preference for one ordering over another. Hence the relation displayed in Figure is considered identical to the one shown in Figure When a relation is implemented as a file or displayed as a table a particular ordering may be specified on the records of the file or the rows of the table. Ordering of Values within a Tuple and an Alternative Definition of a Relation. According to the preceding definition of a relation an n tuple is an ordered list of n values so the ordering of values in a tuple and hence of attributes in a relation schema is important. However at a more abstract level the order of attributes and their values is not that important as long as the correspondence between attributes and values is maintained. An alternative definition of a relation can be given making the ordering of values in a tuple unnecessary. In this definition a relation schema R An is a set of attributes and a relation state r is a finite set of mappings r t t t m where each tuple t i is a mapping from R to D and D is the union of the attribute domains that is D ∪ ∪ ∪ dom. In this definition t[Ai ] must be in dom for ≤ i ≤ n for each mapping t in r. Each mapping t i is called a tuple. According to this definition of tuple as a mapping a tuple can be considered as a set of pairs where each pair gives the value of the mapping from an attribute Ai to a value vi from dom. The ordering of attributes is not Relational Model Concepts Figure Two identical tuples when the order of attributes and values is not part of relation definition. important because the attribute name appears with its value. By this definition the two tuples shown in Figure are identical. This makes sense at an abstract level since there really is no reason to prefer having one attribute value appear before another in a tuple. When a relation is implemented as a file the attributes are physically ordered as fields within a record. We will generally use the first definition of relation where the attributes and the values within tuples are ordered because it simplifies much of the notation. However the alternative definition given here is more Values and NULLs in the Tuples. Each value in a tuple is an atomic value that is it is not divisible into components within the framework of the basic relational model. Hence composite and multivalued attributes . Another student has a NULL for home phone presumably because either he does not have a home phone or he has one but we do not know it . In general we can have several meanings for NULL values such as value unknown value exists but is not available or attribute does not apply to this tuple . An example of the last type of NULL will occur if we add an attribute Visastatus to the STUDENT relation we shall see the alternative definition of relation is useful when we discuss query processing and optimization in Chapter discuss this assumption in more detail in Chapter of the relational model remove these restrictions. For example object relational systems of a Relation. The relation schema can be interpreted as a declaration or a type of assertion. For example the schema of the STUDENT relation of Figure asserts that in general a student entity has a Name Ssn Homephone Address Officephone Age and Gpa. Each tuple in the relation can then be interpreted as a fact or a particular instance of the assertion. For example the first tuple in Figure asserts the fact that there is a STUDENT whose Name is Benjamin Bayer Ssn is Age is and so on. Notice that some relations may represent facts about entities whereas other relations may represent facts about relationships. For example a relation schema MAJORS asserts that students major in academic disciplines. A tuple in this relation relates a student to his or her major discipline. Hence the relational model represents facts about both entities and relationships uniformly as relations. This sometimes compromises understandability because one has to guess whether a relation represents an entity type or a relationship type. We introduce the Entity Relationship model in detail in Chapter where the entity and relationship concepts will be described in detail. The mapping procedures in Chapter show how different constructs of the ER and EER is true for the five tuples in relation STUDENT of Figure These tuples represent five different propositions or facts in the real world. This interpretation is quite useful in the context of logical programming languages such as Prolog because it allows the relational model to be used within these languages of the relation. Any other combination of values makes the predicate false. Relational Model Notation We will use the following notation in our presentation A relation schema R of degree n is denoted by An . Relational Model Constraints and Relational Database Schemas The uppercase letters Q R S denote relation names. The lowercase letters q r s denote relation states. The letters t u v denote tuples. In general the name of a relation schema such as STUDENT also indicates the current set of tuples in that relation the current relation state whereas STUDENT refers only to the relation schema. An attribute A can be qualified with the relation name R to which it belongs by using the dot notation for example or . This is because the same name may be used for two attributes in different relations. However all attribute names in a particular relation must be distinct. An n tuple t in a relation r is denoted by t vn where vi is the value corresponding to attribute Ai . The following notation refers to component values of tuples Both t[Ai ] and refer to the value vi in t for attribute Ai . Both t[Au Aw Az ] and where Au Aw Az is a list of attributes from R refer to the subtuple of values vu vw vz from t corresponding to the attributes specified in the list. As an example consider the tuple t ‘Barbara Benson’ Fontana Lane’ NULL from the STUDENT relation in Figure we have t[Name] ‘Barbara Benson’ and t[Ssn Gpa Age] Relational Model Constraints and Relational Database Schemas So far we have discussed the characteristics of single relations. In a relational database there will typically be many relations and the tuples in those relations are usually related in various ways. The state of the whole database will correspond to the states of all its relations at a particular point in time. There are generally many restrictions or constraints on the actual values in a database state. These constraints are derived from the rules in the miniworld that the database represents as we discussed in Section In this section we discuss the various restrictions on data that can be specified on a relational database in the form of constraints. Constraints on databases can generally be divided into three main categories Constraints that are inherent in the data model. We call these inherent model based constraints or implicit constraints. Constraints that can be directly expressed in schemas of the data model typically by specifying them in the DDL . We have already discussed the ways in which domains can be specified in Section The data types associated with domains typically include standard numeric data types for integers and real numbers . Characters Booleans fixed length strings and variable length strings are also available as are date time timestamp and money or other special data types. Other possible domains may be described by a subrange of values from a data type or as an enumerated data type in which all possible values are explicitly listed. Rather than describe these in detail here we discuss the data types offered by the SQL relational standard in Section Key Constraints and Constraints on NULL Values In the formal relational model a relation is defined as a set of tuples. By definition all elements of a set are distinct hence all tuples in a relation must also be distinct. This means that no two tuples can have the same combination of values for all their attributes. Usually there are other subsets of attributes of a relation schema R with the property that no two tuples in any relation state r of R should have the same combination of values for these attributes. Suppose that we denote one such subset of attributes by SK then for any two distinct tuples t and t in a relation state r of R we have the constraint that t t Relational Model Constraints and Relational Database Schemas Any such set of attributes SK is called a superkey of the relation schema R. A superkey SK specifies a uniqueness constraint that no two distinct tuples in any state r of R can have the same value for SK. Every relation has at least one default superkey the set of all its attributes. A superkey can have redundant attributes however so a more useful concept is that of a key which has no redundancy. A key K of a relation schema R is a superkey of R with the additional property that removing any attribute A from K leaves a set of attributes K that is not a superkey of R any more. Hence a key satisfies two properties Two distinct tuples in any state of the relation cannot have identical values for the attributes in the key. This first property also applies to a superkey. It is a minimal superkey that is a superkey from which we cannot remove any attributes and still have the uniqueness constraint in condition hold. This property is not required by a superkey. Whereas the first property applies to both keys and superkeys the second property is required only for keys. Hence a key is also a superkey but not vice versa. Consider the STUDENT relation of Figure The attribute set Ssn is a key of STUDENT because no two student tuples can have the same value for Ssn. Any set of attributes that includes Ssn for example Ssn Name Age is a superkey. However the superkey Ssn Name Age is not a key of STUDENT because removing Name or Age or both from the set still leaves us with a superkey. In general any superkey formed from a single attribute is also a key. A key with multiple attributes must require all its attributes together to have the uniqueness property. The value of a key attribute can be used to identify uniquely each tuple in the relation. For example the Ssn value identifies uniquely the tuple corresponding to Benjamin Bayer in the STUDENT relation. Notice that a set of attributes constituting a key is a property of the relation schema it is a constraint that should hold on every valid relation state of the schema. A key is determined from the meaning of the attributes and the property is time invariant It must continue to hold when we insert new tuples in the relation. For example we cannot and should not designate the Name attribute of the STUDENT relation in Figure as a key because it is possible that two students with identical names will exist at some point in a valid In general a relation schema may have more than one key. In this case each of the keys is called a candidate key. For example the CAR relation in Figure has two candidate keys Licensenumber and Engineserialnumber. It is common to designate one of the candidate keys as the primary key of the relation. This is the candidate key whose values are used to identify tuples in the relation. We use the convention that the attributes that form the primary key of a relation schema are underlined as shown in Figure Notice that when a relation schema has several candidate keys that Ssn is also a superkey. are sometimes used as keys but then some artifact such as appending an ordinal number must be used to distinguish between identical names. Chapter The Relational Data Model and Relational Database Constraints CAR Licensenumber Engineserialnumber Make Model Year Texas Florida New York California California Texas Ford Oldsmobile Oldsmobile Mercedes Toyota Jaguar Mustang Cutlass Delta Camry XJS Figure The CAR relation with two candidate keys Licensenumber and Engineserialnumber. the choice of one to become the primary key is somewhat arbitrary however it is usually better to choose a primary key with a single attribute or a small number of attributes. The other candidate keys are designated as unique keys and are not underlined. Another constraint on attributes specifies whether NULL values are or are not permitted. For example if every STUDENT tuple must have a valid non NULL value for the Name attribute then Name of STUDENT is constrained to be NOT NULL. Relational Databases and Relational Database Schemas The definitions and constraints we have discussed so far apply to single relations and their attributes. A relational database usually contains many relations with tuples in relations that are related in various ways. In this section we define a relational database and a relational database schema. A relational database schema S is a set of relation schemas S Rm and a set of integrity constraints IC. A relational database DB of S is a set of relation states DB rm such that each ri is a state of Ri and such that the ri relation states satisfy the integrity constraints specified in IC. Figure shows a relational database schema that we call COMPANY EMPLOYEE DEPARTMENT DEPTLOCATIONS PROJECT WORKSON DEPENDENT . The underlined attributes represent primary keys. Figure shows a relational database state corresponding to the COMPANY schema. We will use this schema and database state in this chapter and in Chapters through for developing sample queries in different relational languages. When we refer to a relational database we implicitly include both its schema and its current state. A database state that does not obey all the integrity constraints is relational database state is sometimes called a relational database instance. However as we mentioned earlier we will not use the term instance since it also applies to single tuples. Relational Model Constraints and Relational Database Schemas DEPARTMENT Fname Minit Lname Ssn Bdate Address Sex Salary Superssn Dno EMPLOYEE DEPTLOCATIONS Dnumber Dlocation PROJECT Pname Pnumber Plocation Dnum WORKSON Essn Pno Hours DEPENDENT Essn Dependentname Sex Bdate Relationship Dname Dnumber Mgrssn Mgrstartdate Figure Schema diagram for the COMPANY relational database schema. called an invalid state and a state that satisfies all the constraints in the defined set of integrity constraints IC is called a valid state. In Figure the Dnumber attribute in both DEPARTMENT and DEPTLOCATIONS stands for the same real world concept the number given to a department. That same concept is called Dno in EMPLOYEE and Dnum in PROJECT. Attributes that represent the same real world concept may or may not have identical names in different relations. Alternatively attributes that represent different concepts may have the same name in different relations. For example we could have used the attribute name Name for both Pname of PROJECT and Dname of DEPARTMENT in this case we would have two attributes that share the same name but represent different realworld concepts project names and department names. In some early versions of the relational model an assumption was made that the same real world concept when represented by an attribute would have identical attribute names in all relations. This creates problems when the same real world concept is used in different roles in the same relation. For example the concept of Social Security number appears twice in the EMPLOYEE relation of Figure once in the role of the employee’s SSN and once in the role of the supervisor’s SSN. We are required to give them distinct attribute names Ssn and Superssn respectively because they appear in the same relation and in order to distinguish their meaning. Each relational DBMS must have a data definition language for defining a relational database schema. Current relational DBMSs are mostly using SQL for this purpose. We present the SQL DDL in Sections and DEPTLOCATIONS Dnumber Houston Stafford Bellaire Sugarland Dlocation DEPARTMENT Dname Research Administration Headquarters Dnumber Mgrssn Mgrstartdate WORKSON Essn NULL Pno Hours PROJECT Pname ProductX ProductY ProductZ Computerization Reorganization Newbenefits Houston Bellaire Sugarland Stafford Stafford Houston Pnumber Plocation Dnum DEPENDENT Joy Alice F M M M Theodore Alice Elizabeth Abner Michael Spouse Daughter Son Daughter Spouse Spouse Son Dependentname Sex Bdate Relationship EMPLOYEE Fname John Franklin Jennifer Alicia Ramesh Joyce James Ahmad Narayan English Borg Jabbar M M M M M NULL Zelaya Wallace Smith Wong Castle Spring TX Berry Bellaire TX Fondren Houston TX Voss Houston TX Dallas Houston TX Stone Houston TX Fire Oak Humble TX Rice Houston TX Minit Lname Ssn Bdate Address Sex Dno Salary Superssn B T S K A V E Houston Essn Figure One possible database state for the COMPANY relational database schema. Chapter The Relational Data Model and Relational Database Constraints Integrity constraints are specified on a database schema and are expected to hold on every valid database state of that schema. In addition to domain key and NOT NULL constraints two other types of constraints are considered part of the relational model entity integrity and referential integrity. Integrity Referential Integrity and Foreign Keys The entity integrity constraint states that no primary key value can be NULL. This is because the primary key value is used to identify individual tuples in a relation. Having NULL values for the primary key implies that we cannot identify some tuples. For example if two or more tuples had NULL for their primary keys we may not be able to distinguish them if we try to reference them from other relations. Key constraints and entity integrity constraints are specified on individual relations. The referential integrity constraint is specified between two relations and is used to maintain the consistency among tuples in the two relations. Informally the referential integrity constraint states that a tuple in one relation that refers to another relation must refer to an existing tuple in that relation. For example in Figure the attribute Dno of EMPLOYEE gives the department number for which each employee works hence its value in every EMPLOYEE tuple must match the Dnumber value of some tuple in the DEPARTMENT relation. To define referential integrity more formally first we define the concept of a foreign key. The conditions for a foreign key given below specify a referential integrity constraint between the two relation schemas and A set of attributes FK in relation schema is a foreign key of that references relation if it satisfies the following rules The attributes in FK have the same domain as the primary key attributes PK of the attributes FK are said to reference or refer to the relation A value of FK in a tuple t of the current state either occurs as a value of PK for some tuple t in the current state or is NULL. In the former case we have t t and we say that the tuple t references or refers to the tuple t In this definition is called the referencing relation and is the referenced relation. If these two conditions hold a referential integrity constraint from to is said to hold. In a database of many relations there are usually many referential integrity constraints. To specify these constraints first we must have a clear understanding of the meaning or role that each attribute or set of attributes plays in the various relation schemas of the database. Referential integrity constraints typically arise from the relationships among the entities represented by the relation schemas. For example consider the database shown in Figure In the EMPLOYEE relation the attribute Dno refers to the department for which an employee works hence we designate Dno to be a foreign key of EMPLOYEE referencing the DEPARTMENT relation. This means that a value of Dno in any tuple t of the EMPLOYEE relation must match a value of Relational Model Constraints and Relational Database Schemas Chapter The Relational Data Model and Relational Database Constraints the primary key of DEPARTMENT the Dnumber attribute in some tuple t of the DEPARTMENT relation or the value of Dno can be NULL if the employee does not belong to a department or will be assigned to a department later. For example in Figure the tuple for employee ‘John Smith’ references the tuple for the ‘Research’ department indicating that ‘John Smith’ works for this department. Notice that a foreign key can refer to its own relation. For example the attribute Superssn in EMPLOYEE refers to the supervisor of an employee this is another employee represented by a tuple in the EMPLOYEE relation. Hence Superssn is a foreign key that references the EMPLOYEE relation itself. In Figure the tuple for employee ‘John Smith’ references the tuple for employee ‘Franklin Wong ’ indicating that ‘Franklin Wong’ is the supervisor of ‘John We can diagrammatically display referential integrity constraints by drawing a directed arc from each foreign key to the relation it references. For clarity the arrowhead may point to the primary key of the referenced relation. Figure shows the schema in Figure with the referential integrity constraints displayed in this manner. All integrity constraints should be specified on the relational database schema if we want to enforce these constraints on the database states. Hence the DDL includes provisions for specifying the various types of constraints so that the DBMS can automatically enforce them. Most relational DBMSs support key entity integrity and referential integrity constraints. These constraints are specified as a part of data definition in the DDL. Other Types of Constraints The preceding integrity constraints are included in the data definition language because they occur in most database applications. However they do not include a large class of general constraints sometimes called semantic integrity constraints which may have to be specified and enforced on a relational database. Examples of such constraints are the salary of an employee should not exceed the salary of the employee’s supervisor and the maximum number of hours an employee can work on all projects per week is Such constraints can be specified and enforced within the application programs that update the database or by using a general purpose constraint specification language. Mechanisms called triggers and assertions can be used. In SQL CREATE ASSERTION and CREATE TRIGGER statements can be used for this purpose the user’s query. Chapter also introduces the language called relational calculus which is used to define the new relation declaratively without giving a specific order of operations. In this section we concentrate on the database modification or update operations. There are three basic operations that can change the states of relations in the database Insert Delete and Update . They insert new data delete old data or modify existing data records. Insert is used to insert one or more new tuples in a relation Delete is used to delete tuples and Update is used to change the values of some attributes in existing tuples. Whenever these operations are applied the integrity constraints specified on the relational database schema should not be violated. In this section we discuss the types of constraints that may be violated by each of these operations and the types of actions that may be taken if an operation causes a violation. We use the database shown in Figure for examples and discuss only key constraints entity integrity constraints and the referential integrity constraints shown in Figure For each type of operation we give some examples and discuss any constraints that each operation may violate. The Insert Operation The Insert operation provides a list of attribute values for a new tuple t that is to be inserted into a relation R. Insert can violate any of the four types of constraints discussed in the previous section. Domain constraints can be violated if an attribute value is given that does not appear in the corresponding domain or is not of the appropriate data type. Key constraints can be violated if a key value in the new tuple t already exists in another tuple in the relation r. Entity integrity can be violated if any part of the primary key of the new tuple t is NULL. Referential integrity can be violated if the value of any foreign key in t refers to a tuple that does not exist in the referenced relation. Here are some examples to illustrate this discussion. Operation Insert ‘Cecilia’ ‘F’ ‘Kolonsky’ NULL Windy Lane Katy TX’ F NULL into EMPLOYEE. Result This insertion violates the entity integrity constraint so it is rejected. Operation Insert ‘Alicia’ ‘J’ ‘Zelaya’ Windy Lane Katy TX’ F into EMPLOYEE. Result This insertion violates the key constraint because another tuple with the same Ssn value already exists in the EMPLOYEE relation and so it is rejected. Operation Insert ‘Cecilia’ ‘F’ ‘Kolonsky’ Windswept Katy TX’ F into EMPLOYEE. Result This insertion violates the referential integrity constraint specified on Dno in EMPLOYEE because no corresponding referenced tuple exists in DEPARTMENT with Dnumber Update Operations Transactions and Dealing with Constraint Violations Operation Insert ‘Cecilia’ ‘F’ ‘Kolonsky’ Windy Lane Katy TX’ F NULL into EMPLOYEE. Result This insertion satisfies all constraints so it is acceptable. If an insertion violates one or more constraints the default option is to reject the insertion. In this case it would be useful if the DBMS could provide a reason to the user as to why the insertion was rejected. Another option is to attempt to correct the reason for rejecting the insertion but this is typically not used for violations caused by Insert rather it is used more often in correcting violations for Delete and Update. In the first operation the DBMS could ask the user to provide a value for Ssn and could then accept the insertion if a valid Ssn value is provided. In operation the DBMS could either ask the user to change the value of Dno to some valid value or it could ask the user to insert a DEPARTMENT tuple with Dnumber and could accept the original insertion only after such an operation was accepted. Notice that in the latter case the insertion violation can cascade back to the EMPLOYEE relation if the user attempts to insert a tuple for department with a value for Mgrssn that does not exist in the EMPLOYEE relation. The Delete Operation The Delete operation can violate only referential integrity. This occurs if the tuple being deleted is referenced by foreign keys from other tuples in the database. To specify deletion a condition on the attributes of the relation selects the tuple to be deleted. Here are some examples. Operation Delete the WORKSON tuple with Essn and Pno Result This deletion is acceptable and deletes exactly one tuple. Operation Delete the EMPLOYEE tuple with Ssn Result This deletion is not acceptable because there are tuples in WORKSON that refer to this tuple. Hence if the tuple in EMPLOYEE is deleted referential integrity violations will result. Operation Delete the EMPLOYEE tuple with Ssn Result This deletion will result in even worse referential integrity violations because the tuple involved is referenced by tuples from the EMPLOYEE DEPARTMENT WORKSON and DEPENDENT relations. Several options are available if a deletion operation causes a violation. The first option called restrict is to reject the deletion. The second option called cascade is to attempt to cascade the deletion by deleting tuples that reference the tuple that is being deleted. For example in operation the DBMS could automatically delete the offending tuples from WORKSON with Essn A third option called set null or set default is to modify the referencing attribute values that cause the violation each such value is either set to NULL or changed to reference Chapter The Relational Data Model and Relational Database Constraints another default valid tuple. Notice that if a referencing attribute that causes a violation is part of the primary key it cannot be set to NULL otherwise it would violate entity integrity. Combinations of these three options are also possible. For example to avoid having operation cause a violation the DBMS may automatically delete all tuples from WORKSON and DEPENDENT with Essn Tuples in EMPLOYEE with Superssn and the tuple in DEPARTMENT with Mgrssn can have their Superssn and Mgrssn values changed to other valid values or to NULL. Although it may make sense to delete automatically the WORKSON and DEPENDENT tuples that refer to an EMPLOYEE tuple it may not make sense to delete other EMPLOYEE tuples or a DEPARTMENT tuple. In general when a referential integrity constraint is specified in the DDL the DBMS will allow the database designer to specify which of the options applies in case of a violation of the constraint. We discuss how to specify these options in the SQL DDL in Chapter The Update Operation The Update operation is used to change the values of one or more attributes in a tuple of some relation R. It is necessary to specify a condition on the attributes of the relation to select the tuple to be modified. Here are some examples. Operation Update the salary of the EMPLOYEE tuple with Ssn to Result Acceptable. Operation Update the Dno of the EMPLOYEE tuple with Ssn to Result Acceptable. Operation Update the Dno of the EMPLOYEE tuple with Ssn to Result Unacceptable because it violates referential integrity. Operation Update the Ssn of the EMPLOYEE tuple with Ssn to Result Unacceptable because it violates primary key constraint by repeating a value that already exists as a primary key in another tuple it violates referential integrity constraints because there are other relations that refer to the existing value of Ssn. Updating an attribute that is neither part of a primary key nor of a foreign key usually causes no problems the DBMS need only check to confirm that the new value is of the correct data type and domain. Modifying a primary key value is similar to deleting one tuple and inserting another in its place because we use the primary key to identify tuples. Hence the issues discussed earlier in both Sections and come into play. If a foreign key attribute is modified the DBMS must Summary make sure that the new value refers to an existing tuple in the referenced relation . Similar options exist to deal with referential integrity violations caused by Update as those options discussed for the Delete operation. In fact when a referential integrity constraint is specified in the DDL the DBMS will allow the user to choose separate options to deal with a violation caused by Delete and a violation caused by Update systems are executing transactions at rates that reach several hundred per second. Transaction processing concepts concurrent execution of transactions and recovery from failures will be discussed in Chapters to Summary In this chapter we presented the modeling concepts data structures and constraints provided by the relational model of data. We started by introducing the concepts of domains attributes and tuples. Then we defined a relation schema as a list of attributes that describe the structure of a relation. A relation or relation state is a set of tuples that conforms to the schema. Several characteristics differentiate relations from ordinary tables or files. The first is that a relation is not sensitive to the ordering of tuples. The second involves the ordering of attributes in a relation schema and the corresponding ordering of values within a tuple. We gave an alternative definition of relation that does not require these two orderings but we continued to use the first definition which requires attributes and tuple values to be ordered for convenience. Then we discussed values in tuples and introduced NULL values to represent missing or unknown information. We emphasized that NULL values should be avoided as much as possible. We classified database constraints into inherent model based constraints explicit schema based constraints and application based constraints otherwise known as semantic constraints or business rules. Then we discussed the schema constraints Chapter The Relational Data Model and Relational Database Constraints pertaining to the relational model starting with domain constraints then key constraints including the concepts of superkey candidate key and primary key and the NOT NULL constraint on attributes. We defined relational databases and relational database schemas. Additional relational constraints include the entity integrity constraint which prohibits primary key attributes from being NULL. We described the interrelation referential integrity constraint which is used to maintain consistency of references among tuples from different relations. The modification operations on the relational model are Insert Delete and Update. Each operation may violate certain types of constraints . This represents classes taught in a university with unique Univsection#s. Identify what you think should be various candidate keys and write in your own words the conditions or assumptions under which each candidate key would be valid. Chapter The Relational Data Model and Relational Database Constraints AIRPORT Airportcode Name City State Flightnumber Airline Weekdays FLIGHT FLIGHTLEG Flightnumber Legnumber Departureairportcode Scheduleddeparturetime Arrivalairportcode Scheduledarrivaltime LEGINSTANCE Flightnumber Legnumber Date Numberofavailableseats Airplaneid FARE Flightnumber Farecode Amount Restrictions AIRPLANETYPE Airplanetypename Maxseats Company CANLAND Airplanetypename Airportcode AIRPLANE Airplaneid Totalnumberofseats Airplanetype SEATRESERVATION Flightnumber Legnumber Date Seatnumber Customername Customerphone Departureairportcode Departuretime Arrivalairportcode Arrivaltime Figure The AIRLINE relational database schema. Consider the following six relations for an order processing database application in a company CUSTOMER ORDER ORDERITEM Exercises ITEM SHIPMENT WAREHOUSE Here Ordamt refers to total dollar amount of an order Odate is the date the order was placed and Shipdate is the date an order is shipped from the warehouse. Assume that an order can be shipped from several warehouses. Specify the foreign keys for this schema stating any assumptions you make. What other constraints can you think of for this database Consider the following relations for a database that keeps track of business trips of salespersons in a sales office SALESPERSON TRIP EXPENSE A trip can be charged to one or more accounts. Specify the foreign keys for this schema stating any assumptions you make. Consider the following relations for a database that keeps track of student enrollment in courses and the books adopted for each course STUDENT COURSE ENROLL BOOKADOPTION TEXT Specify the foreign keys for this schema stating any assumptions you make. Consider the following relations for a database that keeps track of automobile sales in a car dealership CAR OPTION SALE SALESPERSON First specify the foreign keys for this schema stating any assumptions you make. Next populate the relations with a few sample tuples and then give an example of an insertion in the SALE and SALESPERSON relations that violates the referential integrity constraints and of another insertion that does not. Database design often involves decisions about the storage of attributes. For example a Social Security number can be stored as one attribute or split into three attributes . However Social Security numbers are usually represented as just one attribute. The decision Chapter The Relational Data Model and Relational Database Constraints is based on how the database will be used. This exercise asks you to think about specific situations where dividing the SSN is useful. Consider a STUDENT relation in a UNIVERSITY database with the following attributes . Note that the cell phone may be from a different city and state from the local phone. A possible tuple of the relation is shown below Name Ssn Localphone Address Cellphone Age Gpa George Shaw Main William Edwards Anytown CA a. Identify the critical missing information from the Localphone and Cellphone attributes. b. Would you store this additional information in the Localphone and Cellphone attributes or add new attributes to the schema for STUDENT c. Consider the Name attribute. What are the advantages and disadvantages of splitting this field from one attribute into three attributes d. What general guideline would you recommend for deciding when to store information in a single attribute and when to split the information e. Suppose the student can have between and phones. Suggest two different designs that allow this type of information. Recent changes in privacy laws have disallowed organizations from using Social Security numbers to identify individuals unless certain restrictions are satisfied. As a result most . universities cannot use SSNs as primary keys . In practice Studentid a unique identifier assigned to every student is likely to be used as the primary key rather than SSN since Studentid can be used throughout the system. a. Some database designers are reluctant to use generated keys for primary keys because they are artificial. Can you propose any natural choices of keys that can be used to identify the student record in a UNIVERSITY database b. Suppose that you are able to guarantee uniqueness of a natural key that includes last name. Are you guaranteed that the last name will not change during the lifetime of the database If last name can change what solutions can you propose for creating a primary key that still includes last name but remains unique c. What are the advantages and disadvantages of using generated keys Selected Bibliography Selected Bibliography The relational model was introduced by Codd in a classic paper. Codd also introduced relational algebra and laid the theoretical foundations for the relational model in a series of papers for his work on the relational model. In a later paper Codd discussed extending the relational model to incorporate more meta data and semantics about the relations he also proposed a three valued logic to deal with uncertainty in relations and incorporating NULLs in the relational algebra. The resulting model is known as RM T. Childs had earlier used set theory to model databases. Later Codd published a book examining over features of the relational data model and database systems. Date provides a retrospective review and analysis of the relational data model. Since Codd’s pioneering work much research has been conducted on various aspects of the relational model. Todd describes an experimental DBMS called PRTV that directly implements the relational algebra operations. Schmidt and Swenson introduce additional semantics into the relational model by classifying different types of relations. Chen’s Entity Relationship model which is discussed in Chapter is a means to communicate the real world semantics of a relational database at the conceptual level. Wiederhold and Elmasri introduce various types of connections between relations to enhance its constraints. Extensions of the relational model are discussed in Chapters and Additional bibliographic notes for other aspects of the relational model and its languages systems extensions and theory are given in Chapters to and Maier and Atzeni and De Antonellis provide an extensive theoretical treatment of the relational data model. This page intentionally left blank Basic SQL The SQL language may be considered one of the major reasons for the commercial success of relational databases. Because it became a standard for relational databases users were less concerned about migrating their database applications from other types of database systems for example network or hierarchical systems to relational systems. This is because even if the users became dissatisfied with the particular relational DBMS product they were using converting to another relational DBMS product was not expected to be too expensive and time consuming because both systems followed the same language standards. In practice of course there are many differences between various commercial relational DBMS packages. However if the user is diligent in using only those features that are part of the standard and if both relational systems faithfully support the standard then conversion between the two systems should be much simplified. Another advantage of having such a standard is that users may write statements in a database application program that can access data stored in two or more relational DBMSs without having to change the database sublanguage if both relational DBMSs support standard SQL. This chapter presents the main features of the SQL standard for commercial relational DBMSs whereas Chapter presented the most important concepts underlying the formal relational data model. In Chapter and was designed and implemented at IBM Research as the interface for an experimental relational database system called SYSTEM R. SQL is now the standard language for commercial relational DBMSs. A joint effort by the American National Standards Institute and the International Standards Organization has led to a standard version of SQL multimedia data and so on. Because SQL is very important we devote two chapters to its features. In this chapter Section describes the SQL DDL commands for creating schemas and tables and gives an overview of the basic data types in SQL. Section presents how basic constraints such as key and referential integrity are specified. Section describes the basic SQL constructs for specifying retrieval queries and Section describes the SQL commands for insertion deletion and data updates. In Chapter we will describe more complex SQL retrieval queries as well as the ALTER commands for changing the schema. We will also describe the CREATE ASSERTION statement which allows the specification of more general constraints on the database. We also introduce the concept of triggers which is presented in SQL had statements for creating and dropping indexes on the files that represent relations but these have been dropped from the SQL standard for some time. SQL Data Definition and Data Types more detail in Chapter and we will describe the SQL facility for defining views on the database in Chapter Views are also called virtual or derived tables because they present the user with what appear to be tables however the information in those tables is derived from previously defined tables. Section lists some SQL features that are presented in other chapters of the book these include transaction control in Chapter security authorization in Chapter active databases in Chapter object oriented features in Chapter and online analytical processing features in Chapter Section summarizes the chapter. Chapters and discuss the various database programming techniques for programming with SQL. SQL Data Definition and Data Types SQL uses the terms table row and column for the formal relational model terms relation tuple and attribute respectively. We will use the corresponding terms interchangeably. The main SQL command for data definition is the CREATE statement which can be used to create schemas tables and domains . Before we describe the relevant CREATE statements we discuss schema and catalog concepts in Section to place our discussion in perspective. Section describes how tables are created and Section describes the most important data types available for attribute specification. Because the SQL specification is very large we give a description of the most important features. Further details can be found in the various SQL standards documents . Schema and Catalog Concepts in SQL Early versions of SQL did not include the concept of a relational database schema all tables were considered part of the same schema. The concept of an SQL schema was incorporated starting with in order to group together tables and other constructs that belong to the same database application. An SQL schema is identified by a schema name and includes an authorization identifier to indicate the user or account who owns the schema as well as descriptors for each element in the schema. Schema elements include tables constraints views domains and other constructs that describe the schema. A schema is created via the CREATE SCHEMA statement which can include all the schema elements’ definitions. Alternatively the schema can be assigned a name and authorization identifier and the elements can be defined later. For example the following statement creates a schema called COMPANY owned by the user with authorization identifier ‘Jsmith’. Note that each statement in SQL ends with a semicolon. CREATE SCHEMA COMPANY AUTHORIZATION ‘Jsmith’ In general not all users are authorized to create schemas and schema elements. The privilege to create schemas tables and other constructs must be explicitly granted to the relevant user accounts by the system administrator or DBA. Chapter Basic SQL In addition to the concept of a schema SQL uses the concept of a catalog a named collection of schemas in an SQL environment. An SQL environment is basically an installation of an SQL compliant RDBMS on a computer A catalog always contains a special schema called INFORMATIONSCHEMA which provides information on all the schemas in the catalog and all the element descriptors in these schemas. Integrity constraints such as referential integrity can be defined between relations only if they exist in schemas within the same catalog. Schemas within the same catalog can also share certain elements such as domain definitions. The CREATE TABLE Command in SQL The CREATE TABLE command is used to specify a new relation by giving it a name and specifying its attributes and initial constraints. The attributes are specified first and each attribute is given a name a data type to specify its domain of values and any attribute constraints such as NOT NULL. The key entity integrity and referential integrity constraints can be specified within the CREATE TABLE statement after the attributes are declared or they can be added later using the ALTER TABLE command make the EMPLOYEE table part of the COMPANY schema. The relations declared through CREATE TABLE statements are called base tables this means that the relation and its tuples are actually created and stored as a file by the DBMS. Base relations are distinguished from virtual relations created through the CREATE VIEW statement are not considered to be ordered within a relation. It is important to note that in Figure there are some foreign keys that may cause errors because they are specified either via circular references or because they refer to a table that has not yet been created. For example the foreign key Superssn in the EMPLOYEE table is a circular reference because it refers to the table itself. The foreign key Dno in the EMPLOYEE table refers to the DEPARTMENT table which has also includes the concept of a cluster of catalogs within an environment. SQL Data Definition and Data Types CREATE TABLE EMPLOYEE FOREIGN KEY REFERENCES EMPLOYEE FOREIGN KEY REFERENCES DEPARTMENT CREATE TABLE DEPARTMENT UNIQUE FOREIGN KEY REFERENCES EMPLOYEE CREATE TABLE DEPTLOCATIONS FOREIGN KEY REFERENCES DEPARTMENT CREATE TABLE PROJECT UNIQUE FOREIGN KEY REFERENCES DEPARTMENT CREATE TABLE WORKSON FOREIGN KEY REFERENCES EMPLOYEE FOREIGN KEY REFERENCES PROJECT CREATE TABLE DEPENDENT FOREIGN KEY REFERENCES EMPLOYEE Figure SQL CREATE TABLE data definition statements for defining the COMPANY schema from Figure Chapter Basic SQL not been created yet. To deal with this type of problem these constraints can be left out of the initial CREATE TABLE statement and then added later using the ALTER TABLE statement and floating point numbers of various precision . Formatted numbers can be declared by using DECIMAL or DEC or NUMERIC where i the precision is the total number of decimal digits and j the scale is the number of digits after the decimal point. The default for scale is zero and the default for precision is implementation defined. Character string data types are either fixed length CHAR or CHARACTER where n is the number of characters or varying length VARCHAR or CHAR VARYING or CHARACTER VARYING where n is the maximum number of characters. When specifying a literal string value it is placed between single quotation marks and it is case sensitive order if a string appears before another string in alphabetic order then is considered to be less than There is also a concatenation operator denoted by || that can concatenate two strings in SQL. For example ‘abc’ || ‘XYZ’ results in a single string ‘abcXYZ’. Another variable length string data type called CHARACTER LARGE OBJECT or CLOB is also available to specify columns that have large text values such as documents. The CLOB maximum length can be specified in kilobytes megabytes or gigabytes . For example specifies a maximum length of megabytes. Bit string data types are either of fixed length n BIT or varying length BIT VARYING where n is the maximum number of bits. The default for n the length of a character string or bit string is Literal bit strings are placed between single quotes but preceded by a B to distinguish is not the case with SQL keywords such as CREATE or CHAR. With keywords SQL is case insensitive meaning that SQL treats uppercase and lowercase letters as equivalent in keywords. nonalphabetic characters there is a defined order. SQL Data Definition and Data Types them from character strings for example Another variable length bitstring data type called BINARY LARGE OBJECT or BLOB is also available to specify columns that have large binary values such as images. As for CLOB the maximum length of a BLOB can be specified in kilobits megabits or gigabits . For example specifies a maximum length of gigabits. A Boolean data type has the traditional values of TRUE or FALSE. In SQL because of the presence of NULL values a three valued logic is used so a third possible value for a Boolean data type is UNKNOWN. We discuss the need for UNKNOWN and the three valued logic in Chapter The DATE data type has ten positions and its components are YEAR MONTH and DAY in the form YYYY MM DD. The TIME data type has at least eight positions with the components HOUR MINUTE and SECOND in the form HH MM SS. Only valid dates and times should be allowed by the SQL implementation. This implies that months should be between and and dates must be between and furthermore a date should be a valid date for the corresponding month. The comparison can be used with dates or times an earlier date is considered to be smaller than a later date and similarly with time. Literal values are represented by single quoted strings preceded by the keyword DATE or TIME for example DATE or TIME In addition a data type TIME where i is called time fractional seconds precision specifies i + additional positions for TIME one position for an additional period separator character and i positions for specifying decimal fractions of a second. A TIME WITH TIME ZONE data type includes an additional six positions for specifying the displacement from the standard universal time zone which is in the range to in units of HOURS MINUTES. If WITH TIME ZONE is not included the default is the local time zone for the SQL session. Some additional data types are discussed below. The list of types discussed here is not exhaustive different implementations have added more data types to SQL. A timestamp data type includes the DATE and TIME fields plus a minimum of six positions for decimal fractions of seconds and an optional WITH TIME ZONE qualifier. Literal values are represented by singlequoted strings preceded by the keyword TIMESTAMP with a blank space between data and time for example TIMESTAMP Another data type related to DATE TIME and TIMESTAMP is the INTERVAL data type. This specifies an interval a relative value that can be used to increment or decrement an absolute value of a date time or timestamp. Intervals are qualified to be either YEAR MONTH intervals or DAY TIME intervals. strings whose length is a multiple of can be specified in hexadecimal notation where the literal string is preceded by X and each hexadecimal character represents bits. Chapter Basic SQL The format of DATE TIME and TIMESTAMP can be considered as a special type of string. Hence they can generally be used in string comparisons by being cast into the equivalent strings. It is possible to specify the data type of each attribute directly as in Figure alternatively a domain can be declared and the domain name used with the attribute specification. This makes it easier to change the data type for a domain that is used by numerous attributes in a schema and improves schema readability. For example we can create a domain SSNTYPE by the following statement CREATE DOMAIN SSNTYPE AS We can use SSNTYPE in place of in Figure for the attributes Ssn and Superssn of EMPLOYEE Mgrssn of DEPARTMENT Essn of WORKSON and Essn of DEPENDENT. A domain can also have an optional default specification via a DEFAULT clause as we discuss later for attributes. Notice that domains may not be available in some implementations of SQL. Specifying Constraints in SQL This section describes the basic constraints that can be specified in SQL as part of table creation. These include key and referential integrity constraints restrictions on attribute domains and NULLs and constraints on individual tuples within a relation. We discuss the specification of more general constraints called assertions in Chapter Specifying Attribute Constraints and Attribute Defaults Because SQL allows NULLs as attribute values a constraint NOT NULL may be specified if NULL is not permitted for a particular attribute. This is always implicitly specified for the attributes that are part of the primary key of each relation but it can be specified for any other attributes whose values are required not to be NULL as shown in Figure It is also possible to define a default value for an attribute by appending the clause DEFAULT value to an attribute definition. The default value is included in any new tuple if an explicit value is not provided for that attribute. Figure illustrates an example of specifying a default manager for a new department and a default department for a new employee. If no default clause is specified the default default value is NULL for attributes that do not have the NOT NULL constraint. Another type of constraint can restrict attribute or domain values using the CHECK clause following an attribute or domain For example suppose that department numbers are restricted to integer numbers between and then we can change the attribute declaration of Dnumber in the DEPARTMENT table CONSTRAINT EMPSUPERFK FOREIGN KEY REFERENCES EMPLOYEE ON DELETE SET NULL ON UPDATE CASCADE CONSTRAINT EMPDEPTFK FOREIGN KEY REFERENCES DEPARTMENT ON DELETE SET DEFAULT ON UPDATE CASCADE CREATE TABLE DEPARTMENT CONSTRAINT DEPTSK UNIQUE CONSTRAINT DEPTMGRFK FOREIGN KEY REFERENCES EMPLOYEE ON DELETE SET DEFAULT ON UPDATE CASCADE CREATE TABLE DEPTLOCATIONS FOREIGN KEY REFERENCES DEPARTMENT ON DELETE CASCADE ON UPDATE CASCADE Figure Example illustrating how default attribute values and referential integrity triggered actions are specified in SQL. The CHECK clause can also be used in conjunction with the CREATE DOMAIN statement. For example we can write the following statement CREATE DOMAIN DNUM AS INTEGER CHECK keys as illustrated in the DEPARTMENT and PROJECT table declarations in Figure The UNIQUE clause can also be specified directly for a secondary key if the secondary key is a single attribute as in the following example Dname UNIQUE Referential integrity is specified via the FOREIGN KEY clause as shown in Figure As we discussed in Section a referential integrity constraint can be violated when tuples are inserted or deleted or when a foreign key or primary key attribute value is modified. The default action that SQL takes for an integrity violation is to reject the update operation that will cause a violation which is known as the RESTRICT option. However the schema designer can specify an alternative action to be taken by attaching a referential triggered action clause to any foreign key constraint. The options include SET NULL CASCADE and SET DEFAULT. An option must be qualified with either ON DELETE or ON UPDATE. We illustrate this with the examples shown in Figure Here the database designer chooses ON DELETE SET NULL and ON UPDATE CASCADE for the foreign key Superssn of EMPLOYEE. This means that if the tuple for a supervising employee is deleted the value of Superssn is automatically set to NULL for all employee tuples that were referencing the deleted employee tuple. On the other hand if the Ssn value for a supervising employee is updated the new value is cascaded to Superssn for all employee tuples referencing the updated employee In general the action taken by the DBMS for SET NULL or SET DEFAULT is the same for both ON DELETE and ON UPDATE The value of the affected referencing attributes is changed to NULL for SET NULL and to the specified default value of the referencing attribute for SET DEFAULT. The action for CASCADE ON DELETE is to delete all the referencing tuples whereas the action for CASCADE ON UPDATE is to change the value of the referencing foreign key attribute to the updated primary key value for all the referencing tuples. It is the responsibility of the database designer to choose the appropriate action and to specify it in the database schema. As a general rule the CASCADE option is suitable for “relationship” relations The CHECK clause can also be used to specify more general constraints using the CREATE ASSERTION statement of SQL. We discuss this in Chapter because it requires the full power of queries which are discussed in Sections and Basic Retrieval Queries in SQL SQL has one basic statement for retrieving information from a database the SELECT statement. The SELECT statement is not the same as the SELECT operation of relational algebra which we discuss in Chapter There are many options and flavors to the SELECT statement in SQL so we will introduce its features gradually. We will use sample queries specified on the schema of Figure and will refer to the sample database state shown in Figure to show the results of some of the sample queries. In this section we present the features of SQL for simple retrieval queries. Features of SQL for specifying more complex retrieval queries are presented in Section Before proceeding we must point out an important distinction between SQL and the formal relational model discussed in Chapter SQL allows a table to have two or more tuples that are identical in all their attribute values. Hence in general an SQL table is not a set of tuples because a set does not allow two identical members rather it is a multiset of tuples. Some SQL relations are constrained to be sets because a key constraint has been declared or because the DISTINCT option has been used with the SELECT statement . We should be aware of this distinction as we discuss the examples. The SELECT FROM WHERE Structure of Basic SQL Queries Queries in SQL can be very complex. We will start with simple queries and then progress to more complex ones in a step by step manner. The basic form of the SELECT statement sometimes called a mapping or a select from where block is Chapter Basic SQL formed of the three clauses SELECT FROM and WHERE and has the following SELECT attribute list FROM table list WHERE condition where attribute list is a list of attribute names whose values are to be retrieved by the query. table list is a list of the relation names required to process the query. condition is a conditional expression that identifies the tuples to be retrieved by the query. In SQL the basic logical comparison operators for comparing attribute values with one another and with literal constants are and . These correspond to the relational algebra operators ≤ ≥ and ≠ respectively and to the C C++ programming language operators and ! . The main syntactic difference is the not equal operator. SQL has additional comparison operators that we will present gradually. We illustrate the basic SELECT statement in SQL with some sample queries. The queries are labeled here with the same query numbers used in Chapter for easy cross reference. Query Retrieve the birth date and address of the employee whose name is ‘John B. Smith’. SELECT Bdate Address FROM EMPLOYEE WHERE Fname ‘John’ AND Minit ‘B’ AND Lname ‘Smith’ This query involves only the EMPLOYEE relation listed in the FROM clause. The query selects the individual EMPLOYEE tuples that satisfy the condition of the WHERE clause then projects the result on the Bdate and Address attributes listed in the SELECT clause. The SELECT clause of SQL specifies the attributes whose values are to be retrieved which are called the projection attributes and the WHERE clause specifies the Boolean condition that must be true for any retrieved tuple which is known as the selection condition. Figure shows the result of query on the database of Figure We can think of an implicit tuple variable or iterator in the SQL query ranging or looping over each individual tuple in the EMPLOYEE table and evaluating the condition in the WHERE clause. Only those tuples that satisfy the condition that is SELECT and FROM clauses are required in all SQL queries. The WHERE is optional Bdate Houston TX Address Fname John Franklin Ramesh Joyce Smith Wong Narayan English Fondren Houston TX Voss Houston TX Fire Oak Humble TX Rice Houston TX Lname Address John Franklin Alicia Zelaya Joyce Ramesh Jennifer Wallace Ahmad Jabbar Smith Wong Narayan English Jennifer James Jennifer Franklin James Franklin Franklin Wallace Borg Wallace Wong Borg Wong Wong Fname John Franklin K Joyce Ramesh A B M M M Narayan English Smith Wong Fire Oak Humble TX Rice Houston TX Fondren Houston TX Voss Houston TX Minit Lname Ssn Bdate Address Sex Dno Salary Superssn Pnumber Wallace Bellaire TX Wallace Bellaire TX Dnum Lname Bdate Address Ssn Research Research Research Research Research Research Research Research Administration Administration Administration Administration Administration Administration Administration Administration Headquarters Headquarters Headquarters Headquarters Headquarters Headquarters Headquarters Headquarters Dname Figure Results of SQL queries when applied to the COMPANY database state shown in Figure those tuples for which the condition evaluates to TRUE after substituting their corresponding attribute values are selected. Query Retrieve the name and address of all employees who work for the ‘Research’ department. SELECT Fname Lname Address FROM EMPLOYEE DEPARTMENT WHERE Dname ‘Research’ AND Dnumber Dno In the WHERE clause of the condition Dname ‘Research’ is a selection condition that chooses the particular tuple of interest in the DEPARTMENT table because Dname is an attribute of DEPARTMENT. The condition Dnumber Dno is called a join condition because it combines two tuples one from DEPARTMENT and one from EMPLOYEE whenever the value of Dnumber in DEPARTMENT is equal to the value of Dno in EMPLOYEE. The result of query is shown in Figure In general any number of selection and join conditions may be specified in a single SQL query. A query that involves only selection and join conditions plus projection attributes is known as a select project join query. The next example is a select project join query with two join conditions. Query For every project located in ‘Stafford’ list the project number the controlling department number and the department manager’s last name address and birth date. SELECT Pnumber Dnum Lname Address Bdate FROM PROJECT DEPARTMENT EMPLOYEE WHERE Dnum Dnumber AND Mgrssn Ssn AND Plocation ‘Stafford’ The join condition Dnum Dnumber relates a project tuple to its controlling department tuple whereas the join condition Mgrssn Ssn relates the controlling department tuple to the employee tuple who manages that department. Each tuple in the result will be a combination of one project one department and one employee that satisfies the join conditions. The projection attributes are used to choose the attributes to be displayed from each combined tuple. The result of query is shown in Figure Ambiguous Attribute Names Aliasing Renaming and Tuple Variables In SQL the same name can be used for two attributes as long as the attributes are in different relations. If this is the case and a multitable query refers to two or more attributes with the same name we must qualify the attribute name with the relation name to prevent ambiguity. This is done by prefixing the relation name to the attribute name and separating the two by a period. To illustrate this suppose that in Figures and the Dno and Lname attributes of the EMPLOYEE relation were Chapter Basic SQL called Dnumber and Name and the Dname attribute of DEPARTMENT was also called Name then to prevent ambiguity query would be rephrased as shown in We must prefix the attributes Name and Dnumber in to specify which ones we are referring to because the same attribute names are used in both relations SELECT Fname Address FROM EMPLOYEE DEPARTMENT WHERE ‘Research’ AND Fully qualified attribute names can be used for clarity even if there is no ambiguity in attribute names. is shown in this manner as is below. We can also create an alias for each table name to avoid repeated typing of long table names . SELECT FROM EMPLOYEE DEPARTMENT WHERE ‘Research’ AND The ambiguity of attribute names also arises in the case of queries that refer to the same relation twice as in the following example. Query For each employee retrieve the employee’s first and last name and the first and last name of his or her immediate supervisor. SELECT FROM EMPLOYEE AS E EMPLOYEE AS S WHERE In this case we are required to declare alternative relation names E and S called aliases or tuple variables for the EMPLOYEE relation. An alias can follow the keyword AS as shown in or it can directly follow the relation name for example by writing EMPLOYEE E EMPLOYEE S in the FROM clause of It is also possible to rename the relation attributes within the query in SQL by giving them aliases. For example if we write EMPLOYEE AS E in the FROM clause Fn becomes an alias for Fname Mi for Minit Ln for Lname and so on. In we can think of E and S as two different copies of the EMPLOYEE relation the first E represents employees in the role of supervisees or subordinates the second S represents employees in the role of supervisors. We can now join the two copies. Of course in reality there is only one EMPLOYEE relation and the join condition is meant to join the relation with itself by matching the tuples that satisfy the join condition . Notice that this is an example of a one level recursive query as we will discuss in Section In earlier versions of SQL it was not possible to specify a general recursive query with an unknown number of levels in a Basic Retrieval Queries in SQL Chapter Basic SQL single SQL statement. A construct for specifying recursive queries has been incorporated into . To retrieve all the attribute values of the selected tuples we do not have to list the attribute names explicitly in SQL we just specify an asterisk which stands for all the attributes. For example query retrieves all the attribute values of any EMPLOYEE who works in DEPARTMENT number Salary Fname Lname Fname Lname James Borg Figure Results of additional SQL queries when applied to the COMPANY database state shown in Figure SQL has directly incorporated some of the set operations from mathematical set theory which are also part of relational algebra set difference and set intersection operations. The relations resulting from these set operations are sets of tuples that is duplicate tuples are eliminated from the result. These set operations apply only to union compatible relations so we must make sure that the two relations on which we apply the operation have the same attributes and that the attributes appear in the same order in both relations. The next example illustrates the use of UNION. Query Make a list of all project numbers for projects that involve an employee whose last name is ‘Smith’ either as a worker or as a manager of the department that controls the project. UNION The first SELECT query retrieves the projects that involve a ‘Smith’ as manager of the department that controls the project and the second retrieves the projects that involve a ‘Smith’ as a worker on the project. Notice that if several employees have the last name ‘Smith’ the project names involving any of them will be retrieved. Applying the UNION operation to the two SELECT queries gives the desired result. SQL also has corresponding multiset operations which are followed by the keyword ALL . Their results are multisets . The behavior of these operations is illustrated by the examples in Figure Basically each tuple whether it is a duplicate or not is considered as a different tuple when applying these operations. some systems the keyword MINUS is used for the set difference operation instead of EXCEPT. Basic Retrieval Queries in SQL T A T A T A R A S A Figure The results of SQL multiset operations. Two tables R and S. R UNION ALL S. R EXCEPT ALL S. R INTERSECT ALL S. Substring Pattern Matching and Arithmetic Operators In this section we discuss several more features of SQL. The first feature allows comparison conditions on only parts of a character string using the LIKE comparison operator. This can be used for string pattern matching. Partial strings are specified using two reserved characters % replaces an arbitrary number of zero or more characters and the underscore replaces a single character. For example consider the following query. Query Retrieve all employees whose address is in Houston Texas. SELECT Fname Lname FROM EMPLOYEE WHERE Address LIKE ‘%Houston TX%’ To retrieve all employees who were born during the we can use Query Here must be the third character of the string so we use the value ‘        ’ with each underscore serving as a placeholder for an arbitrary character. Query Find all employees who were born during the SELECT Fname Lname FROM EMPLOYEE WHERE Bdate LIKE ‘        ’ If an underscore or % is needed as a literal character in the string the character should be preceded by an escape character which is specified after the string using the keyword ESCAPE. For example ‘AB\CD\%EF’ ESCAPE ‘\’ represents the literal string ‘ABCD%EF’ because \ is specified as the escape character. Any character not used in the string can be chosen as the escape character. Also we need a rule to specify apostrophes or single quotation marks if they are to be included in a string because they are used to begin and end strings. If an apostrophe is needed it is represented as two consecutive apostrophes so that it will not be interpreted as ending the string. Notice that substring comparison implies that attribute values Chapter Basic SQL are not atomic values as we had assumed in the formal relational model subtraction multiplication and division can be applied to numeric values or attributes with numeric domains. For example suppose that we want to see the effect of giving all employees who work on the ‘ProductX’ project a percent raise we can issue Query to see what their salaries would become. This example also shows how we can rename an attribute in the query result using AS in the SELECT clause. Query Show the resulting salaries if every employee working on the ‘ProductX’ project is given a percent raise. SELECT AS Increasedsal FROM EMPLOYEE AS E WORKSON AS W PROJECT AS P WHERE AND AND ‘ProductX’ For string data types the concatenate operator || can be used in a query to append two string values. For date time timestamp and interval data types operators include incrementing or decrementing a date time or timestamp by an interval. In addition an interval value is the result of the difference between two date time or timestamp values. Another comparison operator which can be used for convenience is BETWEEN which is illustrated in Query Query Retrieve all employees in department whose salary is between and SELECT FROM EMPLOYEE WHERE needed in the simple query. The WHERE clause identifies the conditions for selecting the tuples from these relations including join conditions if needed. ORDER BY specifies an order for displaying the results of a query. Two additional clauses GROUP BY and HAVING will be described in Section In Chapter we will present more complex features of SQL retrieval queries. These include the following nested queries that allow one query to be included as part of another query aggregate functions that are used to provide summaries of the information in the tables two additional clauses that can be used to provide additional power to aggregate functions and various types of joins that can combine records from various tables in different ways. INSERT DELETE and UPDATE Statements in SQL In SQL three commands can be used to modify the database INSERT DELETE and UPDATE. We discuss each of these in turn. The INSERT Command In its simplest form INSERT is used to add a single tuple to a relation. We must specify the relation name and a list of values for the tuple. The values should be listed in the same order in which the corresponding attributes were specified in the CREATE TABLE command. For example to add a new tuple to the EMPLOYEE relation shown Chapter Basic SQL in Figure and specified in the CREATE TABLE EMPLOYEE command in Figure we can use INSERT INTO EMPLOYEE VALUES A second form of the INSERT statement allows the user to specify explicit attribute names that correspond to the values provided in the INSERT command. This is useful if a relation has many attributes but only a few of those attributes are assigned values in the new tuple. However the values must include all attributes with NOT NULL specification and no default value. Attributes with NULL allowed or DEFAULT values are the ones that can be left out. For example to enter a tuple for a new EMPLOYEE for whom we know only the Fname Lname Dno and Ssn attributes we can use INSERT INTO EMPLOYEE VALUES VALUES INSERT INTO EMPLOYEE VALUES A variation of the INSERT command inserts multiple tuples into a relation in conjunction with creating the relation and loading it with the result of a query. For example to create a temporary table that has the employee last name project name and hours per week for each employee working on a project we can write the statements in and CREATE TABLE WORKSONINFO INSERT DELETE and UPDATE Statements in SQL INSERT INTO WORKSONINFO SELECT FROM PROJECT P WORKSON W EMPLOYEE E WHERE AND A table WORKSONINFO is created by and is loaded with the joined information retrieved from the database by the query in We can now query WORKSONINFO as we would any other relation when we do not need it any more we can remove it by using the DROP TABLE command SQL SQL CLI and its predecessor ODBC and SQL PSM . We discuss these techniques in Chapter We also discuss how to access SQL databases through the Java programming language using JDBC and SQLJ. Each commercial RDBMS will have in addition to the SQL commands a set of commands for specifying physical database design parameters file structures for relations and access paths such as indexes. We called these commands a storage definition language in Chapter Earlier versions of SQL had commands for creating indexes but these were removed from the Summary language because they were not at the conceptual schema level. Many systems still have the CREATE INDEX commands. SQL has transaction control commands. These are used to specify units of database processing for concurrency control and recovery purposes. We discuss these commands in Chapter after we discuss the concept of transactions in more detail. SQL has language constructs for specifying the granting and revoking of privileges to users. Privileges typically correspond to the right to use certain SQL commands to access certain relations. Each relation is assigned an owner and either the owner or the DBA staff can grant to selected users the privilege to use an SQL statement such as SELECT INSERT DELETE or UPDATE to access the relation. In addition the DBA staff can grant the privileges to create schemas tables or views to certain users. These SQL commands called GRANT and REVOKE are discussed in Chapter where we discuss database security and authorization. SQL has language constructs for creating triggers. These are generally referred to as active database techniques since they specify actions that are automatically triggered by events such as database updates. We discuss these features in Section where we discuss active database concepts. SQL has incorporated many features from object oriented models to have more powerful capabilities leading to enhanced relational systems known as object relational. Capabilities such as creating complex structured attributes specifying abstract data types for attributes and tables creating object identifiers for referencing tuples and specifying operations on types are discussed in Chapter SQL and relational databases can interact with new technologies such as XML in SQL differ from the relations defined formally in Chapter Discuss the other differences in terminology. Why does SQL allow duplicate tuples in a table or in a query result List the data types that are allowed for SQL attributes. How does SQL allow implementation of the entity integrity and referential integrity constraints described in Chapter What about referential triggered actions Describe the four clauses in the syntax of a simple SQL retrieval query. Show what type of constructs can be specified in each of the clauses. Which are required and which are optional Exercises Consider the database shown in Figure whose schema is shown in Figure What are the referential integrity constraints that should hold on the schema Write appropriate SQL DDL statements to define the database. Repeat Exercise but use the AIRLINE database schema of Figure Consider the LIBRARY relational database schema shown in Figure Choose the appropriate action for each referential integrity constraint both for the deletion of a referenced tuple and for the update of a primary key attribute value in a referenced tuple. Justify your choices. Write appropriate SQL DDL statements for declaring the LIBRARY relational database schema of Figure Specify the keys and referential triggered actions. How can the key and foreign key constraints be enforced by the DBMS Is the enforcement technique you suggest difficult to implement Can the constraint checks be executed efficiently when updates are applied to the database Specify the following queries in SQL on the COMPANY relational database schema shown in Figure Show the result of each query if it is applied to the COMPANY database in Figure a. Retrieve the names of all employees in department who work more than hours per week on the ProductX project. b. List the names of all employees who have a dependent with the same first name as themselves. Exercises Bookid Title Publishername BOOK BOOKCOPIES Bookid Branchid Noofcopies BOOKAUTHORS Bookid Authorname LIBRARYBRANCH Branchid Branchname Address PUBLISHER Name Address Phone BOOKLOANS Bookid Branchid Cardno Dateout Duedate BORROWER Cardno Name Address Phone Figure A relational database schema for a LIBRARY database. c. Find the names of all employees who are directly supervised by ‘Franklin Wong’. Specify the updates of Exercise using the SQL update commands. Specify the following queries in SQL on the database schema of Figure a. Retrieve the names of all senior students majoring in ‘CS’ . b. Retrieve the names of all courses taught by Professor King in and c. For each section taught by Professor King retrieve the course number semester year and number of students who took the section. d. Retrieve the name and transcript of each senior student REFERENCES EMPLOYEE ON DELETE CASCADE ON UPDATE CASCADE Answer the following questions a. What happens when the following command is run on the database state shown in Figure DELETE EMPLOYEE WHERE Lname ‘Borg’ b. Is it better to CASCADE or SET NULL in case of EMPSUPERFK constraint ON DELETE Write SQL statements to create a table EMPLOYEEBACKUP to back up the EMPLOYEE table shown in Figure Selected Bibliography The SQL language originally named SEQUEL was based on the language SQUARE described by Boyce et al. The syntax of SQUARE was modified into SEQUEL value not available or value not applicable . Consider the following examples to illustrate each of the meanings of NULL. Unknown value. A person’s date of birth is not known so it is represented by NULL in the database. Unavailable or withheld value. A person has a home phone but does not want it to be listed so it is withheld and represented as NULL in the database. Not applicable attribute. An attribute LastCollegeDegree would be NULL for a person who has no college degrees because it does not apply to that person. It is often not possible to determine which of the meanings is intended for example a NULL for the home phone of a person can have any of the three meanings. Hence SQL does not distinguish between the different meanings of NULL. In general each individual NULL value is considered to be different from every other NULL value in the various database records. When a NULL is involved in a comparison operation the result is considered to be UNKNOWN . Hence SQL uses a three valued logic with values TRUE FALSE and UNKNOWN instead of the standard two valued logic with values TRUE or FALSE. It is therefore necessary to define the results of three valued logical expressions when the logical connectives AND OR and NOT are used. Table shows the resulting values. Table Logical Connectives in Three Valued Logic AND TRUE FALSE UNKNOWN TRUE TRUE FALSE UNKNOWN FALSE FALSE FALSE FALSE UNKNOWN UNKNOWN FALSE UNKNOWN OR TRUE FALSE UNKNOWN TRUE TRUE TRUE TRUE FALSE TRUE FALSE UNKNOWN UNKNOWN TRUE UNKNOWN UNKNOWN NOT TRUE FALSE FALSE TRUE UNKNOWN UNKNOWN More Complex SQL Retrieval Queries In Tables and the rows and columns represent the values of the results of comparison conditions which would typically appear in the WHERE clause of an SQL query. Each expression result would have a value of TRUE FALSE or UNKNOWN. The result of combining the two values using the AND logical connective is shown by the entries in Table Table shows the result of using the OR logical connective. For example the result of is FALSE whereas the result of is UNKNOWN. Table shows the result of the NOT logical operation. Notice that in standard Boolean logic only TRUE or FALSE values are permitted there is no UNKNOWN value. In select project join queries the general rule is that only those combinations of tuples that evaluate the logical expression in the WHERE clause of the query to TRUE are selected. Tuple combinations that evaluate to FALSE or UNKNOWN are not selected. However there are exceptions to that rule for certain operations such as outer joins as we shall see in Section SQL allows queries that check whether an attribute value is NULL. Rather than using or to compare an attribute value to NULL SQL uses the comparison operators IS or IS NOT. This is because SQL considers each NULL value as being distinct from every other NULL value so equality comparison is not appropriate. It follows that when a join condition is specified tuples with NULL values for the join attributes are not included in the result of values V and evaluates to TRUE if v is one of the elements in V. The first nested query selects the project numbers of projects that have an employee with last name ‘Smith’ involved as manager while the second nested query selects the project numbers of projects that have an employee with last name ‘Smith’ involved as worker. In the outer query we use the OR logical connective to retrieve a PROJECT tuple if the PNUMBER value of that tuple is in the result of either nested query. Chapter More SQL Complex Queries Triggers Views and Schema Modification SELECT DISTINCT Pnumber FROM PROJECT WHERE Pnumber IN OR Pnumber IN If a nested query returns a single attribute and a single tuple the query result will be a single value. In such cases it is permissible to use instead of IN for the comparison operator. In general the nested query will return a table which is a set or multiset of tuples. SQL allows the use of tuples of values in comparisons by placing them within parentheses. To illustrate this consider the following query SELECT DISTINCT Essn FROM WORKSON WHERE IN This query will select the Essns of all employees who work the same combination on some project that employee ‘John Smith’ within each tuple in WORKSON with the set of type compatible tuples produced by the nested query. In addition to the IN operator a number of other comparison operators can be used to compare a single value v to a set or multiset v . The ANY operator returns TRUE if the value v is equal to some value in the set V and is hence equivalent to IN. The two keywords ANY and SOME have the same effect. Other operators that can be combined with ANY include and . The keyword ALL can also be combined with each of these operators. For example the comparison condition returns TRUE if the value v is greater than all the values in the set V. An example is the following query which returns the names of employees whose salary is greater than the salary of all the employees in department SELECT Lname Fname FROM EMPLOYEE WHERE Salary ALL More Complex SQL Retrieval Queries Notice that this query can also be specified using the MAX aggregate function for that relation. These rules are similar to scope rules for program variables in most programming languages that allow nested procedures and functions. To illustrate the potential ambiguity of attribute names in nested queries consider Query Query Retrieve the name of each employee who has a dependent with the same first name and is the same sex as the employee. SELECT FROM EMPLOYEE AS E WHERE IN In the nested query of we must qualify because it refers to the Sex attribute of EMPLOYEE from the outer query and DEPENDENT also has an attribute called Sex. If there were any unqualified references to Sex in the nested query they would refer to the Sex attribute of DEPENDENT. However we would not have to qualify the attributes Fname and Ssn of EMPLOYEE if they appeared in the nested query because the DEPENDENT relation does not have attributes called Fname and Ssn so there is no ambiguity. It is generally advisable to create tuple variables for all the tables referenced in an SQL query to avoid potential errors and ambiguities as illustrated in Correlated Nested Queries Whenever a condition in the WHERE clause of a nested query references some attribute of a relation declared in the outer query the two queries are said to be correlated. We can understand a correlated query better by considering that the nested query is evaluated once for each tuple in the outer query. For example we can think of as follows For each EMPLOYEE tuple evaluate the nested query which retrieves the Essn values for all DEPENDENT tuples with the same sex and name as that EMPLOYEE tuple if the Ssn value of the EMPLOYEE tuple is in the result of the nested query then select that EMPLOYEE tuple. Chapter More SQL Complex Queries Triggers Views and Schema Modification In general a query written with nested select from where blocks and using the or IN comparison operators can always be expressed as a single block query. For example may be written as in SELECT FROM EMPLOYEE AS E DEPENDENT AS D WHERE AND AND The EXISTS and UNIQUE Functions in SQL The EXISTS function in SQL is used to check whether the result of a correlated nested query is empty or not. The result of EXISTS is a Boolean value TRUE if the nested query result contains at least one tuple or FALSE if the nested query result contains no tuples. We illustrate the use of EXISTS and NOT EXISTS with some examples. First we formulate Query in an alternative form that uses EXISTS as in SELECT FROM EMPLOYEE AS E WHERE EXISTS EXISTS and NOT EXISTS are typically used in conjunction with a correlated nested query. In the nested query references the Ssn Fname and Sex attributes of the EMPLOYEE relation from the outer query. We can think of as follows For each EMPLOYEE tuple evaluate the nested query which retrieves all DEPENDENT tuples with the same Essn Sex and Dependentname as the EMPLOYEE tuple if at least one tuple EXISTS in the result of the nested query then select that EMPLOYEE tuple. In general EXISTS returns TRUE if there is at least one tuple in the result of the nested query Q and it returns FALSE otherwise. On the other hand NOT EXISTS returns TRUE if there are no tuples in the result of nested query Q and it returns FALSE otherwise. Next we illustrate the use of NOT EXISTS. Query Retrieve the names of employees who have no dependents. SELECT Fname Lname FROM EMPLOYEE WHERE NOT EXISTS In the correlated nested query retrieves all DEPENDENT tuples related to a particular EMPLOYEE tuple. If none exist the EMPLOYEE tuple is selected because the WHERE clause condition will evaluate to TRUE in this case. We can explain as follows For each EMPLOYEE tuple the correlated nested query selects all DEPENDENT tuples whose Essn value matches the EMPLOYEE Ssn if the result is More Complex SQL Retrieval Queries empty no dependents are related to the employee so we select that EMPLOYEE tuple and retrieve its Fname and Lname. Query List the names of managers who have at least one dependent. SELECT Fname Lname FROM EMPLOYEE WHERE EXISTS AND EXISTS One way to write this query is shown in where we specify two nested correlated queries the first selects all DEPENDENT tuples related to an EMPLOYEE and the second selects all DEPARTMENT tuples managed by the EMPLOYEE. If at least one of the first and at least one of the second exists we select the EMPLOYEE tuple. Can you rewrite this query using only a single nested query or no nested queries The query Retrieve the name of each employee who works on all the projects controlled by department number can be written using EXISTS and NOT EXISTS in SQL systems. We show two ways of specifying this query in SQL as and This is an example of certain types of queries that require universal quantification as we will discuss in Section One way to write this query is to use the construct EXCEPT as explained next and checking whether the result is This option is shown as SELECT Fname Lname FROM EMPLOYEE WHERE NOT EXISTS In the first subquery selects all projects controlled by department and the second subquery selects all projects that the particular employee being considered works on. If the set difference of the first subquery result MINUS the second subquery result is empty it means that the employee works on all the projects and is therefore selected. The second option is shown as Notice that we need two level nesting in and that this formulation is quite a bit more complex than which uses NOT EXISTS and EXCEPT. that EXCEPT is the set difference operator. The keyword MINUS is also sometimes used for example in Oracle. Chapter More SQL Complex Queries Triggers Views and Schema Modification SELECT Lname Fname FROM EMPLOYEE WHERE NOT EXISTS AND NOT EXISTS In the outer nested query selects any WORKSON tuples whose Pno is of a project controlled by department if there is not a WORKSON tuple with the same Pno and the same Ssn as that of the EMPLOYEE tuple under consideration in the outer query. If no such tuple exists we select the EMPLOYEE tuple. The form of matches the following rephrasing of Query Select each employee such that there does not exist a project controlled by department that the employee does not work on. It corresponds to the way we will write this query in tuple relation calculus which returns TRUE if there are no duplicate tuples in the result of query Q otherwise it returns FALSE. This can be used to test whether the result of a nested query is a set or a multiset. Explicit Sets and Renaming of Attributes in SQL We have seen several queries with a nested query in the WHERE clause. It is also possible to use an explicit set of values in the WHERE clause rather than a nested query. Such a set is enclosed in parentheses in SQL. Query Retrieve the Social Security numbers of all employees who work on project numbers or SELECT DISTINCT Essn FROM WORKSON WHERE Pno IN In SQL it is possible to rename any attribute that appears in the result of a query by adding the qualifier AS followed by the desired new name. Hence the AS construct can be used to alias both attribute and relation names and it can be used in both the SELECT and FROM clauses. For example shows how query from Section can be slightly changed to retrieve the last name of each employee and his or her supervisor while renaming the resulting attribute names as Employeename and Supervisorname. The new names will appear as column headers in the query result. SELECT AS Employeename AS Supervisorname FROM EMPLOYEE AS E EMPLOYEE AS S WHERE More Complex SQL Retrieval Queries Joined Tables in SQL and Outer Joins The concept of a joined table was incorporated into SQL to permit users to specify a table resulting from a join operation in the FROM clause of a query. This construct may be easier to comprehend than mixing together all the select and join conditions in the WHERE clause. For example consider query which retrieves the name and address of every employee who works for the ‘Research’ department. It may be easier to specify the join of the EMPLOYEE and DEPARTMENT relations first and then to select the desired tuples and attributes. This can be written in SQL as in SELECT Fname Lname Address FROM WHERE Dname ‘Research’ The FROM clause in contains a single joined table. The attributes of such a table are all the attributes of the first table EMPLOYEE followed by all the attributes of the second table DEPARTMENT. The concept of a joined table also allows the user to specify different types of join such as NATURAL JOIN and various types of OUTER JOIN. In a NATURAL JOIN on two relations R and S no join condition is specified an implicit EQUIJOIN condition for each pair of attributes with the same name from R and S is created. Each such pair of attributes is included only once in the resulting relation . If the names of the join attributes are not the same in the base relations it is possible to rename the attributes so that they match and then to apply NATURAL JOIN. In this case the AS construct can be used to rename a relation and all its attributes in the FROM clause. This is illustrated in where the DEPARTMENT relation is renamed as DEPT and its attributes are renamed as Dname Dno Mssn and Msdate. The implied join condition for this NATURAL JOIN is because this is the only pair of attributes with the same name after renaming SELECT Fname Lname Address FROM WHERE Dname ‘Research’ The default type of join in a joined table is called an inner join where a tuple is included in the result only if a matching tuple exists in the other relation. For example in query only employees who have a supervisor are included in the result an EMPLOYEE tuple whose value for Superssn is NULL is excluded. If the user requires that all employees be included an OUTER JOIN must be used explicitly . In SQL this is handled by explicitly specifying the keyword OUTER JOIN in a joined table as illustrated in SELECT AS Employeename AS Supervisorname FROM Chapter More SQL Complex Queries Triggers Views and Schema Modification There are a variety of outer join operations which we shall discuss in more detail in Section In SQL the options available for specifying joined tables include INNER JOIN LEFT OUTER JOIN RIGHT OUTER JOIN and FULL OUTER JOIN. In the latter three options the keyword OUTER may be omitted. If the join attributes have the same name one can also specify the natural join variation of outer joins by using the keyword NATURAL before the operation . The keyword CROSS JOIN is used to specify the CARTESIAN PRODUCT operation JOIN EMPLOYEE ON Mgrssn Ssn WHERE Plocation ‘Stafford’ Not all SQL implementations have implemented the new syntax of joined tables. In some systems a different syntax was used to specify outer joins by using the comparison operators + + and + + for left right and full outer join respectively when specifying the join condition. For example this syntax is available in Oracle. To specify the left outer join in using this syntax we could write the query as follows SELECT FROM EMPLOYEE E EMPLOYEE S WHERE + Aggregate Functions in SQL In Section we will introduce the concept of an aggregate function as a relational algebra operation. Aggregate functions are used to summarize information from multiple tuples into a single tuple summary. Grouping is used to create subgroups of tuples before summarization. Grouping and aggregation are required in many database applications and we will introduce their use in SQL through examples. A number of built in aggregate functions exist COUNT SUM MAX MIN and AVG. The COUNT function returns the number of tuples or values as specified in a aggregate functions for more advanced statistical calculation were added in More Complex SQL Retrieval Queries query. The functions SUM MAX MIN and AVG can be applied to a set or multiset of numeric values and return respectively the sum maximum value minimum value and average of those values. These functions can be used in the SELECT clause or in a HAVING clause . The functions MAX and MIN can also be used with attributes that have nonnumeric domains if the domain values have a total ordering among one We illustrate the use of these functions with sample queries. Query Find the sum of the salaries of all employees the maximum salary the minimum salary and the average salary. SELECT SUM MAX MIN AVG FROM EMPLOYEE If we want to get the preceding function values for employees of a specific department say the ‘Research’ department we can write Query where the EMPLOYEE tuples are restricted by the WHERE clause to those employees who work for the ‘Research’ department. Query Find the sum of the salaries of all employees of the ‘Research’ department as well as the maximum salary the minimum salary and the average salary in this department. SELECT SUM MAX MIN AVG FROM WHERE Dname ‘Research’ Queries and Retrieve the total number of employees in the company and the number of employees in the ‘Research’ department SELECT COUNT FROM EMPLOYEE SELECT COUNT FROM EMPLOYEE DEPARTMENT WHERE DNO DNUMBER AND DNAME ‘Research’ Here the asterisk refers to the rows so COUNT returns the number of rows in the result of the query. We may also use the COUNT function to count values in a column rather than tuples as in the next example. Query Count the number of distinct salary values in the database. SELECT COUNT FROM EMPLOYEE If we write COUNT instead of COUNT in then duplicate values will not be eliminated. However any tuples with NULL for SALARY order means that for any two values in the domain it can be determined that one appears before the other in the defined order for example DATE TIME and TIMESTAMP domains have total orderings on their values as do alphabetic strings. Chapter More SQL Complex Queries Triggers Views and Schema Modification will not be counted. In general NULL values are discarded when aggregate functions are applied to a particular column . The preceding examples summarize a whole relation or a selected subset of tuples and hence all produce single tuples or single values. They illustrate how functions are applied to retrieve a summary value or summary tuple from the database. These functions can also be used in selection conditions involving nested queries. We can specify a correlated nested query with an aggregate function and then use the nested query in the WHERE clause of an outer query. For example to retrieve the names of all employees who have two or more dependents FROM DEPENDENT WHERE Ssn Essn The correlated nested query counts the number of dependents that each employee has if this is greater than or equal to two the employee tuple is selected. Grouping The GROUP BY and HAVING Clauses In many cases we want to apply the aggregate functions to subgroups of tuples in a relation where the subgroups are based on some attribute values. For example we may want to find the average salary of employees in each department or the number of employees who work on each project. In these cases we need to partition the relation into nonoverlapping subsets of tuples. Each group will consist of the tuples that have the same value of some attribute called the grouping attribute. We can then apply the function to each such group independently to produce summary information about each group. SQL has a GROUP BY clause for this purpose. The GROUP BY clause specifies the grouping attributes which should also appear in the SELECT clause so that the value resulting from applying each aggregate function to a group of tuples appears along with the value of the grouping attribute. Query For each department retrieve the department number the number of employees in the department and their average salary. SELECT Dno COUNT AVG FROM EMPLOYEE GROUP BY Dno In the EMPLOYEE tuples are partitioned into groups each group having the same value for the grouping attribute Dno. Hence each group contains the employees who work in the same department. The COUNT and AVG functions are applied to each such group of tuples. Notice that the SELECT clause includes only the grouping attribute and the aggregate functions to be applied on each group of tuples. Figure illustrates how grouping works on it also shows the result of More Complex SQL Retrieval Queries Dno Count Avg Result of Pname ProductY Computerization Reorganization Newbenefits Count Result of These groups are not selected by the HAVING condition of Grouping EMPLOYEE tuples by the value of Dno After applying the WHERE clause but before applying HAVING After applying the HAVING clause condition Fname John Franklin Ramesh K Jennifer Alicia Joyce A Ahmad James V B S Narayan English Jabbar Bong Smith Wong Zelaya Wallace Minit Lname Dno NULL Superssn Salary . . . Pname ProductX ProductX ProductY ProductZ ProductY ProductY ProductZ Computerization Computerization Computerization Reorganization Newbenefits Reorganization Reorganization Newbenefits Newbenefits NULL Pnumber Hours . . . Pname ProductY ProductY ProductY Computerization Computerization Computerization Reorganization Reorganization Reorganization Newbenefits Newbenefits Newbenefits NULL Pnumber Essn Pno Hours . . . Ssn . . . . . . Essn Pno . . . Figure Results of GROUP BY and HAVING. Chapter More SQL Complex Queries Triggers Views and Schema Modification If NULLs exist in the grouping attribute then a separate group is created for all tuples with a NULL value in the grouping attribute. For example if the EMPLOYEE table had some tuples that had NULL for the grouping attribute Dno there would be a separate group for those tuples in the result of Query For each project retrieve the project number the project name and the number of employees who work on that project. SELECT Pnumber Pname COUNT FROM PROJECT WORKSON WHERE Pnumber Pno GROUP BY Pnumber Pname shows how we can use a join condition in conjunction with GROUP BY. In this case the grouping and functions are applied after the joining of the two relations. Sometimes we want to retrieve the values of these functions only for groups that satisfy certain conditions. For example suppose that we want to modify Query so that only projects with more than two employees appear in the result. SQL provides a HAVING clause which can appear in conjunction with a GROUP BY clause for this purpose. HAVING provides a condition on the summary information regarding the group of tuples associated with each value of the grouping attributes. Only the groups that satisfy the condition are retrieved in the result of the query. This is illustrated by Query Query For each project on which more than two employees work retrieve the project number the project name and the number of employees who work on the project. SELECT Pnumber Pname COUNT FROM PROJECT WORKSON WHERE Pnumber Pno GROUP BY Pnumber Pname HAVING COUNT Notice that while selection conditions in the WHERE clause limit the tuples to which functions are applied the HAVING clause serves to choose whole groups. Figure illustrates the use of HAVING and displays the result of Query For each project retrieve the project number the project name and the number of employees from department who work on the project. SELECT Pnumber Pname COUNT FROM PROJECT WORKSON EMPLOYEE WHERE Pnumber Pno AND Ssn Essn AND GROUP BY Pnumber Pname Here we restrict the tuples in the relation to those that satisfy the condition specified in the WHERE clause namely that they work in department number Notice that we must be extra careful when two different conditions apply . For example suppose that we want More Complex SQL Retrieval Queries to count the total number of employees whose salaries exceed in each department but only for departments where more than five employees work. Here the condition FROM DEPARTMENT EMPLOYEE WHERE Dnumber Dno AND GROUP BY Dname HAVING COUNT This is incorrect because it will select only departments that have more than five employees who each earn more than The rule is that the WHERE clause is executed first to select individual tuples or joined tuples the HAVING clause is applied later to select individual groups of tuples. Hence the tuples are already restricted to employees who earn more than before the function in the HAVING clause is applied. One way to write this query correctly is to use a nested query as shown in Query Query For each department that has more than five employees retrieve the department number and the number of its employees who are making more than SELECT Dnumber COUNT FROM DEPARTMENT EMPLOYEE WHERE Dnumber Dno AND AND Discussion and Summary of SQL Queries A retrieval query in SQL can consist of up to six clauses but only the first two SELECT and FROM are mandatory. The query can span several lines and is ended by a semicolon. Query terms are separated by spaces and parentheses can be used to group relevant parts of a query in the standard way. The clauses are specified in the following order with the clauses between square brackets [ ] being optional SELECT attribute and function list FROM table list [ WHERE condition ] [ GROUP BY grouping attribute ] [ HAVING group condition ] [ ORDER BY attribute list ] The SELECT clause lists the attributes or functions to be retrieved. The FROM clause specifies all relations needed in the query including joined relations but not those in nested queries. The WHERE clause specifies the conditions for selecting the tuples from these relations including join conditions if needed. GROUP BY Chapter More SQL Complex Queries Triggers Views and Schema Modification specifies grouping attributes whereas HAVING specifies a condition on the groups being selected rather than on the individual tuples. The built in aggregate functions COUNT SUM MIN MAX and AVG are used in conjunction with grouping but they can also be applied to all the selected tuples in a query without a GROUP BY clause. Finally ORDER BY specifies an order for displaying the result of a query. In order to formulate queries correctly it is useful to consider the steps that define the meaning or semantics of each query. A query is evaluated by first applying the FROM clause followed by the WHERE clause to select and join tuples and then by GROUP BY and HAVING. Conceptually ORDER BY is applied at the end to sort the query result. If none of the last three clauses are specified we can think conceptually of a query as being executed as follows For each combination of tuples one from each of the relations specified in the FROM clause evaluate the WHERE clause if it evaluates to TRUE place the values of the attributes specified in the SELECT clause from this tuple combination in the result of the query. Of course this is not an efficient way to implement the query in a real system and each DBMS has special query optimization routines to decide on an execution plan that is efficient to execute. We discuss query processing and optimization in Chapter In general there are numerous ways to specify the same query in SQL. This flexibility in specifying queries has advantages and disadvantages. The main advantage is that users can choose the technique with which they are most comfortable when specifying a query. For example many queries may be specified with join conditions in the WHERE clause or by using joined relations in the FROM clause or with some form of nested queries and the IN comparison operator. Some users may be more comfortable with one approach whereas others may be more comfortable with another. From the programmer’s and the system’s point of view regarding query optimization it is generally preferable to write a query with as little nesting and implied ordering as possible. The disadvantage of having numerous ways of specifying the same query is that this may confuse the user who may not know which technique to use to specify particular types of queries. Another problem is that it may be more efficient to execute a query specified in one way than the same query specified in an alternative way. Ideally this should not be the case The DBMS should process the same query in the same way regardless of how the query is specified. But this is quite difficult in practice since each DBMS has different methods for processing queries specified in different ways. Thus an additional burden on the user is to determine which of the alternative specifications is the most efficient to execute. Ideally the user should worry only about specifying the query correctly whereas the DBMS would determine how to execute the query efficiently. In practice however it helps if the user is aware of which types of constructs in a query are more expensive to process than others that we presented in Section These built in constraints can be specified within the CREATE TABLE statement of SQL The constraint name SALARYCONSTRAINT is followed by the keyword CHECK which is followed by a condition in parentheses that must hold true on every database state for the assertion to be satisfied. The constraint name can be used later to refer to the constraint or to modify or drop it. The DBMS is responsible for ensuring that the condition is not violated. Any WHERE clause condition can be used but many constraints can be specified using the EXISTS and NOT EXISTS style of SQL conditions. Whenever some tuples in the database cause the condition of an ASSERTION statement to evaluate to FALSE the constraint is violated. The constraint is satisfied by a database state if no combination of tuples in that database state violates the constraint. The basic technique for writing such assertions is to specify a query that selects any tuples that violate the desired condition. By including this query inside a NOT EXISTS Chapter More SQL Complex Queries Triggers Views and Schema Modification clause the assertion will specify that the result of this query must be empty so that the condition will always be TRUE. Thus the assertion is violated if the result of the query is not empty. In the preceding example the query selects all employees whose salaries are greater than the salary of the manager of their department. If the result of the query is not empty the assertion is violated. Note that the CHECK clause and constraint condition can also be used to specify constraints on individual attributes and domains in SQL FOR EACH ROW WHEN INFORMSUPERVISOR The trigger is given the name SALARYVIOLATION which can be used to remove or deactivate the trigger later. A typical trigger has three components The event These are usually database update operations that are explicitly applied to the database. In this example the events are inserting a new employee record changing an employee’s salary or changing an employee’s supervisor. The person who writes the trigger must make sure that all possible events are accounted for. In some cases it may be necessary to write more than one trigger to cover all possible cases. These events are specified after the keyword BEFORE in our example which means that the trigger should be executed before the triggering operation is executed. An alternative is to use the keyword AFTER which specifies that the trigger should be executed after the operation specified in the event is completed. The condition that determines whether the rule action should be executed Once the triggering event has occurred an optional condition may be evaluated. If no condition is specified the action will be executed once the event occurs. If a condition is specified it is first evaluated and only if it evaluates to true will the rule action be executed. The condition is specified in the WHEN clause of the trigger. The action to be taken The action is usually a sequence of SQL statements but it could also be a database transaction or an external program that will be automatically executed. In this example the action is to execute the stored procedure INFORMSUPERVISOR. Triggers can be used in various applications such as maintaining database consistency monitoring database updates and updating derived data automatically. A more complete discussion is given in Section Views in SQL In this section we introduce the concept of a view in SQL. We show how views are specified and then we discuss the problem of updating views and how views can be implemented by the DBMS. Concept of a View in SQL A view in SQL terminology is a single table that is derived from other These other tables can be base tables or previously defined views. A view does not necessarily used in SQL the term view is more limited than the term user view discussed in Chapters and since a user view would possibly include many relations. Chapter More SQL Complex Queries Triggers Views and Schema Modification DEPTINFO Deptname Noofemps Totalsal Fname Lname Pname Hours Figure Two views specified on the database schema of Figure exist in physical form it is considered to be a virtual table in contrast to base tables whose tuples are always physically stored in the database. This limits the possible update operations that can be applied to views but it does not provide any limitations on querying a view. We can think of a view as a way of specifying a table that we need to reference frequently even though it may not exist physically. For example referring to the COMPANY database in Figure we may frequently issue queries that retrieve the employee name and the project names that the employee works on. Rather than having to specify the join of the three tables EMPLOYEE WORKSON and PROJECT every time we issue this query we can define a view that is specified as the result of these joins. Then we can issue queries on the view which are specified as singletable retrievals rather than as retrievals involving two joins on three tables. We call the EMPLOYEE WORKSON and PROJECT tables the defining tables of the view. Specification of Views in SQL In SQL the command to specify a view is CREATE VIEW. The view is given a table name a list of attribute names and a query to specify the contents of the view. If none of the view attributes results from applying functions or arithmetic operations we do not have to specify new attribute names for the view since they would be the same as the names of the attributes of the defining tables in the default case. The views in and create virtual tables whose schemas are illustrated in Figure when applied to the database schema of Figure CREATE VIEW AS SELECT Fname Lname Pname Hours FROM EMPLOYEE PROJECT WORKSON WHERE Ssn Essn AND Pno Pnumber CREATE VIEW DEPTINFO AS SELECT Dname COUNT SUM FROM DEPARTMENT EMPLOYEE WHERE Dnumber Dno GROUP BY Dname In we did not specify any new attribute names for the view in this case inherits the names of the view attributes from the defining tables EMPLOYEE PROJECT and WORKSON. View Views in SQL explicitly specifies new attribute names for the view DEPTINFO using a one to one correspondence between the attributes specified in the CREATE VIEW clause and those specified in the SELECT clause of the query that defines the view. We can now specify SQL queries on a view or virtual table in the same way we specify queries involving base tables. For example to retrieve the last name and first name of all employees who work on the ‘ProductX’ project we can utilize the view and specify the query as in SELECT Fname Lname FROM WHERE Pname ‘ProductX’ The same query would require the specification of two joins if specified on the base relations directly one of the main advantages of a view is to simplify the specification of certain queries. Views are also used as a security and authorization mechanism into a query on the underlying base tables. For example the query would be automatically modified to the following query by the DBMS SELECT Fname Lname FROM EMPLOYEE PROJECT WORKSON WHERE Ssn Essn AND Pno Pnumber AND Pname ‘ProductX’ The disadvantage of this approach is that it is inefficient for views defined via complex queries that are time consuming to execute especially if multiple queries are going to be applied to the same view within a short period of time. The second strategy called view materialization involves physically creating a temporary view table when the view is first queried and keeping that table on the assumption that Chapter More SQL Complex Queries Triggers Views and Schema Modification other queries on the view will follow. In this case an efficient strategy for automatically updating the view table when the base tables are updated must be developed in order to keep the view up to date. Techniques using the concept of incremental update have been developed for this purpose where the DBMS can determine what new tuples must be inserted deleted or modified in a materialized view table when a database update is applied to one of the defining base tables. The view is generally kept as a materialized table as long as it is being queried. If the view is not queried for a certain period of time the system may then automatically remove the physical table and recompute it from scratch when future queries reference the view. Updating of views is complicated and can be ambiguous. In general an update on a view defined on a single table without any aggregate functions can be mapped to an update on the underlying base table under certain conditions. For a view involving joins an update operation may be mapped to update operations on the underlying base relations in multiple ways. Hence it is often not possible for the DBMS to determine which of the updates is intended. To illustrate potential problems with updating a view defined on multiple tables consider the view and suppose that we issue the command to update the PNAME attribute of ‘John Smith’ from ‘ProductX’ to ‘ProductY’. This view update is shown in SET Pname ‘ProductY’ WHERE Lname ‘Smith’ AND Fname ‘John’ AND Pname ‘ProductX’ This query can be mapped into several updates on the base relations to give the desired update effect on the view. In addition some of these updates will create additional side effects that affect the result of other queries. For example here are two possible updates and on the base relations corresponding to the view update operation in UPDATEWORKSON SET Pno WHERE Essn IN AND Pno UPDATEPROJECT SET Pname ‘ProductY’ WHERE Pname ‘ProductX’ Update relates ‘John Smith’ to the ‘ProductY’ PROJECT tuple instead of the ‘ProductX’ PROJECT tuple and is the most likely desired update. However Schema Change Statements in SQL would also give the desired update effect on the view but it accomplishes this by changing the name of the ‘ProductX’ tuple in the PROJECT relation to ‘ProductY’. It is quite unlikely that the user who specified the view update wants the update to be interpreted as in since it also has the side effect of changing all the view tuples with Pname ‘ProductX’. Some view updates may not make much sense for example modifying the Totalsal attribute of the DEPTINFO view does not make sense because Totalsal is defined to be the sum of the individual employee salaries. This request is shown as UPDATE DEPTINFO SET WHERE Dname ‘Research’ A large number of updates on the underlying base relations can satisfy this view update. Generally a view update is feasible when only one possible update on the base relations can accomplish the desired update effect on the view. Whenever an update on the view can be mapped to more than one update on the underlying base relations we must have a certain procedure for choosing one of the possible updates as the most likely one. Some researchers have developed methods for choosing the most likely update while other researchers prefer to have the user choose the desired update mapping during view definition. In summary we can make the following observations A view with a single defining table is updatable if the view attributes contain the primary key of the base relation as well as all attributes with the NOT NULL constraint that do not have default values specified. Views defined on multiple tables using joins are generally not updatable. Views defined using grouping and aggregate functions are not updatable. In SQL the clause WITH CHECK OPTION must be added at the end of the view definition if a view is to be updated. This allows the system to check for view updatability and to plan an execution strategy for view updates. It is also possible to define a view table in the FROM clause of an SQL query. This is known as an in line view. In this case the view is defined within the query itself. Schema Change Statements in SQL In this section we give an overview of the schema evolution commands available in SQL which can be used to alter a schema by adding or dropping tables attributes constraints and other schema elements. This can be done while the database is operational and does not require recompilation of the database schema. Certain checks must be done by the DBMS to ensure that the changes do not affect the rest of the database and make it inconsistent. Chapter More SQL Complex Queries Triggers Views and Schema Modification The DROP Command The DROP command can be used to drop named schema elements such as tables domains or constraints. One can also drop a schema. For example if a whole schema is no longer needed the DROP SCHEMA command can be used. There are two drop behavior options CASCADE and RESTRICT. For example to remove the COMPANY database schema and all its tables domains and other elements the CASCADE option is used as follows DROP SCHEMA COMPANY CASCADE If the RESTRICT option is chosen in place of CASCADE the schema is dropped only if it has no elements in it otherwise the DROP command will not be executed. To use the RESTRICT option the user must first individually drop each element in the schema then drop the schema itself. If a base relation within a schema is no longer needed the relation and its definition can be deleted by using the DROP TABLE command. For example if we no longer wish to keep track of dependents of employees in the COMPANY database of Figure we can get rid of the DEPENDENT relation by issuing the following command DROP TABLE DEPENDENT CASCADE If the RESTRICT option is chosen instead of CASCADE a table is dropped only if it is not referenced in any constraints or views changing a column definition and adding or dropping table constraints. For example to add an attribute for keeping track of jobs of employees to the EMPLOYEE base relation in the COMPANY schema reference the column. For example the following command removes the attribute Address from the EMPLOYEE base table ALTER TABLE DROP COLUMN Address CASCADE It is also possible to alter a column definition by dropping an existing default clause or by defining a new default clause. The following examples illustrate this clause ALTER TABLE ALTER COLUMN Mgrssn DROP DEFAULT ALTER TABLE ALTER COLUMN Mgrssn SET DEFAULT One can also change the constraints specified on a table by adding or dropping a named constraint. To be dropped a constraint must have been given a name when it was specified. For example to drop the constraint named EMPSUPERFK in Figure from the EMPLOYEE relation we write ALTER TABLE DROP CONSTRAINT EMPSUPERFK CASCADE Once this is done we can redefine a replacement constraint by adding a new constraint to the relation if needed. This is specified by using the ADD keyword in the ALTER TABLE statement followed by the new constraint which can be named or unnamed and can be of any of the table constraint types discussed. The preceding subsections gave an overview of the schema evolution commands of SQL. It is also possible to create new tables and views within a database schema using the appropriate commands. There are many other details and options we refer the interested reader to the SQL documents listed in the Selected Bibliography at the end of this chapter. Summary In this chapter we presented additional features of the SQL database language. We started in Section by presenting more complex features of SQL retrieval queries including nested queries joined tables outer joins aggregate functions and grouping. In Section we described the CREATE ASSERTION statement which allows the specification of more general constraints on the database and introduced the concept of triggers and the CREATE TRIGGER statement. Then in Section we described the SQL facility for defining views on the database. Views are also called Chapter More SQL Complex Queries Triggers Views and Schema Modification Table Summary of SQL Syntax CREATE TABLE table name DROP TABLE table name ALTER TABLE table name ADD column name column type SELECT [ DISTINCT ] attribute list FROM [ WHERE condition ] [ GROUP BY grouping attributes [ HAVING group selection condition ] ] [ ORDER BY column name [ order ] column name [ order ] ] attribute list grouping attributes column name column name order INSERT INTO table name [ ] | select statement DELETE FROM table name [ WHERE selection condition ] UPDATE table name SET column name value expression column name value expression [ WHERE selection condition ] CREATE [ UNIQUE] INDEX index name ON table name [ CLUSTER ] DROP INDEX index name CREATE VIEW view name [ ] AS select statement DROP VIEW view name NOTE The commands for creating and dropping indexes are not part of standard SQL. virtual or derived tables because they present the user with what appear to be tables however the information in those tables is derived from previously defined tables. Section introduced the SQL ALTER TABLE statement which is used for modifying the database tables and constraints. Table summarizes the syntax of various SQL statements. This summary is not meant to be comprehensive or to describe every possible SQL construct rather it is meant to serve as a quick reference to the major types of constructs available in SQL. We use BNF notation where nonterminal symbols are shown in angled brackets optional parts are shown in square brackets repetitions are shown in braces and alternatives are shown in parentheses . b. Retrieve the names and major departments of all students who do not have a grade of A in any of their courses. In SQL specify the following queries on the database in Figure using the concept of nested queries and concepts described in this chapter. a. Retrieve the names of all employees who work in the department that has the employee with the highest salary among all employees. b. Retrieve the names of all employees whose supervisor’s supervisor has for Ssn. Chapter More SQL Complex Queries Triggers Views and Schema Modification c. Retrieve the names of employees who make at least more than the employee who is paid the least in the company. Specify the following views in SQL on the COMPANY database schema shown in Figure a. A view that has the department name manager name and manager salary for every department. b. A view that has the employee name supervisor name and employee salary for each employee who works in the ‘Research’ department. c. A view that has the project name controlling department name number of employees and total hours worked per week on the project for each project. d. A view that has the project name controlling department name number of employees and total hours worked per week on the project for each project with more than one employee working on it. Consider the following view DEPTSUMMARY defined on the COMPANY database in Figure CREATE VIEW DEPTSUMMARY AS SELECT Dno COUNT SUM AVG FROM EMPLOYEE GROUP BY Dno State which of the following queries and updates would be allowed on the view. If a query or update would be allowed show what the corresponding query or update on the base relations would look like and give its result when applied to the database in Figure a. SELECT FROM DEPTSUMMARY b. SELECT D C FROM DEPTSUMMARY WHERE TOTALS c. SELECT D AVERAGES FROM DEPTSUMMARY WHERE C . The relational algebra is very important for several reasons. First it provides a formal foundation for relational model operations. Second and perhaps more important it is used as a basis for implementing and optimizing queries in the query processing and optimization modules that are integral parts of relational database management systems as we shall discuss in Chapter Third some of its concepts are incorporated into the SQL standard query language for RDBMSs. chapter Chapter The Relational Algebra and Relational Calculus Although most commercial RDBMSs in use today do not provide user interfaces for relational algebra queries the core operations and functions in the internal modules of most relational systems are based on relational algebra operations. We will define these operations in detail in Sections through of this chapter. Whereas the algebra defines a set of operations for the relational model the relational calculus provides a higher level declarative language for specifying relational queries. A relational calculus expression creates a new relation. In a relational calculus expression there is no order of operations to specify how to retrieve the query result only what information the result should contain. This is the main distinguishing feature between relational algebra and relational calculus. The relational calculus is important because it has a firm basis in mathematical logic and because the standard query language for RDBMSs has some of its foundations in a variation of relational calculus known as the tuple relational The relational algebra is often considered to be an integral part of the relational data model. Its operations can be divided into two groups. One group includes set operations from mathematical set theory these are applicable because each relation is defined to be a set of tuples in the formal relational model . The other group consists of operations developed specifically for relational databases these include SELECT PROJECT and JOIN among others. First we describe the SELECT and PROJECT operations in Section because they are unary operations that operate on single relations. Then we discuss set operations in Section In Section we discuss JOIN and other complex binary operations which operate on two tables by combining related tuples based on join conditions. The COMPANY relational database shown in Figure is used for our examples. Some common database requests cannot be performed with the original relational algebra operations so additional operations were created to express these requests. These include aggregate functions which are operations that can summarize data from the tables as well as additional types of JOIN and UNION operations known as OUTER JOINs and OUTER UNIONs. These operations which were added to the original relational algebra because of their importance to many database applications are described in Section We give examples of specifying queries that use relational operations in Section Some of these same queries were used in Chapters and By using the same query numbers in this chapter the reader can contrast how the same queries are written in the various query languages. In Sections and we describe the other main formal language for relational databases the relational calculus. There are two variations of relational calculus. The tuple relational calculus is described in Section and the domain relational calculus is described in Section Some of the SQL constructs discussed in Chapters and are based on the tuple relational calculus. The relational calculus is a formal language based on the branch of mathematical logic called predicate is based on tuple relational calculus but also incorporates some of the operations from the relational algebra and its extensions as illustrated in Chapters and Unary Relational Operations SELECT and PROJECT In tuple relational calculus variables range over tuples whereas in domain relational calculus variables range over the domains of attributes. In Appendix C we give an overview of the Query By Example language which is a graphical user friendly relational language based on domain relational calculus. Section summarizes the chapter. For the reader who is interested in a less detailed introduction to formal relational languages Sections and may be skipped. Unary Relational Operations SELECT and PROJECT The SELECT Operation The SELECT operation is used to choose a subset of the tuples from a relation that satisfies a selection condition. One can consider the SELECT operation to be a filter that keeps only those tuples that satisfy a qualifying condition. Alternatively we can consider the SELECT operation to restrict the tuples in a relation to only those tuples that satisfy the condition. The SELECT operation can also be visualized as a horizontal partition of the relation into two sets of tuples those tuples that satisfy the condition and are selected and those tuples that do not satisfy the condition and are discarded. For example to select the EMPLOYEE tuples whose department is or those whose salary is greater than we can individually specify each of these two conditions with a SELECT operation as follows In general the SELECT operation is denoted by σ selection condition where the symbol σ is used to denote the SELECT operator and the selection condition is a Boolean expression specified on the attributes of relation R. Notice that R is generally a relational algebra expression whose result is a relation the simplest such expression is just the name of a database relation. The relation resulting from the SELECT operation has the same attributes as R. The Boolean expression specified in selection condition is made up of a number of clauses of the form attribute name comparison op constant value or attribute name comparison op attribute name this chapter no familiarity with first order predicate calculus which deals with quantified variables and values is assumed. SELECT operation is different from the SELECT clause of SQL. The SELECT operation chooses tuples from a table and is sometimes called a RESTRICT or FILTER operation. Chapter The Relational Algebra and Relational Calculus where attribute name is the name of an attribute of R comparison op is normally one of the operators ≤ ≥ ≠ and constant value is a constant value from the attribute domain. Clauses can be connected by the standard Boolean operators and or and not to form a general selection condition. For example to select the tuples for all employees who either work in department and make over per year or work in department and make over we can specify the following SELECT operation AND OR AND The result is shown in Figure Notice that all the comparison operators in the set ≤ ≥ ≠ can apply to attributes whose domains are ordered values such as numeric or date domains. Domains of strings of characters are also considered to be ordered based on the collating sequence of the characters. If the domain of an attribute is a set of unordered values then only the comparison operators in the set ≠ can be used. An example of an unordered domain is the domain Color ‘red’ ‘blue’ ‘green’ ‘white’ ‘yellow’ where no order is specified among the various colors. Some domains allow additional types of comparison operators for example a domain of character strings may allow the comparison operator SUBSTRINGOF. In general the result of a SELECT operation can be determined as follows. The selection condition is applied independently to each individual tuple t in R. This is done by substituting each occurrence of an attribute Ai in the selection condition with its value in the tuple t[Ai ]. If the condition evaluates to TRUE then tuple t is Fname Minit Lname Ssn Bdate Address Sex Salary Superssn Dno Franklin Jennifer Ramesh T Wong Wallace Narayan Voss Houston TX Berry Bellaire TX Fire Oak Humble TX M M Lname Fname Salary Smith Wong Zelaya Wallace Narayan English Jabbar Borg John Franklin Alicia Jennifer Ramesh Joyce Ahmad James Sex Salary M M M M M S K Figure Results of SELECT and PROJECT operations. AND OR AND . πLname Fname Salary. πSex Salary. Unary Relational Operations SELECT and PROJECT selected. All the selected tuples appear in the result of the SELECT operation. The Boolean conditions AND OR and NOT have their normal interpretation as follows AND is TRUE if both and are TRUE otherwise it is FALSE. OR is TRUE if either or or both are TRUE otherwise it is FALSE. is TRUE if cond is FALSE otherwise it is FALSE. The SELECT operator is unary that is it is applied to a single relation. Moreover the selection operation is applied to each tuple individually hence selection conditions cannot involve more than one tuple. The degree of the relation resulting from a SELECT operation its number of attributes is the same as the degree of R. The number of tuples in the resulting relation is always less than or equal to the number of tuples in R. That is |σc | ≤ |R| for any condition C. The fraction of tuples selected by a selection condition is referred to as the selectivity of the condition. Notice that the SELECT operation is commutative that is Hence a sequence of SELECTs can be applied in any order. In addition we can always combine a cascade of SELECT operations into a single SELECT operation with a conjunctive condition that is condn In SQL the SELECT condition is typically specified in the WHERE clause of a query. For example the following operation AND would correspond to the following SQL query SELECT FROM EMPLOYEE WHERE AND The PROJECT Operation If we think of a relation as a table the SELECT operation chooses some of the rows from the table while discarding other rows. The PROJECT operation on the other hand selects certain columns from the table and discards the other columns. If we are interested in only certain attributes of a relation we use the PROJECT operation to project the relation over these attributes only. Therefore the result of the PROJECT operation can be visualized as a vertical partition of the relation into two relations one has the needed columns and contains the result of the operation and the other contains the discarded columns. For example to list each employee’s first and last name and salary we can use the PROJECT operation as follows πLname Fname Salary Chapter The Relational Algebra and Relational Calculus The resulting relation is shown in Figure The general form of the PROJECT operation is π attribute list where π is the symbol used to represent the PROJECT operation and attribute list is the desired sublist of attributes from the attributes of relation R. Again notice that R is in general a relational algebra expression whose result is a relation which in the simplest case is just the name of a database relation. The result of the PROJECT operation has only the attributes specified in attribute list in the same order as they appear in the list. Hence its degree is equal to the number of attributes in attribute list . If the attribute list includes only nonkey attributes of R duplicate tuples are likely to occur. The PROJECT operation removes any duplicate tuples so the result of the PROJECT operation is a set of distinct tuples and hence a valid relation. This is known as duplicate elimination. For example consider the following PROJECT operation πSex Salary The result is shown in Figure Notice that the tuple ‘F’ appears only once in Figure even though this combination of values appears twice in the EMPLOYEE relation. Duplicate elimination involves sorting or some other technique to detect duplicates and thus adds more processing. If duplicates are not eliminated the result would be a multiset or bag of tuples rather than a set. This was not permitted in the formal relational model but is allowed in SQL would correspond to the following SQL query SELECT DISTINCT Sex Salary FROM EMPLOYEE Notice that if we remove the keyword DISTINCT from this SQL query then duplicates will not be eliminated. This option is not available in the formal relational algebra. Unary Relational Operations SELECT and PROJECT Sequences of Operations and the RENAME Operation The relations shown in Figure that depict operation results do not have any names. In general for most queries we need to apply several relational algebra operations one after the other. Either we can write the operations as a single relational algebra expression by nesting the operations or we can apply one operation at a time and create intermediate result relations. In the latter case we must give names to the relations that hold the intermediate results. For example to retrieve the first name last name and salary of all employees who work in department number we must apply a SELECT and a PROJECT operation. We can write a single relational algebra expression also known as an in line expression as follows πFname Lname Figure shows the result of this in line relational algebra expression. Alternatively we can explicitly show the sequence of operations giving a name to each intermediate relation as follows ← RESULT ← πFname Lname It is sometimes simpler to break down a complex sequence of operations by specifying intermediate result relations than to write a single relational algebra expression. We can also use this technique to rename the attributes in the intermediate and TEMP Fname John Franklin Ramesh Joyce Minit B K A Lname Smith Wong Narayan English Ssn Bdate Address Fondren Houston TX Voss Houston TX Fire Oak Humble TX Rice Houston TX Sex M M M Salary Superssn Dno Smith Wong Narayan English Fname Lname Salary Joh Franklin Ramesh Joyce Smith Wong Narayan English Firstname Lastname Salary John Franklin Ramesh Joyce R Figure Results of a sequence of operations. πFname Lname Salary Using intermediate relations and renaming of attributes. Chapter The Relational Algebra and Relational Calculus result relations. This can be useful in connection with more complex operations such as UNION and JOIN as we shall see. To rename the attributes in a relation we simply list the new attribute names in parentheses as in the following example TEMP ← R ← πFname Lname Salary These two operations are illustrated in Figure If no renaming is applied the names of the attributes in the resulting relation of a SELECT operation are the same as those in the original relation and in the same order. For a PROJECT operation with no renaming the resulting relation has the same attribute names as those in the projection list and in the same order in which they appear in the list. We can also define a formal RENAME operation which can rename either the relation name or the attribute names or both as a unary operator. The general RENAME operation when applied to a relation R of degree n is denoted by any of the following three forms Bn or ρS or Bn where the symbol ρ is used to denote the RENAME operator S is the new relation name and Bn are the new attribute names. The first expression renames both the relation and its attributes the second renames the relation only and the third renames the attributes only. If the attributes of R are An in that order then each Ai is renamed as Bi . In SQL a single query typically represents a complex relational algebra expression. Renaming in SQL is accomplished by aliasing using AS as in the following example SELECT AS Firstname AS Lastname AS Salary FROM EMPLOYEE AS E WHERE Relational Algebra Operations from Set Theory The UNION INTERSECTION and MINUS Operations The next group of relational algebra operations are the standard mathematical operations on sets. For example to retrieve the Social Security numbers of all employees who either work in department or directly supervise an employee who works in department we can use the UNION operation as a single relational algebra expression this becomes Result ← πSsn ∪ πSuperssn Relational Algebra Operations from Set Theory ← ← ← RESULT ← ∪ The relation has the Ssn of all employees who work in department whereas has the Ssn of all employees who directly supervise an employee who works in department The UNION operation produces the tuples that are in either or or both . These are binary operations that is each is applied to two sets . When these operations are adapted to relational databases the two relations on which any of these three operations are applied must have the same type of tuples this condition has been called union compatibility or type compatibility. Two relations An and Bn are said to be union compatible if they have the same degree n and if dom dom for f i f n. This means that the two relations have the same number of attributes and each corresponding pair of attributes has the same domain. We can define the three operations UNION INTERSECTION and SET DIFFERENCE on two union compatible relations R and S as follows UNION The result of this operation denoted by R ∪ S is a relation that includes all tuples that are either in R or in S or in both R and S. Duplicate tuples are eliminated. INTERSECTION The result of this operation denoted by R ∩ S is a relation that includes all tuples that are in both R and S. SET DIFFERENCE The result of this operation denoted by R – S is a relation that includes all tuples that are in R but not in S. We will adopt the convention that the resulting relation has the same attribute names as the first relation R. It is always possible to rename the attributes in the result using the rename operator. R E S U Ssn R E S U LT Ssn R E S U Ssn Figure Result of the UNION operation RESULT ← ∪ STUDENT Fn Susan Ramesh Johnny Barbara Amy Jimmy Ernest Ln Yao Shah Kohler Jones Ford Wang Gilbert Fn Susan Ramesh Johnny Barbara Amy Jimmy Ernest Ln Yao Shah Kohler Jones Ford Wang Gilbert John Smith Ricardo Browne Francis Johnson Fn Johnny Barbara Amy Jimmy Ernest Ln Kohler Jones Ford Wang Gilbert Fn Susan Ramesh Ln Yao Shah INSTRUCTOR Fname John Ricardo Susan Francis Ramesh Lname Smith Browne Yao Johnson Shah Fname John Ricardo Francis Lname Smith Browne Johnson Chapter The Relational Algebra and Relational Calculus Figure illustrates the three operations. The relations STUDENT and INSTRUCTOR in Figure are union compatible and their tuples represent the names of students and the names of instructors respectively. The result of the UNION operation in Figure shows the names of all students and instructors. Note that duplicate tuples appear only once in the result. The result of the INTERSECTION operation ∪ T and ∩ T R ∩ The MINUS operation is not commutative that is in general R − S ≠ S − R Figure The set operations UNION INTERSECTION and MINUS. Two union compatible relations. STUDENT ∪ INSTRUCTOR. STUDENT ∩ INSTRUCTOR. STUDENT − INSTRUCTOR. INSTRUCTOR − STUDENT. Relational Algebra Operations from Set Theory Figure shows the names of students who are not instructors and Figure shows the names of instructors who are not students. Note that INTERSECTION can be expressed in terms of union and set difference as follows R ∩ S − − In SQL there are three operations UNION INTERSECT and EXCEPT that correspond to the set operations described here. In addition there are multiset operations that do not eliminate duplicates Operation Next we discuss the CARTESIAN PRODUCT operation also known as CROSS PRODUCT or CROSS JOIN which is denoted by ×. This is also a binary set operation but the relations on which it is applied do not have to be union compatible. In its binary form this set operation produces a new element by combining every member from one relation with every member from the other relation . In general the result of An × Bm is a relation Q with degree n + m attributes An Bm in that order. The resulting relation Q has one tuple for each combination of tuples one from R and one from S. Hence if R has nR tuples and S has nS tuples then R × S will have nR nS tuples. The n ary CARTESIAN PRODUCT operation is an extension of the above concept which produces new tuples by concatenating all possible combinations of tuples from n underlying relations. In general the CARTESIAN PRODUCT operation applied by itself is generally meaningless. It is mostly useful when followed by a selection that matches values of attributes coming from the component relations. For example suppose that we want to retrieve a list of names of each female employee’s dependents. We can do this as follows FEMALEEMPS ← σSex ‘F’ EMPNAMES ← πFname Lname Ssn EMPDEPENDENTS ← EMPNAMES × DEPENDENT ACTUALDEPENDENTS ← σSsn Essn RESULT ← πFname Lname Dependentname The resulting relations from this sequence of operations are shown in Figure The EMPDEPENDENTS relation is the result of applying the CARTESIAN PRODUCT operation to EMPNAMES from Figure with DEPENDENT from Figure In EMPDEPENDENTS every tuple from EMPNAMES is combined with every tuple from DEPENDENT giving a result that is not very meaningful . We want to combine a female employee tuple only with her particular dependents namely the DEPENDENT tuples whose Chapter The Relational Algebra and Relational Calculus Fname FEMALEEMPS Alicia Jennifer Joyce A S Minit English Zelaya Wallace Lname Spring TX Ssn Bdate Rice Houston TX Bellaire TX Address Sex Dno Salary Superssn Fname EMPNAMES Alicia Jennifer Joyce English Zelaya Wallace Lname Ssn Fname EMPDEPENDENTS Alicia Alicia Alicia Alicia Alicia Alicia Alicia Jennifer Jennifer Jennifer Jennifer Jennifer Joyce Jennifer Jennifer Joyce Joyce Zelaya Zelaya Zelaya Zelaya Zelaya Zelaya Wallace Wallace Wallace Wallace Wallace Wallace English Zelaya English Wallace English Lname Ssn Essn Abner Theodore Joy M Dependentname Sex . . . . . . . . . . . . Michael Elizabeth Alice M M . . . . . . . . . Alice Joy Theodore M . . . . . . . . . Abner Alice Michael M M . . . . . . . . . Elizabeth Theodore Alice . . . . . . . . . Joy M . . . . . . Bdate Joyce Joyce Joyce Joyce English English English English Abner Alice Michael M M . . . . . . Elizabeth . . . . . . Fname ACTUALDEPENDENTS Lname Ssn Essn Dependentname Sex Bdate . . . Jennifer Wallace Abner M . . . Fname R E S U LT Lname Dependentname Jennifer Wallace Abner Figure The Cartesian Product operation. Binary Relational Operations JOIN and DIVISION Essn value match the Ssn value of the EMPLOYEE tuple. The ACTUALDEPENDENTS relation accomplishes this. The EMPDEPENDENTS relation is a good example of the case where relational algebra can be correctly applied to yield results that make no sense at all. It is the responsibility of the user to make sure to apply only meaningful operations to relations. The CARTESIAN PRODUCT creates tuples with the combined attributes of two relations. We can SELECT related tuples only from the two relations by specifying an appropriate selection condition after the Cartesian product as we did in the preceding example. Because this sequence of CARTESIAN PRODUCT followed by SELECT is quite commonly used to combine related tuples from two relations a special operation called JOIN was created to specify this sequence as a single operation. We discuss the JOIN operation next. In SQL CARTESIAN PRODUCT can be realized by using the CROSS JOIN option in joined tables The first operation is illustrated in Figure Note that Mgrssn is a foreign key of the DEPARTMENT relation that references Ssn the primary key of the EMPLOYEE relation. This referential integrity constraint plays a role in having matching tuples in the referenced relation EMPLOYEE. The JOIN operation can be specified as a CARTESIAN PRODUCT operation followed by a SELECT operation. However JOIN is very important because it is used very frequently when specifying database queries. Consider the earlier example illustrating CARTESIAN PRODUCT which included the following sequence of operations EMPDEPENDENTS ← EMPNAMES × DEPENDENT ACTUALDEPENDENTS ← σSsn Essn Chapter The Relational Algebra and Relational Calculus DEPTMGR Dname Dnumber Mgrssn Fname Minit Lname Ssn Research Franklin T Wong Administration Jennifer S Wallace Headquarters James E Borg . . . . . . . . . . . . . . . . . . . . . . . . Figure Result of the JOIN operation DEPTMGR ← DEPARTMENT Mgrssn SsnEMPLOYEE. These two operations can be replaced with a single JOIN operation as follows ACTUALDEPENDENTS ← EMPNAMES Ssn EssnDEPENDENT The general form of a JOIN operation on two An and Bm is R join condition S The result of the JOIN is a relation Q with n + m attributes An Bm in that order Q has one tuple for each combination of tuples one from R and one from S whenever the combination satisfies the join condition. This is the main difference between CARTESIAN PRODUCT and JOIN. In JOIN only combinations of tuples satisfying the join condition appear in the result whereas in the CARTESIAN PRODUCT all combinations of tuples are included in the result. The join condition is specified on attributes from the two relations R and S and is evaluated for each combination of tuples. Each tuple combination for which the join condition evaluates to TRUE is included in the resulting relation Q as a single combined tuple. A general join condition is of the form condition AND condition condition where each condition is of the form Ai θ Bj Ai is an attribute of R Bj is an attribute of S Ai and Bj have the same domain and θ is one of the comparison operators ≤ ≥ ≠ . A JOIN operation with such a general join condition is called a THETA JOIN. Tuples whose join attributes are NULL or for which the join condition is FALSE do not appear in the result. In that sense the JOIN operation does not necessarily preserve all of the information in the participating relations because tuples that do not get combined with matching ones in the other relation do not appear in the result. notice that R and S can be any relations that result from general relational algebra expressions. Binary Relational Operations JOIN and DIVISION Variations of JOIN The EQUIJOIN and NATURAL JOIN The most common use of JOIN involves join conditions with equality comparisons only. Such a JOIN where the only comparison operator used is is called an EQUIJOIN. Both previous examples were EQUIJOINs. Notice that in the result of an EQUIJOIN we always have one or more pairs of attributes that have identical values in every tuple. For example in Figure the values of the attributes Mgrssn and Ssn are identical in every tuple of DEPTMGR because the equality join condition specified on these two attributes requires the values to be identical in every tuple in the result. Because one of each pair of attributes with identical values is superfluous a new operation called NATURAL JOIN denoted by was created to get rid of the second attribute in an EQUIJOIN The standard definition of NATURAL JOIN requires that the two join attributes have the same name in both relations. If this is not the case a renaming operation is applied first. Suppose we want to combine each PROJECT tuple with the DEPARTMENT tuple that controls the project. In the following example first we rename the Dnumber attribute of DEPARTMENT to Dnum so that it has the same name as the Dnum attribute in PROJECT and then we apply NATURAL JOIN PROJDEPT ← PROJECT ρ The same query can be done in two steps by creating an intermediate table DEPT as follows DEPT ← ρ PROJDEPT ← PROJECT DEPT The attribute Dnum is called the join attribute for the NATURAL JOIN operation because it is the only attribute with the same name in both relations. The resulting relation is illustrated in Figure In the PROJDEPT relation each tuple combines a PROJECT tuple with the DEPARTMENT tuple for the department that controls the project but only one join attribute value is kept. If the attributes on which the natural join is specified already have the same names in both relations renaming is unnecessary. For example to apply a natural join on the Dnumber attributes of DEPARTMENT and DEPTLOCATIONS it is sufficient to write DEPTLOCS ← DEPARTMENT DEPTLOCATIONS The resulting relation is shown in Figure which combines each department with its locations and has one tuple for each location. In general the join condition for NATURAL JOIN is constructed by equating each pair of join attributes that have the same name in the two relations and combining these conditions with AND. There can be a list of join attributes from each relation and each corresponding pair must have the same name. JOIN is basically an EQUIJOIN followed by the removal of the superfluous attributes. Chapter The Relational Algebra and Relational Calculus Pname PROJDEPT ProductX ProductY ProductZ Computerization Reorganization Newbenefits Pnumber Houston Bellaire Sugarland Stafford Stafford Houston Plocation Dnum Research Research Research Administration Administration Headquarters Dname Mgrssn Mgrstartdate Dname DEPTLOCS Dnumber Mgrssn Research Research Research Administration Headquarters Houston Bellaire Stafford Sugarland Houston Mgrstartdate Location Figure Results of two NATURAL JOIN operations. PROJDEPT ← PROJECT DEPT. DEPTLOCS ← DEPARTMENT DEPTLOCATIONS. A more general but nonstandard definition for NATURAL JOIN is Q ← R In this case specifies a list of i attributes from R and specifies a list of i attributes from S. The lists are used to form equality comparison conditions between pairs of corresponding attributes and the conditions are then ANDed together. Only the list corresponding to attributes of the first relation is kept in the result Q. Notice that if no combination of tuples satisfies the join condition the result of a JOIN is an empty relation with zero tuples. In general if R has nR tuples and S has nS tuples the result of a JOIN operation R join condition S will have between zero and nR nS tuples. The expected size of the join result divided by the maximum size nR nS leads to a ratio called join selectivity which is a property of each join condition. If there is no join condition all combinations of tuples qualify and the JOIN degenerates into a CARTESIAN PRODUCT also called CROSS PRODUCT or CROSS JOIN. As we can see a single JOIN operation is used to combine data from two relations so that related information can be presented in a single table. These operations are also known as inner joins to distinguish them from a different join variation called Binary Relational Operations JOIN and DIVISION outer joins Mgrssn SsnEMPLOYEE This combines each project tuple with its controlling department tuple into a single tuple and then combines that tuple with an employee tuple that is the department manager. The net result is a consolidated relation in which each tuple contains this project department manager combined information. In SQL JOIN can be realized in several different ways. The first method is to specify the join conditions in the WHERE clause along with any other selection conditions. This is very common and is illustrated by queries and in Sections and as well as by many other query examples in Chapters and The second way is to use a nested relation as illustrated by queries and in Section Another way is to use the concept of joined tables as illustrated by the queries and in Section The construct of joined tables was added to to allow the user to specify explicitly all the various types of joins because the other methods were more limited. It also allows the user to clearly distinguish join conditions from the selection conditions in the WHERE clause. A Complete Set of Relational Algebra Operations It has been shown that the set of relational algebra operations σ π ∪ ρ – × is a complete set that is any of the other original relational algebra operations can be expressed as a sequence of operations from this set. For example the INTERSECTION operation can be expressed by using UNION and MINUS as follows R ∩ S ≡ – ∪ Although strictly speaking INTERSECTION is not required it is inconvenient to specify this complex expression every time we wish to specify an intersection. As another example a JOIN operation can be specified as a CARTESIAN PRODUCT followed by a SELECT operation as we discussed R condition S ≡ σ condition Similarly a NATURAL JOIN can be specified as a CARTESIAN PRODUCT preceded by RENAME and followed by SELECT and PROJECT operations. Hence the various JOIN operations are also not strictly necessary for the expressive power of the relational algebra. However they are important to include as separate operations because they are convenient to use and are very commonly applied in database applications. Other operations have been included in the basic relational algebra for convenience rather than necessity. We discuss one of these the DIVISION operation in the next section. Chapter The Relational Algebra and Relational Calculus Essn SSNPNOS Pno A R B SMITHPNOS Pno S A T B SSNS Ssn Figure The DIVISION operation. Dividing SSNPNOS by SMITHPNOS. T ← R ÷ S. The DIVISION Operation The DIVISION operation denoted by ÷ is useful for a special kind of query that sometimes occurs in database applications. An example is Retrieve the names of employees who work on all the projects that ‘John Smith’ works on. To express this query using the DIVISION operation proceed as follows. First retrieve the list of project numbers that ‘John Smith’ works on in the intermediate relation SMITHPNOS SMITH ← σFname ‘John’ AND Lname ‘Smith’ SMITHPNOS ← πPno Next create a relation that includes a tuple Pno Essn whenever the employee whose Ssn is Essn works on the project whose number is Pno in the intermediate relation SSNPNOS SSNPNOS ← πEssn Pno Finally apply the DIVISION operation to the two relations which gives the desired employees’ Social Security numbers SSNS ← SSNPNOS ÷ SMITHPNOS RESULT ← πFname Lname The preceding operations are shown in Figure Binary Relational Operations JOIN and DIVISION In general the DIVISION operation is applied to two relations R ÷ S where the attributes of R are a subset of the attributes of S that is X ⊆ Z. Let Y be the set of attributes of R that are not attributes of S that is Y Z – X . The result of DIVISION is a relation T that includes a tuple t if tuples t R appear in R with t R [Y] t and with t R [X] t S for every tuple t S in S. This means that for a tuple t to appear in the result T of the DIVISION the values in t must appear in R in combination with every tuple in S. Note that in the formulation of the DIVISION operation the tuples in the denominator relation S restrict the numerator relation R by selecting those tuples in the result that match all values present in the denominator. It is not necessary to know what those values are as they can be computed by another operation as illustrated in the SMITHPNOS relation in the above example. Figure illustrates a DIVISION operation where X A Y B and Z A B . Notice that the tuples and appear in R in combination with all three tuples in S that is why they appear in the resulting relation T. All other values of B in R do not appear with all the tuples in S and are not selected does not appear with and does not appear with The DIVISION operation can be expressed as a sequence of π × and – operations as follows ← πY ← πY T ← – The DIVISION operation is defined for convenience for dealing with queries that involve universal quantification are available and then replacing that internal node by the relation that results from executing the operation. The execution terminates when the root node is executed and produces the result relation for the query. Chapter The Relational Algebra and Relational Calculus Table Operations of Relational Algebra OPERATION PURPOSE NOTATION SELECT Selects all tuples that satisfy the selection condition from a relation R. σ selection condition PROJECT Produces a new relation with only some of the attributes of R and removes duplicate tuples. π attribute list THETA JOIN Produces all combinations of tuples from and that satisfy the join condition. join condition EQUIJOIN Produces all the combinations of tuples from and that satisfy a join condition with only equality comparisons. join condition OR that includes all tuples t[X] in that appear in in combination with every tuple from where Z X ∪ Y. ÷ Figure shows a query tree for Query Dnum Dnumber Mgrssn Ssn In Figure the three leaf nodes P D and E represent the three relations PROJECT DEPARTMENT and EMPLOYEE. The relational algebra operations in the expression Additional Relational Operations π σ ‘Stafford’ E D P EMPLOYEE DEPARTMENT PROJECT Figure Query tree corresponding to the relational algebra expression for are represented by internal tree nodes. The query tree signifies an explicit order of execution in the following sense. In order to execute the node marked in Figure must begin execution before node because some resulting tuples of operation must be available before we can begin to execute operation Similarly node must begin to execute and produce results before node can start execution and so on. In general a query tree gives a good visual representation and understanding of the query in terms of the relational operations it uses and is recommended as an additional means for expressing queries in relational algebra. We will revisit query trees when we discuss query processing and optimization in Chapter Additional Relational Operations Some common database requests which are needed in commercial applications for RDBMSs cannot be performed with the original relational algebra operations described in Sections through In this section we define additional operations to express these requests. These operations enhance the expressive power of the original relational algebra. Generalized Projection The generalized projection operation extends the projection operation by allowing functions of attributes to be included in the projection list. The generalized form can be expressed as Fn Chapter The Relational Algebra and Relational Calculus where Fn are functions over the attributes in relation R and may involve arithmetic operations and constant values. This operation is helpful when developing reports where computed values have to be produced in the columns of a query result. As an example consider the relation EMPLOYEE A report may be required to show Net Salary Salary – Deduction Bonus Yearsservice and Tax Salary. Then a generalized projection combined with renaming may be used as follows REPORT ← ρ . Aggregate Functions and Grouping Another type of request that cannot be expressed in the basic relational algebra is to specify mathematical aggregate functions on collections of values from the database. Examples of such functions include retrieving the average or total salary of all employees or the total number of employee tuples. These functions are used in simple statistical queries that summarize information from the database tuples. Common functions applied to collections of numeric values include SUM AVERAGE MAXIMUM and MINIMUM. The COUNT function is used for counting tuples or values. Another common type of request involves grouping the tuples in a relation by the value of some of their attributes and then applying an aggregate function independently to each group. An example would be to group EMPLOYEE tuples by Dno so that each group includes the tuples for employees working in the same department. We can then list each Dno value along with say the average salary of employees within the department or the number of employees who work in the department. We can define an AGGREGATE FUNCTION operation using the symbol ℑ where grouping attributes is a list of attributes of the relation specified in R and function list is a list of pairs. In each such pair function is one of the allowed functions such as SUM AVERAGE MAXIMUM MINIMUM COUNT and attribute is an attribute of the relation specified by R. The is no single agreed upon notation for specifying aggregate functions. In some cases a “script A” is used. Additional Relational Operations Countssn Dno Countssn Averagesalary Averagesalary Dno Noofemployees Averagesal R Figure The aggregate function operation. a. ρR . b. Dno ℑ COUNT Ssn AVERAGE Salary. c. ℑ COUNT Ssn AVERAGE Salary. resulting relation has the grouping attributes plus one attribute for each element in the function list. For example to retrieve each department number the number of employees in the department and their average salary while renaming the resulting attributes as indicated below we write ρR The result of this operation on the EMPLOYEE relation of Figure is shown in Figure In the above example we specified a list of attribute names between parentheses in the RENAME operation for the resulting relation R. If no renaming is applied then the attributes of the resulting relation that correspond to the function list will each be the concatenation of the function name with the attribute name in the form For example Figure shows the result of the following operation Dno ℑ COUNT Ssn AVERAGE Salary If no grouping attributes are specified the functions are applied to all the tuples in the relation so the resulting relation has a single tuple only. For example Figure shows the result of the following operation ℑ COUNT Ssn AVERAGE Salary It is important to note that in general duplicates are not eliminated when an aggregate function is applied this way the normal interpretation of functions such as that this is an arbitrary notation we are suggesting. There is no standard notation. Chapter The Relational Algebra and Relational Calculus SUM and AVERAGE is It is worth emphasizing that the result of applying an aggregate function is a relation not a scalar number even if it has a single value. This makes the relational algebra a closed mathematical system. Recursive Closure Operations Another type of operation that in general cannot be specified in the basic original relational algebra is recursive closure. This operation is applied to a recursive relationship between tuples of the same type such as the relationship between an employee and a supervisor. This relationship is described by the foreign key Superssn of the EMPLOYEE relation in Figures and and it relates each employee tuple to another employee tuple . An example of a recursive operation is to retrieve all supervisees of an employee e at all levels that is all employees e directly supervised by e all employees eℑ directly supervised by each employee e all employees e directly supervised by each employee e and so on. It is relatively straightforward in the relational algebra to specify all employees supervised by e at a specific level by joining the table with itself one or more times. However it is difficult to specify all supervisees at all levels. For example to specify the Ssns of all employees e directly supervised at level one by the employee e whose name is ‘James Borg’ ← πSsn Superssn ← To retrieve all employees supervised by Borg at level is all employees e supervised by some employee e who is directly supervised by Borg we can apply another JOIN to the result of the first query as follows ← To get both sets of employees supervised at levels and by ‘James Borg’ we can apply the UNION operation to the two results as follows RESULT ← ∪ The results of these queries are illustrated in Figure Although it is possible to retrieve employees at each level and then take their UNION we cannot in general specify a query such as “retrieve the supervisees of ‘James Borg’ at all levels” without utilizing a looping mechanism unless we know the maximum number of An operation called the transitive closure of relations has been proposed to compute the recursive relationship as far as the recursion proceeds. SQL the option of eliminating duplicates before applying the aggregate function is available by including the keyword DISTINCT Ssn RESULT Ssn ∪ Ssn Figure A two level recursive query. OUTER JOIN Operations Next we discuss some additional extensions to the JOIN operation that are necessary to specify certain types of queries. The JOIN operations described earlier match tuples that satisfy the join condition. For example for a NATURAL JOIN operation R S only tuples from R that have matching tuples in S and vice versa appear in the result. Hence tuples without a matching tuple are eliminated from the JOIN result. Tuples with NULL values in the join attributes are also eliminated. This type of join where tuples with no match are eliminated is known as an inner join. The join operations we described earlier in Section are all inner joins. This amounts to the loss of information if the user wants the result of the JOIN to include all the tuples in one or more of the component relations. A set of operations called outer joins were developed for the case where the user wants to keep all the tuples in R or all those in S or all those in both relations in the result of the JOIN regardless of whether or not they have matching tuples in the other relation. This satisfies the need of queries in which tuples from two tables are Chapter The Relational Algebra and Relational Calculus RESULT Fname Minit Lname Dname John Franklin Alicia Jennifer Ramesh Joyce Ahmad James B T J S K A V E Smith Wong Zelaya Wallace Narayan English Jabbar Borg NULL Research NULL Administration NULL NULL NULL Headquarters Figure The result of a LEFT OUTER JOIN operation. to be combined by matching corresponding rows but without losing any tuples for lack of matching values. For example suppose that we want a list of all employee names as well as the name of the departments they manage if they happen to manage a department if they do not manage one we can indicate it with a NULL value. We can apply an operation LEFT OUTER JOIN denoted by to retrieve the result as follows TEMP ← RESULT ← πFname Minit Lname Dname The LEFT OUTER JOIN operation keeps every tuple in the first or left relation R in R S if no matching tuple is found in S then the attributes of S in the join result are filled or padded with NULL values. The result of these operations is shown in Figure A similar operation RIGHT OUTER JOIN denoted by keeps every tuple in the second or right relation S in the result of R S. A third operation FULL OUTER JOIN denoted by keeps all tuples in both the left and the right relations when no matching tuples are found padding them with NULL values as needed. The three outer join operations are part of the standard compatible. This operation will take the UNION of tuples in two relations R and S that are partially compatible meaning that only some of their attributes say X are union compatible. The attributes that are union compatible are represented only once in the result and those attributes that are not union compatible from either Examples of Queries in Relational Algebra relation are also kept in the result relation T. It is therefore the same as a FULL OUTER JOIN on the common attributes. Two tuples t in R and t in S are said to match if t These will be combined into a single tuple in t. Tuples in either relation that have no matching tuple in the other relation are padded with NULL values. For example an OUTER UNION can be applied to two relations whose schemas are STUDENT and INSTRUCTOR. Tuples from the two relations are matched based on having the same combination of values of the shared attributes Name Ssn Department. The resulting relation STUDENTORINSTRUCTOR will have the following attributes STUDENTORINSTRUCTOR All the tuples from both relations are included in the result but tuples with the same combination will appear only once in the result. Tuples appearing only in STUDENT will have a NULL for the Rank attribute whereas tuples appearing only in INSTRUCTOR will have a NULL for the Advisor attribute. A tuple that exists in both relations which represent a student who is also an instructor will have values for all its Notice that the same person may still appear twice in the result. For example we could have a graduate student in the Mathematics department who is an instructor in the Computer Science department. Although the two tuples representing that person in STUDENT and INSTRUCTOR will have the same values they will not agree on the Department value and so will not be matched. This is because Department has two different meanings in STUDENT and INSTRUCTOR . If we wanted to apply the OUTER UNION based on the same combination only we should rename the Department attribute in each table to reflect that they have different meanings and designate them as not being part of the union compatible attributes. For example we could rename the attributes as MajorDept in STUDENT and WorkDept in INSTRUCTOR. Examples of Queries in Relational Algebra The following are additional examples to illustrate the use of the relational algebra operations. All examples refer to the database in Figure In general the same query can be stated in numerous ways using the various operations. We will state each query in one way and leave it to the reader to come up with equivalent formulations. Query Retrieve the name and address of all employees who work for the ‘Research’ department. that OUTER UNION is equivalent to a FULL OUTER JOIN if the join attributes are all the common attributes of the two relations. Chapter The Relational Algebra and Relational Calculus RESEARCHDEPT ← σDname ‘Research’ RESEARCHEMPS ← RESULT ← πFname Lname Address As a single in line expression this query becomes πFname Lname Address This query could be specified in other ways for example the order of the JOIN and SELECT operations could be reversed or the JOIN could be replaced by a NATURAL JOIN after renaming one of the join attributes to match the other join attribute name. Query For every project located in ‘Stafford’ list the project number the controlling department number and the department manager’s last name address and birth date. STAFFORDPROJS ← σPlocation ‘Stafford’ CONTRDEPTS ← PROJDEPTMGRS ← RESULT ← πPnumber Dnum Lname Address Bdate In this example we first select the projects located in Stafford then join them with their controlling departments and then join the result with the department managers. Finally we apply a project operation on the desired attributes. Query Find the names of employees who work on all the projects controlled by department number ← ρ EMPPROJ ← ρ RESULTEMPSSNS ← EMPPROJ ÷ RESULT ← πLname Fname In this query we first create a table that contains the project numbers of all projects controlled by department Then we create a table EMPPROJ that holds tuples and apply the division operation. Notice that we renamed the attributes so that they will be correctly used in the division operation. Finally we join the result of the division which holds only Ssn values with the EMPLOYEE table to retrieve the desired attributes from EMPLOYEE. Query Make a list of project numbers for projects that involve an employee whose last name is ‘Smith’ either as a worker or as a manager of the department that controls the project. SMITHS ← πSsn SMITHWORKERPROJS ← πPno MGRS ← πLname Dnumber SMITHMANAGEDDEPTS ← πDnumber SMITHMGRPROJS ← πPnumber RESULT ← Examples of Queries in Relational Algebra In this query we retrieved the project numbers for projects that involve an employee named Smith as a worker in SMITHWORKERPROJS. Then we retrieved the project numbers for projects that involve an employee named Smith as manager of the department that controls the project in SMITHMGRPROJS. Finally we applied the UNION operation on SMITHWORKERPROJS and SMITHMGRPROJS. As a single in line expression this query becomes πPno ∪ πPno Ssn MgrssnDEPARTMENT Dnumber DnumPROJECT Query List the names of all employees with two or more dependents. Strictly speaking this query cannot be done in the basic relational algebra. We have to use the AGGREGATE FUNCTION operation with the COUNT aggregate function. We assume that dependents of the same employee have distinct Dependentname values. Noofdependents ← Essn ℑ COUNT Dependentname ← RESULT ← πLname EMPLOYEE Query Retrieve the names of employees who have no dependents. This is an example of the type of query that uses the MINUS operation. ALLEMPS ← πSsn EMPSWITHDEPS ← πEssn EMPSWITHOUTDEPS ← RESULT ← πLname Fname We first retrieve a relation with all employee Ssns in ALLEMPS. Then we create a table with the Ssns of employees who have at least one dependent in EMPSWITHDEPS. Then we apply the SET DIFFERENCE operation to retrieve employees Ssns with no dependents in EMPSWITHOUTDEPS and finally join this with EMPLOYEE to retrieve the desired attributes. As a single in line expression this query becomes πLname Fname – ρSsn EMPLOYEE Query List the names of managers who have at least one dependent. MGRS ← πMgrssn EMPSWITHDEPS ← πEssn MGRSWITHDEPS ← RESULT ← πLname Fname In this query we retrieve the Ssns of managers in MGRS and the Ssns of employees with at least one dependent in EMPSWITHDEPS then we apply the SET INTERSECTION operation to get the Ssns of managers who have at least one dependent. Chapter The Relational Algebra and Relational Calculus As we mentioned earlier the same query can be specified in many different ways in relational algebra. In particular the operations can often be applied in various orders. In addition some operations can be used to replace others for example the INTERSECTION operation in can be replaced by a NATURAL JOIN. As an exercise try to do each of these sample queries using different We showed how to write queries as single relational algebra expressions for queries and Try to write the remaining queries as single expressions. In Chapters and and in Sections and we show how these queries are written in other relational languages. The Tuple Relational Calculus In this and the next section we introduce another formal query language for the relational model called relational calculus. This section introduces the language known as tuple relational calculus and Section introduces a variation called domain relational calculus. In both variations of relational calculus we write one declarative expression to specify a retrieval request hence there is no description of how or in what order to evaluate a query. A calculus expression specifies what is to be retrieved rather than how to retrieve it. Therefore the relational calculus is considered to be a nonprocedural language. This differs from relational algebra where we must write a sequence of operations to specify a retrieval request in a particular order of applying the operations thus it can be considered as a procedural way of stating a query. It is possible to nest algebra operations to form a single expression however a certain order among the operations is always explicitly specified in a relational algebra expression. This order also influences the strategy for evaluating the query. A calculus expression may be written in different ways but the way it is written has no bearing on how a query should be evaluated. It has been shown that any retrieval that can be specified in the basic relational algebra can also be specified in relational calculus and vice versa in other words the expressive power of the languages is identical. This led to the definition of the concept of a relationally complete language. A relational query language L is considered relationally complete if we can express in L any query that can be expressed in relational calculus. Relational completeness has become an important basis for comparing the expressive power of high level query languages. However as we saw in Section certain frequently required queries in database applications cannot be expressed in basic relational algebra or calculus. Most relational query languages are relationally complete but have more expressive power than relational algebra or relational calculus because of additional operations such as aggregate functions grouping and ordering. As we mentioned in the introduction to this chapter the relational calculus is important for two reasons. First it has a firm basis in mathematical logic. Second the standard query language for RDBMSs has some of its foundations in the tuple relational calculus. queries are optimized Tuple Variables and Range Relations The tuple relational calculus is based on specifying a number of tuple variables. Each tuple variable usually ranges over a particular database relation meaning that the variable may take as its value any individual tuple from that relation. A simple tuple relational calculus query is of the form t | COND where t is a tuple variable and COND is a conditional expression involving t that evaluates to either TRUE or FALSE for different assignments of tuples to the variable t. The result of such a query is the set of all tuples t that evaluate COND to TRUE. These tuples are said to satisfy COND. For example to find all employees whose salary is above we can write the following tuple calculus expression t | EMPLOYEE AND The condition EMPLOYEE specifies that the range relation of tuple variable t is EMPLOYEE. Each EMPLOYEE tuple t that satisfies the condition will be retrieved. Notice that references attribute Salary of tuple variable t this notation resembles how attribute names are qualified with relation names or aliases in SQL as we saw in Chapter In the notation of Chapter is the same as writing t[Salary]. The above query retrieves all attribute values for each selected EMPLOYEE tuple t. To retrieve only some of the attributes say the first and last names we write | EMPLOYEE AND Informally we need to specify the following information in a tuple relational calculus expression For each tuple variable t the range relation R of t. This value is specified by a condition of the form R. If we do not specify a range relation then the variable t will range over all possible tuples “in the universe” as it is not restricted to any one relation. A condition to select particular combinations of tuples. As tuple variables range over their respective range relations the condition is evaluated for every possible combination of tuples to identify the selected combinations for which the condition evaluates to TRUE. A set of attributes to be retrieved the requested attributes. The values of these attributes are retrieved for each selected combination of tuples. Before we discuss the formal syntax of tuple relational calculus consider another query. Chapter The Relational Algebra and Relational Calculus Query Retrieve the birth date and address of the employee whose name is John B. Smith. | EMPLOYEE AND ‘John’ AND ‘B’ AND ‘Smith’ In tuple relational calculus we first specify the requested attributes and for each selected tuple t. Then we specify the condition for selecting a tuple following the bar namely that t be a tuple of the EMPLOYEE relation whose Fname Minit and Lname attribute values are ‘John’ ‘B’ and ‘Smith’ respectively. Expressions and Formulas in Tuple Relational Calculus A general expression of the tuple relational calculus is of the form t t t | COND where t t t n t t n+m are tuple variables each Ai is an attribute of the relation on which t i ranges and COND is a condition or formula. of the tuple relational calculus. A formula is made up of predicate calculus atoms which can be one of the following An atom of the form R where R is a relation name and t i is a tuple variable. This atom identifies the range of the tuple variable t i as the relation whose name is R. It evaluates to TRUE if t i is a tuple in the relation R and evaluates to FALSE otherwise. An atom of the form t i .A op t j .B where op is one of the comparison operators in the set ≤ ≥ ≠ t i and t j are tuple variables A is an attribute of the relation on which t i ranges and B is an attribute of the relation on which t j ranges. An atom of the form t i .A op c or c op t j .B where op is one of the comparison operators in the set ≤ ≥ ≠ t i and t j are tuple variables A is an attribute of the relation on which t i ranges B is an attribute of the relation on which t j ranges and c is a constant value. Each of the preceding atoms evaluates to either TRUE or FALSE for a specific combination of tuples this is called the truth value of an atom. In general a tuple variable t ranges over all possible tuples in the universe. For atoms of the form R if t is assigned to a tuple that is a member of the specified relation R the atom is TRUE otherwise it is FALSE. In atoms of types and if the tuple variables are assigned to tuples such that the values of the specified attributes of the tuples satisfy the condition then the atom is TRUE. A formula is made up of one or more atoms connected via the logical operators AND OR and NOT and is defined recursively by Rules and as follows Rule Every atom is a formula. called a well formed formula or WFF in mathematical logic. The Tuple Relational Calculus Rule If and are formulas then so are AND OR NOT and NOT The truth values of these formulas are derived from their component formulas and as follows a. AND is TRUE if both and are TRUE otherwise it is FALSE. b. OR is FALSE if both and are FALSE otherwise it is TRUE. c. NOT is TRUE if is FALSE it is FALSE if is TRUE. d. NOT is TRUE if is FALSE it is FALSE if is TRUE. The Existential and Universal Quantifiers In addition two special symbols called quantifiers can appear in formulas these are the universal quantifier and the existential quantifier . Truth values for formulas with quantifiers are described in Rules and below first however we need to define the concepts of free and bound tuple variables in a formula. Informally a tuple variable t is bound if it is quantified meaning that it appears in an or clause otherwise it is free. Formally we define a tuple variable in a formula as free or bound according to the following rules An occurrence of a tuple variable in a formula F that is an atom is free in F. An occurrence of a tuple variable t is free or bound in a formula made up of logical AND OR and depending on whether it is free or bound in or . Notice that in a formula of the form F AND or F OR a tuple variable may be free in and bound in or vice versa in this case one occurrence of the tuple variable is bound and the other is free in F. All free occurrences of a tuple variable t in F are bound in a formula F of the form F or F . The tuple variable is bound to the quantifier specified in F. For example consider the following formulas ‘Research’ The tuple variable d is free in both and whereas it is bound to the quantifier in Variable t is bound to the quantifier in We can now give Rules and for the definition of a formula we started earlier Rule If F is a formula then so is where t is a tuple variable. The formula is TRUE if the formula F evaluates to TRUE for some tuple assigned to free occurrences of t in F otherwise is FALSE. Rule If F is a formula then so is where t is a tuple variable. The formula is TRUE if the formula F evaluates to TRUE for every tuple assigned to free occurrences of t in F otherwise is FALSE. The quantifier is called an existential quantifier because a formula is TRUE if there exists some tuple that makes F TRUE. For the universal quantifier Chapter The Relational Algebra and Relational Calculus is TRUE if every possible tuple that can be assigned to free occurrences of t in F is substituted for t and F is TRUE for every such substitution. It is called the universal or for all quantifier because every tuple in the universe of tuples must make F TRUE to make the quantified formula TRUE. Sample Queries in Tuple Relational Calculus We will use some of the same queries from Section to give a flavor of how the same queries are specified in relational algebra and in relational calculus. Notice that some queries are easier to specify in the relational algebra than in the relational calculus and vice versa. Query List the name and address of all employees who work for the ‘Research’ department. | EMPLOYEE AND AND ‘Research’ AND The only free tuple variables in a tuple relational calculus expression should be those that appear to the left of the bar . In t is the only free variable it is then bound successively to each tuple. If a tuple satisfies the conditions specified after the bar in the attributes Fname Lname and Address are retrieved for each such tuple. The conditions EMPLOYEE and DEPARTMENT specify the range relations for t and d. The condition ‘Research’ is a selection condition and corresponds to a SELECT operation in the relational algebra whereas the condition is a join condition and is similar in purpose to the JOIN operation AND EMPLOYEE AND ‘Stafford’ AND AND AND In there are two free tuple variables p and m. Tuple variable d is bound to the existential quantifier. The query condition is evaluated for every combination of tuples assigned to p and m and out of all possible combinations of tuples to which p and m are bound only the combinations that satisfy the condition are selected. Several tuple variables in a query can range over the same relation. For example to specify each employee retrieve the employee’s first and last name and the first and last name of his or her immediate supervisor we specify two tuple variables e and s that both range over the EMPLOYEE relation | EMPLOYEE AND EMPLOYEE AND Query List the name of each employee who works on some project controlled by department number This is a variation of in which all is The Tuple Relational Calculus [ ] [ ] ‘Stafford’ PDE ‘Stafford’ Figure Query graph for changed to some. In this case we need two join conditions and two existential quantifiers. | EMPLOYEE AND AND WORKSON AND AND AND Query Make a list of project numbers for projects that involve an employee whose last name is ‘Smith’ either as a worker or as manager of the controlling department for the project. | PROJECT AND AND WORKSON AND AND ‘Smith’ AND OR AND DEPARTMENT AND AND AND ‘Smith’ Compare this with the relational algebra version of this query in Section The UNION operation in relational algebra can usually be substituted with an OR connective in relational calculus. Notation for Query Graphs In this section we describe a notation that has been proposed to represent relational calculus queries that do not involve complex quantification in a graphical form. These types of queries are known as select project join queries because they only involve these three relational algebra operations. The notation may be expanded to more general queries but we do not discuss these extensions here. This graphical representation of a query is called a query graph. Figure shows the query graph for Relations in the query are represented by relation nodes which are displayed as single circles. Constant values typically from the query selection conditions are represented by constant nodes which are displayed as double circles or ovals. Selection and join conditions are represented by the graph edges as shown in Figure Finally the attributes to be retrieved from each relation are displayed in square brackets above each relation. Chapter The Relational Algebra and Relational Calculus The query graph representation does not indicate a particular order to specify which operations to perform first and is hence a more neutral representation of a select project join query than the query tree representation AND and OR replace one another a negated formula becomes unnegated and an unnegated formula becomes negated. Some special cases of this transformation can be stated as follows where the ≡ symbol stands for equivalent to ≡ NOT ≡ NOT AND Q ≡ NOT OR NOT OR Q ≡ NOT AND NOT OR Q ≡ NOT AND NOT AND Q ≡ NOT OR NOT Notice also that the following is TRUE where the ⇒ symbol stands for implies ⇒ NOT ⇒ NOT Using the Universal Quantifier in Queries Whenever we use a universal quantifier it is quite judicious to follow a few rules to ensure that our expression makes sense. We discuss these rules with respect to the query Query List the names of employees who work on all the projects controlled by department number One way to specify this query is to use the universal quantifier as shown | EMPLOYEE AND OR NOT OR AND AND The Tuple Relational Calculus We can break up into its basic components as follows | EMPLOYEE AND F F OR OR AND AND We want to make sure that a selected employee e works on all the projects controlled by department but the definition of universal quantifier says that to make the quantified formula TRUE the inner formula must be TRUE for all tuples in the universe. The trick is to exclude from the universal quantification all tuples that we are not interested in by making the condition TRUE for all such tuples. This is necessary because a universally quantified tuple variable such as x in must evaluate to TRUE for every possible tuple assigned to it to make the quantified formula TRUE. The first tuples to exclude are those that are not in the relation R of interest. In using the expression NOT inside the universally quantified formula evaluates to TRUE all tuples x that are not in the PROJECT relation. Then we exclude the tuples we are not interested in from R itself. In using the expression evaluates to TRUE all tuples x that are in the PROJECT relation but are not controlled by department Finally we specify a condition that must hold on all the remaining tuples in R. Hence we can explain as follows For the formula F to be TRUE we must have the formula F be TRUE for all tuples in the universe that can be assigned to x. However in we are only interested in F being TRUE for all tuples of the PROJECT relation that are controlled by department Hence the formula F is of the form OR The ‘NOT OR condition is TRUE for all tuples not in the PROJECT relation and has the effect of eliminating these tuples from consideration in the truth value of For every tuple in the PROJECT relation must be TRUE if F is to be TRUE. Using the same line of reasoning we do not want to consider tuples in the PROJECT relation that are not controlled by department number since we are only interested in PROJECT tuples whose Therefore we can write IF THEN which is equivalent to Using the general transformation from universal to existential quantifiers given in Section we can rephrase the query in as shown in which uses a negated existential quantifier instead of the universal quantifier | EMPLOYEE AND AND AND AND AND We now give some additional examples of queries that use quantifiers. Query List the names of employees who have no dependents. | EMPLOYEE AND AND Using the general transformation rule we can rephrase as follows | EMPLOYEE AND OR NOT Query List the names of managers who have at least one dependent. | EMPLOYEE AND AND DEPENDENT AND AND This query is handled by interpreting managers who have at least one dependent as managers for whom there exists some dependent. Safe Expressions Whenever we use universal quantifiers existential quantifiers or negation of predicates in a calculus expression we must make sure that the resulting expression makes sense. A safe expression in relational calculus is one that is guaranteed to yield a finite number of tuples as its result otherwise the expression is called unsafe. For example the expression t | NOT is unsafe because it yields all tuples in the universe that are not EMPLOYEE tuples which are infinitely numerous. If we follow the rules for discussed earlier we will get a safe expression when using universal quantifiers. We can define safe expressions more precisely by introducing the concept of the domain of a tuple relational calculus expression This is the set of all values that either appear as constant values in the expression or exist in any tuple in the relations referenced in the expression. For example the domain of t | NOT is the set of all attribute values appearing in some tuple of the EMPLOYEE relation . The domain The Domain Relational Calculus of the expression would include all values appearing in EMPLOYEE PROJECT and WORKSON . An expression is said to be safe if all values in its result are from the domain of the expression. Notice that the result of t | NOT is unsafe since it will in general include tuples from outside the EMPLOYEE relation such values are not in the domain of the expression. All of our other examples are safe expressions. The Domain Relational Calculus There is another type of relational calculus called the domain relational calculus or simply domain calculus. Historically while SQL which is related to domain calculus was being developed almost concurrently at the IBM . Watson Research Center in Yorktown Heights New York. The formal specification of the domain calculus was proposed after the development of the QBE language and system. Domain calculus differs from tuple calculus in the type of variables used in formulas Rather than having variables range over tuples the variables range over single values from domains of attributes. To form a relation of degree n for a query result we must have n of these domain variables one for each attribute. An expression of the domain calculus is of the form xn | xn xn+m where xn xn+m are domain variables that range over domains and COND is a condition or formula of the domain relational calculus. A formula is made up of atoms. The atoms of a formula are slightly different from those for the tuple calculus and can be one of the following An atom of the form xj where R is the name of a relation of degree j and each xi ≤ i ≤ j is a domain variable. This atom states that a list of values of xj must be a tuple in the relation whose name is R where xi is the value of the ith attribute value of the tuple. To make a domain calculus expression more concise we can drop the commas in a list of variables thus we can write xn | AND instead of xn | AND An atom of the form xi op xj where op is one of the comparison operators in the set ≤ ≥ ≠ and xi and xj are domain variables. An atom of the form xi op c or c op xj where op is one of the comparison operators in the set ≤ ≥ ≠ xi and xj are domain variables and c is a constant value. Chapter The Relational Algebra and Relational Calculus As in tuple calculus atoms evaluate to either TRUE or FALSE for a specific set of values called the truth values of the atoms. In case if the domain variables are assigned values corresponding to a tuple of the specified relation R then the atom is TRUE. In cases and if the domain variables are assigned values that satisfy the condition then the atom is TRUE. In a similar way to the tuple relational calculus formulas are made up of atoms variables and quantifiers so we will not repeat the specifications for formulas here. Some examples of queries specified in the domain calculus follow. We will use lowercase letters l m n x y z for domain variables. Query List the birth date and address of the employee whose name is ‘John B. Smith’. u v | AND q ‘John’ AND r ‘B’ AND s ‘Smith’ We need ten variables for the EMPLOYEE relation one to range over each of the domains of attributes of EMPLOYEE in order. Of the ten variables q r s only u and v are free because they appear to the left of the bar and hence should not be bound to a quantifier. We first specify the requested attributes Bdate and Address by the free domain variables u for BDATE and v for ADDRESS. Then we specify the condition for selecting a tuple following the bar namely that the sequence of values assigned to the variables qrstuvwxyz be a tuple of the EMPLOYEE relation and that the values for q r and s be equal to ‘John’ ‘B’ and ‘Smith’ respectively. For convenience we will quantify only those variables actually appearing in a condition Query Retrieve the name and address of all employees who work for the ‘Research’ department. q s v | AND DEPARTMENT AND l ‘Research’ AND m z A condition relating two domain variables that range over attributes from two relations such as m z in is a join condition whereas a condition that relates a domain variable to a constant such as l ‘Research’ is a selection condition. that the notation of quantifying only the domain variables actually used in conditions and of showing a predicate such as EMPLOYEE without separating domain variables with commas is an abbreviated notation used for convenience it is not the correct formal notation. this is not a formally accurate notation. Summary Query For every project located in ‘Stafford’ list the project number the controlling department number and the department manager’s last name birth date and address. i k s u v | AND EMPLOYEE AND DEPARTMENT AND k m AND n t AND j ‘Stafford’ Query List the names of employees who have no dependents. q s | AND AND t l can be restated using universal quantifiers instead of the existential quantifiers as shown in q s | AND OR NOT Query List the names of managers who have at least one dependent. s q | AND DEPARTMENT AND DEPENDENT AND t j AND l t As we mentioned earlier it can be shown that any query that can be expressed in the basic relational algebra can also be expressed in the domain or tuple relational calculus. Also any safe expression in the domain or tuple relational calculus can be expressed in the basic relational algebra. The QBE language was based on the domain relational calculus although this was realized later after the domain calculus was formalized. QBE was one of the first graphical query languages with minimum syntax developed for database systems. It was developed at IBM Research and is available as an IBM commercial product as part of the Query Management Facility interface option to The basic ideas used in QBE have been applied in several other commercial products. Because of its important place in the history of relational languages we have included an overview of QBE in Appendix C. Summary In this chapter we presented two formal languages for the relational model of data. They are used to manipulate relations and produce new relations as answers to queries. We discussed the relational algebra and its operations which are used to specify a sequence of operations to specify a query. Then we introduced two types of relational calculi called tuple calculus and domain calculus. In Sections through we introduced the basic relational algebra operations and illustrated the types of queries for which each is used. First we discussed the unary relational operators SELECT and PROJECT as well as the RENAME operation. Then we discussed binary set theoretic operations requiring that relations on which they Chapter The Relational Algebra and Relational Calculus are applied be union compatible these include UNION INTERSECTION and SET DIFFERENCE. The CARTESIAN PRODUCT operation is a set operation that can be used to combine tuples from two relations producing all possible combinations. It is rarely used in practice however we showed how CARTESIAN PRODUCT followed by SELECT can be used to define matching tuples from two relations and leads to the JOIN operation. Different JOIN operations called THETA JOIN EQUIJOIN and NATURAL JOIN were introduced. Query trees were introduced as a graphical representation of relational algebra queries which can also be used as the basis for internal data structures that the DBMS can use to represent a query. We discussed some important types of queries that cannot be stated with the basic relational algebra operations but are important for practical situations. We introduced GENERALIZED PROJECTION to use functions of attributes in the projection list and the AGGREGATE FUNCTION operation to deal with aggregate types of statistical requests that summarize the information in the tables. We discussed recursive queries for which there is no direct support in the algebra but which can be handled in a step by step approach as we demonstrated. Then we presented the OUTER JOIN and OUTER UNION operations which extend JOIN and UNION and allow all information in source relations to be preserved in the result. The last two sections described the basic concepts behind relational calculus which is based on the branch of mathematical logic called predicate calculus. There are two types of relational calculi the tuple relational calculus which uses tuple variables that range over tuples of relations and the domain relational calculus which uses domain variables that range over domains . In relational calculus a query is specified in a single declarative statement without specifying any order or method for retrieving the query result. Hence relational calculus is often considered to be a higher level declarative language than the relational algebra because a relational calculus expression states what we want to retrieve regardless of how the query may be executed. We discussed the syntax of relational calculus queries using both tuple and domain variables. We introduced query graphs as an internal representation for queries in relational calculus. We also discussed the existential quantifier and the universal quantifier . We saw that relational calculus variables are bound by these quantifiers. We described in detail how queries with universal quantification are written and we discussed the problem of specifying safe queries whose results are finite. We also discussed rules for transforming universal into existential quantifiers and vice versa. It is the quantifiers that give expressive power to the relational calculus making it equivalent to the basic relational algebra. There is no analog to grouping and aggregation functions in basic relational calculus although some extensions have been suggested. Review Questions List the operations of relational algebra and the purpose of each. Exercises What is union compatibility Why do the UNION INTERSECTION and DIFFERENCE operations require that the relations on which they are applied be union compatible Discuss some types of queries for which renaming of attributes is necessary in order to specify the query unambiguously. Discuss the various types of inner join operations. Why is theta join required What role does the concept of foreign key play when specifying the most common types of meaningful join operations What is the FUNCTION operation What is it used for How are the OUTER JOIN operations different from the INNER JOIN operations How is the OUTER UNION operation different from UNION In what sense does relational calculus differ from relational algebra and in what sense are they similar How does tuple relational calculus differ from domain relational calculus Discuss the meanings of the existential quantifier and the universal quantifier . Define the following terms with respect to the tuple calculus tuple variable range relation atom formula and expression. Define the following terms with respect to the domain calculus domain variable range relation atom formula and expression. What is meant by a safe expression in relational calculus When is a query language called relationally complete Exercises Show the result of each of the sample queries in Section as it would apply to the database state in Figure Specify the following queries on the COMPANYrelational database schema shown in Figure using the relational operators discussed in this chapter. Also show the result of each query as it would apply to the database state in Figure a. Retrieve the names of all employees in department who work more than hours per week on the ProductX project. b. List the names of all employees who have a dependent with the same first name as themselves. c. Find the names of all employees who are directly supervised by ‘Franklin Wong’. d. For each project list the project name and the total hours per week spent on that project. Chapter The Relational Algebra and Relational Calculus e. Retrieve the names of all employees who work on every project. f. Retrieve the names of all employees who do not work on any project. g. For each department retrieve the department name and the average salary of all employees working in that department. h. Retrieve the average salary of all female employees. i. Find the names and addresses of all employees who work on at least one project located in Houston but whose department has no location in Houston. j. List the last names of all department managers who have no dependents. Consider the AIRLINE relational database schema shown in Figure which was described in Exercise Specify the following queries in relational algebra a. For each flight list the flight number the departure airport for the first leg of the flight and the arrival airport for the last leg of the flight. b. List the flight numbers and weekdays of all flights or flight legs that depart from Houston Intercontinental Airport and arrive in Los Angeles International Airport . c. List the flight number departure airport code scheduled departure time arrival airport code scheduled arrival time and weekdays of all flights or flight legs that depart from some airport in the city of Houston and arrive at some airport in the city of Los Angeles. d. List all fare information for flight number e. Retrieve the number of available seats for flight number on Consider the LIBRARY relational database schema shown in Figure which is used to keep track of books borrowers and book loans. Referential integrity constraints are shown as directed arcs in Figure as in the notation of Figure Write down relational expressions for the following queries a. How many copies of the book titled The Lost Tribe are owned by the library branch whose name is ‘Sharpstown’ b. How many copies of the book titled The Lost Tribe are owned by each library branch c. Retrieve the names of all borrowers who do not have any books checked out. d. For each book that is loaned out from the Sharpstown branch and whose Duedate is today retrieve the book title the borrower’s name and the borrower’s address. e. For each library branch retrieve the branch name and the total number of books loaned out from that branch. Exercises Bookid Title Publishername BOOK BOOKCOPIES Bookid Branchid Noofcopies BOOKAUTHORS Bookid Authorname LIBRARYBRANCH Branchid Branchname Address PUBLISHER Name Address Phone BOOKLOANS Bookid Branchid Cardno Dateout Duedate BORROWER Cardno Name Address Phone Figure A relational database schema for a LIBRARY database. f. Retrieve the names addresses and number of books checked out for all borrowers who have more than five books checked out. g. For each book authored by Stephen King retrieve the title and the number of copies owned by the library branch whose name is Central. Specify the following queries in relational algebra on the database schema given in Exercise a. List the Order# and Shipdate for all orders shipped from Warehouse# b. List the WAREHOUSE information from which the CUSTOMER named Jose Lopez was supplied his orders. Produce a listing Order# Warehouse#. Chapter The Relational Algebra and Relational Calculus PQR ABC a b a b c b Figure TABLE TABLE A database state for the relations and T c. Produce a listing Cname Nooforders Avgorderamt where the middle column is the total number of orders by the customer and the last column is the average order amount for that customer. d. List the orders that were not shipped within days of ordering. e. List the Order# for orders that were shipped from all warehouses that the company has in New York. Specify the following queries in relational algebra on the database schema given in Exercise a. Give the details for trips that exceeded in expenses. b. Print the Ssns of salespeople who took trips to Honolulu. c. Print the total trip expenses incurred by the salesperson with SSN Specify the following queries in relational algebra on the database schema given in Exercise a. List the number of courses taken by all students named John Smith in Winter for courses offered by the ‘CS’ department that have used more than two books. c. List any department that has all its adopted books published by ‘Pearson Publishing’. Consider the two tables and shown in Figure Show the results of the following operations a. b. c. d. e. ∪ f. AND Exercises Specify the following queries in relational algebra on the database schema in Exercise a. For the salesperson named ‘Jane Doe’ list the following information for all the cars she sold Serial# Manufacturer Saleprice. b. List the Serial# and Model of cars that have no options. c. Consider the NATURAL JOIN operation between SALESPERSON and SALE. What is the meaning of a left outer join for these tables Explain with an example. d. Write a query in relational algebra involving selection and one set operation and say in words what the query does. Specify queries a b c e f i and j of Exercise in both tuple and domain relational calculus. Specify queries a b c and d of Exercise in both tuple and domain relational calculus. Specify queries c d and f of Exercise in both tuple and domain relational calculus. In a tuple relational calculus query with n tuple variables what would be the typical minimum number of join conditions Why What is the effect of having a smaller number of join conditions Rewrite the domain relational calculus queries that followed in Section in the style of the abbreviated notation of where the objective is to minimize the number of domain variables by writing constants in place of variables wherever possible. Consider this query Retrieve the Ssns of employees who work on at least those projects on which the employee with works. This may be stated as where x is a tuple variable that ranges over the PROJECT relation. P ≡ EMPLOYEE with works on PROJECT x. Q ≡ EMPLOYEE e works on PROJECT x. Express the query in tuple relational calculus using the rules ≡ NOT . ≡ OR Q . Show how you can specify the following relational algebra operations in both tuple and domain relational calculus. a. σA C b. π A B c. R S d. R ∪ S e. R ∩ S Chapter The Relational Algebra and Relational Calculus f. R S g. R × S h. R ÷ S Suggest extensions to the relational calculus so that it may express the following types of operations that were discussed in Section aggregate functions and grouping OUTER JOIN operations recursive closure queries. A nested query is a query within a query. More specifically a nested query is a parenthesized query whose result can be used as a value in a number of places such as instead of a relation. Specify the following queries on the database specified in Figure using the concept of nested queries and the relational operators discussed in this chapter. Also show the result of each query as it would apply to the database state in Figure a. List the names of all employees who work in the department that has the employee with the highest salary among all employees. b. List the names of all employees whose supervisor’s supervisor has for Ssn. c. List the names of employees who make at least more than the employee who is paid the least in the company. State whether the following conclusions are true or false a. NOT OR Q → AND b. NOT → x c. → x Laboratory Exercises Specify and execute the following queries in relational algebra using the RA interpreter on the COMPANY database schema in Figure a. List the names of all employees in department who work more than hours per week on the ProductX project. b. List the names of all employees who have a dependent with the same first name as themselves. c. List the names of employees who are directly supervised by Franklin Wong. d. List the names of employees who work on every project. e. List the names of employees who do not work on any project. f. List the names and addresses of employees who work on at least one project located in Houston but whose department has no location in Houston. g. List the names of department managers who have no dependents. Laboratory Exercises Consider the following MAILORDER relational schema describing the data for a mail order company. PARTS CUSTOMERS EMPLOYEES ZIPCODES ORDERS ODETAILS Qoh stands for quantity on hand the other attribute names are selfexplanatory. Specify and execute the following queries using the RA interpreter on the MAILORDER database schema. a. Retrieve the names of parts that cost less than b. Retrieve the names and cities of employees who have taken orders for parts costing more than c. Retrieve the pairs of customer number values of customers who live in the same ZIP Code. d. Retrieve the names of customers who have ordered parts from employees living in Wichita. e. Retrieve the names of customers who have ordered parts costing less than f. Retrieve the names of customers who have not placed an order. g. Retrieve the names of customers who have placed exactly two orders. Consider the following GRADEBOOK relational schema describing the data for a grade book of a particular instructor. CATALOG STUDENTS COURSES ENROLLS Specify and execute the following queries using the RA interpreter on the GRADEBOOK database schema. a. Retrieve the names of students enrolled in the Automata class during the fall term. b. Retrieve the Sid values of students who have enrolled in and c. Retrieve the Sid values of students who have enrolled in or d. Retrieve the names of students who have not enrolled in any class. e. Retrieve the names of students who have enrolled in all courses in the CATALOG table. Chapter The Relational Algebra and Relational Calculus Consider a database that consists of the following relations. SUPPLIER PART PROJECT SUPPLY The database records information about suppliers parts and projects and includes a ternary relationship between suppliers parts and projects. This relationship is a many many many relationship. Specify and execute the following queries using the RA interpreter. a. Retrieve the part numbers that are supplied to exactly two projects. b. Retrieve the names of suppliers who supply more than two parts to project c. Retrieve the part numbers that are supplied by every supplier. d. Retrieve the project names that are supplied by supplier only. e. Retrieve the names of suppliers who supply at least two different parts each to at least two different projects. Specify and execute the following queries for the database in Exercise using the RA interpreter. a. Retrieve the names of students who have enrolled in a course that uses a textbook published by Addison Wesley. b. Retrieve the names of courses in which the textbook has been changed at least once. c. Retrieve the names of departments that adopt textbooks published by Addison Wesley only. d. Retrieve the names of departments that adopt textbooks written by Navathe and published by Addison Wesley. e. Retrieve the names of students who have never used a book written by Navathe and published by Addison Wesley. Repeat Laboratory Exercises through in domain relational calculus by using the DRC interpreter. Selected Bibliography Codd defined the basic relational algebra. Date discusses outer joins. Work on extending relational operations is discussed by Carlis and Ozsoyoglu et al. Cammarata et al. extends the relational model integrity constraints and joins. Codd introduced the language Alpha which is based on concepts of tuple relational calculus. Alpha also includes the notion of aggregate functions which goes beyond relational calculus. The original formal definition of relational calculus Selected Bibliography was given by Codd which also provided an algorithm that transforms any tuple relational calculus expression to relational algebra. The QUEL Model Conceptual modeling is a very important phase in designing a successful database application. Generally the term database application refers to a particular database and the associated programs that implement the database queries and updates. For example a BANK database application that keeps track of customer accounts would include programs that implement database updates corresponding to customer deposits and withdrawals. These programs provide user friendly graphical user interfaces utilizing forms and menus for the end users of the application the bank tellers in this example. Hence a major part of the database application will require the design implementation and testing of these application programs. Traditionally the design and testing of application programs has been considered to be part of software engineering rather than database design. In many software design tools the database design methodologies and software engineering methodologies are intertwined since these activities are strongly related. In this chapter we follow the traditional approach of concentrating on the database structures and constraints during conceptual database design. The design of application programs is typically covered in software engineering courses. We present the modeling concepts of the Entity Relationship model which is a popular high level conceptual data model. This model and its variations are frequently used for the conceptual design of database applications and many database design tools employ its concepts. We describe the basic data structuring concepts and constraints of the ER model and discuss their use in the design of conceptual schemas for database applications. We also present the diagrammatic notation associated with the ER model known as ER diagrams. chapter Chapter Data Modeling Using the Entity Relationship Model Object modeling methodologies such as the Unified Modeling Language are becoming increasingly popular in both database and software design. These methodologies go beyond database design to specify detailed design of software modules and their interactions using various types of diagrams. An important part of these methodologies namely class similar in many ways to the ER diagrams. In class diagrams operations on objects are specified in addition to specifying the database schema structure. Operations can be used to specify the functional requirements during database design as we will discuss in Section We present some of the UML notation and concepts for class diagrams that are particularly relevant to database design in Section and briefly compare these to ER notation and concepts. Additional UML notation and concepts are presented in Section and in Chapter This chapter is organized as follows Section discusses the role of high level conceptual data models in database design. We introduce the requirements for a sample database application in Section to illustrate the use of concepts from the ER model. This sample database is also used throughout the book. In Section we present the concepts of entities and attributes and we gradually introduce the diagrammatic technique for displaying an ER schema. In Section we introduce the concepts of binary relationships and their roles and structural constraints. Section introduces weak entity types. Section shows how a schema design is refined to include relationships. Section reviews the notation for ER diagrams summarizes the issues and common pitfalls that occur in schema design and discusses how to choose the names for database schema constructs. Section introduces some UML class diagram concepts compares them to ER model concepts and applies them to the same database example. Section discusses more complex types of relationships. Section summarizes the chapter. The material in Sections and may be excluded from an introductory course. If a more thorough coverage of data modeling concepts and conceptual database design is desired the reader should continue to Chapter where we describe extensions to the ER model that lead to the Enhanced ER model which includes concepts such as specialization generalization inheritance and union types . We also introduce some additional UML concepts and notation in Chapter Using High Level Conceptual Data Models for Database Design Figure shows a simplified overview of the database design process. The first step shown is requirements collection and analysis. During this step the database designers interview prospective database users to understand and document their data requirements. The result of this step is a concisely written set of users’ requirements. These requirements should be specified in as detailed and complete a form as possible. In parallel with specifying the data requirements it is useful to specify class is similar to an entity type in many ways. Using High Level Conceptual Data Models for Database Design Functional Requirements REQUIREMENTS COLLECTION AND ANALYSIS Miniworld Data Requirements CONCEPTUAL DESIGN Conceptual Schema LOGICAL DESIGN Logical Schema PHYSICAL DESIGN Internal Schema Application Programs TRANSACTION I M PLE M E NTATION APPLICATION PROGRAM DESIGN DBMS specific DBMS independent High Level Transaction Specification FUNCTIONAL ANALYSIS Figure A simplified diagram to illustrate the main phases of database design. the known functional requirements of the application. These consist of the userdefined operations that will be applied to the database including both retrievals and updates. In software design it is common to use data flow diagrams sequence diagrams scenarios and other techniques to specify functional requirements. We will not discuss any of these techniques here they are usually described in detail in software engineering texts. We give an overview of some of these techniques in Chapter Once the requirements have been collected and analyzed the next step is to create a conceptual schema for the database using a high level conceptual data model. This step is called conceptual design. The conceptual schema is a concise description of Chapter Data Modeling Using the Entity Relationship Model the data requirements of the users and includes detailed descriptions of the entity types relationships and constraints these are expressed using the concepts provided by the high level data model. Because these concepts do not include implementation details they are usually easier to understand and can be used to communicate with nontechnical users. The high level conceptual schema can also be used as a reference to ensure that all users’ data requirements are met and that the requirements do not conflict. This approach enables database designers to concentrate on specifying the properties of the data without being concerned with storage and implementation details. This makes it is easier to create a good conceptual database design. During or after the conceptual schema design the basic data model operations can be used to specify the high level user queries and operations identified during functional analysis. This also serves to confirm that the conceptual schema meets all the identified functional requirements. Modifications to the conceptual schema can be introduced if some functional requirements cannot be specified using the initial schema. The next step in database design is the actual implementation of the database using a commercial DBMS. Most current commercial DBMSs use an implementation data model such as the relational or the object relational database model so the conceptual schema is transformed from the high level data model into the implementation data model. This step is called logical design or data model mapping its result is a database schema in the implementation data model of the DBMS. Data model mapping is often automated or semiautomated within the database design tools. The last step is the physical design phase during which the internal storage structures file organizations indexes access paths and physical design parameters for the database files are specified. In parallel with these activities application programs are designed and implemented as database transactions corresponding to the highlevel transaction specifications. We discuss the database design process in more detail in Chapter We present only the basic ER model concepts for conceptual schema design in this chapter. Additional modeling concepts are discussed in Chapter when we introduce the EER model. A Sample Database Application In this section we describe a sample database application called COMPANY which serves to illustrate the basic ER model concepts and their use in schema design. We list the data requirements for the database here and then create its conceptual schema step by step as we introduce the modeling concepts of the ER model. The COMPANY database keeps track of a company’s employees departments and projects. Suppose that after the requirements collection and analysis phase the database designers provide the following description of the miniworld the part of the company that will be represented in the database. Entity Types Entity Sets Attributes and Keys The company is organized into departments. Each department has a unique name a unique number and a particular employee who manages the department. We keep track of the start date when that employee began managing the department. A department may have several locations. A department controls a number of projects each of which has a unique name a unique number and a single location. We store each employee’s name Social Security address salary sex and birth date. An employee is assigned to one department but may work on several projects which are not necessarily controlled by the same department. We keep track of the current number of hours per week that an employee works on each project. We also keep track of the direct supervisor of each employee . We want to keep track of the dependents of each employee for insurance purposes. We keep each dependent’s first name sex birth date and relationship to the employee. Figure shows how the schema for this database application can be displayed by means of the graphical notation known as ER diagrams. This figure will be explained gradually as the ER model concepts are presented. We describe the stepby step process of deriving this schema from the stated requirements and explain the ER diagrammatic notation as we introduce the ER model concepts. Entity Types Entity Sets Attributes and Keys The ER model describes data as entities relationships and attributes. In Section we introduce the concepts of entities and their attributes. We discuss entity types and key attributes in Section Then in Section we specify the initial conceptual design of the entity types for the COMPANY database. Relationships are described in Section Entities and Attributes Entities and Their Attributes. The basic object that the ER model represents is an entity which is a thing in the real world with an independent existence. An entity may be an object with a physical existence or it may be an object with a conceptual existence . Each entity has attributes the particular properties that describe it. For example an EMPLOYEE entity may be described by the employee’s name age address salary and job. A particular entity will have a Social Security number or SSN is a unique nine digit identifier assigned to each individual in the United States to keep track of his or her employment benefits and taxes. Other countries may have similar identification schemes such as personal identification card numbers. Chapter Data Modeling Using the Entity Relationship Model EMPLOYEE Fname Minit Lname Name Address Sex Salary Ssn Bdate Supervisor Supervisee SUPERVISION N Hours WORKSON CONTROLS M N DEPENDENTSOF Name Location N PROJECT DEPARTMENT Locations Name Number Number Numberofemployees MANAGES Startdate WORKSFOR N N DEPENDENT Name Sex Birthdate Relationship Figure An ER schema diagram for the COMPANY database. The diagrammatic notation is introduced gradually throughout this chapter and is summarized in Figure value for each of its attributes. The attribute values that describe each entity become a major part of the data stored in the database. Figure shows two entities and the values of their attributes. The EMPLOYEE entity has four attributes Name Address Age and Homephone their values are ‘John Smith ’ Kirby Houston Texas and respectively. The COMPANY entity has three attributes Name Headquarters and President their values are ‘Sunco Oil’ ‘Houston’ and ‘John Smith’ respectively. Several types of attributes occur in the ER model simple versus composite singlevalued versus multivalued and stored versus derived. First we define these attribute Entity Types Entity Sets Attributes and Keys Name John Smith Name Sunco Oil Headquarters Houston President John Smith Address Kirby Houston Texas Age e c Homephone Figure Two entities EMPLOYEE and COMPANY and their attributes. Address Streetaddress City Number Street Apartmentnumber State Zip Figure A hierarchy of composite attributes. types and illustrate their use via examples. Then we discuss the concept of a NULL value for an attribute. Composite versus Simple Attributes. Composite attributes can be divided into smaller subparts which represent more basic attributes with independent meanings. For example the Address attribute of the EMPLOYEE entity shown in Figure can be subdivided into Streetaddress City State and Zip with the values Kirby’ ‘Houston’ ‘Texas’ and Attributes that are not divisible are called simple or atomic attributes. Composite attributes can form a hierarchy for example Streetaddress can be further subdivided into three simple component attributes Number Street and Apartmentnumber as shown in Figure The value of a composite attribute is the concatenation of the values of its component simple attributes. Composite attributes are useful to model situations in which a user sometimes refers to the composite attribute as a unit but at other times refers specifically to its components. If the composite attribute is referenced only as a whole there is no Code is the name used in the United States for a five digit postal code such as which can be extended to nine digits such as We use the five digit Zip in our examples. Chapter Data Modeling Using the Entity Relationship Model need to subdivide it into component attributes. For example if there is no need to refer to the individual components of an address then the whole address can be designated as a simple attribute. Single Valued versus Multivalued Attributes. Most attributes have a single value for a particular entity such attributes are called single valued. For example Age is a single valued attribute of a person. In some cases an attribute can have a set of values for the same entity for instance a Colors attribute for a car or a Collegedegrees attribute for a person. Cars with one color have a single value whereas two tone cars have two color values. Similarly one person may not have a college degree another person may have one and a third person may have two or more degrees therefore different people can have different numbers of values for the Collegedegrees attribute. Such attributes are called multivalued. A multivalued attribute may have lower and upper bounds to constrain the number of values allowed for each individual entity. For example the Colors attribute of a car may be restricted to have between one and three values if we assume that a car can have three colors at most. Stored versus Derived Attributes. In some cases two attribute values are related for example the Age and Birthdate attributes of a person. For a particular person entity the value of Age can be determined from the current date and the value of that person’s Birthdate. The Age attribute is hence called a derived attribute and is said to be derivable from the Birthdate attribute which is called a stored attribute. Some attribute values can be derived from related entities for example an attribute Numberofemployees of a DEPARTMENT entity can be derived by counting the number of employees related to that department. NULL Values. In some cases a particular entity may not have an applicable value for an attribute. For example the Apartmentnumber attribute of an address applies only to addresses that are in apartment buildings and not to other types of residences such as single family homes. Similarly a Collegedegrees attribute applies only to people with college degrees. For such situations a special value called NULL is created. An address of a single family home would have NULL for its Apartmentnumber attribute and a person with no college degree would have NULL for Collegedegrees. NULL can also be used if we do not know the value of an attribute for a particular entity for example if we do not know the home phone number of ‘John Smith’ in Figure The meaning of the former type of NULL is not applicable whereas the meaning of the latter is unknown. The unknown category of NULL can be further classified into two cases. The first case arises when it is known that the attribute value exists but is missing for instance if the Height attribute of a person is listed as NULL. The second case arises when it is not known whether the attribute value exists for example if the Homephone attribute of a person is NULL. Complex Attributes. Notice that in general composite and multivalued attributes can be nested arbitrarily. We can represent arbitrary nesting by grouping com Entity Types Entity Sets Attributes and Keys Addressphone Address City State Zip Figure A complex attribute Addressphone. Entity Type Name Entity Set COMPANY Name Headquarters President EMPLOYEE Name Age Salary Figure Two entity types EMPLOYEE and COMPANY and some member entities of each. ponents of a composite attribute between parentheses and separating the components with commas and by displaying multivalued attributes between braces . Such attributes are called complex attributes. For example if a person can have more than one residence and each residence can have a single address and multiple phones an attribute Addressphone for a person can be specified as shown in Figure Both Phone and Address are themselves composite attributes. Entity Types Entity Sets Keys and Value Sets Entity Types and Entity Sets. A database usually contains groups of entities that are similar. For example a company employing hundreds of employees may want to store similar information concerning each of the employees. These employee entities share the same attributes but each entity has its own value for each attribute. An entity type defines a collection of entities that have the same attributes. Each entity type in the database is described by its name and attributes. Figure shows two entity types EMPLOYEE and COMPANY and a list of some of the attributes for those familiar with XML we should note that complex attributes are similar to complex elements in XML Model Model Make Vehicleid Year Color Registration State Number CAR CAR Ford Mustang convertible red black CAR Nissan Maxima blue Chrysler LeBaron white blue CAR Registration Vehicleid Make Model Year Color Figure The CAR entity type with two key attributes Registration and Vehicleid. ER diagram notation. Entity set with three entities. each. A few individual entities of each type are also illustrated along with the values of their attributes. The collection of all entities of a particular entity type in the database at any point in time is called an entity set the entity set is usually referred to using the same name as the entity type. For example EMPLOYEE refers to both a type of entity as well as the current set of all employee entities in the database. An entity type is represented in ER . Sometimes several attributes together form a key meaning that the combination of the attribute values must be distinct for each entity. If a set of attributes possesses this property the proper way to represent this in the ER model that we describe here is to define a composite attribute and designate it as a key attribute of the entity type. Notice that such a composite key must be minimal that is all component attributes must be included in the composite attribute to have the uniqueness property. Superfluous attributes must not be included in a key. In ER diagrammatic notation each key attribute has its name underlined inside the oval as illustrated in Figure Specifying that an attribute is a key of an entity type means that the preceding uniqueness property must hold for every entity set of the entity type. Hence it is a constraint that prohibits any two entities from having the same value for the key attribute at the same time. It is not the property of a particular entity set rather it is a constraint on any entity set of the entity type at any point in time. This key constraint is derived from the constraints of the miniworld that the database represents. Some entity types have more than one key attribute. For example each of the Vehicleid and Registration attributes of the entity type CAR of Attributes. Each simple attribute of an entity type is associated with a value set which specifies the set of values that may be assigned to that attribute for each individual entity. In Figure if the range of ages allowed for employees is between and we can specify the value set of the Age attribute of EMPLOYEE to be the set of integer numbers between and Similarly we can specify the value set for the Name attribute to be the set of strings of alphabetic characters separated by blank characters and so on. Value sets are not displayed in ER diagrams and are typically specified using the basic data types available in most programming languages such as integer string Boolean float enumerated type subrange and so on. Additional data types to represent common database types such as date time and other concepts are also employed. Chapter Data Modeling Using the Entity Relationship Model Mathematically an attribute A of entity set E whose value set is V can be defined as a function from E to the power P of V A E → P We refer to the value of attribute A for entity e as A. The previous definition covers both single valued and multivalued attributes as well as NULLs. A NULL value is represented by the empty set. For single valued attributes A is restricted to being a singleton set for each entity e in E whereas there is no restriction on multivalued For a composite attribute A the value set V is the power set of the Cartesian product of P where Vn are the value sets of the simple component attributes that form A V P × × × P The value set provides all possible values. Usually only a small number of these values exist in the database at a particular time. Those values represent the data from the current state of the miniworld. They correspond to the data as it actually exists in the miniworld. Initial Conceptual Design of the COMPANY Database We can now define the entity types for the COMPANY database based on the requirements described in Section After defining several entity types and their attributes here we refine our design in Section after we introduce the concept of a relationship. According to the requirements listed in Section we can identify four entity types one corresponding to each of the four items in the specification key attributes because each was specified to be unique. An entity type PROJECT with attributes Name Number Location and Controllingdepartment. Both Name and Number are key attributes. An entity type EMPLOYEE with attributes Name Ssn Sex Address Salary Birthdate Department and Supervisor. Both Name and Address may be composite attributes however this was not specified in the requirements. We must go back to the users to see if any of them will refer to the individual components of Name Firstname Middleinitial Lastname or of Address. An entity type DEPENDENT with attributes Employee Dependentname Sex Birthdate and Relationship . power set P of a set V is the set of all subsets of V. singleton set is a set with only one element . Entity Types Entity Sets Attributes and Keys Address Sex Birthdate Project Hours Workson Fname Minit Lname Department Salary Supervisor Name EMPLOYEE Ssn Sex Relationship Employee Dependentname DEPENDENT Birthdate Location Number Controllingdepartment Name PROJECT Managerstartdate Number DEPARTMENT Manager Name Locations Figure Preliminary design of entity types for the COMPANY database. Some of the shown attributes will be refined into relationships. So far we have not represented the fact that an employee can work on several projects nor have we represented the number of hours per week an employee works on each project. This characteristic is listed as part of the third requirement in Section and it can be represented by a multivalued composite attribute of EMPLOYEE called Workson with the simple components . Alternatively it can be represented as a multivalued composite attribute of PROJECT called Workers with the simple components . We choose the first alternative in Figure which shows each of the entity types just described. The Name attribute of EMPLOYEE is shown as a composite attribute presumably after consultation with the users. Chapter Data Modeling Using the Entity Relationship Model Relationship Types Relationship Sets Roles and Structural Constraints In Figure there are several implicit relationships among the various entity types. In fact whenever an attribute of one entity type refers to another entity type some relationship exists. For example the attribute Manager of DEPARTMENT refers to an employee who manages the department the attribute Controllingdepartment of PROJECT refers to the department that controls the project the attribute Supervisor of EMPLOYEE refers to another employee the attribute Department of EMPLOYEE refers to the department for which the employee works and so on. In the ER model these references should not be represented as attributes but as relationships which are discussed in this section. The COMPANY database schema will be refined in Section to represent relationships explicitly. In the initial design of entity types relationships are typically captured in the form of attributes. As the design is refined these attributes get converted into relationships between entity types. This section is organized as follows Section introduces the concepts of relationship types relationship sets and relationship instances. We define the concepts of relationship degree role names and recursive relationships in Section and then we discuss structural constraints on relationships such as cardinality ratios and existence dependencies in Section Section shows how relationship types can also have attributes. Relationship Types Sets and Instances A relationship type R among n entity types En defines a set of associations or a relationship set among entities from these entity types. As for the case of entity types and entity sets a relationship type and its corresponding relationship set are customarily referred to by the same name R. Mathematically the relationship set R is a set of relationship instances ri where each ri associates n individual entities en and each entity ej in ri is a member of entity set Ej f j f n. Hence a relationship set is a mathematical relation on En alternatively it can be defined as a subset of the Cartesian product of the entity sets × × × En. Each of the entity types E En is said to participate in the relationship type R similarly each of the individual entities en is said to participate in the relationship instance ri en . Informally each relationship instance ri in R is an association of entities where the association includes exactly one entity from each participating entity type. Each such relationship instance ri represents the fact that the entities participating in ri are related in some way in the corresponding miniworld situation. For example consider a relationship type WORKSFOR between the two entity types EMPLOYEE and DEPARTMENT which associates each employee with the department for which the employee works in the corresponding entity set. Each relationship instance in the relationship set WORKSFOR associates one EMPLOYEE entity and one DEPARTMENT entity. Figure illustrates this example where each relationship Relationship Types Relationship Sets Roles and Structural Constraints EMPLOYEE WORKSFOR DEPARTMENT e e d Figure Some instances in the WORKSFOR relationship set which represents a relationship type WORKSFOR between EMPLOYEE and DEPARTMENT. instance ri is shown connected to the EMPLOYEE and DEPARTMENT entities that participate in ri . In the miniworld represented by Figure employees and work for department employees and work for department and employees and work for department In ER diagrams relationship types are displayed as diamond shaped boxes which are connected by straight lines to the rectangular boxes representing the participating entity types. The relationship name is displayed in the diamond shaped box Model SUPPLIER PART SUPPLY PROJECT p r j s Figure Some relationship instances in the SUPPLY ternary relationship set. Relationships as Attributes. It is sometimes convenient to think of a binary relationship type in terms of attributes as we discussed in Section Consider the WORKSFOR relationship type in Figure One can think of an attribute called Department of the EMPLOYEE entity type where the value of Department for each EMPLOYEE entity is the DEPARTMENT entity for which that employee works. Hence the value set for this Department attribute is the set of all DEPARTMENT entities which is the DEPARTMENT entity set. This is what we did in Figure when we specified the initial design of the entity type EMPLOYEE for the COMPANY database. However when we think of a binary relationship as an attribute we always have two options. In this example the alternative is to think of a multivalued attribute Employee of the entity type DEPARTMENT whose values for each DEPARTMENT entity is the set of EMPLOYEE entities who work for that department. The value set of this Employee attribute is the power set of the EMPLOYEE entity set. Either of these two attributes Department of EMPLOYEE or Employee of DEPARTMENT can represent the WORKSFOR relationship type. If both are represented they are constrained to be inverses of each concept of representing relationship types as attributes is used in a class of data models called functional data models. In object databases and once in the role of supervisee . Each relationship instance ri in SUPERVISION associates two employee entities ej and ek one of which plays the role of supervisor and the other the role of supervisee. In Figure the lines marked represent the supervisor role and those marked represent the supervisee role hence supervises and supervises and and supervises and In this example each relationship instance must be connected with two lines one marked with and the other with . Chapter Data Modeling Using the Entity Relationship Model EMPLOYEE MANAGES DEPARTMENT e d r Figure A relationship MANAGES. Constraints on Binary Relationship Types Relationship types usually have certain constraints that limit the possible combinations of entities that may participate in the corresponding relationship set. These constraints are determined from the miniworld situation that the relationships represent. For example in Figure if the company has a rule that each employee must work for exactly one department then we would like to describe this constraint in the schema. We can distinguish two main types of binary relationship constraints cardinality ratio and participation. Cardinality Ratios for Binary Relationships. The cardinality ratio for a binary relationship specifies the maximum number of relationship instances that an entity can participate in. For example in the WORKSFOR binary relationship type DEPARTMENT EMPLOYEE is of cardinality ratio meaning that each department can be related to any number of but an employee can be related to only one department. This means that for this particular relationship WORKSFOR a particular department entity can be related to any number of employees . On the other hand an employee can be related to a maximum of one department. The possible cardinality ratios for binary relationship types are and M N. An example of a binary relationship is MANAGES . Relationship Types Relationship Sets Roles and Structural Constraints EMPLOYEE WORKSON PROJECT e r p Figure An M N relationship WORKSON. world rule is that an employee can work on several projects and a project can have several employees. Cardinality ratios for binary relationships are represented on ER diagrams by displaying M and N on the diamonds as shown in Figure Notice that in this notation we can either specify no maximum or a maximum of one on participation. An alternative notation Model refer to the cardinality ratio and participation constraints taken together as the structural constraints of a relationship type. In ER diagrams total participation is displayed as a double line connecting the participating entity type to the relationship whereas partial participation is represented by a single line or a minimum of one . The alternative notation entity. For a relationship type a relationship attribute can be migrated only to the entity type on the N side of the relationship. For example in Figure if the WORKSFOR relationship also has an attribute Startdate that indicates when an employee started working for a department this attribute can be included as an attribute of EMPLOYEE. This is because each employee works for only one department and hence participates in at most one relationship instance in WORKSFOR. In both and relationship types the decision where to place a relationship attribute as a relationship type attribute or as an attribute of a participating entity type is determined subjectively by the schema designer. For M N relationship types some attributes may be determined by the combination of participating entities in a relationship instance not by any single entity. Such attributes must be specified as relationship attributes. An example is the Hours attribute of the M N relationship WORKSON with respect to its identifying relationship because a weak entity cannot be identified without an owner entity. However not every existence dependency results in a weak entity type. For example a DRIVERLICENSE entity cannot exist unless it is related to a PERSON entity even though it has its own key and hence is not a weak entity. Consider the entity type DEPENDENT related to EMPLOYEE which is used to keep track of the dependents of each employee via a relationship Birthdate Sex and Relationship . Two dependents of two distinct employees may by chance have the same values for Name Birthdate Sex and Relationship but they are still distinct entities. They are identified as distinct entities only after determining the particular employee entity to which each dependent is related. Each employee entity is said to own the dependent entities that are related to it. A weak entity type normally has a partial key which is the attribute that can uniquely identify weak entities that are related to the same owner entity. In our example if we assume that no two dependents of the same employee ever have the same first name the attribute Name of DEPENDENT is the partial key. In the worst case a composite attribute of all the weak entity’s attributes will be the partial key. In ER diagrams both a weak entity type and its identifying relationship are distinguished by surrounding their boxes and diamonds with double lines attributes. In the preceding example we could specify a multivalued attribute Dependents for EMPLOYEE which is a composite attribute with component attributes Name Birthdate Sex and Relationship. The choice of which representation to use is made by the database designer. One criterion that may be used is to choose the identifying entity type is also sometimes called the parent entity type or the dominant entity type. weak entity type is also sometimes called the child entity type or the subordinate entity type. partial key is sometimes called the discriminator. Chapter Data Modeling Using the Entity Relationship Model weak entity type representation if there are many attributes. If the weak entity participates independently in relationship types other than its identifying relationship type then it should not be modeled as a complex attribute. In general any number of levels of weak entity types can be defined an owner entity type may itself be a weak entity type. In addition a weak entity type may have more than one identifying entity type and an identifying relationship type of degree higher than two as we illustrate in Section Refining the ER Design for the COMPANY Database We can now refine the database design in Figure by changing the attributes that represent relationships into relationship types. The cardinality ratio and participation constraint of each relationship type are determined from the requirements listed in Section If some cardinality ratio or dependency cannot be determined from the requirements the users must be questioned further to determine these structural constraints. In our example we specify the following relationship types MANAGES a relationship type between EMPLOYEE and DEPARTMENT. EMPLOYEE participation is partial. DEPARTMENT participation is not clear from the requirements. We question the users who say that a department must have a manager at all times which implies total The attribute Startdate is assigned to this relationship type. WORKSFOR a relationship type between DEPARTMENT and EMPLOYEE. Both participations are total. CONTROLS a relationship type between DEPARTMENT and PROJECT. The participation of PROJECT is total whereas that of DEPARTMENT is determined to be partial after consultation with the users indicates that some departments may control no projects. SUPERVISION a relationship type between EMPLOYEE and EMPLOYEE . Both participations are determined to be partial after the users indicate that not every employee is a supervisor and not every employee has a supervisor. WORKSON determined to be an M N relationship type with attribute Hours after the users indicate that a project can have several employees working on it. Both participations are determined to be total. DEPENDENTSOF a relationship type between EMPLOYEE and DEPENDENT which is also the identifying relationship for the weak entity rules in the miniworld that determine the constraints are sometimes called the business rules since they are determined by the business or organization that will utilize the database. ER Diagrams Naming Conventions and Design Issues type DEPENDENT. The participation of EMPLOYEE is partial whereas that of DEPENDENT is total. After specifying the above six relationship types we remove from the entity types in Figure all attributes that have been refined into relationships. These include Manager and Managerstartdate from DEPARTMENT Controllingdepartment from PROJECT Department Supervisor and Workson from EMPLOYEE and Employee from DEPENDENT. It is important to have the least possible redundancy when we design the conceptual schema of a database. If some redundancy is desired at the storage level or at the user view level it can be introduced later as discussed in Section ER Diagrams Naming Conventions and Design Issues Summary of Notation for ER Diagrams Figures through illustrate examples of the participation of entity types in relationship types by displaying their sets or extensions the individual entity instances in an entity set and the individual relationship instances in a relationship set. In ER diagrams the emphasis is on representing the schemas rather than the instances. This is more useful in database design because a database schema changes rarely whereas the contents of the entity sets change frequently. In addition the schema is obviously easier to display because it is much smaller. Figure displays the COMPANY ER database schema as an ER diagram. We now review the full ER diagram notation. Entity types such as EMPLOYEE DEPARTMENT and PROJECT are shown in rectangular boxes. Relationship types such as WORKSFOR MANAGES CONTROLS and WORKSON are shown in diamond shaped boxes attached to the participating entity types with straight lines. Attributes are shown in ovals and each attribute is attached by a straight line to its entity type or relationship type. Component attributes of a composite attribute are attached to the oval representing the composite attribute as illustrated by the Name attribute of EMPLOYEE. Multivalued attributes are shown in double ovals as illustrated by the Locations attribute of DEPARTMENT. Key attributes have their names underlined. Derived attributes are shown in dotted ovals as illustrated by the Numberofemployees attribute of DEPARTMENT. Weak entity types are distinguished by being placed in double rectangles and by having their identifying relationship placed in double diamonds as illustrated by the DEPENDENT entity type and the DEPENDENTSOF identifying relationship type. The partial key of the weak entity type is underlined with a dotted line. In Figure the cardinality ratio of each binary relationship type is specified by attaching a M or N on each participating edge. The cardinality ratio of DEPARTMENT EMPLOYEE in MANAGES is whereas it is for DEPARTMENT EMPLOYEE in WORKSFOR and M N for WORKSON. The participation Chapter Data Modeling Using the Entity Relationship Model constraint is specified by a single line for partial participation and by double lines for total participation . In Figure we show the role names for the SUPERVISION relationship type because the same EMPLOYEE entity type plays two distinct roles in that relationship. Notice that the cardinality ratio is from supervisor to supervisee because each employee in the role of supervisee has at most one direct supervisor whereas an employee in the role of supervisor can supervise zero or more employees. Figure summarizes the conventions for ER diagrams. It is important to note that there are many other alternative diagrammatic notations . Proper Naming of Schema Constructs When designing a database schema the choice of names for entity types attributes relationship types and roles is not always straightforward. One should choose names that convey as much as possible the meanings attached to the different constructs in the schema. We choose to use singular names for entity types rather than plural ones because the entity type name applies to each individual entity belonging to that entity type. In our ER diagrams we will use the convention that entity type and relationship type names are uppercase letters attribute names have their initial letter capitalized and role names are lowercase letters. We have used this convention in Figure As a general practice given a narrative description of the database requirements the nouns appearing in the narrative tend to give rise to entity type names and the verbs tend to indicate names of relationship types. Attribute names generally arise from additional nouns that describe the nouns corresponding to entity types. Another naming consideration involves choosing binary relationship names to make the ER diagram of the schema readable from left to right and from top to bottom. We have generally followed this guideline in Figure To explain this naming convention further we have one exception to the convention in Figure DEPENDENTSOF relationship type which reads from bottom to top. When we describe this relationship we can say that the DEPENDENT entities are DEPENDENTSOF an EMPLOYEE . To change this to read from top to bottom we could rename the relationship type to HASDEPENDENTS which would then read as follows An EMPLOYEE entity HASDEPENDENTS of type DEPENDENT . Notice that this issue arises because each binary relationship can be described starting from either of the two participating entity types as discussed in the beginning of Section Design Choices for ER Conceptual Design It is occasionally difficult to decide whether a particular concept in the miniworld should be modeled as an entity type an attribute or a relationship type. In this ER Diagrams Naming Conventions and Design Issues Symbol Meaning Entity Weak Entity Indentifying Relationship Relationship Composite Attribute . . . Key Attribute Attribute Derived Attribute Multivalued Attribute E R E Total Participation of E in R E R E Cardinality Ratio N for E E in R N Structural Constraint on Participation of E in R R E Figure Summary of the notation for ER diagrams. Chapter Data Modeling Using the Entity Relationship Model section we give some brief guidelines as to which construct should be chosen in particular situations. In general the schema design process should be considered an iterative refinement process where an initial design is created and then iteratively refined until the most suitable design is reached. Some of the refinements that are often used include the following A concept may be first modeled as an attribute and then refined into a relationship because it is determined that the attribute is a reference to another entity type. It is often the case that a pair of such attributes that are inverses of one another are refined into a binary relationship. We discussed this type of refinement in detail in Section It is important to note that in our notation once an attribute is replaced by a relationship the attribute itself should be removed from the entity type to avoid duplication and redundancy. Similarly an attribute that exists in several entity types may be elevated or promoted to an independent entity type. For example suppose that several entity types in a UNIVERSITY database such as STUDENT INSTRUCTOR and COURSE each has an attribute Department in the initial design the designer may then choose to create an entity type DEPARTMENT with a single attribute Deptname and relate it to the three entity types via appropriate relationships. Other attributes relationships of DEPARTMENT may be discovered later. An inverse refinement to the previous case may be applied for example if an entity type DEPARTMENT exists in the initial design with a single attribute Deptname and is related to only one other entity type STUDENT. In this case DEPARTMENT may be reduced or demoted to an attribute of STUDENT. Section discusses choices concerning the degree of a relationship. In Chapter we discuss other refinements concerning specialization generalization. Chapter discusses additional top down and bottom up refinements that are common in large scale conceptual schema design. Alternative Notations for ER Diagrams There are many alternative diagrammatic notations for displaying ER diagrams. Appendix A gives some of the more popular notations. In Section we introduce the Unified Modeling Language notation for class diagrams which has been proposed as a standard for conceptual object modeling. In this section we describe one alternative ER notation for specifying structural constraints on relationships which replaces the cardinality ratio M N and single double line notation for participation constraints. This notation involves associating a pair of integer numbers with each participation of an entity type E in a relationship type R where ≤ min ≤ max and max ≥ The numbers mean that for each entity e in E e must participate in at least min and at most ER Diagrams Naming Conventions and Design Issues EMPLOYEE Minit Lname Name Address Sex Salary Ssn Bdate Supervisor Employee Department Managed Department Supervisee SUPERVISION Hours WORKSON CONTROLS DEPENDENTSOF Name Location PROJECT DEPARTMENT Locations Name Number Number Numberofemployees MANAGES Startdate WORKSFOR DEPENDENT Name Sex Birthdate Relationship Controlling Department Controlled Project Project Worker Employee Dependent Fname Figure ER diagrams for the company schema with structural constraints specified using notation and role names. some notations particularly those used in object modeling methodologies such as UML the is placed on the opposite sides to the ones we have shown. For example for the WORKSFOR relationship in Figure the would be on the DEPARTMENT side and the would be on the EMPLOYEE side. Here we used the original notation from Abrial max relationship instances in R at any point in time. In this method min implies partial participation whereas min implies total participation. Figure displays the COMPANY database schema using the Usually one uses either the cardinality ratio single line double line notation or the notation. The Chapter Data Modeling Using the Entity Relationship Model supervisee Name Namedom Fname Minit Lname Ssn Bdate Date Sex M F Address Salary age changedepartment changeprojects . . . Sex M F Birthdate Date Relationship DEPENDENT . . . supervisor Dependentname EMPLOYEE Name Number addemployee numberofemployees changemanager . . . DEPARTMENT Name Number addemployee addproject changemanager . . . PROJECT Startdate MANAGES CONTROLS Hours WORKSON Name LOCATION Multiplicity Notation in OMT Aggregation Notation in UML Whole Part WORKSFOR Figure The COMPANY conceptual schema in UML class diagram notation. notation is more precise and we can use it to specify some structural constraints for relationship types of higher degree. However it is not sufficient for specifying some key constraints on higher degree relationships as discussed in Section Figure also displays all the role names for the COMPANY database schema. Example of Other Notation UML Class Diagrams The UML methodology is being used extensively in software design and has many types of diagrams for various software design purposes. We only briefly present the basics of UML class diagrams here and compare them with ER diagrams. In some ways class diagrams can be considered as an alternative notation to ER diagrams. Additional UML notation and concepts are presented in Section and in Chapter Figure shows how the COMPANY ER database schema in Figure can be displayed using UML class diagram notation. The entity types in Figure are modeled as classes in Figure An entity in ER corresponds to an object in UML. Example of Other Notation UML Class Diagrams In UML class diagrams a class is displayed as a box the middle section includes the attributes and the last section includes operations that can be applied to individual objects of the class. Operations are not specified in ER diagrams. Consider the EMPLOYEE class in Figure Its attributes are Name Ssn Bdate Sex Address and Salary. The designer can optionally specify the domain of an attribute if desired by placing a colon followed by the domain name or description as illustrated by the Name Sex and Bdate attributes of EMPLOYEE in Figure A composite attribute is modeled as a structured domain as illustrated by the Name attribute of EMPLOYEE. A multivalued attribute will generally be modeled as a separate class as illustrated by the LOCATION class in Figure Relationship types are called associations in UML terminology and relationship instances are called links. A binary association is represented as a line connecting the participating classes and may optionally have a name. A relationship attribute called a link attribute is placed in a box that is connected to the association’s line by a dashed line. The notation described in Section is used to specify relationship constraints which are called multiplicities in UML terminology. Multiplicities are specified in the form and an asterisk indicates no maximum limit on participation. However the multiplicities are placed on the opposite ends of the relationship when compared with the notation discussed in Section . In the unidirectional case the line connecting the classes is displayed with an arrow to indicate that only one direction for accessing related objects is needed. If no arrow is displayed the bidirectional case is assumed which is the default. For example if we always expect to access the manager of a department starting from a DEPARTMENT object we would draw the association line representing the MANAGES association with an arrow from DEPARTMENT to EMPLOYEE. In addition relationship instances may be specified to be ordered. For example we could specify that the employee objects related to each department through the WORKSFOR association should be ordered by their Salary attribute Chapter Data Modeling Using the Entity Relationship Model value. Association names are optional in UML and relationship attributes are displayed in a box attached with a dashed line to the line representing the association aggregation for each operation plus a functional description of each operation. UML has function descriptions and sequence diagrams to specify some of the operation details but these are beyond the scope of our discussion. Chapter will introduce some of these diagrams. Weak entities can be modeled using the construct called qualified association in UML this can represent both the identifying relationship and the partial key which is placed in a box attached to the owner class. This is illustrated by the DEPENDENT class and its qualified aggregation to EMPLOYEE in Figure The partial key Dependentname is called the discriminatorin UML terminology since its value distinguishes the objects associated with the same EMPLOYEE. Qualified associations are not restricted to modeling weak entities and they can be used to model other situations in UML. This section is not meant to be a complete description of UML class diagrams but rather to illustrate one popular type of alternative diagrammatic notation that can be used for representing ER modeling concepts. Relationship Types of Degree Higher than Two In Section we defined the degree of a relationship type as the number of participating entity types and called a relationship type of degree two binary and a relationship type of degree three ternary. In this section we elaborate on the differences between binary and higher degree relationships when to choose higher degree versus binary relationships and how to specify constraints on higher degree relationships. Choosing between Binary and Ternary Relationships The ER diagram notation for a ternary relationship type is shown in Figure which displays the schema for the SUPPLY relationship type that was displayed at the entity set relationship set or instance level in Figure Recall that the relationship set of SUPPLY is a set of relationship instances where s is a SUPPLIER who is currently supplying a PART p to a PROJECT j. In general a relationship type R of degree n will have n edges in an ER diagram one connecting R to each participating entity type. Relationship Types of Degree Higher than Two SUPPLY Sname Partno SUPPLIER Quantity PROJECT PART Projname Partno PART N Sname SUPPLIER Projname PROJECT N Quantity SUPPLY N Partno M N CANSUPPLY N M Sname SUPPLIER Projname PROJECT USES PART M N SUPPLIES SP SS SPJ Figure Ternary relationship types. The SUPPLY relationship. Three binary relationships not equivalent to SUPPLY. SUPPLY represented as a weak entity type. Figure shows an ER diagram for three binary relationship types CANSUPPLY USES and SUPPLIES. In general a ternary relationship type represents different information than do three binary relationship types. Consider the three binary relationship types CANSUPPLY USES and SUPPLIES. Suppose that CANSUPPLY between SUPPLIER and PART includes an instance whenever supplier s can supply part p USES between PROJECT and PART includes an instance whenever project j uses part p and SUPPLIES between SUPPLIER and PROJECT includes an instance whenever supplier s supplies Chapter Data Modeling Using the Entity Relationship Model some part to project j. The existence of three relationship instances and in CANSUPPLY USES and SUPPLIES respectively does not necessarily imply that an instance exists in the ternary relationship SUPPLY because the meaning is different. It is often tricky to decide whether a particular relationship should be represented as a relationship type of degree n or should be broken down into several relationship types of smaller degrees. The designer must base this decision on the semantics or meaning of the particular situation being represented. The typical solution is to include the ternary relationship plus one or more of the binary relationships if they represent different meanings and if all are needed by the application. Some database design tools are based on variations of the ER model that permit only binary relationships. In this case a ternary relationship such as SUPPLY must be represented as a weak entity type with no partial key and with three identifying relationships. The three participating entity types SUPPLIER PART and PROJECT are together the owner entity types whenever INSTRUCTOR i offers COURSE c during SEMESTER s. The three binary relationship types shown in Figure have the following meanings CANTEACH relates a course to the instructors who can teach that course TAUGHTDURING relates a semester to the instructors who taught some course during that semester and OFFEREDDURING Cnumber CANTEACH Lname INSTRUCTOR Semyear Semester Year SEMESTER OFFEREDDURING COURSE OFFERS TAUGHTDURING Figure Another example of ternary versus binary relationship types. Relationship Types of Degree Higher than Two Deptdate Department Date RESULTSIN Name CANDIDATE Cname COMPANY INTERVIEW JOBOFFER CCI Figure A weak entity type INTERVIEW with a ternary identifying relationship type. relates a semester to the courses offered during that semester by any instructor. These ternary and binary relationships represent different information but certain constraints should hold among the relationships. For example a relationship instance should not exist in OFFERS unless an instance exists in TAUGHTDURING an instance exists in OFFEREDDURING and an instance exists in CANTEACH. However the reverse is not always true we may have instances and in the three binary relationship types with no corresponding instance in OFFERS. Note that in this example based on the meanings of the relationships we can infer the instances of TAUGHTDURING and OFFEREDDURING from the instances in OFFERS but we cannot infer the instances of CANTEACH therefore TAUGHTDURING and OFFEREDDURING are redundant and can be left out. Although in general three binary relationships cannot replace a ternary relationship they may do so under certain additional constraints. In our example if the CANTEACH relationship is then the ternary relationship OFFERS can be left out because it can be inferred from the three binary relationships CANTEACH TAUGHTDURING and OFFEREDDURING. The schema designer must analyze the meaning of each specific situation to decide which of the binary and ternary relationship types are needed. Notice that it is possible to have a weak entity type with a ternary identifying relationship type. In this case the weak entity type can have several owner entity types. An example is shown in Figure This example shows part of a database that keeps track of candidates interviewing for jobs at various companies and may be part of an employment agency database for example. In the requirements a candidate can have multiple interviews with the same company but a job offer is made based on one of the interviews. Here INTERVIEW is represented as a weak entity with two owners CANDIDATE and COMPANY and with the partial key Deptdate. An INTERVIEW entity is uniquely identified by a candidate a company and the combination of the date and department of the interview. Chapter Data Modeling Using the Entity Relationship Model Constraints on Ternary Relationships There are two notations for specifying structural constraints on n ary relationships and they specify different constraints. They should thus both be used if it is important to fully specify the structural constraints on a ternary or higher degree relationship. The first notation is based on the cardinality ratio notation of binary relationships displayed in Figure Here a M or N is specified on each participation arc where s is a SUPPLIER j is a PROJECT and p is a PART. Suppose that the constraint exists that for a particular project part combination only one supplier will be used . In this case we place on the SUPPLIER participation and M N on the PROJECT PART participations in Figure This specifies the constraint that a particular combination can appear at most once in the relationship set because each such combination uniquely determines a single supplier. Hence any relationship instance is uniquely identified in the relationship set by its combination which makes a key for the relationship set. In this notation the participations that have a specified on them are not required to be part of the identifying key for the relationship If all three cardinalities are M or N then the key will be the combination of all three participants. The second notation is based on the notation displayed in Figure for binary relationships. A on a participation here specifies that each entity is related to at least min and at most max relationship instances in the relationship set. These constraints have no bearing on determining the key of an n ary relationship where n but specify a different type of constraint that places restrictions on how many relationship instances each entity can participate in. Summary In this chapter we presented the modeling concepts of a high level conceptual data model the Entity Relationship model. We started by discussing the role that a high level data model plays in the database design process and then we presented a sample set of database requirements for the COMPANY database which is one of the examples that is used throughout this book. We defined the basic ER model concepts of entities and their attributes. Then we discussed NULL values and presented notation allows us to determine the key of the relationship relation as we discuss in Chapter is also true for cardinality ratios of binary relationships. constraints can determine the keys for binary relationships though. Summary the various types of attributes which can be nested arbitrarily to produce complex attributes Simple or atomic Composite Multivalued We also briefly discussed stored versus derived attributes. Then we discussed the ER model concepts at the schema or “intension” level Entity types and their corresponding entity sets Key attributes of entity types Value sets of attributes Relationship types and their corresponding relationship sets Participation roles of entity types in relationship types We presented two methods for specifying the structural constraints on relationship types. The first method distinguished two types of structural constraints Cardinality ratios M N for binary relationships Participation constraints We noted that alternatively another method of specifying structural constraints is to specify minimum and maximum numbers on the participation of each entity type in a relationship type. We discussed weak entity types and the related concepts of owner entity types identifying relationship types and partial key attributes. Entity Relationship schemas can be represented diagrammatically as ER diagrams. We showed how to design an ER schema for the COMPANY database by first defining the entity types and their attributes and then refining the design to include relationship types. We displayed the ER diagram for the COMPANY database schema. We discussed some of the basic concepts of UML class diagrams and how they relate to ER modeling concepts. We also described ternary and higher degree relationship types in more detail and discussed the circumstances under which they are distinguished from binary relationships. The ER modeling concepts we have presented thus far entity types relationship types attributes keys and structural constraints can model many database applications. However more complex applications such as engineering design medical information systems and telecommunications require additional concepts if we want to model them with greater accuracy. We discuss some advanced modeling concepts in Chapter and revisit further advanced data modeling techniques in Chapter Chapter Data Modeling Using the Entity Relationship Model Review Questions Discuss the role of a high level data model in the database design process. List the various cases where use of a NULL value would be appropriate. Define the following terms entity attribute attribute value relationship instance composite attribute multivalued attribute derived attribute complex attribute key attribute and value set . What is an entity type What is an entity set Explain the differences among an entity an entity type and an entity set. Explain the difference between an attribute and a value set. What is a relationship type Explain the differences among a relationship instance a relationship type and a relationship set. What is a participation role When is it necessary to use role names in the description of relationship types Describe the two alternatives for specifying structural constraints on relationship types. What are the advantages and disadvantages of each Under what conditions can an attribute of a binary relationship type be migrated to become an attribute of one of the participating entity types When we think of relationships as attributes what are the value sets of these attributes What class of data models is based on this concept What is meant by a recursive relationship type Give some examples of recursive relationship types. When is the concept of a weak entity used in data modeling Define the terms owner entity type weak entity type identifying relationship type and partial key. Can an identifying relationship of a weak entity type be of a degree greater than two Give examples to illustrate your answer. Discuss the conventions for displaying an ER schema as an ER diagram. Discuss the naming conventions used for ER schema diagrams. Exercises Consider the following set of requirements for a UNIVERSITY database that is used to keep track of students’ transcripts. This is similar but not identical to the database shown in Figure a. The university keeps track of each student’s name student number Social Security number current address and phone number permanent address and phone number birth date sex class major department minor department and degree program Exercises . Some user applications need to refer to the city state and ZIP Code of the student’s permanent address and to the student’s last name. Both Social Security number and student number have unique values for each student. b. Each department is described by a name department code office number office phone number and college. Both name and code have unique values for each department. c. Each course has a course name description course number number of semester hours level and offering department. The value of the course number is unique for each course. d. Each section has an instructor semester year course and section number. The section number distinguishes sections of the same course that are taught during the same semester year its values are up to the number of sections taught during each semester. e. A grade report has a student section letter grade and numeric grade or Design an ER schema for this application and draw an ER diagram for the schema. Specify key attributes of each entity type and structural constraints on each relationship type. Note any unspecified requirements and make appropriate assumptions to make the specification complete. Composite and multivalued attributes can be nested to any number of levels. Suppose we want to design an attribute for a STUDENT entity type to keep track of previous college education. Such an attribute will have one entry for each college previously attended and each such entry will be composed of college name start and end dates degree entries and transcript entries . Each degree entry contains the degree name and the month and year the degree was awarded and each transcript entry contains a course name semester year and grade. Design an attribute to hold this information. Use the conventions in Figure Show an alternative design for the attribute described in Exercise that uses only entity types and relationship types. Consider the ER diagram in Figure which shows a simplified schema for an airline reservations system. Extract from the ER diagram the requirements and constraints that produced this schema. Try to be as precise as possible in your requirements and constraints specification. In Chapters and we discussed the database environment and database users. We can consider many entity types to describe such an environment such as DBMS stored database DBA and catalog data dictionary. Try to specify all the entity types that can fully describe a database system and its environment then specify the relationship types among them and draw an ER diagram to describe such a general database environment. Chapter Data Modeling Using the Entity Relationship Model Restrictions M N N N N N AIRPORT City State AIRPLANE TYPE Deptime Arrtime Name Scheduleddeptime INSTANCEOF Weekdays Airline Instances N N Airportcode Number Scheduledarrtime CAN LAND TYPE N DEPARTS N ARRIVES N ASSIGNED ARRIVAL AIRPORT DEPARTURE AIRPORT N SEAT Typename Maxseats Code AIRPLANE Airplaneid Totalnoofseats LEGS FLIGHT FLIGHTLEG Le gno FARES FARE Amount Customername Cphone Date Noofavailseats RESERVATION Seatno Company LEGINSTANCE Notes A LEG is a nonstop portion of a flight. A LEGINSTANCE is a particular occurrence of a LEG on a particular date. Figure An ER diagram for an AIRLINE database schema. Design an ER schema for keeping track of information about votes taken in the . House of Representatives during the current two year congressional session. The database needs to keep track of each . STATE’s Name and include the Region of the state . Each Exercises CONGRESSPERSON in the House of Representatives is described by his or her Name plus the District represented the Startdate when the congressperson was first elected and the political Party to which he or she belongs . The database keeps track of each BILL including the Billname the Dateofvote on the bill whether the bill Passedorfailed and the Sponsor who sponsored that is proposed the bill . The database also keeps track of how each congressperson voted on each bill . Draw an ER schema diagram for this application. State clearly any assumptions you make. A database is being constructed to keep track of the teams and games of a sports league. A team has a number of players not all of whom participate in each game. It is desired to keep track of the players participating in each game for each team the positions they played in that game and the result of the game. Design an ER schema diagram for this application stating any assumptions you make. Choose your favorite sport . Consider the ER diagram shown in Figure for part of a BANK database. Each bank can have multiple branches and each branch can have multiple accounts and loans. a. List the strong entity types in the ER diagram. b. Is there a weak entity type If so give its name partial key and identifying relationship. c. What constraints do the partial key and the identifying relationship of the weak entity type specify in this diagram d. List the names of all relationship types and specify the constraint on each participation of an entity type in a relationship type. Justify your choices. e. List concisely the user requirements that led to this ER schema design. f. Suppose that every customer must have at least one account but is restricted to at most two loans at a time and that a bank branch cannot have more than loans. How does this show up on the constraints Consider the ER diagram in Figure Assume that an employee may work in up to two departments or may not be assigned to any department. Assume that each department must have one and may have up to three phone numbers. Supply constraints on this diagram. State clearly any additional assumptions you make. Under what conditions would the relationship HASPHONE be redundant in this example Consider the ER diagram in Figure Assume that a course may or may not use a textbook but that a text by definition is a book that is used in some course. A course may not use more than five books. Instructors teach from Chapter Data Modeling Using the Entity Relationship Model BANK LOAN Balance Type Loanno Amount N N N N M M Code Name N BANKBRANCH AC LC ACCTS LOANS BRANCHES ACCOUNT CUSTOMER Acctno Name Phone Addr Type Addr Addr Branchno Ssn Figure An ER diagram for a BANK database schema. EMPLOYEE DEPARTMENT HASPHONE CONTAI NS WORKSIN PHONE Figure Part of an ER diagram for a COMPANY database. Exercises two to four courses. Supply constraints on this diagram. State clearly any additional assumptions you make. If we add the relationship ADOPTS to indicate the textbook that an instructor uses for a course should it be a binary relationship between INSTRUCTOR and TEXT or a ternary relationship between all three entity types What constraints would you put on it Why Consider an entity type SECTION in a UNIVERSITY database which describes the section offerings of courses. The attributes of SECTION are Sectionnumber Semester Year Coursenumber Instructor Roomno Building Weekdays and Hours . Assume that Sectionnumber is unique for each course within a particular semester year combination . There are several composite keys for section and some attributes are components of more than one key. Identify three composite keys and show how they can be represented in an ER schema diagram. Cardinality ratios often dictate the detailed design of a database. The cardinality ratio depends on the real world meaning of the entity types involved and is defined by the specific application. For the following binary relationships suggest cardinality ratios based on the common sense meaning of the entity types. Clearly state any assumptions you make. Entity Cardinality Ratio Entity STUDENT  SOCIALSECURITYCARD STUDENT  TEACHER CLASSROOM  WALL INSTRUCTOR COURSE USES TEACHES TEXT Figure Part of an ER diagram for a COURSES database. Chapter Data Modeling Using the Entity Relationship Model COUNTRY  CURRENTPRESIDENT COURSE  TEXTBOOK ITEM  ORDER STUDENT  CLASS CLASS  INSTRUCTOR INSTRUCTOR  OFFICE EBAYAUCTION ITEM  EBAYBID Consider the ER schema for the MOVIES database in Figure Assume that MOVIES is a populated database. ACTOR is used as a generic term and includes actresses. Given the constraints shown in the ER schema respond to the following statements with True False or Maybe. Assign a response of Maybe to statements that while not explicitly shown to be True cannot be proven False based on the schema as shown. Justify each answer. ACTOR MOVIE LEADROLE PERFORMSIN DIRECTOR DIRECTS ALSOA DIRECTOR PRODUCER PRODUCES ACTOR PRODUCER M M N N N N Figure An ER diagram for a MOVIES database schema. Laboratory Exercises a. There are no actors in this database that have been in no movies. b. There are some actors who have acted in more than ten movies. c. Some actors have done a lead role in multiple movies. d. A movie can have only a maximum of two lead actors. e. Every director has been an actor in some movie. f. No producer has ever been an actor. g. A producer cannot be an actor in some other movie. h. There are movies with more than a dozen actors. i. Some producers have been a director as well. j. Most movies have one director and one producer. k. Some movies have one director but several producers. l. There are some actors who have done a lead role directed a movie and produced some movie. m. No movie has a director who also acted in that movie. Given the ER schema for the MOVIES database in Figure draw an instance diagram using three movies that have been released recently. Draw instances of each entity type MOVIES ACTORS PRODUCERS DIRECTORS involved make up instances of the relationships as they exist in reality for those movies. Illustrate the UML Diagram for Exercise Your UML design should observe the following requirements a. A student should have the ability to compute his her GPA and add or drop majors and minors. b. Each department should be to able add or delete courses and hire or terminate faculty. c. Each instructor should be able to assign or change a student’s grade for a course. Note Some of these functions may be spread over multiple classes. Laboratory Exercises Consider the UNIVERSITY database described in Exercise Build the ER schema for this database using a data modeling tool such as ERwin or Rational Rose. Consider a MAILORDER database in which employees take orders for parts from customers. The data requirements are summarized as follows The mail order company has employees each identified by a unique employee number first and last name and Zip Code. Each customer of the company is identified by a unique customer number first and last name and Zip Code. Chapter Data Modeling Using the Entity Relationship Model Each part sold by the company is identified by a unique part number a part name price and quantity in stock. Each order placed by a customer is taken by an employee and is given a unique order number. Each order contains specified quantities of one or more parts. Each order has a date of receipt as well as an expected ship date. The actual ship date is also recorded. Design an Entity Relationship diagram for the mail order database and build the design using a data modeling tool such as ERwin or Rational Rose. Consider a MOVIE database in which data is recorded about the movie industry. The data requirements are summarized as follows Each movie is identified by title and year of release. Each movie has a length in minutes. Each has a production company and each is classified under one or more genres . Each movie has one or more directors and one or more actors appear in it. Each movie also has a plot outline. Finally each movie has zero or more quotable quotes each of which is spoken by a particular actor appearing in the movie. Actors are identified by name and date of birth and appear in one or more movies. Each actor has a role in the movie. Directors are also identified by name and date of birth and direct one or more movies. It is possible for a director to act in a movie . Production companies are identified by name and each has an address. A production company produces one or more movies. Design an Entity Relationship diagram for the movie database and enter the design using a data modeling tool such as ERwin or Rational Rose. Consider a CONFERENCEREVIEW database in which researchers submit their research papers for consideration. Reviews by reviewers are recorded for use in the paper selection process. The database system caters primarily to reviewers who record answers to evaluation questions for each paper they review and make recommendations regarding whether to accept or reject the paper. The data requirements are summarized as follows Authors of papers are uniquely identified by e mail id. First and last names are also recorded. Each paper is assigned a unique identifier by the system and is described by a title abstract and the name of the electronic file containing the paper. A paper may have multiple authors but one of the authors is designated as the contact author. Reviewers of papers are uniquely identified by e mail address. Each reviewer’s first name last name phone number affiliation and topics of interest are also recorded. Selected Bibliography Each paper is assigned between two and four reviewers. A reviewer rates each paper assigned to him or her on a scale of to in four categories technical merit readability originality and relevance to the conference. Finally each reviewer provides an overall recommendation regarding each paper. Each review contains two types of written comments one to be seen by the review committee only and the other as feedback to the author. Design an Entity Relationship diagram for the CONFERENCEREVIEW database and build the design using a data modeling tool such as ERwin or Rational Rose. Consider the ER diagram for the AIRLINE database shown in Figure Build this design using a data modeling tool such as ERwin or Rational Rose. Selected Bibliography The Entity Relationship model was introduced by Chen and related work appears in Schmidt and Swenson Wiederhold and Elmasri and Senko Since then numerous modifications to the ER model have been suggested. We have incorporated some of these in our presentation. Structural constraints on relationships are discussed in Abrial Elmasri and Wiederhold and Lenzerini and Santucci Multivalued and composite attributes are incorporated in the ER model in Elmasri et al. Although we did not discuss languages for the ER model and its extensions there have been several proposals for such languages. Elmasri and Wiederhold proposed the GORDAS query language for the ER model. Another ER query language was proposed by Markowitz and Raz Senko presented a query language for Senko’s DIAM model. A formal set of operations called the ER algebra was presented by Parent and Spaccapietra Gogolla and Hohenstein presented another formal language for the ER model. Campbell et al. presented a set of ER operations and showed that they are relationally complete. A conference for the dissemination of research results related to the ER model has been held regularly since The conference now known as the International Conference on Conceptual Modeling has been held in Los Angeles Model The ER modeling concepts discussed in Chapter are sufficient for representing many database schemas for traditional database applications which include many data processing applications in business and industry. Since the late however designers of database applications have tried to design more accurate database schemas that reflect the data properties and constraints more precisely. This was particularly important for newer applications of database technology such as databases for engineering design and manufacturing telecommunications complex software systems and Geographic Information Systems among many other applications. These types of databases have more complex requirements than do the more traditional applications. This led to the development of additional semantic data modeling concepts that were incorporated into conceptual data models such as the ER model. Various semantic data models have been proposed in the literature. Many of these concepts were also developed independently in related areas of computer science such as the knowledge representation area of artificial intelligence and the object modeling area in software engineering. In this chapter we describe features that have been proposed for semantic data models and show how the ER model can be enhanced to include these concepts leading to the Enhanced ER We start in Section by incorporating the concepts of class subclass relationships and type inheritance into the ER model. Then in Section we add the concepts of specialization and generalization. Section chapter has also been used to stand for Extended ER model. stands for computer aided design computer aided manufacturing. Chapter The Enhanced Entity Relationship Model discusses the various types of constraints on specialization generalization and Section shows how the UNION construct can be modeled by including the concept of category in the EER model. Section gives a sample UNIVERSITY database schema in the EER model and summarizes the EER model concepts by giving formal definitions. We will use the terms object and entity interchangeably in this chapter because many of these concepts are commonly used in object oriented models. We present the UML class diagram notation for representing specialization and generalization in Section and briefly compare these with EER notation and concepts. This serves as an example of alternative notation and is a continuation of Section which presented basic UML class diagram notation that corresponds to the basic ER model. In Section we discuss the fundamental abstractions that are used as the basis of many semantic data models. Section summarizes the chapter. For a detailed introduction to conceptual modeling Chapter should be considered a continuation of Chapter However if only a basic introduction to ER modeling is desired this chapter may be omitted. Alternatively the reader may choose to skip some or all of the later sections of this chapter that is the union of objects of different entity types. Associated with these concepts is the important mechanism of attribute and relationship inheritance. Unfortunately no standard terminology exists for these concepts so we use the most common terminology. Alternative terminology is given in footnotes. We also describe a diagrammatic technique for displaying these concepts when they arise in an EER schema. We call the resulting schema diagrams enhanced ER or EER diagrams. The first Enhanced ER model concept we take up is that of a subtype or subclass of an entity type. As we discussed in Chapter an entity type is used to represent both a type of entity and the entity set or collection of entities of that type that exist in the database. For example the entity type EMPLOYEE describes the type of each employee entity and also refers to the current set of EMPLOYEE entities in the COMPANY database. In many cases an entity type has numerous subgroupings or subtypes of its entities that are meaningful and need to be represented explicitly because of their significance to the database application. For example the entities that are members of the EMPLOYEE entity type may be distinguished further into SECRETARY ENGINEER MANAGER TECHNICIAN SALARIEDEMPLOYEE HOURLYEMPLOYEE and so on. The set of entities in each of the latter groupings is a subset of the entities that belong to the EMPLOYEE entity set meaning that every entity that is a member of one of these Subclasses Superclasses and Inheritance subgroupings is also an employee. We call each of these subgroupings a subclass or subtype of the EMPLOYEE entity type and the EMPLOYEE entity type is called the superclass or supertype for each of these subclasses. Figure shows how to represent these concepts diagramatically in EER diagrams. relationship because of the way we refer to the concept. We say a SECRETARY is an EMPLOYEE a TECHNICIAN is an EMPLOYEE and so on. Chapter The Enhanced Entity Relationship Model An entity cannot exist in the database merely by being a member of a subclass it must also be a member of the superclass. Such an entity can be included optionally as a member of any number of subclasses. For example a salaried employee who is also an engineer belongs to the two subclasses ENGINEER and SALARIEDEMPLOYEE of the EMPLOYEE entity type. However it is not necessary that every entity in a superclass is a member of some subclass. An important concept associated with subclasses is that of type inheritance. Recall that the type of an entity is defined by the attributes it possesses and the relationship types in which it participates. Because an entity in the subclass represents the same real world entity from the superclass it should possess values for its specific attributes as well as values of its attributes as a member of the superclass. We say that an entity that is a member of a subclass inherits all the attributes of the entity as a member of the superclass. The entity also inherits all the relationships in which the superclass participates. Notice that a subclass with its own specific attributes and relationships together with all the attributes and relationships it inherits from the superclass can be considered an entity type in its own Specialization and Generalization Specialization Specialization is the process of defining a set of subclasses of an entity type this entity type is called the superclass of the specialization. The set of subclasses that forms a specialization is defined on the basis of some distinguishing characteristic of the entities in the superclass. For example the set of subclasses SECRETARY ENGINEER TECHNICIAN is a specialization of the superclass EMPLOYEE that distinguishes among employee entities based on the job type of each employee entity. We may have several specializations of the same entity type based on different distinguishing characteristics. For example another specialization of the EMPLOYEE entity type may yield the set of subclasses SALARIEDEMPLOYEE HOURLYEMPLOYEE this specialization distinguishes among employees based on the method of pay. Figure shows how we represent a specialization diagrammatically in an EER diagram. The subclasses that define a specialization are attached by lines to a circle that represents the specialization which is connected in turn to the superclass. The subset symbol on each line connecting a subclass to the circle indicates the direction of the superclass subclass Attributes that apply only to entities of a particular subclass such as TypingSpeed of SECRETARY are attached to the rectangle representing that subclass. These are called specific attributes has only one type. This is generally too restrictive for conceptual database modeling. are many alternative notations for specialization we present the UML notation in Section and other proposed notations in Appendix A. Specialization and Generalization attributes of the subclass. Similarly a subclass can participate in specific relationship types such as the HOURLYEMPLOYEE subclass participating in the BELONGSTO relationship in Figure We will explain the d symbol in the circles in Figure and additional EER diagram notation shortly. Figure shows a few entity instances that belong to subclasses of the SECRETARY ENGINEER TECHNICIAN specialization. Again notice that an entity that belongs to a subclass represents the same real world entity as the entity connected to it in the EMPLOYEE superclass even though the same entity is shown twice for is shown in both EMPLOYEE and SECRETARY in Figure As the figure suggests a superclass subclass relationship such as EMPLOYEE SECRETARY somewhat resembles a relationship at the instance level Model There are two main reasons for including class subclass relationships and specializations in a data model. The first is that certain attributes may apply to some but not all entities of the superclass. A subclass is defined in order to group the entities to which these attributes apply. The members of the subclass may still share the majority of their attributes with the other members of the superclass. For example in Figure the SECRETARY subclass has the specific attribute Typingspeed whereas the ENGINEER subclass has the specific attribute Engtype but SECRETARY and ENGINEER share their other inherited attributes from the EMPLOYEE entity type. The second reason for using subclasses is that some relationship types may be participated in only by entities that are members of the subclass. For example if only HOURLYEMPLOYEES can belong to a trade union we can represent that fact by creating the subclass HOURLYEMPLOYEE of EMPLOYEE and relating the subclass to an entity type TRADEUNION via the BELONGSTO relationship type as illustrated in Figure In summary the specialization process allows us to do the following Define a set of subclasses of an entity type Establish additional specific attributes with each subclass Establish additional specific relationship types between each subclass and other entity types or other subclasses Generalization We can think of a reverse process of abstraction in which we suppress the differences among several entity types identify their common features and generalize them into a single superclass of which the original entity types are special subclasses. For example consider the entity types CAR and TRUCK shown in Figure Because they have several common attributes they can be generalized into the entity type VEHICLE as shown in Figure Both CAR and TRUCK are now subclasses of the generalized superclass VEHICLE. We use the term generalization to refer to the process of defining a generalized entity type from the given entity types. Notice that the generalization process can be viewed as being functionally the inverse of the specialization process. Hence in Figure we can view CAR TRUCK as a specialization of VEHICLE rather than viewing VEHICLE as a generalization of CAR and TRUCK. Similarly in Figure we can view EMPLOYEE as a generalization of SECRETARY TECHNICIAN and ENGINEER. A diagrammatic notation to distinguish between generalization and specialization is used in some design methodologies. An arrow pointing to the generalized superclass represents a generalization whereas arrows pointing to the specialized subclasses represent a specialization. We will not use this notation because the decision as to which process is followed in a particular situation is often subjective. Appendix A gives some of the suggested alternative diagrammatic notations for schema diagrams and class diagrams. So far we have introduced the concepts of subclasses and superclass subclass relationships as well as the specialization and generalization processes. In general a Constraints and Characteristics of Specialization and Generalization Hierarchies Maxspeed Vehicleid Noofpassengers Licenseplateno CAR Price Price Licenseplateno Noofaxles Vehicleid Tonnage TRUCK Vehicleid Price Licenseplateno VEHICLE Noofpassengers Maxspeed CAR TRUCK Noofaxles Tonnage d Figure Generalization. Two entity types CAR and TRUCK. Generalizing CAR and TRUCK into the superclass VEHICLE. superclass or subclass represents a collection of entities of the same type and hence also describes an entity type that is why superclasses and subclasses are all shown in rectangles in EER diagrams like entity types. Next we discuss the properties of specializations and generalizations in more detail. Constraints and Characteristics of Specialization and Generalization Hierarchies First we discuss constraints that apply to a single specialization or a single generalization. For brevity our discussion refers only to specialization even though it applies to both specialization and generalization. Then we discuss differences between specialization generalization lattices and hierarchies and elaborate on the differences between the specialization and generalization processes during conceptual database schema design. Constraints on Specialization and Generalization In general we may have several specializations defined on the same entity type as shown in Figure In such a case entities may belong to subclasses Chapter The Enhanced Entity Relationship Model d Minit Lname Name Ssn Birthdate Address Jobtype Fname Tgrade Engtype ‘Technician’ Jobtype ‘Secretary’ ‘Engineer’ Typingspeed SECR ETARY TECHNICIAN ENGINEER EMPLOYEE Figure EER diagram notation for an attribute defined specialization on Jobtype. in each of the specializations. However a specialization may also consist of a single subclass only such as the MANAGER specialization in Figure in such a case we do not use the circle notation. In some specializations we can determine exactly the entities that will become members of each subclass by placing a condition on the value of some attribute of the superclass. Such subclasses are called predicate defined subclasses. For example if the EMPLOYEE entity type has an attribute Jobtype as shown in Figure we can specify the condition of membership in the SECRETARY subclass by the condition which we call the defining predicate of the subclass. This condition is a constraint specifying that exactly those entities of the EMPLOYEE entity type whose attribute value for Jobtype is ‘Secretary’ belong to the subclass. We display a predicate defined subclass by writing the predicate condition next to the line that connects the subclass to the specialization circle. If all subclasses in a specialization have their membership condition on the same attribute of the superclass the specialization itself is called an attribute defined specialization and the attribute is called the defining attribute of the In this case all the entities with the same value for the attribute belong to the same subclass. We display an attribute defined specialization by placing the defining attribute name next to the arc from the circle to the superclass as shown in Figure When we do not have a condition for determining membership in a subclass the subclass is called user defined. Membership in such a subclass is determined by the database users when they apply the operation to add an entity to the subclass hence membership is specified individually for each entity by the user not by any condition that may be evaluated automatically. an attribute is called a discriminator in UML terminology. Constraints and Characteristics of Specialization and Generalization Hierarchies Partno Description Manufacturedate PART Drawingno PURCHASEDPART Suppliername Batchno Listprice o MANUFACTUREDPART Figure EER diagram notation for an overlapping specialization. Two other constraints may apply to a specialization. The first is the disjointness constraint which specifies that the subclasses of the specialization must be disjoint. This means that an entity can be a member of at most one of the subclasses of the specialization. A specialization that is attribute defined implies the disjointness constraint . Figure illustrates this case where the d in the circle stands for disjoint. The d notation also applies to user defined subclasses of a specialization that must be disjoint as illustrated by the specialization HOURLYEMPLOYEE SALARIEDEMPLOYEE in Figure If the subclasses are not constrained to be disjoint their sets of entities may be overlapping that is the same entity may be a member of more than one subclass of the specialization. This case which is the default is displayed by placing an o in the circle as shown in Figure The second constraint on specialization is called the completeness constraint which may be total or partial. A total specialization constraint specifies that every entity in the superclass must be a member of at least one subclass in the specialization. For example if every EMPLOYEE must be either an HOURLYEMPLOYEE or a SALARIEDEMPLOYEE then the specialization HOURLYEMPLOYEE SALARIEDEMPLOYEE in Figure is a total specialization of EMPLOYEE. This is shown in EER diagrams by using a double line to connect the superclass to the circle. A single line is used to display a partial specialization which allows an entity not to belong to any of the subclasses. For example if some EMPLOYEE entities do not belong to any of the subclasses SECRETARY ENGINEER TECHNICIAN in Figures and then that specialization is Notice that the disjointness and completeness constraints are independent. Hence we have the following four possible constraints on specialization Disjoint total Disjoint partial Overlapping total Overlapping partial notation of using single or double lines is similar to that for partial or total participation of an entity type in a relationship type as described in Chapter Chapter The Enhanced Entity Relationship Model Of course the correct constraint is determined from the real world meaning that applies to each specialization. In general a superclass that was identified through the generalization process usually is total because the superclass is derived from the subclasses and hence contains only the entities that are in the subclasses. Certain insertion and deletion rules apply to specialization as a consequence of the constraints specified earlier. Some of these rules are as follows Deleting an entity from a superclass implies that it is automatically deleted from all the subclasses to which it belongs. Inserting an entity in a superclass implies that the entity is mandatorily inserted in all predicate defined subclasses for which the entity satisfies the defining predicate. Inserting an entity in a superclass of a total specialization implies that the entity is mandatorily inserted in at least one of the subclasses of the specialization. The reader is encouraged to make a complete list of rules for insertions and deletions for the various types of specializations. Specialization and Generalization Hierarchies and Lattices A subclass itself may have further subclasses specified on it forming a hierarchy or a lattice of specializations. For example in Figure ENGINEER is a subclass of EMPLOYEE and is also a superclass of ENGINEERINGMANAGER this represents the real world constraint that every engineering manager is required to be an engineer. A specialization hierarchy has the constraint that every subclass participates as a subclass in only one class subclass relationship that is each subclass has only H O U R LY  E M P LOY E E SALARIEDEMPLOYEE ENGINEERINGMANAGER SECR ETARY TECHNICIAN ENGINEER MANAGER EMPLOYEE Figure A specialization lattice with shared subclass ENGINEERINGMANAGER. Constraints and Characteristics of Specialization and Generalization Hierarchies one parent which results in a tree structure or strict hierarchy. In contrast for a specialization lattice a subclass can be a subclass in more than one class subclass relationship. Hence Figure is a lattice. Figure shows another specialization lattice of more than one level. This may be part of a conceptual schema for a UNIVERSITY database. Notice that this arrangement would have been a hierarchy except for the STUDENTASSISTANT subclass which is a subclass in two distinct class subclass relationships. The requirements for the part of the UNIVERSITY database shown in Figure are the following The database keeps track of three types of persons employees alumni and students. A person can belong to one two or all three of these types. Each person has a name SSN sex address and birth date. Every employee has a salary and there are three types of employees faculty staff and student assistants. Each employee belongs to exactly one of these types. For each alumnus a record of the degree or degrees that he or she STAFF Percenttime FACULTY Name Sex Address PERSON Salary EMPLOYEE Majordept Birthdate ALUMNUS d o STUDENT ASSISTANT STUDENT Degrees Year Degree Major GRADUATE STUDENT d UNDERGRADUATE STUDENT R ESEARCHASSISTANT d TEACH I NGASSISTANT Position Rank Degreeprogram Class Project Course Ssn Figure A specialization lattice with multiple inheritance for a UNIVERSITY database. Chapter The Enhanced Entity Relationship Model earned at the university is kept including the name of the degree the year granted and the major department. Each student has a major department. Each faculty has a rank whereas each staff member has a staff position. Student assistants are classified further as either research assistants or teaching assistants and the percent of time that they work is recorded in the database. Research assistants have their research project stored whereas teaching assistants have the current course they work on. Students are further classified as either graduate or undergraduate with the specific attributes degree program for graduate students and class for undergraduates. In Figure all person entities represented in the database are members of the PERSON entity type which is specialized into the subclasses EMPLOYEE ALUMNUS STUDENT . This specialization is overlapping for example an alumnus may also be an employee and may also be a student pursuing an advanced degree. The subclass STUDENT is the superclass for the specialization GRADUATESTUDENT UNDERGRADUATESTUDENT while EMPLOYEE is the superclass for the specialization STUDENTASSISTANT FACULTY STAFF . Notice that STUDENTASSISTANT is also a subclass of STUDENT. Finally STUDENTASSISTANT is the superclass for the specialization into RESEARCHASSISTANT TEACHINGASSISTANT . In such a specialization lattice or hierarchy a subclass inherits the attributes not only of its direct superclass but also of all its predecessor superclasses all the way to the root of the hierarchy or lattice if necessary. For example an entity in GRADUATESTUDENT inherits all the attributes of that entity as a STUDENT and as a PERSON. Notice that an entity may exist in several leaf nodes of the hierarchy where a leaf node is a class that has no subclasses of its own. For example a member of GRADUATESTUDENT may also be a member of RESEARCHASSISTANT. A subclass with more than one superclass is called a shared subclass such as ENGINEERINGMANAGER in Figure This leads to the concept known as multiple inheritance where the shared subclass ENGINEERINGMANAGER directly inherits attributes and relationships from multiple classes. Notice that the existence of at least one shared subclass leads to a lattice if no shared subclasses existed we would have a hierarchy rather than a lattice and only single inheritance would exist. An important rule related to multiple inheritance can be illustrated by the example of the shared subclass STUDENTASSISTANT in Figure which inherits attributes from both EMPLOYEE and STUDENT. Here both EMPLOYEE and STUDENT inherit the same attributes from PERSON. The rule states that if an attribute originating in the same superclass is inherited more than once via different paths in the lattice then it should be included only once in the shared subclass . Hence the attributes of PERSON are inherited only once in the STUDENTASSISTANT subclass in Figure Constraints and Characteristics of Specialization and Generalization Hierarchies some models the class is further restricted to be a leaf node in the hierarchy or lattice. It is important to note here that some models and languages are limited to single inheritance and do not allow multiple inheritance . It is also important to note that some models do not allow an entity to have multiple types and hence an entity can be a member of only one leaf In such a model it is necessary to create additional subclasses as leaf nodes to cover all possible combinations of classes that may have some entity that belongs to all these classes simultaneously. For example in the overlapping specialization of PERSON into EMPLOYEE ALUMNUS STUDENT it would be necessary to create seven subclasses of PERSON in order to cover all possible types of entities E A S EA ES AS and EAS. Obviously this can lead to extra complexity. Although we have used specialization to illustrate our discussion similar concepts apply equally to generalization as we mentioned at the beginning of this section. Hence we can also speak of generalization hierarchies and generalization lattices. Utilizing Specialization and Generalization in Refining Conceptual Schemas Now we elaborate on the differences between the specialization and generalization processes and how they are used to refine conceptual schemas during conceptual database design. In the specialization process we typically start with an entity type and then define subclasses of the entity type by successive specialization that is we repeatedly define more specific groupings of the entity type. For example when designing the specialization lattice in Figure we may first specify an entity type PERSON for a university database. Then we discover that three types of persons will be represented in the database university employees alumni and students. We create the specialization EMPLOYEE ALUMNUS STUDENT for this purpose and choose the overlapping constraint because a person may belong to more than one of the subclasses. We specialize EMPLOYEE further into STAFF FACULTY STUDENTASSISTANT and specialize STUDENT into GRADUATESTUDENT UNDERGRADUATESTUDENT . Finally we specialize STUDENTASSISTANT into RESEARCHASSISTANT TEACHINGASSISTANT . This successive specialization corresponds to a top down conceptual refinement process during conceptual schema design. So far we have a hierarchy then we realize that STUDENTASSISTANT is a shared subclass since it is also a subclass of STUDENT leading to the lattice. It is possible to arrive at the same hierarchy or lattice from the other direction. In such a case the process involves generalization rather than specialization and corresponds to a bottom up conceptual synthesis. For example the database designers may first discover entity types such as STAFF FACULTY ALUMNUS GRADUATESTUDENT UNDERGRADUATESTUDENT RESEARCHASSISTANT TEACHINGASSISTANT and so on then they generalize GRADUATESTUDENT Chapter The Enhanced Entity Relationship Model UNDERGRADUATESTUDENT into STUDENT then they generalize RESEARCHASSISTANT TEACHINGASSISTANT into STUDENTASSISTANT then they generalize STAFF FACULTY STUDENTASSISTANT into EMPLOYEE and finally they generalize EMPLOYEE ALUMNUS STUDENT into PERSON. In structural terms hierarchies or lattices resulting from either process may be identical the only difference relates to the manner or order in which the schema superclasses and subclasses were created during the design process. In practice it is likely that neither the generalization process nor the specialization process is followed strictly but that a combination of the two processes is employed. New classes are continually incorporated into a hierarchy or lattice as they become apparent to users and designers. Notice that the notion of representing data and knowledge by using superclass subclass hierarchies and lattices is quite common in knowledge based systems and expert systems which combine database technology with artificial intelligence techniques. For example frame based knowledge representation schemes closely resemble class hierarchies. Specialization is also common in software engineering design methodologies that are based on the object oriented paradigm. Modeling of UNION Types Using Categories All of the superclass subclass relationships we have seen thus far have a single superclass. A shared subclass such as ENGINEERINGMANAGER in the lattice in Figure is the subclass in three distinct superclass subclass relationships where each of the three relationships has a single superclass. However it is sometimes necessary to represent a single superclass subclass relationship with more than one superclass where the superclasses represent different entity types. In this case the subclass will represent a collection of objects that is a subset of the UNION of distinct entity types we call such a subclass a union type or a category. For example suppose that we have three entity types PERSON BANK and COMPANY. In a database for motor vehicle registration an owner of a vehicle can be a person a bank or a company. We need to create a class that includes entities of all three types to play the role of vehicle owner. A category OWNER that is a subclass of the UNION of the three entity sets of COMPANY BANK and PERSON can be created for this purpose. We display categories in an EER diagram as shown in Figure The superclasses COMPANY BANK and PERSON are connected to the circle with the ∪ symbol which stands for the set union operation. An arc with the subset symbol connects the circle to the OWNER category. If a defining predicate is needed it is displayed next to the line from the superclass to which the predicate applies. In Figure we have two categories OWNER which is a subclass of the union of PERSON BANK and COMPANY and REGISTEREDVEHICLE which is a subclass of the union of CAR and TRUCK. use of the term category is based on the ECR model OWNER and REGISTEREDVEHICLE. A category has two or more superclasses that may represent distinct entity types whereas other superclass subclass relationships always have a single superclass. To better understand the difference we can compare a category such as OWNER in Figure with the ENGINEERINGMANAGER shared subclass in Figure The latter is a subclass of each of the three superclasses ENGINEER MANAGER and SALARIEDEMPLOYEE so an entity that is a member of ENGINEERINGMANAGER must exist in all three. This represents the constraint that an engineering manager must be an ENGINEER a MANAGER and a SALARIEDEMPLOYEE that is ENGINEERINGMANAGER is a subset of the intersection of the three classes . On the other hand a category is a subset of the union of its superclasses. Hence an entity that is a member of OWNER must exist in only one of the super Chapter The Enhanced Entity Relationship Model classes. This represents the constraint that an OWNER may be a COMPANY a BANK or a PERSON in Figure Attribute inheritance works more selectively in the case of categories. For example in Figure each OWNER entity inherits the attributes of a COMPANY a PERSON or a BANK depending on the superclass to which the entity belongs. On the other hand a shared subclass such as ENGINEERINGMANAGER . In general a specialization or generalization such as that in Figure if it were partial would not preclude VEHICLE from containing other types of entities such as motorcycles. However a category such as REGISTEREDVEHICLE in Figure implies that only cars and trucks but not other types of entities can be members of REGISTEREDVEHICLE. A category can be total or partial. A total category holds the union of all entities in its superclasses whereas a partial category can hold a subset of the union. A total category is represented diagrammatically by a double line connecting the category and the circle whereas a partial category is indicated by a single line. The superclasses of a category may have different key attributes as demonstrated by the OWNER category in Figure or they may have the same key attribute as demonstrated by the REGISTEREDVEHICLE category. Notice that if a category is total it may be represented alternatively as a total specialization . In this case the choice of which representation to use is subjective. If the two classes represent the same type of entities and share numerous attributes including the same key attributes specialization generalization is preferred otherwise categorization is more appropriate. It is important to note that some modeling methodologies do not have union types. In these models a union type must be represented in a roundabout way office [Foffice] office phone [Fphone] and salary [Salary]. All faculty members are related to the academic department with which they are affiliated [BELONGS] . A specific attribute of STUDENT is [Class] graduate Each STUDENT is also related to his or her major and minor departments [MAJOR] and [MINOR] to the course sections he or she is currently attending [REGISTERED] and to the courses completed [TRANSCRIPT]. Each TRANSCRIPT instance includes the grade the student received [Grade] in a section of a course. GRADSTUDENT is a subclass of STUDENT with the defining predicate Class For each graduate student we keep a list of previous degrees in a composite multivalued attribute [Degrees]. We also relate the graduate student to a faculty advisor [ADVISOR] and to a thesis committee [COMMITTEE] if one exists. An academic department has the attributes name [Dname] telephone [Dphone] and office number [Office] and is related to the faculty member who is its chairperson [CHAIRS] and to the college to which it belongs [CD]. Each college has attributes college name [Cname] office number [Coffice] and the name of its dean [Dean]. A course has attributes course number [C#] course name [Cname] and course description [Cdesc]. Several sections of each course are offered with each section having the attributes section number [Sec#] and the year and quarter in which the section was offered if that instructor is in the database. The category INSTRUCTORRESEARCHER is a subset of the union of FACULTY and GRADSTUDENT and includes all faculty as well as graduate students who are supported by teaching or research. Finally the entity type GRANT keeps track of research grants and contracts awarded to the university. Each grant has attributes grant title [Title] grant number [No] the awarding agency [Agency] and the starting assume that the quarter system rather than the semester system is used in this university. Chapter The Enhanced Entity Relationship Model Foffice Salary Rank Fphone FACULTY College Degree Year N M N M Degrees Class M N N M N N Qtr Currentqtr and Year Currentyear N N M N N Cname C# Cdesc N Office Dphone Dname N N Fname Minit Lname Name Ssn Bdate Sex No Street Aptno City State Zip Address U ADVISOR COMMITTEE CHAIRS BELONGS MINOR MAJOR C D D C Agency Stdate Title No Start Time End CURRENTSECTION Grade Sec# Year Qtr Cname Coffice Dean PERSON GRADSTUDENT STUDENT GRANT SUPPORT REGISTERED TRANSCRIPT SECTION TEACH DEPARTMENT COLLEGE COURSE C S INSTRUCTORRESEARCHER P I Figure An EER conceptual schema for a UNIVERSITY database. A Sample UNIVERSITY EER Schema Design Choices and Formal Definitions date [Stdate]. A grant is related to one principal investigator [PI] and to all researchers it supports [SUPPORT]. Each instance of support has as attributes the starting date of support [Start] the ending date of the support [End] and the percentage of time being spent on the project [Time] by the researcher being supported. Design Choices for Specialization Generalization It is not always easy to choose the most appropriate conceptual design for a database application. In Section we presented some of the typical issues that confront a database designer when choosing among the concepts of entity types relationship types and attributes to represent a particular miniworld situation as an ER schema. In this section we discuss design guidelines and choices for the EER concepts of specialization generalization and categories . As we mentioned in Section conceptual database design should be considered as an iterative refinement process until the most suitable design is reached. The following guidelines can help to guide the design process for EER concepts In general many specializations and subclasses can be defined to make the conceptual model accurate. However the drawback is that the design becomes quite cluttered. It is important to represent only those subclasses that are deemed necessary to avoid extreme cluttering of the conceptual schema. If a subclass has few specific attributes and no specific relationships it can be merged into the superclass. The specific attributes would hold NULL values for entities that are not members of the subclass. A type attribute could specify whether an entity is a member of the subclass. Similarly if all the subclasses of a specialization generalization have few specific attributes and no specific relationships they can be merged into the superclass and replaced with one or more type attributes that specify the subclass or subclasses that each entity belongs to . Union types and categories should generally be avoided unless the situation definitely warrants this type of construct which does occur in some practical situations. If possible we try to model using specialization generalization as discussed at the end of Section The choice of disjoint overlapping and total partial constraints on specialization generalization is driven by the rules in the miniworld being modeled. If the requirements do not indicate any particular constraints the default would generally be overlapping and partial since this does not specify any restrictions on subclass membership. Chapter The Enhanced Entity Relationship Model As an example of applying these guidelines consider Figure where no specific attributes are shown. We could merge all the subclasses into the EMPLOYEE entity type and add the following attributes to EMPLOYEE An attribute Jobtype whose value set ‘Secretary’ ‘Engineer’ ‘Technician’ would indicate which subclass in the first specialization each employee belongs to. An attribute Paymethod whose value set ‘Salaried’ ‘Hourly’ would indicate which subclass in the second specialization each employee belongs to. An attribute Isamanager whose value set ‘Yes’ ‘No’ would indicate whether an individual employee entity is a manager or not. Formal Definitions for the EER Model Concepts We now summarize the EER model concepts and give formal definitions. A is a set or collection of entities this includes any of the EER schema constructs of group entities such as entity types subclasses superclasses and categories. A subclass S is a class whose entities must always be a subset of the entities in another class called the superclass C of the superclass subclass relationship. We denote such a relationship by C S. For such a superclass subclass relationship we must always have S ⊆ C A specialization Z Sn is a set of subclasses that have the same superclass G that is G Si is a superclass subclass relationship for i n. G is called a generalized entity type . Z is said to be total if we always have Otherwise Z is said to be partial. Z is said to be disjoint if we always have Si ∩ Sj ∅ for i ≠ j Otherwise Z is said to be overlapping. A subclass S of C is said to be predicate defined if a predicate p on the attributes of C is used to specify which entities in C are members of S that is S C[p] where C[p] is the set of entities in C that satisfy p. A subclass that is not defined by a predicate is called user defined. A specialization Z is said to be attribute defined if a predicate where A is an attribute of G and ci is a constant value from the domain of A n ∪Si G use of the word class here differs from its more common use in object oriented programming languages such as C++. In C++ a class is a structured type definition along with its applicable functions . Example of Other Notation Representing Specialization and Generalization in UML Class Diagrams is used to specify membership in each subclass Si in Z. Notice that if ci ≠ cj for i ≠ j and A is a single valued attribute then the specialization will be disjoint. A category T is a class that is a subset of the union of n defining superclasses Dn n and is formally specified as follows T ⊆ ∪ ∪ Dn A predicate pi on the attributes of Di can be used to specify the members of each Di that are members of T. If a predicate is specified on every Di we get T ∪ ∪ Dn[pn] We should now extend the definition of relationship type given in Chapter by allowing any class not only any entity type to participate in a relationship. Hence we should replace the words entity type with class in that definition. The graphical notation of EER is consistent with ER because all classes are represented by rectangles. Example of Other Notation Representing Specialization and Generalization in UML Class Diagrams We now discuss the UML notation for generalization specialization and inheritance. We already presented basic UML class diagram notation and terminology in Section Figure illustrates a possible UML class diagram corresponding to the EER diagram in Figure The basic notation for specialization generalization are called leaf classes. The above discussion and example in Figure and the presentation in Section gave a brief overview of UML class diagrams and terminology. We focused on the concepts that are relevant to ER and EER database modeling rather than those concepts that are more relevant to software engineering. In UML there are many details that we have not discussed because they are outside the scope of this book and are mainly relevant to software engineering. For example classes can be of various types Abstract classes define attributes and operations but do not have objects corresponding to those classes. These are mainly used to specify a set of attributes and operations that can be inherited. Concrete classes can have objects instantiated to belong to the class. Template classes specify a template that can be further used to define other classes. Chapter The Enhanced Entity Relationship Model In database design we are mainly concerned with specifying concrete classes whose collections of objects are permanently stored in the database. The bibliographic notes at the end of this chapter give some references to books that describe complete details of UML. Additional material related to UML is covered in Chapter Project changeproject . . . RESEARCH ASSISTANT Course assigntocourse . . . TEACHING ASSISTANT Degreeprogram changedegreeprogram . . . GRADUATE STUDENT Class changeclassification . . . UNDERGRADUATE STUDENT Position hirestaff . . . STAFF Rank promote . . . FACULTY Percenttime hirestudent . . . STUDENTASSISTANT Year Degree Major DEGREE . . . Salary hireemp . . . EMPLOYEE newalumnus . . . ALUMNUS Majordept changemajor . . . STUDENT Name Ssn Birthdate Sex Address age . . . PERSON Figure A UML class diagram corresponding to the EER diagram in Figure illustrating UML notation for specialization generalization. Data Abstraction Knowledge Representation and Ontology Concepts Data Abstraction Knowledge Representation and Ontology Concepts In this section we discuss in general terms some of the modeling concepts that we described quite specifically in our presentation of the ER and EER models in Chapter and earlier in this chapter. This terminology is not only used in conceptual data modeling but also in artificial intelligence literature when discussing knowledge representation . This section discusses the similarities and differences between conceptual modeling and knowledge representation and introduces some of the alternative terminology and a few additional concepts. The goal of KR techniques is to develop concepts for accurately modeling some domain of knowledge by creating an that describes the concepts of the domain and how these concepts are interrelated. Such an ontology is used to store and manipulate knowledge for drawing inferences making decisions or answering questions. The goals of KR are similar to those of semantic data models but there are some important similarities and differences between the two disciplines Both disciplines use an abstraction process to identify common properties and important aspects of objects in the miniworld while suppressing insignificant differences and unimportant details. Both disciplines provide concepts relationships constraints operations and languages for defining data and representing knowledge. KR is generally broader in scope than semantic data models. Different forms of knowledge such as rules incomplete and default knowledge and temporal and spatial knowledge are represented in KR schemes. Database models are being expanded to include some of these concepts needs to be stored. ontology is somewhat similar to a conceptual schema but with more knowledge rules and exceptions. Chapter The Enhanced Entity Relationship Model We now discuss four abstraction concepts that are used in semantic data models such as the EER model as well as in KR schemes classification and instantiation identification specialization and generalization and aggregation and association. The paired concepts of classification and instantiation are inverses of one another as are generalization and specialization. The concepts of aggregation and association are also related. We discuss these abstract concepts and their relation to the concrete representations used in the EER model to clarify the data abstraction process and to improve our understanding of the related process of conceptual schema design. We close the section with a brief discussion of ontology which is being used widely in recent knowledge representation research. Classification and Instantiation The process of classification involves systematically assigning similar objects entities to object classes entity types. We can now describe or reason about the classes rather than the individual objects. Collections of objects that share the same types of attributes relationships and constraints are classified into classes in order to simplify the process of discovering their properties. Instantiation is the inverse of classification and refers to the generation and specific examination of distinct objects of a class. An object instance is related to its object class by the IS ANINSTANCE OF or IS A MEMBER OF relationship. Although EER diagrams do not display instances the UML diagrams allow a form of instantiation by permitting the display of individual objects. We did not describe this feature in our introduction to UML class diagrams. In general the objects of a class should have a similar type structure. However some objects may display properties that differ in some respects from the other objects of the class these exception objects also need to be modeled and KR schemes allow more varied exceptions than do database models. In addition certain properties apply to the class as a whole and not to the individual objects KR schemes allow such class properties. UML diagrams also allow specification of class properties. In the EER model entities are classified into entity types according to their basic attributes and relationships. Entities are further classified into subclasses and categories based on additional similarities and differences among them. Relationship instances are classified into relationship types. Hence entity types subclasses categories and relationship types are the different concepts that are used for classification in the EER model. The EER model does not provide explicitly for class properties but it may be extended to do so. In UML objects are classified into classes and it is possible to display both class properties and individual objects. Knowledge representation models allow multiple classification schemes in which one class is an instance of another class . Notice that this cannot be represented directly in the EER model because we have only two levels classes and instances. The only relationship among classes in the EER model is a superclass subclass relationship whereas in some KR schemes an additional class instance relationship can be represented directly in a class hierarchy. An instance may itself be another class allowing multiple level classification schemes. Data Abstraction Knowledge Representation and Ontology Concepts Identification Identification is the abstraction process whereby classes and objects are made uniquely identifiable by means of some identifier. For example a class name uniquely identifies a whole class within a schema. An additional mechanism is necessary for telling distinct object instances apart by means of object identifiers. Moreover it is necessary to identify multiple manifestations in the database of the same real world object. For example we may have a tuple ‘Matthew Clarke’ in a PERSON relation and another tuple ‘CS’ in a STUDENT relation that happen to represent the same real world entity. There is no way to identify the fact that these two database objects represent the same real world entity unless we make a provision at design time for appropriate cross referencing to supply this identification. Hence identification is needed at two levels To distinguish among database objects and classes To identify database objects and to relate them to their real world counterparts In the EER model identification of schema constructs is based on a system of unique names for the constructs in a schema. For example every class in an EER schema whether it is an entity type a subclass a category or a relationship type must have a distinct name. The names of attributes of a particular class must also be distinct. Rules for unambiguously identifying attribute name references in a specialization or generalization lattice or hierarchy are needed as well. At the object level the values of key attributes are used to distinguish among entities of a particular entity type. For weak entity types entities are identified by a combination of their own partial key values and the entities they are related to in the owner entity type. Relationship instances are identified by some combination of the entities that they relate to depending on the cardinality ratio specified. Specialization and Generalization Specialization is the process of classifying a class of objects into more specialized subclasses. Generalization is the inverse process of generalizing several classes into a higher level abstract class that includes the objects in all these classes. Specialization is conceptual refinement whereas generalization is conceptual synthesis. Subclasses are used in the EER model to represent specialization and generalization. We call the relationship between a subclass and its superclass an IS A SUBCLASS OF relationship or simply an IS A relationship. This is the same as the IS A relationship discussed earlier in Section Aggregation and Association Aggregation is an abstraction concept for building composite objects from their component objects. There are three cases where this concept can be related to the EER model. The first case is the situation in which we aggregate attribute values of Chapter The Enhanced Entity Relationship Model an object to form the whole object. The second case is when we represent an aggregation relationship as an ordinary relationship. The third case which the EER model does not provide for explicitly involves the possibility of combining objects that are related by a particular relationship instance into a higher level aggregate object. This is sometimes useful when the higher level aggregate object is itself to be related to another object. We call the relationship between the primitive objects and their aggregate object IS A PART OF the inverse is called IS A COMPONENTOF. UML provides for all three types of aggregation. The abstraction of association is used to associate objects from several independent classes. Hence it is somewhat similar to the second use of aggregation. It is represented in the EER model by relationship types and in UML by associations. This abstract relationship is called IS ASSOCIATED WITH. In order to understand the different uses of aggregation better consider the ER schema shown in Figure which stores information about interviews by job applicants to various companies. The class COMPANY is an aggregation of the attributes Cname and Caddress whereas JOBAPPLICANT is an aggregate of Ssn Name Address and Phone. The relationship attributes Contactname and Contactphone represent the name and phone number of the person in the company who is responsible for the interview. Suppose that some interviews result in job offers whereas others do not. We would like to treat INTERVIEW as a class to associate it with JOBOFFER. The schema shown in Figure is incorrect because it requires each interview relationship instance to have a job offer. The schema shown in Figure is not allowed because the ER model does not allow relationships among relationships. One way to represent this situation is to create a higher level aggregate class composed of COMPANY JOBAPPLICANT and INTERVIEW and to relate this class to JOBOFFER as shown in Figure Although the EER model as described in this book does not have this facility some semantic data models do allow it and call the resulting object a composite or molecular object. Other models treat entity types and relationship types uniformly and hence permit relationships among relationships as illustrated in Figure To represent this situation correctly in the ER model as described here we need to create a new weak entity type INTERVIEW as shown in Figure and relate it to JOBOFFER. Hence we can always represent these situations correctly in the ER model by creating additional entity types although it may be conceptually more desirable to allow direct representation of aggregation as in Figure or to allow relationships among relationships as in Figure The main structural distinction between aggregation and association is that when an association instance is deleted the participating objects may continue to exist. However if we support the notion of an aggregate object for example a CAR that is made up of objects ENGINE CHASSIS and TIRES then deleting the aggregate CAR object amounts to deleting all its component objects. COMPANY JOBAPPLICANT Cname Caddress Name Ssn Phone Address Contactname Contactphone Date INTERVIEW JOBOFFER COMPANY INTERVIEW JOBAPPLICANT RESULTSIN JOBOFFER COMPANY INTERVIEW JOBAPPLICANT JOBOFFER COMPANY INTERVIEW JOBAPPLICANT RESULTSIN JOBOFFER COMPANY JOBAPPLICANT Cname Caddress Name Ssn Phone Address Contactphone Contactname R E S U LTSI N CJI Date INTERVIEW Data Abstraction Knowledge Representation and Ontology Concepts Figure Aggregation. The relationship type INTERVIEW. Including JOBOFFER in a ternary relationship type . Having the RESULTSIN relationship participate in other relationships . Using aggregation and a composite object . Correct representation in ER. Chapter The Enhanced Entity Relationship Model Ontologies and the Semantic Web In recent years the amount of computerized data and information available on the Web has spiraled out of control. Many different models and formats are used. In addition to the database models that we present in this book much information is stored in the form of documents which have considerably less structure than database information does. One ongoing project that is attempting to allow information exchange among computers on the Web is called the Semantic Web which attempts to create knowledge representation models that are quite general in order to allow meaningful information exchange and search among machines. The concept of ontology is considered to be the most promising basis for achieving the goals of the Semantic Web and is closely related to knowledge representation. In this section we give a brief introduction to what ontology is and how it can be used as a basis to automate information understanding search and exchange. The study of ontologies attempts to describe the structures and relationships that are possible in reality through some common vocabulary therefore it can be considered as a way to describe the knowledge of a certain community about reality. Ontology originated in the fields of philosophy and metaphysics. One commonly used definition of ontology is a specification of a conceptualization. In this definition a conceptualization is the set of concepts that are used to represent the part of reality or knowledge that is of interest to a community of users. Specification refers to the language and vocabulary terms that are used to specify the conceptualization. The ontology includes both specification and conceptualization. For example the same conceptualization may be specified in two different languages giving two separate ontologies. Based on this quite general definition there is no consensus on what an ontology is exactly. Some possible ways to describe ontologies are as follows A thesaurus describes the relationships between words that represent various concepts. A taxonomy describes how concepts of a particular area of knowledge are related using structures similar to those used in a specialization or generalization. A detailed database schema is considered by some to be an ontology that describes the concepts and relationships of a miniworld from reality. A logical theory uses concepts from mathematical logic to try to define concepts and their interrelationships. Usually the concepts used to describe ontologies are quite similar to the concepts we discussed in conceptual modeling such as entities attributes relationships specializations and so on. The main difference between an ontology and say a database schema is that the schema is usually limited to describing a small subset of a definition is given in Gruber Review Questions world from reality in order to store and manage data. An ontology is usually considered to be more general in that it attempts to describe a part of reality or a domain of interest as completely as possible. Summary In this chapter we discussed extensions to the ER model that improve its representational capabilities. We called the resulting model the enhanced ER or EER model. We presented the concept of a subclass and its superclass and the related mechanism of attribute relationship inheritance. We saw how it is sometimes necessary to create additional classes of entities either because of additional specific attributes or because of specific relationship types. We discussed two main processes for defining superclass subclass hierarchies and lattices specialization and generalization. Next we showed how to display these new constructs in an EER diagram. We also discussed the various types of constraints that may apply to specialization or generalization. The two main constraints are total partial and disjoint overlapping. In addition a defining predicate for a subclass or a defining attribute for a specialization may be specified. We discussed the differences between user defined and predicate defined subclasses and between user defined and attribute defined specializations. Finally we discussed the concept of a category or union type which is a subset of the union of two or more classes and we gave formal definitions of all the concepts presented. We introduced some of the notation and terminology of UML for representing specialization and generalization. In Section we briefly discussed the discipline of knowledge representation and how it is related to semantic data modeling. We also gave an overview and summary of the types of abstract data representation concepts classification and instantiation identification specialization and generalization and aggregation and association. We saw how EER and UML concepts are related to each of these. Review Questions What is a subclass When is a subclass needed in data modeling Define the following terms superclass of a subclass superclass subclass relationship IS A relationship specialization generalization category specific attributes and specific relationships. Discuss the mechanism of attribute relationship inheritance. Why is it useful Discuss user defined and predicate defined subclasses and identify the differences between the two. Discuss user defined and attribute defined specializations and identify the differences between the two. Chapter The Enhanced Entity Relationship Model Discuss the two main types of constraints on specializations and generalizations. What is the difference between a specialization hierarchy and a specialization lattice What is the difference between specialization and generalization Why do we not display this difference in schema diagrams How does a category differ from a regular shared subclass What is a category used for Illustrate your answer with examples. For each of the following UML terms and LOANS . Suppose that it is also desirable to keep track of each ACCOUNT’s TRANSACTIONS and each LOAN’s PAYMENTS both of these include the amount date and time. Modify the BANK schema using ER and EER concepts of specialization and generalization. State any assumptions you make about the additional requirements. Exercises The following narrative describes a simplified version of the organization of Olympic facilities planned for the summer Olympics. Draw an EER diagram that shows the entity types attributes relationships and specializations for this application. State any assumptions you make. The Olympic facilities are divided into sports complexes. Sports complexes are divided into one sport and multisport types. Multisport complexes have areas of the complex designated for each sport with a location indicator . A complex has a location chief organizing individual total occupied area and so on. Each complex holds a series of events . For each event there is a planned date duration number of participants number of officials and so on. A roster of all officials will be maintained together with the list of events each official will be involved in. Different equipment is needed for the events as well as for maintenance. The two types of facilities will have different types of information. For each type the number of facilities needed is kept together with an approximate budget. Identify all the important concepts represented in the library database case study described below. In particular identify the abstractions of classification aggregation identification and specialization generalization. Specify cardinality constraints whenever possible. List details that will affect the eventual design but that have no bearing on the conceptual design. List the semantic constraints separately. Draw an EER diagram of the library database. Case Study The Georgia Tech Library has approximately members titles and volumes . About percent of the volumes are out on loan at any one time. The librarians ensure that the books that members want to borrow are available when the members want to borrow them. Also the librarians must know how many copies of each book are in the library or out on loan at any given time. A catalog of books is available online that lists books by author title and subject area. For each title in the library a book description is kept in the catalog that ranges from one sentence to several pages. The reference librarians want to be able to access this description when members request information about a book. Library staff includes chief librarian departmental associate librarians reference librarians check out staff and library assistants. Books can be checked out for days. Members are allowed to have only five books out at a time. Members usually return books within three to four weeks. Most members know that they have one week of grace before a notice is sent to them so they try to return books before the grace period ends. About percent of the members have to be sent reminders to return books. Most overdue books are returned within a month of the due date. Approximately percent of the overdue books are either kept or never returned. The most active members of the library are defined as those who Chapter The Enhanced Entity Relationship Model borrow books at least ten times during the year. The top percent of membership does percent of the borrowing and the top percent of the membership does percent of the borrowing. About percent of the members are totally inactive in that they are members who never borrow. To become a member of the library applicants fill out a form including their SSN campus and home mailing addresses and phone numbers. The librarians issue a numbered machine readable card with the member’s photo on it. This card is good for four years. A month before a card expires a notice is sent to a member for renewal. Professors at the institute are considered automatic members. When a new faculty member joins the institute his or her information is pulled from the employee records and a library card is mailed to his or her campus address. Professors are allowed to check out books for three month intervals and have a two week grace period. Renewal notices to professors are sent to their campus address. The library does not lend some books such as reference books rare books and maps. The librarians must differentiate between books that can be lent and those that cannot be lent. In addition the librarians have a list of some books they are interested in acquiring but cannot obtain such as rare or outof print books and books that were lost or destroyed but have not been replaced. The librarians must have a system that keeps track of books that cannot be lent as well as books that they are interested in acquiring. Some books may have the same title therefore the title cannot be used as a means of identification. Every book is identified by its International Standard Book Number a unique international code assigned to all books. Two books with the same title can have different ISBNs if they are in different languages or have different bindings . Editions of the same book have different ISBNs. The proposed database system must be designed to keep track of the members the books the catalog and the borrowing activity. Design a database to keep track of information for an art museum. Assume that the following requirements were collected The museum has a collection of ARTOBJECTS. Each ARTOBJECT has a unique Idno an Artist a Year a Title and a Description. The art objects are categorized in several ways as discussed below. ARTOBJECTS are categorized based on their type. There are three main types PAINTING SCULPTURE and STATUE plus another type called OTHER to accommodate objects that do not fall into one of the three main types. A PAINTING has a Painttype material on which it is Drawnon and Style . A SCULPTURE or a statue has a Material from which it was created Height Weight and Style. Exercises An art object in the OTHER category has a Type and Style. ARTOBJECTs are categorized as either PERMANENTCOLLECTION and BORROWED. Information captured about objects in the PERMANENTCOLLECTION includes Dateacquired Status and Cost. Information captured about BORROWED objects includes the Collection from which it was borrowed Dateborrowed and Datereturned. Information describing the country or culture of Origin and Epoch is captured for each ARTOBJECT. The museum keeps track of ARTIST information if known Name DateBorn Datedied Countryoforigin Epoch Mainstyle and Description. The Name is assumed to be unique. Different EXHIBITIONS occur each having a Name Startdate and Enddate. EXHIBITIONS are related to all the art objects that were on display during the exhibition. Information is kept on other COLLECTIONS with which the museum interacts including Name Type Description Address Phone and current Contactperson. Draw an EER schema diagram for this application. Discuss any assumptions you make and that justify your EER design choices. Figure shows an example of an EER diagram for a small private airport database that is used to keep track of airplanes their owners airport employees and pilots. From the requirements for this database the following information was collected Each AIRPLANE has a registration number [Reg#] is of a particular plane type [OFTYPE] and is stored in a particular hangar [STOREDIN]. Each PLANETYPE has a model number [Model] a capacity [Capacity] and a weight [Weight]. Each HANGAR has a number [Number] a capacity [Capacity] and a location [Location]. The database also keeps track of the OWNERs of each plane [OWNS] and the EMPLOYEEs who have maintained the plane [MAINTAIN]. Each relationship instance in OWNS relates an AIRPLANE to an OWNER and includes the purchase date [Pdate]. Each relationship instance in MAINTAIN relates an EMPLOYEE to a service record [SERVICE]. Each plane undergoes service many times hence it is related by [PLANESERVICE] to a number of SERVICE records. A SERVICE record includes as attributes the date of maintenance [Date] the number of hours spent on the work [Hours] and the type of work done [Workcode]. We use a weak entity type [SERVICE] to represent airplane service because the airplane registration number is used to identify a service record. An OWNER is either a person or a corporation. Hence we use a union type [OWNER] that is a subset of the union of corporation [CORPORATION] and person [PERSON] entity types. Both pilots [PILOT] and employees [EMPLOYEE] are subclasses of PERSON. Each PILOT has Chapter The Enhanced Entity Relationship Model specific attributes license number [Licnum] and restrictions [Restr] each EMPLOYEE has specific attributes salary [Salary] and shift worked [Shift]. All PERSON entities in the database have data kept on their Social Security number [Ssn] name [Name] address [Address] and telephone number [Phone]. For CORPORATION entities the data kept includes name [Name] address [Address] and telephone number [Phone]. The database also keeps track of the types of planes each pilot is authorized to fly [FLIES] and the types of planes each employee can do maintenance work on [WORKSON]. Number Location Capacity Name Phone Address Name Ssn Phone Add ress Restr Licnum Date workcode N N N PLANETYPE Model Capacity Pdate Weight MAINTAIN M M N OFTYPE STOREDIN M N OWNS FLIES WORKSON N N M Reg# Date Hours HANGAR PILOT EMPLOYEE Salary PLANESERVICE SERVICE Workcode AIRPLANE Shift U CORPORATION PERSON OWNER Figure EER schema for a SMALLAIRPORT database. Exercises Entity Set Has a Relationship with Has an Attribute that is Is a Specialization of Is a Generalization of Entity Set or Attribute MOTHER PERSON DAUGHTER MOTHER STUDENT PERSON STUDENT Studentid SCHOOL STUDENT SCHOOL CLASSROOM ANIMAL HORSE HORSE Breed HORSE Age EMPLOYEE SSN FURNITURE CHAIR CHAIR Weight HUMAN WOMAN SOLDIER PERSON ENEMYCOMBATANT PERSON Show how the SMALLAIRPORT EER schema in Figure may be represented in UML notation. in UML so you do not have to map the categories in this and the following Show how the UNIVERSITY EER schema in Figure may be represented in UML notation. Consider the entity sets and attributes shown in the table below. Place a checkmark in one column in each row to indicate the relationship between the far left and right columns. a. The left side has a relationship with the right side. b. The right side is an attribute of the left side. c. The left side is a specialization of the right side. d. The left side is a generalization of the right side. Draw a UML diagram for storing a played game of chess in a database. You may look at for an application similar to what you are designing. State clearly any assumptions you make in your UML diagram. A sample of assumptions you can make about the scope is as follows The game of chess is played between two players. The game is played on an × board like the one shown below Chapter The Enhanced Entity Relationship Model The players are assigned a color of black or white at the start of the game. Each player starts with the following pieces a. king b. queen c. rooks d. bishops e. knights f. pawns Every piece has its own initial position. Every piece has its own set of legal moves based on the state of the game. You do not need to worry about which moves are or are not legal except for the following issues a. A piece may move to an empty square or capture an opposing piece. b. If a piece is captured it is removed from the board. c. If a pawn moves to the last row it is “promoted” by converting it to another piece . Note Some of these functions may be spread over multiple classes. Draw an EER diagram for a game of chess as described in Exercise Focus on persistent storage aspects of the system. For example the system would need to retrieve all the moves of every game played in sequential order. Which of the following EER diagrams is are incorrect and why State clearly any assumptions you make. a. b. E d R E R N o Laboratory Exercises c. Consider the following EER diagram that describes the computer systems at a company. Provide your own attributes and key for each entity type. Supply max cardinality constraints justifying your choice. Write a complete narrative description of what this EER diagram represents. R N o M MEMORY VIDEOCARD d LAPTOP DESKTOP INSTALLED d COMPUTER SOFTWARE OPERATING SYSTEM INSTALLEDOS SUPPORTS COMPONENT OPTIONS SOUNDCARD MEMOPTIONS KEYBOARD MOUSE d ACCESSORY MONITOR SOLDWITH Laboratory Exercises Consider a GRADEBOOK database in which instructors within an academic department record points earned by individual students in their classes. The data requirements are summarized as follows Each student is identified by a unique identifier first and last name and an e mail address. Each instructor teaches certain courses each term. Each course is identified by a course number a section number and the term in which it is taught. For each course he or she teaches the Chapter The Enhanced Entity Relationship Model instructor specifies the minimum number of points required in order to earn letter grades A B C D and F. For example points for an A points for a B points for a C and so forth. Students are enrolled in each course taught by the instructor. Each course has a number of grading components . Each grading component has a maximum number of points participate in the sale of items. The data requirements for this system are summarized as follows The online site has members each of whom is identified by a unique member number and is described by an e mail address name password home address and phone number. A member may be a buyer or a seller. A buyer has a shipping address recorded in the database. A seller has a bank account number and routing number recorded in the database. Items are placed by a seller for sale and are identified by a unique item number assigned by the system. Items are also described by an item title a description starting bid price bidding increment the start date of the auction and the end date of the auction. Items are also categorized based on a fixed classification hierarchy . Buyers make bids for items they are interested in. Bid price and time of bid is recorded. The bidder at the end of the auction with the highest bid price is declared the winner and a transaction between buyer and seller may then proceed. The buyer and seller may record feedback regarding their completed transactions. Feedback contains a rating of the other party participating in the transaction and a comment. Design an Enhanced Entity Relationship diagram for the ONLINEAUCTION database and build the design using a data modeling tool such as ERwin or Rational Rose. Consider a database system for a baseball organization such as the major leagues. The data requirements are summarized as follows The personnel involved in the league include players coaches managers and umpires. Each is identified by a unique personnel id. They are also described by their first and last names along with the date and place of birth. Players are further described by other attributes such as their batting orientation and have a lifetime batting average . Within the players group is a subset of players called pitchers. Pitchers have a lifetime ERA associated with them. Teams are uniquely identified by their names. Teams are also described by the city in which they are located and the division and league in which they play . Teams have one manager a number of coaches and a number of players. Games are played between two teams with one designated as the home team and the other the visiting team on a particular date. The score are recorded for each team. The team with the most runs is declared the winner of the game. With each finished game a winning pitcher and a losing pitcher are recorded. In case there is a save awarded the save pitcher is also recorded. With each finished game the number of hits obtained by each player is also recorded. Design an Enhanced Entity Relationship diagram for the BASEBALL database and enter the design using a data modeling tool such as ERwin or Rational Rose. Consider the EER diagram for the UNIVERSITY database shown in Figure Enter this design using a data modeling tool such as ERwin or Rational Rose. Make a list of the differences in notation between the diagram in the text and the corresponding equivalent diagrammatic notation you end up using with the tool. Consider the EER diagram for the small AIRPORT database shown in Figure Build this design using a data modeling tool such as ERwin or Rational Rose. Be careful as to how you model the category OWNER in this diagram. Consider the UNIVERSITY database described in Exercise You already developed an ER schema for this database using a data modeling tool such as Laboratory Exercises ERwin or Rational Rose in Lab Exercise Modify this diagram by classifying COURSES as either UNDERGRADCOURSES or GRADCOURSES and INSTRUCTORS as either JUNIORPROFESSORS or SENIORPROFESSORS. Include appropriate attributes for these new entity types. Then establish relationships indicating that junior instructors teach undergraduate courses while senior instructors teach graduate courses. Selected Bibliography Many papers have proposed conceptual or semantic data models. We give a representative list here. One group of papers including Abrial Senko’s DIAM model the NIAM method model of Elmasri et al. Smith and Smith present the concepts of generalization and aggregation. The semantic data model of Hammer and McLeod introduced the concepts of class subclass lattices as well as other advanced modeling concepts. A survey of semantic data modeling appears in Hull and King Eick discusses design and transformations of conceptual schemas. Analysis of constraints for n ary relationships is given in Soutou UML is described in detail in Booch Rumbaugh and Jacobson Fowler and Scott and Stevens and Pooley give concise introductions to UML concepts. Fensel discuss the Semantic Web and application of ontologies. Uschold and Gruninger and Gruber discuss ontologies. The June issue of Communications of the ACM is devoted to ontology concepts and applications. Fensel is a book that discusses ontologies and e commerce. Chapter The Enhanced Entity Relationship Model Relational Database Design by ER and EER to Relational Mapping This chapter discusses how to design a relational database schema based on a conceptual schema design. Figure presented a high level view of the database design process and in this chapter we focus on the logical database design or data model mapping step of database design. We present the procedures to create a relational schema from an Entity Relationship or an Enhanced ER schema. Our discussion relates the constructs of the ER and EER models presented in Chapters and to the constructs of the relational model presented in Chapters through Many computeraided software engineering tools are based on the ER or EER models or other similar models as we have discussed in Chapters and Many tools use ER or EER diagrams or variations to develop the schema graphically and then convert it automatically into a relational database schema in the DDL of a specific relational DBMS by employing algorithms similar to the ones presented in this chapter. We outline a seven step algorithm in Section to convert the basic ER model constructs entity types binary relationships n ary relationships and attributes into relations. Then in Section we continue the mapping algorithm by describing how to map EER model constructs specialization generalization and union types into relations. Section summarizes the chapter. chapter Chapter Relational Database Design by ER and EER to Relational Mapping Relational Database Design Using ER to Relational Mapping ER to Relational Mapping Algorithm In this section we describe the steps of an algorithm for ER to relational mapping. We use the COMPANY database example to illustrate the mapping procedure. The COMPANY ER schema is shown again in Figure and the corresponding COMPANY relational database schema is shown in Figure to illustrate the mapEMPLOYEE Fname Minit Lname Name Address Sex Salary Ssn Bdate Supervisor Supervisee SUPERVISION N Hours WORKSON CONTROLS M N DEPENDENTSOF Name Location N PROJECT DEPARTMENT Locations Name Number Number Numberofemployees MANAGES Startdate WORKSFOR N N DEPENDENT Name Sex Birthdate Relationship Figure The ER conceptual schema diagram for the COMPANY database. Relational Database Design Using ER to Relational Mapping DEPARTMENT Fname Minit Lname Ssn Bdate Address Sex Salary Superssn Dno EMPLOYEE DEPTLOCATIONS Dnumber Dlocation PROJECT Pname Pnumber Plocation Dnum WORKSON Essn Pno Hours DEPENDENT Essn Dependentname Sex Bdate Relationship Dname Dnumber Mgrssn Mgrstartdate Figure Result of mapping the COMPANY ER schema into a relational database schema. ping steps. We assume that the mapping will create tables with simple single valued attributes. The relational model constraints defined in Chapter which include primary keys unique keys and referential integrity constraints on the relations will also be specified in the mapping results. Step Mapping of Regular Entity Types. For each regular entity type E in the ER schema create a relation R that includes all the simple attributes of E. Include only the simple component attributes of a composite attribute. Choose one of the key attributes of E as the primary key for R. If the chosen key of E is a composite then the set of simple attributes that form it will together form the primary key of R. If multiple keys were identified for E during the conceptual design the information describing the attributes that form each additional key is kept in order to specify secondary keys of relation R. Knowledge about keys is also kept for indexing purposes and other types of analyses. In our example we create the relations EMPLOYEE DEPARTMENT and PROJECT in Figure to correspond to the regular entity types EMPLOYEE DEPARTMENT and PROJECT in Figure The foreign key and relationship attributes if any are not included yet they will be added during subsequent steps. These include the DEPARTMENT Fname Minit Lname Ssn Bdate Address Sex Salary EMPLOYEE WORKSON Essn Pno Hours Dname Dnumber DEPTLOCATIONS Dnumber Dlocation PROJECT Pname Pnumber Plocation DEPENDENT Essn Dependentname Sex Bdate Relationship Chapter Relational Database Design by ER and EER to Relational Mapping Figure Illustration of some mapping steps. Entity relations after step Additional weak entity relation after step Relationship relation after step Relation representing multivalued attribute after step attributes Superssn and Dno of EMPLOYEE Mgrssn and Mgrstartdate of DEPARTMENT and Dnum of PROJECT. In our example we choose Ssn Dnumber and Pnumber as primary keys for the relations EMPLOYEE DEPARTMENT and PROJECT respectively. Knowledge that Dname of DEPARTMENT and Pname of PROJECT are secondary keys is kept for possible use later in the design. The relations that are created from the mapping of entity types are sometimes called entity relations because each tuple represents an entity instance. The result after this mapping step is shown in Figure Step Mapping of Weak Entity Types. For each weak entity type W in the ER schema with owner entity type E create a relation R and include all simple attributes of W as attributes of R. In addition include as foreign key attributes of R the primary key attribute of the relation that correspond to the owner entity type this takes care of mapping the identifying relationship type of W. The primary key of R is the combination of the primary key of the owner and the partial key of the weak entity type W if any. If there is a weak entity type whose owner is also a weak entity type then should be mapped before to determine its primary key first. In our example we create the relation DEPENDENT in this step to correspond to the weak entity type DEPENDENT option for the referential triggered action of the relationship type R as attributes of S. In our example we map the relationship type MANAGES from Figure by choosing the participating entity type DEPARTMENT to serve in the role of S because its participation in the MANAGES relationship type is total . We include the primary key of the EMPLOYEE relation as foreign key in the DEPARTMENT relation and rename it Mgrssn. We also include the simple attribute Startdate of the MANAGES relationship type in the DEPARTMENT relation and rename it Mgrstartdate because each Chapter Relational Database Design by ER and EER to Relational Mapping tuple in R represents a relationship instance that relates one tuple from S with one tuple from T. The relation R will include the primary key attributes of S and T as foreign keys to S and T. The primary key of R will be one of the two foreign keys and the other foreign key will be a unique key of R. The drawback is having an extra relation and requiring an extra join operation when combining related tuples from the tables. Step Mapping of Binary Relationship Types. For each regular binary relationship type R identify the relation S that represents the participating entity type at the N side of the relationship type. Include as foreign key in S the primary key of the relation T that represents the other entity type participating in R we do this because each entity instance on the N side is related to at most one entity instance on the of the relationship type. Include any simple attributes of the relationship type as attributes of S. In our example we now map the relationship types WORKSFOR CONTROLS and SUPERVISION from Figure For WORKSFOR we include the primary key Dnumber of the DEPARTMENT relation as foreign key in the EMPLOYEE relation and call it Dno. For SUPERVISION we include the primary key of the EMPLOYEE relation as foreign key in the EMPLOYEE relation itself because the relationship is recursive and call it Superssn. The CONTROLS relationship is mapped to the foreign key attribute Dnum of PROJECT which references the primary key Dnumber of the DEPARTMENT relation. These foreign keys are shown in Figure An alternative approach is to use the relationship relation option as in the third option for binary relationships. We create a separate relation R whose attributes are the primary keys of S and T which will also be foreign keys to S and T. The primary key of R is the same as the primary key of S. This option can be used if few tuples in S participate in the relationship to avoid excessive NULL values in the foreign key. Step Mapping of Binary M N Relationship Types. For each binary M N relationship type R create a new relation S to represent R. Include as foreign key attributes in S the primary keys of the relations that represent the participating entity types their combination will form the primary key of S. Also include any simple attributes of the M N relationship type as attributes of S. Notice that we cannot represent an M N relationship type by a single foreign key attribute in one of the participating relations because of the M N cardinality ratio we must create a separate relationship relation S. In our example we map the M N relationship type WORKSON from Figure by creating the relation WORKSON in Figure We include the primary keys of the PROJECT and EMPLOYEE relations as foreign keys in WORKSON and rename them Pno and Essn respectively. We also include an attribute Hours in WORKSON to represent the Hours attribute of the relationship type. The primary key of the WORKSON relation is the combination of the foreign key attributes Essn Pno . This relationship relation is shown in Figure Relational Database Design Using ER to Relational Mapping The propagate option for the referential triggered action approach as we discussed earlier. This alternative is particularly useful when few relationship instances exist in order to avoid NULL values in foreign keys. In this case the primary key of the relationship relation will be only one of the foreign keys that reference the participating entity relations. For a relationship the primary key of the relationship relation will be the foreign key that references the entity relation on the N side. For a relationship either foreign key can be used as the primary key of the relationship relation. Step Mapping of Multivalued Attributes. For each multivalued attribute A create a new relation R. This relation R will include an attribute corresponding to A plus the primary key attribute K as a foreign key in R of the relation that represents the entity type or relationship type that has A as a multivalued attribute. The primary key of R is the combination of A and K. If the multivalued attribute is composite we include its simple components. In our example we create a relation DEPTLOCATIONS option for the referential triggered action as attributes of S. The primary key of S is usually a combination of all the foreign keys that reference the relations representing the participating entity types. However if the cardinality constraints on any of the entity types E participating in R is then the primary key of S should not include the foreign key attribute that references the relation E corresponding to E . For example consider the relationship type SUPPLY in Figure This can be mapped to the relation SUPPLY shown in Figure whose primary key is the combination of the three foreign keys Sname Partno Projname . Discussion and Summary of Mapping for ER Model Constructs Table summarizes the correspondences between ER and relational model constructs and constraints. One of the main points to note in a relational schema in contrast to an ER schema is that relationship types are not represented explicitly instead they are represented by having two attributes A and B one a primary key and the other a foreign key included in two relations S and T. Two tuples in S and T are related when they have the same value for A and B. By using the EQUIJOIN operation over and we can combine all pairs of related tuples from S and T and materialize the relationship. When a binary or relationship type is involved a single join operation is usually needed. For a binary M N relationship type two join operations are needed whereas for n ary relationship types n joins are needed to fully materialize the relationship instances. Relational Database Design Using ER to Relational Mapping Table Correspondence between ER and Relational Models ER MODEL RELATIONAL MODEL Entity type Entity relation or relationship type Foreign key M N relationship type Relationship relation and two foreign keys n ary relationship type Relationship relation and n foreign keys Simple attribute Attribute Composite attribute Set of simple component attributes Multivalued attribute Relation and foreign key Value set Domain Key attribute Primary key For example to form a relation that includes the employee name project name and hours that the employee works on each project we need to connect each EMPLOYEE tuple to the related PROJECT tuples via the WORKSON relation in Figure Hence we must apply the EQUIJOIN operation to the EMPLOYEE and WORKSON relations with the join condition Ssn Essn and then apply another EQUIJOIN operation to the resulting relation and the PROJECT relation with join condition Pno Pnumber. In general when multiple relationships need to be traversed numerous join operations must be specified. A relational database user must always be aware of the foreign key attributes in order to use them correctly in combining related tuples from two or more relations. This is sometimes considered to be a drawback of the relational data model because the foreign key primary key correspondences are not always obvious upon inspection of relational schemas. If an EQUIJOIN is performed among attributes of two relations that do not represent a foreign key primary key relationship the result can often be meaningless and may lead to spurious data. For example the reader can try joining the PROJECT and DEPTLOCATIONS relations on the condition Dlocation Plocation and examine the result for an attribute in a single tuple. For example because department has three locations three tuples exist in the DEPTLOCATIONS relation in Figure each tuple specifies one of the locations. In our example we apply EQUIJOIN to DEPTLOCATIONS and DEPARTMENT on the Dnumber attribute to get the values of all locations along with other DEPARTMENT attributes. In the resulting relation the values of the other DEPARTMENT attributes are repeated in separate tuples for every location that a department has. Chapter Relational Database Design by ER and EER to Relational Mapping The basic relational algebra does not have a NEST or COMPRESS operation that would produce a set of tuples of the form ‘Houston’ ‘Stafford’ ‘Bellaire’ ‘Sugarland’ ‘Houston’ from the DEPTLOCATIONS relation in Figure This is a serious drawback of the basic normalized or flat version of the relational model. The object data model and object relational systems such as the SECRETARY TECHNICIAN ENGINEER subclasses of EMPLOYEE in Figure We can add a further step to our ER to relational mapping algorithm from Section which has seven steps to handle the mapping of specialization. Step which follows gives the most common options other mappings are also possible. We discuss the conditions under which each option should be used. We use Attrs to denote the attributes of relation R and PK to denote the primary key of R. First we describe the mapping formally then we illustrate it with examples. Step Options for Mapping Specialization or Generalization. Convert each specialization with m subclasses Sm and superclass C where the attributes of C are k and k is the key into relation schemas using one of the following options Option Multiple relations superclass and subclasses. Create a relation L for C with attributes Attrs k an and PK k. Create a relation Li for each subclass Si ≤ i ≤ m with the attributes Attrs k ∪ attributes of Si and PK k. This option works for any specialization . Option Multiple relations subclass relations only. Create a relation Li for each subclass Si ≤ i ≤ m with the attributes Attrs attributes of Si ∪ k an and PK k. This option only works for a specialization whose subclasses are total one of the subclasses . Additionally it is only recommended if the specialization has the disjointedness constraint k an ∪ attributes of ∪ ∪ attributes of Sm ∪ t and PK k. The attribute t is called a type EMPLOYEE Ssn Ssn Ssn Ssn Vehicleid Vehicleid Partno Mapping EER Model Constructs to Relations discriminating attribute whose value indicates the subclass to which each tuple belongs if any. This option works only for a specialization whose subclasses are disjoint and has the potential for generating many NULL values if many specific attributes exist in the subclasses. Option Single relation with multiple type attributes. Create a single relation schema L with attributes Attrs k an ∪ attributes of ∪ ∪ attributes of Sm ∪ t t t m and PK k. Each t i ≤ i ≤ m is a Boolean type attribute indicating whether a tuple belongs to subclass Si . This option is used for a specialization whose subclasses are overlapping . Options and can be called the multiple relation options whereas options and can be called the single relation options. Option creates a relation L for the superclass C and its attributes plus a relation Li for each subclass Si each Li includes the specific attributes of Si plus the primary key of the superclass C which is propagated to Li and becomes its primary key. It also becomes a foreign key to the superclass relation. An EQUIJOIN operation on the primary key between any Li and L produces all the specific and inherited attributes of the entities in Si . This option is illustrated in Figure for the EER schema in Figure Option works for any constraints on the specialization disjoint or overlapping total or partial. Notice that the constraint π k ⊆ π k must hold for each Li . This specifies a foreign key from each Li to L as well as an inclusion dependency Li .k Mapping the EER schema in Figure using option Mapping the EER schema in Figure using option Mapping the EER schema in Figure using option Mapping Figure using option with Boolean type fields Mflag and Pflag. Chapter Relational Database Design by ER and EER to Relational Mapping In option the EQUIJOIN operation between each subclass and the superclass is built into the schema and the relation L is done away with as illustrated in Figure for the EER specialization in Figure This option works well only when both the disjoint and total constraints hold. If the specialization is not total an entity that does not belong to any of the subclasses Si is lost. If the specialization is not disjoint an entity belonging to more than one subclass will have its inherited attributes from the superclass C stored redundantly in more than one Li . With option no relation holds all the entities in the superclass C consequently we must apply an OUTER UNION operation attribute t to indicate to which of the m subclasses each tuple belongs hence the domain of t could be m . If the specialization is partial t can have NULL values in tuples that do not belong to any subclass. If the specialization is attribute defined that attribute serves the purpose of t and t is not needed this option is illustrated in Figure for the EER specialization in Figure Option is designed to handle overlapping subclasses by including m Boolean type fields one for each subclass. It can also be used for disjoint subclasses. Each type field t i can have a domain yes no where a value of yes indicates that the tuple is a member of subclass Si . If we use this option for the EER specialization in Figure we would include three types attributes Isasecretary Isaengineer and Isatechnician instead of the Jobtype attribute in Figure Notice that it is also possible to create a single type attribute of m bits instead of the m type fields. Figure shows the mapping of the specialization from Figure using option When we have a multilevel specialization hierarchy or lattice we do not have to follow the same mapping option for all the specializations. Instead we can use one mapping option for part of the hierarchy or lattice and other options for other parts. Figure shows one possible mapping into relations for the EER lattice in Figure Here we used option for PERSON EMPLOYEE ALUMNUS STUDENT option for EMPLOYEE STAFF FACULTY STUDENTASSISTANT by including the type attribute Employeetype and option for STUDENTASSISTANT RESEARCHASSISTANT TEACHING ASSISTANT by including the type attributes Taflag and Raflag in EMPLOYEE STUDENT Mapping EER Model Constructs to Relations EMPLOYEE Salary Employeetype Position Rank Percenttime Raflag Taflag Project Course STUDENT Majordept Gradflag Undergradflag Degreeprogram Class Studentassistflag Name Birthdate Sex Address PERSON Ssn ALUMNUS ALUMNUSDEGREES Ssn Year Major Ssn Ssn Ssn Degree Figure Mapping the EER specialization lattice in Figure using multiple options. STUDENTASSISTANT by including the type attributes Studentassistflag in STUDENT and STUDENT GRADUATESTUDENT UNDERGRADUATESTUDENT by including the type attributes Gradflag and Undergradflag in STUDENT. In Figure all attributes whose names end with type or flag are type fields. Mapping of Shared Subclasses A shared subclass such as ENGINEERINGMANAGER in Figure is a subclass of several superclasses indicating multiple inheritance. These classes must all have the same key attribute otherwise the shared subclass would be modeled as a category as we discussed in Section We can apply any of the options discussed in step to a shared subclass subject to the restrictions discussed in step of the mapping algorithm. In Figure options and are used for the shared subclass STUDENTASSISTANT. Option is used in the EMPLOYEE relation and option is used in the STUDENT relation . Mapping of Categories We add another step to the mapping procedure step handle categories. A category is a subclass of the union of two or more superclasses that can have different keys because they can be of different entity types in Figure to relations. Step Mapping of Union Types . For mapping a category whose defining superclasses have different keys it is customary to specify a new key attribute called a surrogate key when creating a relation to correspond to the category. The keys of the defining classes are different so we cannot use any one of them exclusively to identify all entities in the category. In our example in Figure we create a relation OWNER to correspond to the OWNER category as illustrated in Figure and include any attributes of the category in this relation. The primary key of the OWNER relation is the surrogate key which we called Ownerid. We also include the surrogate key attribute Ownerid as foreign key in each relation corresponding to a superclass of the category to specify the correspondence in values between the surrogate key and the key of each superclass. Notice that if a particular PERSON entity is not a member of OWNER it would have a NULL value for its Ownerid attribute in its corresponding tuple in the PERSON relation and it would not have a tuple in the OWNER relation. It is also recommended to add a type attribute . Exercises For a category whose superclasses have the same key such as VEHICLE in Figure there is no need for a surrogate key. The mapping of the REGISTEREDVEHICLE category which illustrates this case is also shown in Figure Summary In Section we showed how a conceptual schema design in the ER model can be mapped to a relational database schema. An algorithm for ER to relational mapping was given and illustrated by examples from the COMPANY database. Table summarized the correspondences between the ER and relational model constructs and constraints. Next we added additional steps to the algorithm in Section for mapping the constructs from the EER model into the relational model. Similar algorithms are incorporated into graphical database design tools to create a relational schema from a conceptual schema design automatically. Review Questions Discuss the correspondences between the ER model constructs and the relational model constructs. Show how each ER model construct can be mapped to the relational model and discuss any alternative mappings. Discuss the options for mapping EER model constructs to relations. Exercises Try to map the relational schema in Figure into an ER schema. This is part of a process known as reverse engineering where a conceptual schema is created for an existing implemented database. State any assumptions you make. Figure shows an ER schema for a database that can be used to keep track of transport ships and their locations for maritime authorities. Map this schema into a relational schema and specify all primary keys and foreign keys. Map the BANK ER schema of Exercise in Chapters and and data dependency theory and relational normalization algorithms in Chapters and The overall database design activity has to undergo a systematic process called the design methodology whether the target database is managed by an RDBMS an object database management system . To accommodate DBMSs many organizations have created the position of database administrator and database administration departments to oversee and control database life cycle activities. Similarly information technology and information resource management departments have The Role of Information Systems in Organizations been recognized by large organizations as being key to successful business management for the following reasons Data is regarded as a corporate resource and its management and control is considered central to the effective working of the organization. More functions in organizations are computerized increasing the need to keep large volumes of data available in an up to the minute current state. As the complexity of the data and applications grows complex relationships among the data need to be modeled and maintained. There is a tendency toward consolidation of information resources in many organizations. Many organizations are reducing their personnel costs by letting end users perform business transactions. This is evident with travel services financial services higher education government and many other types of services. This trend was realized early on by online retail goods outlets and customerto business electronic commerce such as and eBay. In these organizations a publicly accessible and updatable operational database must be designed and made available for the customer transactions. Many capabilities provided by database systems have made them integral components in computer based information systems. The following are some of the key features that they offer Integrating data across multiple applications into a single database. Support for developing new applications in a short time by using high level languages like SQL. Providing support for casual access for browsing and querying by managers while supporting major production level transaction processing for customers. From the early through the the move was toward creating large centralized repositories of data managed by a single centralized DBMS. Since then the trend has been toward utilizing distributed systems because of the following developments Personal computers and database system like software products such as Excel Visual FoxPro Access and SQL Anywhere and public domain products such as MySQL and PostgreSQL are being heavily utilized by users who previously belonged to the category of casual and occasional database users. Many administrators secretaries engineers scientists architects and students belong to this category. As a result the practice of creating personal databases is gaining popularity. It is sometimes possible to check out a copy of part of a large database from a mainframe computer or a database server work on it from a personal workstation and then restore it on the mainframe. Similarly users can design and create their own databases and then merge them into a larger one. Chapter Practical Database Design Methodology and Use of UML Diagrams The advent of distributed and client server DBMSs and OracleDesigner and Oracle Developer Suite are being used with built in facilities to link applications to multiple back end database servers. Many organizations now use data dictionary systems or information repositories which are mini DBMSs that manage meta data that is data that describes the database structure constraints applications authorizations users and so on. These are often used as an integral tool for information resource management. A useful data dictionary system should store and manage the following types of information a. Descriptions of the schemas of the database system. b. Detailed information on physical database design such as storage structures access paths and file and record sizes. c. Descriptions of the types of database users their responsibilities and their access rights. d. High level descriptions of the database transactions and applications and of the relationships of users to transactions. e. The relationship between database transactions and the data items referenced by them. This is useful in determining which transactions are affected when certain data definitions are changed. f. Usage statistics such as frequencies of queries and transactions and access counts to different portions of the database. g. The history of any changes made to the database and applications and documentation that describes the reasons for these changes. This is sometimes referred to as data provenance. This meta data is available to DBAs designers and authorized users as online system documentation. This improves the control of DBAs over the information system as well as the users’ understanding and use of the system. The advent of data warehousing technology which includes all resources that are involved in the collection management use and dissemination of the information resources of the organization. In a computerized environment these resources include the data itself the DBMS software the computer system hardware and storage media the personnel who use and manage the data the application programs that accesses and updates the data and the application programmers who develop these applications. Thus the database system is part of a much larger organizational information system. In this section we examine the typical life cycle of an information system and how the database system fits into this life cycle. The information system life cycle has been called the macro life cycle whereas the database system life cycle has been referred to as the micro life cycle. The distinction between them is becoming less pronounced for information systems where databases are a major integral component. The macro life cycle typically includes the following phases Feasibility analysis. This phase is concerned with analyzing potential application areas identifying the economics of information gathering and dissemination performing preliminary cost benefit studies determining the complexity of data and processes and setting up priorities among applications. Requirements collection and analysis. Detailed requirements are collected by interacting with potential users and user groups to identify their particular problems and needs. Interapplication dependencies communication and reporting procedures are identified. Design. This phase has two aspects the design of the database system and the design of the application systems that use and process the database through retrievals and updates. Implementation. The information system is implemented the database is loaded and the database transactions are implemented and tested. Validation and acceptance testing. The acceptability of the system in meeting users’ requirements and performance criteria is validated. The system is tested against performance criteria and behavior specifications. Chapter Practical Database Design Methodology and Use of UML Diagrams Deployment operation and maintenance. This may be preceded by conversion of users from an older system as well as by user training. The operational phase starts when all system functions are operational and have been validated. As new requirements or applications crop up they pass through the previous phases until they are validated and incorporated into the system. Monitoring of system performance and system maintenance are important activities during the operational phase. The Database Application System Life Cycle Activities related to the micro life cycle which focuses on the database application system include the following System definition. The scope of the database system its users and its applications are defined. The interfaces for various categories of users the response time constraints and storage and processing needs are identified. Database design. A complete logical and physical design of the database system on the chosen DBMS is prepared. Database implementation. This comprises the process of specifying the conceptual external and internal database definitions creating the database files and implementing the software applications. Loading or data conversion. The database is populated either by loading the data directly or by converting existing files into the database system format. Application conversion. Any software applications from a previous system are converted to the new system. Testing and validation. The new system is tested and validated. Testing and validation of application programs can be a very involved process and the techniques that are employed are usually covered in software engineering courses. There are automated tools that assist in this process but a discussion is outside the scope of this textbook. Operation. The database system and its applications are put into operation. Usually the old and the new systems are operated in parallel for a period of time. Monitoring and maintenance. During the operational phase the system is constantly monitored and maintained. Growth and expansion can occur in both data content and software applications. Major modifications and reorganizations may be needed from time to time. Activities and are part of the design and implementation phases of the larger information system macro life cycle. Our emphasis in Section is on activities and which cover the database design and implementation phases. Most databases in organizations undergo all of the preceding life cycle activities. The conversion activities and are not applicable when both the database and the applications are new. When an organization moves from an established system to a new one The Database Design and Implementation Process activities and tend to be very time consuming and the effort to accomplish them is often underestimated. In general there is often feedback among the various steps because new requirements frequently arise at every stage. Figure shows the feedback loop affecting the conceptual and logical design phases as a result of system implementation and tuning. The Database Design and Implementation Process Now we focus on activities and of the database application system life cycle which are database design and implementation. The problem of database design can be stated as follows Design the logical and physical structure of one or more databases to accommodate the information needs of the users in an organization for a defined set of applications. Phase Requirements collection and analysis Phase Conceptual database design Phase Choice o f D B M S Phase Data model mapping Phase Physical design Phase System implementation and tuning Data content structure and constraints Data requirements Conceptual Schema design Logical Schema and view design Internal Schema design DDL statements SDL statements Database applications Processing requirements Transaction and application design Transaction and application implementation Frequencies performance constraints Figure Phases of database design and implementation for large databases. Chapter Practical Database Design Methodology and Use of UML Diagrams The goals of database design are multiple Satisfy the information content requirements of the specified users and applications. Provide a natural and easy to understand structuring of the information. Support processing requirements and any performance objectives such as response time processing time and storage space. These goals are very hard to accomplish and measure and they involve an inherent tradeoff if one attempts to achieve more naturalness and understandability of the model it may be at the cost of performance. The problem is aggravated because the database design process often begins with informal and incomplete requirements. In contrast the result of the design activity is a rigidly defined database schema that cannot easily be modified once the database is implemented. We can identify six main phases of the overall database design and implementation process Requirements collection and analysis Conceptual database design Choice of a DBMS Data model mapping Physical database design Database system implementation and tuning The design process consists of two parallel activities as illustrated in Figure The first activity involves the design of the data content structure and constraints of the database the second relates to the design of database applications. To keep the figure simple we have avoided showing most of the interactions between these sides but the two activities are closely intertwined. For example by analyzing database applications we can identify data items that will be stored in the database. In addition the physical database design phase during which we choose the storage structures and access paths of database files depends on the applications that will use these files for querying and updating. On the other hand we usually specify the design of database applications by referring to the database schema constructs which are specified during the first activity. Clearly these two activities strongly influence one another. Traditionally database design methodologies have primarily focused on the first of these activities whereas software design has focused on the second this may be called data driven versus process driven design. It now is recognized by database designers and software engineers that the two activities should proceed hand in hand and design tools are increasingly combining them. The six phases mentioned previously do not typically progress strictly in sequence. In many cases we may have to modify the design from an earlier phase during a later phase. These feedback loops among phases and also within phases are common. We show only a couple of feedback loops in Figure but many more exist between various phases. We have also shown some interaction between the data and the process sides of the figure many more interactions exist in reality. Phase in Figure involves collecting information about the intended use of the database The Database Design and Implementation Process and Phase concerns database implementation and redesign. The heart of the database design process comprises Phases and we briefly summarize these phases Conceptual database design the conceptual schema from the high level data model used in Phase into the data model of the chosen DBMS. We can start this phase after choosing a specific type of DBMS for example if we decide to use some relational DBMS but have not yet decided on which particular one. We call the latter system independent logical design. In terms of the three level DBMS architecture discussed in Chapter the result of this phase is a conceptual schema in the chosen data model. In addition the design of external schemas for specific applications is often done during this phase. Physical database design . More recently techniques have been developed such as Contextual Design which involve the designers becoming immersed in the workplace in which the application is to be used. To help customer representatives better understand the proposed system it is common to walk through workflow or transaction scenarios or to create a mock up rapid prototype of the application. The preceding modes help structure and refine requirements but leave them still in an informal state. To transform requirements into a better structured representation requirements specification techniques are used. These include object The Database Design and Implementation Process oriented analysis data flow diagrams and the refinement of application goals. These methods use diagramming techniques for organizing and presenting information processing requirements. Additional documentation in the form of text tables charts and decision requirements usually accompanies the diagrams. There are techniques that produce a formal specification that can be checked mathematically for consistency and what if symbolic analyses. These methods may become standard in the future for those parts of information systems that serve mission critical functions and which therefore must work as planned. The modelbased formal specification methods of which the Z notation and methodology is a prominent example can be thought of as extensions of the ER model and are therefore the most applicable to information system design. Some computer aided techniques called Upper CASE tools have been proposed to help check the consistency and completeness of specifications which are usually stored in a single repository and can be displayed and updated as the design progresses. Other tools are used to trace the links between requirements and other design entities such as code modules and test cases. Such traceability databases are especially important in conjunction with enforced change management procedures for systems where the requirements change frequently. They are also used in contractual projects where the development organization must provide documentary evidence to the customer that all the requirements have been implemented. The requirements collection and analysis phase can be quite time consuming but it is crucial to the success of the information system. Correcting a requirements error is more expensive than correcting an error made during implementation because the effects of a requirements error are usually pervasive and much more downstream work has to be reimplemented as a result. Not correcting a significant error means that the system will not satisfy the customer and may not even be used at all. Requirements gathering and analysis is the subject of entire books. Phase Conceptual Database Design The second phase of database design involves two parallel The first activity conceptual schema design examines the data requirements resulting from Phase and produces a conceptual database schema. The second activity transaction and application design examines the database applications analyzed in Phase and produces high level specifications for these applications. Phase Conceptual Schema Design. The conceptual schema produced by this phase is usually contained in a DBMS independent high level data model for the following reasons The goal of conceptual schema design is a complete understanding of the database structure meaning interrelationships and constraints. phase of design is discussed in great detail in the first seven chapters of Batini et al. we summarize that discussion here. Chapter Practical Database Design Methodology and Use of UML Diagrams This is best achieved independently of a specific DBMS because each DBMS typically has idiosyncrasies and restrictions that should not be allowed to influence the conceptual schema design. The conceptual schema is invaluable as a stable description of the database contents. The choice of DBMS and later design decisions may change without changing the DBMS independent conceptual schema. A good understanding of the conceptual schema is crucial for database users and application designers. Use of a high level data model that is more expressive and general than the data models of individual DBMSs is therefore quite important. The diagrammatic description of the conceptual schema can serve as a vehicle of communication among database users designers and analysts. Because high level data models usually rely on concepts that are easier to understand than lower level DBMS specific data models or syntactic definitions of data any communication concerning the schema design becomes more exact and more straightforward. In this phase of database design it is important to use a conceptual high level data model with the following characteristics Expressiveness. The data model should be expressive enough to distinguish different types of data relationships and constraints. Simplicity and understandability. The model should be simple enough for typical nonspecialist users to understand and use its concepts. Minimality. The model should have a small number of basic concepts that are distinct and nonoverlapping in meaning. Diagrammatic representation. The model should have a diagrammatic notation for displaying a conceptual schema that is easy to interpret. Formality. A conceptual schema expressed in the data model must represent a formal unambiguous specification of the data. Hence the model concepts must be defined accurately and unambiguously. Some of these requirements the first one in particular sometimes conflict with the other requirements. Many high level conceptual models have been proposed for database design model presented in Chapter and we will assume that it is being used in this phase. Conceptual schema design including data modeling is becoming an integral part of object oriented analysis and design methodologies. The UML has class diagrams that are largely based on extensions of the EER model. Approaches to Conceptual Schema Design. For conceptual schema design we must identify the basic components of the schema the entity types relationship types and attributes. We should also specify key attributes cardinality and participation constraints on relationships weak entity types and specialization generalization hierarchies lattices. There are two approaches to designing the conceptual schema which is derived from the requirements collected during Phase The Database Design and Implementation Process The first approach is the centralized schema design approach in which the requirements of the different applications and user groups from Phase are merged into a single set of requirements before schema design begins. A single schema corresponding to the merged set of requirements is then designed. When many users and applications exist merging all the requirements can be an arduous and time consuming task. The assumption is that a centralized authority the DBA is responsible for deciding how to merge the requirements and for designing the conceptual schema for the whole database. Once the conceptual schema is designed and finalized external schemas for the various user groups and applications can be specified by the DBA. The second approach is the view integration approach in which the requirements are not merged. Rather a schema is designed for each user group or application based only on its own requirements. Thus we develop one high level schema for each such user group or application. During a subsequent view integration phase these schemas are merged or integrated into a global conceptual schema for the entire database. The individual views can be reconstructed as external schemas after view integration. The main difference between the two approaches lies in the manner and stage in which multiple views or requirements of the many users and applications are reconciled and merged. In the centralized approach the reconciliation is done manually by the DBA staff prior to designing any schemas and is applied directly to the requirements collected in Phase This places the burden to reconcile the differences and conflicts among user groups on the DBA staff. The problem has been typically dealt with by using external consultants design experts who apply their specific methods for resolving these conflicts. Because of the difficulties of managing this task the view integration approach has been proposed as an alternative technique. In the view integration approach each user group or application actually designs its own conceptual schema from its requirements with assistance from the DBA staff. Then an integration process is applied to these schemas by the DBA to form the global integrated schema. Although view integration can be done manually its application to a large database involving dozens of user groups requires a methodology and the use of automated tools. The correspondences among the attributes entity types and relationship types in various views must be specified before the integration can be applied. Additionally problems such as integrating conflicting views and verifying the consistency of the specified interschema correspondences must be dealt with. Strategies for Schema Design. Given a set of requirements whether for a single user or for a large user community we must create a conceptual schema that satisfies these requirements. There are various strategies for designing such a schema. Most strategies follow an incremental approach that is they start with some important schema constructs derived from the requirements and then they incrementally modify refine and build on them. We now discuss some of these strategies Top down strategy. We start with a schema containing high level abstractions and then apply successive top down refinements. For example we may Chapter Practical Database Design Methodology and Use of UML Diagrams specify only a few high level entity types and then as we specify their attributes split them into lower level entity types and specify the relationships. The process of specialization to refine an entity type into subclasses that we illustrated in Sections and and a relationship between them. Refinement typically forces a designer to ask more questions and extract more constraints and details for example the cardinality ratios between COURSE and INSTRUCTOR are obtained during refinement. Figure shows the bottom up refinement primitive of generating relationships among the entity types FACULTY and STUDENT. Two relationships are identified ADVISES and COMMITTEECHAIROF. The bottom up refinement using categorization is illustrated in Figure where the new concept of VEHICLEOWNER is discovered from the existing entity types FACULTY STAFF and STUDENT this process of creating a category and the related diagrammatic notation follows what we introduced in Section Schema Integration. For large databases with many expected users and applications the view integration approach of designing individual schemas and then merging them can be used. Because the individual views can be kept relatively small design of the schemas is simplified. However a methodology for integrating the views into a global database schema is needed. Schema integration can be divided into the following subtasks The Database Design and Implementation Process FACULTY TEACHES COURSE COURSE FACULTY TEACHES OFFERS OFFEREDBY SEMINAR Name INSTRUCTOR Course# Sec# Semester Instructor COURSEOFFERING Course# Sec# Semester COURSE Figure Examples of topdown refinement. Generating a new entity type. Decomposing an entity type into two entity types and a relationship type. Identifying correspondences and conflicts among the schemas. Because the schemas are designed individually it is necessary to specify constructs in the schemas that represent the same real world concept. These correspondences must be identified before integration can proceed. During this process several types of conflicts among the schemas may be discovered a. Naming conflicts. These are of two types synonyms and homonyms. A synonym occurs when two schemas use different names to describe the same concept for example an entity type CUSTOMER in one schema may describe the same concept as an entity type CLIENT in another schema. A homonym occurs when two schemas use the same name to describe different concepts for example an entity type PART may represent computer parts in one schema and furniture parts in another schema. b. Type conflicts. The same concept may be represented in two schemas by different modeling constructs. For example the concept of a Chapter Practical Database Design Methodology and Use of UML Diagrams FACULTY FACULTY STUDENT STUDENT FACULTY STAFF STUDENT ADVISES VEHICLEOWNER FACULTY STAFF STUDENT ISA FACULTY ISA STAFF ISA STUDENT PARKINGDECAL COMMITTEE CHAIROF PARKINGDECAL Figure Examples of bottom up refinement. Discovering and adding new relationships. Discovering a new category and relating it. DEPARTMENT may be an entity type in one schema and an attribute in another. c. Domain conflicts. An attribute may have different domains in two schemas. For example Ssn may be declared as an integer in one schema and as a character string in the other. A conflict of the unit of measure could occur if one schema represented Weight in pounds and the other used kilograms. d. Conflicts among constraints. Two schemas may impose different constraints for example the key of an entity type may be different in each schema. Another example involves different structural constraints on a relationship such as TEACHES one schema may represent it as while the other schema represents it as M N . The Database Design and Implementation Process Modifying views to conform to one another. Some schemas are modified so that they conform to other schemas more closely. Some of the conflicts identified in the first subtask are resolved during this step. Merging of views. The global schema is created by merging the individual schemas. Corresponding concepts are represented only once in the global schema and mappings between the views and the global schema are specified. This is the most difficult step to achieve in real life databases involving dozens or hundreds of entities and relationships. It involves a considerable amount of human intervention and negotiation to resolve conflicts and to settle on the most reasonable and acceptable solutions for a global schema. Restructuring. As a final optional step the global schema may be analyzed and restructured to remove any redundancies or unnecessary complexity. Some of these ideas are illustrated by the rather simple example presented in Figures and In Figure two views are merged to create a bibliographic database. During identification of correspondences between the two views we discover that RESEARCHER and AUTHOR are synonyms as are CONTRIBUTEDBY and WRITTENBY. Further we decide to modify VIEW to include a SUBJECT for ARTICLE as shown in Figure to conform to VIEW Figure shows the result of merging MODIFIED VIEW with VIEW We generalize the entity types ARTICLE and BOOK into the entity type PUBLICATION with their common attribute Title. The relationships CONTRIBUTEDBY and WRITTENBY are merged as are the entity types RESEARCHER and AUTHOR. The attribute Publisher applies only to the entity type BOOK whereas the attribute Size and the relationship type PUBLISHEDIN apply only to ARTICLE. This simple example illustrates the complexity of the merging process and how the meaning of the various concepts must be accounted for in simplifying the resultant schema design. For real life designs the process of schema integration requires a more disciplined and systematic approach. Several strategies have been proposed for the view integration process in a DBMS independent way. When a database system is being designed the designers are aware of many known applications that will run on the database once it is implemented. An important part of database design is to specify the functional characteristics of these transactions early on in the design process. This ensures that the database schema will include all the information required by these transactions. In addition knowing the relative importance of the various transactions and the expected rates of their invocation plays a crucial part during the physical database design and the internal functional flow of control designers can specify a transaction in a conceptual and system independent way. Transactions usually can be grouped into three categories retrieval transactions which are used to retrieve data for display on a screen or for printing of a report update transactions which are used to enter new data or to modify existing data in the database and mixed transactions which are used for more complex applications that do some retrieval and some update. For example consider an airline reservations database. A retrieval transaction could first list all morning flights on a given date between two cities. An update transaction could be to book a seat on a particular flight. A mixed transaction may first display some data such as showing a customer reservation on some flight and then update the database such as canceling the reservation by deleting it or by adding a flight segment to an existing reservation. Transactions may originate in a front end tool such as PowerBuilder which collect parameters online and then send a transaction to the DBMS as a Several techniques for requirements specification include notation for specifying processes which in this context are more complex operations that can consist of several transactions. Process modeling tools like BPwin as well as workflow modeling tools are becoming popular to identify information flows in organizations. The UML language which provides for data modeling via class and object diagrams has a variety of process modeling diagrams including state transition diagrams activity diagrams sequence diagrams and collaboration diagrams. All of these refer to philosophy has been followed for over years in popular products like CICS which serves as a tool to generate transactions for legacy DBMSs like IMS. The Database Design and Implementation Process activities events and operations within the information system the inputs and outputs of the processes the sequencing or synchronization requirements and other conditions. It is possible to refine these specifications and extract individual transactions from them. Other proposals for specifying transactions include TAXIS GALILEO and GORDAS . Some of these have been implemented into prototype systems and tools. Process modeling still remains an active area of research. Transaction design is just as important as schema design but it is often considered to be part of software engineering rather than database design. Many current design methodologies emphasize one over the other. One should go through Phases and in parallel using feedback loops for refinement until a stable design of schema and transactions is Phase Choice of a DBMS The choice of a DBMS is governed by a number of factors some technical others economic and still others concerned with the politics of the organization. The technical factors focus on the suitability of the DBMS for the task at hand. Issues to consider are the type of DBMS the storage structures and access paths that the DBMS supports the user and programmer interfaces available the types of high level query languages the availability of development tools the ability to interface with other DBMSs via standard interfaces the architectural options related to client server operation and so on. Nontechnical factors include the financial status and the support organization of the vendor. In this section we concentrate on discussing the economic and organizational factors that affect the choice of DBMS. The following costs must be considered Software acquisition cost. This is the up front cost of buying the software including programming language options different interface options tools recovery backup options special access methods and documentation. The correct DBMS version for a specific operating system must be selected. Typically the development tools design tools and additional language support are not included in basic pricing. Maintenance cost. This is the recurring cost of receiving standard maintenance service from the vendor and for keeping the DBMS version up todate. Hardware acquisition cost. New hardware may be needed such as additional memory terminals disk drives and controllers or specialized DBMS storage and archival storage. Database creation and conversion cost. This is the cost of either creating the database system from scratch or converting an existing system to the new transaction modeling is covered in Batini et al. Chapters and The joint functional and data analysis philosophy is advocated throughout that book. Chapter Practical Database Design Methodology and Use of UML Diagrams DBMS software. In the latter case it is customary to operate the existing system in parallel with the new system until all the new applications are fully implemented and tested. This cost is hard to project and is often underestimated. Personnel cost. Acquisition of DBMS software for the first time by an organization is often accompanied by a reorganization of the data processing department. Positions of DBA and staff exist in most companies that have adopted DBMSs. Training cost. Because DBMSs are often complex systems personnel must often be trained to use and program the DBMS. Training is required at all levels including programming and application development physical design and database administration. Operating cost. The cost of continued operation of the database system is typically not worked into an evaluation of alternatives because it is incurred regardless of the DBMS selected. The benefits of acquiring a DBMS are not so easy to measure and quantify. A DBMS has several intangible advantages over traditional file systems such as ease of use consolidation of company wide information wider availability of data and faster access to information. With Web based access certain parts of the data can be made globally accessible to employees as well as external users. More tangible benefits include reduced application development cost reduced redundancy of data and better control and security. Although databases have been firmly entrenched in most organizations the decision of whether to move an application from a filebased to a database centered approach still comes up. This move is generally driven by the following factors Data complexity. As data relationships become more complex the need for a DBMS is greater. Sharing among applications. The need for a DBMS is greater when applications share common data stored redundantly in multiple files. Dynamically evolving or growing data. If the data changes constantly it is easier to cope with these changes using a DBMS than using a file system. Frequency of ad hoc requests for data. File systems are not at all suitable for ad hoc retrieval of data. Data volume and need for control. The sheer volume of data and the need to control it sometimes demands a DBMS. It is difficult to develop a generic set of guidelines for adopting a single approach to data management within an organization whether relational object oriented or object relational. If the data to be stored in the database has a high level of complexity and deals with multiple data types the typical approach may be to consider an object or object relational Also the benefits of inheritance among classes the discussion in Chapter concerning this issue. The Database Design and Implementation Process and the corresponding advantage of reuse favor these approaches. Finally several economic and organizational factors affect the choice of one DBMS over another Organization wide adoption of a certain philosophy. This is often a dominant factor affecting the acceptability of a certain data model a certain vendor or a certain development methodology and tools . Familiarity of personnel with the system. If the programming staff within the organization is familiar with a particular DBMS it may be favored to reduce training cost and learning time. Availability of vendor services. The availability of vendor assistance in solving problems with the system is important since moving from a nonDBMS to a DBMS environment is generally a major undertaking and requires much vendor assistance at the start. Another factor to consider is the DBMS portability among different types of hardware. Many commercial DBMSs now have versions that run on many hardware software configurations . The need of applications for backup recovery performance integrity and security must also be considered. Many DBMSs are currently being designed as total solutions to the informationprocessing and information resource management needs within organizations. Most DBMS vendors are combining their products with the following options or built in features Text editors and browsers Report generators and listing utilities Communication software Data entry and display features such as forms screens and menus with automatic editing features Inquiry and access tools that can be used on the World Wide Web Graphical database design tools A large amount of third party software is available that provides added functionality to a DBMS in each of the above areas. In rare cases it may be preferable to develop in house software rather than use a DBMS for example if the applications are very well defined and are all known beforehand. Under such circumstances an in house custom designed system may be appropriate to implement the known applications in the most efficient way. In most cases however new applications that were not foreseen at design time come up after system implementation. This is precisely why DBMSs have become very popular They facilitate the incorporation of new applications with only incremental modifications to the existing design of a database. Such design evolution or schema evolution is a feature present to various degrees in commercial DBMSs. Chapter Practical Database Design Methodology and Use of UML Diagrams Phase Data Model Mapping The next phase of database design is to create a conceptual schema and external schemas in the data model of the selected DBMS by mapping those schemas produced in Phase The mapping can proceed in two stages System independent mapping. In this stage the mapping does not consider any specific characteristics or special cases that apply to the particular DBMS implementation of the data model. We discussed DBMS independent mapping of an ER schema to a relational schema in Section and of EER schema constructs to relational schemas in Section Tailoring the schemas to a specific DBMS. Different DBMSs implement a data model by using specific modeling features and constraints. We may have to adjust the schemas obtained in step to conform to the specific implementation features of a data model as used in the selected DBMS. The result of this phase should be DDL statements in the language of the chosen DBMS that specify the conceptual and external level schemas of the database system. But if the DDL statements include some physical design parameters a complete DDL specification must wait until after the physical database design phase is completed. Many automated CASE design tools of the selected DBMS are compiled and used to create the database schemas and database files. The database can then be loaded with the data. If data is to be converted from an earlier computerized system conversion routines may be needed to reformat the data for loading into the new database. Database programs are implemented by the application programmers by referring to the conceptual specifications of transactions and then writing and testing program code with embedded DML commands. Once the transactions are ready and the data is loaded into the database the design and implementation phase is over and the operational phase of the database system begins. Most systems include a monitoring utility to collect performance statistics which are kept in the system catalog or data dictionary for later analysis. These include statistics on the number of invocations of predefined transactions or queries input output activity against files counts of file disk pages or index records and fre Chapter Practical Database Design Methodology and Use of UML Diagrams quency of index usage. As the database system requirements change it often becomes necessary to add or remove existing tables and to reorganize some files by changing primary access methods or by dropping old indexes and constructing new ones. Some queries or transactions may be rewritten for better performance. Database tuning continues as long as the database is in existence as long as performance problems are discovered and while the requirements keep changing is the Unified Modeling Language approach. It provides a mechanism in the form of diagrammatic notation and associated language syntax to cover the entire life cycle. Presently UML can be used by software developers data modelers database designers and so on to define the detailed specification of an application. They also use it to specify the environment consisting of users software communications and hardware to implement and deploy the application. UML combines commonly accepted concepts from many object oriented methods and methodologies . It is generic and is language independent and platform independent. Software architects can model any type of application running on any operating system programming language or network in UML. That has made the approach very widely applicable. Tools like Rational Rose are currently popular for drawing UML diagrams they enable software developers to develop clear and easy to understand models for specifying visualizing constructing and documenting components of software systems. Since the scope of UML extends to software and application development at large we will not cover all aspects of UML here. Our goal is to show some relevant UML notations that are commonly used in the requirements collection and analysis phase of database design as well as the conceptual design phase . contribution of Abrar Ul Haque to the UML and Rational Rose sections is much appreciated. Use of UML Diagrams as an Aid to Database Design Specification UML has many types of diagrams. Class diagrams which can represent the end result of conceptual database design were discussed in Sections and To arrive at the class diagrams the application requirements may be gathered and specified using use case diagrams sequence diagrams and statechart diagrams. In the rest of this section we introduce the different types of UML diagrams briefly to give the reader an idea of the scope of UML. Then we describe a small sample application to illustrate the use of some of these diagrams and show how they lead to the eventual class diagram as the final conceptual database design. The diagrams presented in this section pertain to the standard UML notation and have been drawn using Rational Rose. Section is devoted to a general discussion of the use of Rational Rose in database application design. UML for Database Application Design UML was developed as a software engineering methodology. As we mentioned earlier in Section most software systems have sizable database components. The database community has started embracing UML and now some database designers and developers are using UML for data modeling as well as for subsequent phases of database design. The advantage of UML is that even though its concepts are based on object oriented techniques the resulting models of structure and behavior can be used to design relational object oriented or object relational databases . One of the major contributions of the UML approach has been to bring the traditional database modelers analysts and designers together with the software application developers. In Figure we showed the phases of database design and implementation and how they apply to these two groups. UML also allows us to do behavioral functional and dynamic modeling by introducing various types of diagrams. This results in a more complete specification description of the overall database application. In the following sections we summarize the different types of UML diagrams and then give an example of the use case sequence and statechart diagrams in a sample application. Different Types of Diagrams in UML UML defines nine types of diagrams divided into these two categories Structural Diagrams. These describe the structural or static relationships among schema objects data objects and software components. They include class diagrams object diagrams component diagrams and deployment diagrams. Behavioral Diagrams. Their purpose is to describe the behavioral or dynamic relationships among components. They include use case diagrams sequence diagrams collaboration diagrams statechart diagrams and activity diagrams. Chapter Practical Database Design Methodology and Use of UML Diagrams We introduce the nine types briefly below. The structural diagrams include A. Class Diagrams. Class diagrams capture the static structure of the system and act as foundation for other models. They show classes interfaces collaborations dependencies generalizations associations and other relationships. Class diagrams are a very useful way to model the conceptual database schema. We showed examples of class diagrams for the COMPANY database schema in Figure and for a generalization hierarchy in Figure Package Diagrams. Package diagrams are a subset of class diagrams. They organize elements of the system into related groups called packages. A package may be a collection of related classes and the relationships between them. Package diagrams help minimize dependencies in a system. B. Object Diagrams. Object diagrams show a set of individual objects and their relationships and are sometimes referred to as instance diagrams. They give a static view of a system at a particular time and are normally used to test class diagrams for accuracy. C. Component Diagrams. Component diagrams illustrate the organizations and dependencies among software components. A component diagram typically consists of components interfaces and dependency relationships. A component may be a source code component a runtime component or an executable component. It is a physical building block in the system and is represented as a rectangle with two small rectangles or tabs overlaid on its left side. An interface is a group of operations used or created by a component and is usually represented by a small circle. Dependency relationship is used to model the relationship between two components and is represented by a dotted arrow pointing from a component to the component it depends on. For databases component diagrams stand for stored data such as tablespaces or partitions. Interfaces refer to applications that use the stored data. D. Deployment Diagrams. Deployment diagrams represent the distribution of components across the hardware topology. They depict the physical resources in a system including nodes components and connections and are basically used to show the configuration of runtime processing elements and the software processes that reside on them . Next we briefly describe the various types of behavioral diagrams and expand on those that are of particular interest. E. Use Case Diagrams. Use case diagrams are used to model the functional interactions between users and the system. A scenario is a sequence of steps describing an interaction between a user and a system. A use case is a set of scenarios that have a common goal. The use case diagram was introduced by to visualize use cases. A use case diagram shows actors interacting with use cases and can be understood easily without the knowledge of any notation. An individual use case is Jacobson et al. Use of UML Diagrams as an Aid to Database Design Specification Base use Base use Use case Base use Extended use case Include Include Extend Included use case Figure The use case diagram notation. shown as an oval and stands for a specific task performed by the system. An actor shown with a stick person symbol represents an external user which may be a human user a representative group of users a certain role of a person in the organization or anything external to the system and describes as use cases the specific tasks the system performs. Since they do not specify any implementation detail and are supposed to be easy to understand they are used as a vehicle for communicating between the end users and developers to help in easier user validation at an early stage. Test plans can also be described using use case diagrams. Figure shows the use case diagram notation. The include relationship is used to factor out some common behavior from two or more of the original use cases it is a form of reuse. For example in a university environment shown in Figure the use cases Register for course and Enter grades in which the actors student and professor are involved include a common use case called Validate user. If a use case incorporates two or more significantly different scenarios based on circumstances or varying conditions the extend relationship is used to show the subcases attached to the base case. Interaction Diagrams. The next two types of UML behavioral diagrams interaction diagrams are used to model the dynamic aspects of a system. They consist of a set of messages exchanged between a set of objects. There are two types of interaction diagrams sequence and collaboration. F. Sequence Diagrams. Sequence diagrams describe the interactions between various objects over time. They basically give a dynamic view of the system by Chapter Practical Database Design Methodology and Use of UML Diagrams Student Professor Register for course Enter grades Validate user Apply for aid Financial aid officer Include Include Figure A sample use case diagram for a UNIVERSITY database. showing the flow of messages between objects. Within the sequence diagram an object or an actor is shown as a box at the top of a dashed vertical line which is called the object’s lifeline. For a database this object is typically something physical a book in a warehouse that would be represented in the database an external document or form such as an order form or an external visual screen that may be part of a user interface. The lifeline represents the existence of an object over time. Activation which indicates when an object is performing an action is represented as a rectangular box on a lifeline. Each message is represented as an arrow between the lifelines of two objects. A message bears a name and may have arguments and control information to explain the nature of the interaction. The order of messages is read from top to bottom. A sequence diagram also gives the option of self call which is basically just a message from an object to itself. Condition and Iteration markers can also be shown in sequence diagrams to specify when the message should be sent and to specify the condition to send multiple markers. A return dashed line shows a return from the message and is optional unless it carries a special meaning. Object deletion is shown with a large X. Figure explains some of the notation used in sequence diagrams. G. Collaboration Diagrams. Collaboration diagrams represent interactions among objects as a series of sequenced messages. In collaboration diagrams the emphasis is on the structural organization of the objects that send and receive messages whereas in sequence diagrams the emphasis is on the time ordering of the messages. Collaboration diagrams show objects as icons and number the messages numbered messages represent an ordering. The spatial layout of collaboration diagrams allows linkages among objects that show their structural relationships. Use of collaboration and sequence diagrams to represent interactions is a matter of choice as they can be used for somewhat similar purposes we will hereafter use only sequence diagrams. Use of UML Diagrams as an Aid to Database Design Specification Object Class or Actor Lifeline Focus of control activation Message to self Message Object Class or Actor Object Class or Actor Object deconstruction termination Return Figure The sequence diagram notation. H. Statechart Diagrams. Statechart diagrams describe how an object’s state changes in response to external events. To describe the behavior of an object it is common in most object oriented techniques to draw a statechart diagram to show all the possible states an object can get into in its lifetime. The UML statecharts are based on David statecharts. They show a state machine consisting of states transitions events and actions and are very useful in the conceptual design of the application that works against a database of stored objects. The important elements of a statechart diagram shown in Figure are as follows States. Shown as boxes with rounded corners they represent situations in the lifetime of an object. Transitions. Shown as solid arrows between the states they represent the paths between different states of an object. They are labeled by the eventname [guard] action the event triggers the transition and the action results from it. The guard is an additional and optional condition that specifies a condition under which the change of state may not occur. Start Initial State. Shown by a solid circle with an outgoing arrow to a state. Stop Final State. Shown as a double lined filled circle with an arrow pointing into it from a state. Statechart diagrams are useful in specifying how an object’s reaction to a message depends on its state. An event is something done to an object such as receiving a message an action is something that an object does such as sending a message. Harel Chapter Practical Database Design Methodology and Use of UML Diagrams Start initial state Transition State State State State consists of three parts Name Activities Embedded machine Activities and embedded machine are optional Stop accepting final state Name Do action Figure The statechart diagram notation. I. Activity Diagrams. Activity diagrams present a dynamic view of the system by modeling the flow of control from activity to activity. They can be considered as flowcharts with states. An activity is a state of doing something which could be a real world process or an operation on some object or class in the database. Typically activity diagrams are used to model workflow and internal business operations for an application. A Modeling and Design Example UNIVERSITY Database In this section we will briefly illustrate the use of some of the UML diagrams we presented above to design a simple database in a university setting. A large number of details are left out to conserve space only a stepwise use of these diagrams that leads toward a conceptual design and the design of program components is illustrated. As we indicated before the eventual DBMS on which this database gets implemented may be relational object oriented or object relational. That will not change the stepwise analysis and modeling of the application using the UML diagrams. Imagine a scenario with students enrolling in courses that are offered by professors. The registrar’s office is in charge of maintaining a schedule of courses in a course catalog. They have the authority to add and delete courses and to do schedule changes. They also set enrollment limits on courses. The financial aid office is in Use of UML Diagrams as an Aid to Database Design Specification charge of processing student aid applications for which the students have to apply. Assume that we have to design a database that maintains the data about students professors courses financial aid and so on. We also want to design some of the applications that enable us to do course registration financial aid application processing and maintaining of the university wide course catalog by the registrar’s office. The above requirements may be depicted by a series of UML diagrams. As mentioned previously one of the first steps involved in designing a database is to gather customer requirements by using use case diagrams. Suppose one of the requirements in the UNIVERSITY database is to allow the professors to enter grades for the courses they are teaching and for the students to be able to register for courses and apply for financial aid. The use case diagram corresponding to these use cases can be drawn as shown in Figure Another helpful element when designing a system is to graphically represent some of the states the system can be in to visualize the various states the system can be in during the course of an application. For example in our UNIVERSITY database the various states that the system goes through when the registration for a course with seats is opened can be represented by the statechart diagram in Figure This shows the states of a course while enrollment is in process. The first state sets the count of students enrolled to zero. During the enrolling state the Enroll student transition continues as long as the count of enrolled students is less than When the count reaches the state to close the section is entered. In a real system additional states and or transitions could be added to allow a student to drop a section and any other needed actions. Next we can design a sequence diagram to visualize the execution of the use cases. For the university database the sequence diagram corresponds to the use case student requests to register and selects a particular course to register is shown in Figure Enroll student [count Course enrollment Enroll student set count Enrolling Entry register student Section closing Canceled Cancel Cancel Count Cancel Exit closesection Do enroll students Figure A sample statechart diagram for the UNIVERSITY database. Chapter Practical Database Design Methodology and Use of UML Diagrams getSeatsLeft getPreq true && [getSeatsLeft True] update Schedule Registration requestRegistration getCourseListing selectCourse addCourse getPreReq Student Catalog Course Schedule Figure A sequence diagram for the UNIVERSITY database. The catalog is first browsed to get course listings. Then when the student selects a course to register in prerequisites and course capacity are checked and the course is then added to the student’s schedule if the prerequisites are met and there is space in the course. These UML diagrams are not the complete specification of the UNIVERSITY database. There will be other use cases for the various applications of the actors including registrar student professor and so on. A complete methodology for how to arrive at the class diagrams from the various diagrams we illustrated in this section is outside our scope here. Design methodologies remain a matter of judgment and personal preferences. However the designer should make sure that the class diagram will account for all the specifications that have been given in the form of the use cases statechart and sequence diagrams. The class diagram in Figure shows a possible class diagram for this application with the structural relationships and the operations within the classes. These classes will need to be implemented to develop the UNIVERSITY database and together with the operations they will implement the complete class schedule enrollment aid application. Only some of the attributes and methods are shown in Figure It is likely that these class diagrams will be modified as more details are specified and more functions evolve in the UNIVERSITY application. Rational Rose A UML Based Design Tool REGISTRATION . . . findCourseAdd cancelCourse addCourse viewSchedule . . . CATALOG . . . getPreReq getSeatsLeft getCourseListing . . . FINANCIALAID aidType aidAmount assignAid denyAid SCHEDULE . . . updateSchedule showSchedule . . . STUDENT . . . requestRegistration applyAid . . . PROFESSOR . . . enterGrades offerCourse . . . COURSE time classroom seats . . . dropCourse addCourse . . . PERSON Name Ssn . . . viewSchedule . . . Figure The design of the UNIVERSITY database as a class diagram. Rational Rose A UML Based Design Tool Rational Rose for Database Design Rational Rose is one of the modeling tools used in the industry to develop information systems. It was acquired by IBM in As we pointed out in the first two sections of this chapter a database is a central component of most information systems. Rational Rose provides the initial specification in UML that eventually leads to the database development. Many extensions have been made in the latest versions of Rose for data modeling and now it provides support for conceptual logical and physical database modeling and design. Chapter Practical Database Design Methodology and Use of UML Diagrams Rational Rose Data Modeler Rational Rose Data Modeler is a visual modeling tool for designing databases. Because it is UML based it provides a common tool and language to bridge the communication gap between database designers and application developers. This makes it possible for database designers developers and analysts to work together capture and share business requirements and track them as they change throughout the process. Also by allowing the designers to model and design all specifications on the same platform using the same notation it improves the design process and reduces the risk of errors. The process modeling capabilities in Rational Rose allow the modeling of the behavior of database applications as we saw in the short example above in the form of use cases . The Rose Data Modeler also provides the capability to forward engineer a database in terms of constantly changing requirements and reverse engineer an existing implemented database into its conceptual design. Data Modeling Using Rational Rose Data Modeler There are many tools and options available in Rose Data Modeler for data modeling. Reverse Engineering. Reverse engineering of a database allows the user to create a conceptual data model based on an existing database schema specified in a DDL file. We can use the reverse engineering wizard in Rational Rose Data Modeler for this purpose. The reverse engineering wizard basically reads the schema in the database or DDL file and recreates it as a data model. While doing so it also includes the names of all quoted identifier entities. Forward Engineering and DDL Generation. We can also create a data model directly from scratch in Rose. Having created the data we can also use it to generate the DDL for a specific DBMS. There is a forward engineering wizard in the Rose Data Modeler that reads the schema in the data model or reads both the schema in the data model and the tablespaces in the data storage model and generates the appropriate DDL code in a DDL file. The wizard also provides the option of generating a database by executing the generated DDL file. term data model used by Rational Rose Data Modeler corresponds to our notion of an application model or conceptual schema. Rational Rose A UML Based Design Tool Conceptual Design in UML Notation. Rational Rose allows modeling of databases using UML notation. ER diagrams most often used in the conceptual design of databases can be easily built using the UML notation as class diagrams in Rational Rose. For example the ER schema of our COMPANY database from Chapter can be redrawn in Rose using UML notation as shown in Figure The textual specification in Figure can be converted to the graphical representation shown in Figure by using the data model diagram option in Rose. Figure is similar to Figure except that it is using the notation provided by Rational Rose. Hence it can be considered as an ER diagram using UML notation with the inclusion of methods and other details. Identifying relationships specify that an object in a child class between two independent classes. It is possible to update the schemas directly in their text or graphical form. For example if the relationship between the EMPLOYEE and PROJECT called WORKSON was deleted Rose would automatically update or delete all the foreign keys in the relevant tables. EMPLOYEE Fname Minit Lname Sex Salary Integer Address Ssn Integer Bdate Date Number Integer Projectnumber Integer Name Employeessn Integer P K F K F K F K F K DEPARTMENT Number Integer Name Location Noofemployees Integer Mgrssn Integer Mgrstartdate Date Ssn Integer F K P K DEPENDENT Identifying HASDEPENDENTS Non Identifying WORKSON Non Identifying CONTROLS Non Identifying SUPERVISION Non Identifying MANAGES Non Identifying WORKSFOR Name Sex Birthdate Date Relationship Ssn Integer P K PROJECT Number Integer Name Location Departmentnumber Integer Hours PK P K F K Figure A graphical data model diagram in Rational Rose for the COMPANY database. Rational Rose A UML Based Design Tool An important difference in Figure from our previous ER notation shown in Chapters and is that foreign key attributes actually appear in the class diagrams in Rational Rose. This is common in several diagrammatic notations to make the conceptual design closer to the way it is realized in the relational model implementation. In Chapters and the conceptual ER and EER diagrams and the UML class diagrams did not include foreign key attributes which were added to the relational schema during the mapping process to an object model design and vice versa. For example the logical data model shown in Figure can be converted to an object model. This sort of mapping allows a deep understanding of the relationships between the conceptual model and implementation model and helps in keeping them both up to date when changes are made to either model during the development process. Figure shows the Employee table after converting it to a class in an object model. The various tabs in the window can then be used to enter display different types of information. They include operations attributes and relationships for that class. Synchronization between the Conceptual Design and the Actual Database. Rose Data Modeler allows keeping the data model and database implementation synchronized. It allows visualizing both the data model and the database and then based on the differences it gives the option to update the model or change the database. Extensive Domain Support. The Rose Data Modeler allows database designers to create a standard set of user defined data types Phase and Phase in the design process that we discussed in Section Discussion of Phase is deferred to Chapter after we present storage and indexing techniques and query optimization. We discussed Phases and in detail with the use of the UML notation in Section and pointed out the features of the tool Rational Rose which supports these phases in Section As we mentioned Rational Rose is more than just a database design tool. It is a software development tool and does database modeling and schema design in the form of class diagrams as part of its overall object oriented application development methodology. In this section we summarize the features and shortcomings of the set of commercial tools that are focused on automating the process of conceptual logical and physical design of databases. When database technology was first introduced most database design was carried out manually by expert designers who used their experience and knowledge in the design process. However at least two factors indicated that some form of automation had to be utilized if possible As an application involves more and more complexity of data in terms of relationships and constraints the number of options or different designs to Automated Database Design Tools EMPLOYEE Fname Minit Lname Ssn Bdate Sex Address Salary age changedepartment changeprojects DEPARTMENT Name Number addemployee noofemployee changemajor n+supervisee +supervisor WORKSFOR PROJECT Name Number addemployee addproject changemanager MANAGES Startdate WORKS ON Hours LOCATION Name DEPENDENT Dependent name Sex Birthdate Relationship Controls Figure The COMPANY database class diagram tools for database design. Rational Rose is a good example of a modern CASE tool. Typically these tools consist of a combination of the following facilities Diagramming. This allows the designer to draw a conceptual schema diagram in some tool specific notation. Most notations include entity types relationship types that are shown either as separate boxes or simply as directed or undirected lines cardinality constraints Chapter Practical Database Design Methodology and Use of UML Diagrams shown alongside the lines or in terms of the different types of arrowheads or min max constraints attributes keys and so Some tools display inheritance hierarchies and use additional notation for showing the partialversus total and disjoint versus overlapping nature of the specialization generalization. The diagrams are internally stored as conceptual designs and are available for modification as well as generation of reports crossreference listings and other uses. Model mapping. This implements mapping algorithms similar to the ones we presented in Sections and The mapping is system specific most tools generate schemas in SQL DDL for Oracle Informix Sybase and other RDBMSs. This part of the tool is most amenable to automation. The designer can further edit the produced DDL files if needed. Design normalization. This utilizes a set of functional dependencies that are supplied at the conceptual design or after the relational schemas are produced during logical design. Then design decomposition algorithms tools. Some vendors like Platinum provide a tool for data modeling and schema design and another for process modeling and functional design . Other tools use expert system technology to guide the design process by including design expertise in the form of rules. Expert system technology is also useful in the requirements collection and analysis phase which is typically a laborious and frustrating process. The trend is to use both meta data repositories and design tools to achieve better designs for complex databases. Without a claim of being exhaustive Table lists some popular database design and application modeling tools. Companies in the table are listed alphabetically. Summary We started this chapter by discussing the role of information systems in organizations database systems are looked upon as a part of information systems in largescale applications. We discussed how databases fit within an information system for information resource management in an organization and the life cycle they go through. Then we discussed the six phases of the design process. The three phases commonly included as a part of database design are conceptual design logical design and physical design. We also discussed the initial phase of requirements collection and analysis which is often considered to be a predesign phase. Additionally at some point during the design a specific DBMS Chapter Practical Database Design Methodology and Use of UML Diagrams package must be chosen. We discussed some of the organizational criteria that come into play in selecting a DBMS. As performance problems are detected and as new applications are added designs have to be modified. The importance of designing both the schema and the applications was highlighted. We discussed different approaches to conceptual schema design and the difference between centralized schema design and the view integration approach. We introduced UML diagrams as an aid to the specification of database models and designs. We presented the entire range of structural and behavioral diagrams and then we described the notational detail about the following types of diagrams use case sequence and statechart. We showed how a few requirements for the UNIVERSITY database are specified using these diagrams and can be used to develop the conceptual design of the database. Only illustrative details and not the complete specification were supplied. Then we discussed a specific software development tool Rational Rose and the Rose Data Modeler that provides support for the conceptual design and logical design phases of database design. Rose is a much broader tool for design of information systems at large. Finally we briefly discussed the functionality and desirable features of commercial automated database design tools that are more focused on database design as opposed to Rose. A tabular summary of features was presented. Table Some of the Currently Available Automated Database Design Tools Company Tool Functionality Embarcadero ER Studio Database modeling in ER and Technologies DBArtisan Database administration and space and security management Oracle Developer and Database modeling application Designer development Persistence Inc. PowerTier Mapping from O O to relational model Platinum Technology Platinum ModelMart Data process and business component ERwin BPwin AllFusion modeling Component Modeler Popkin Software Telelogic System Architect Data modeling object modeling process modeling structured analysis design Rational Rational Rose Modeling in UML and application XDE Developer Plus generation in C++ and Java Resolution Ltd. XCase Conceptual modeling up to code maintenance Sybase Enterprise Application Suite Data modeling business logic modeling Visio Visio Enterprise Data modeling design and reengineering Visual Basic and Visual C++ Review Questions What are the six phases of database design Discuss each phase. Which of the six phases are considered the main activities of the database design process itself Why Why is it important to design the schemas and applications in parallel Why is it important to use an implementation independent data model during conceptual schema design What models are used in current design tools Why Discuss the importance of requirements collection and analysis. Consider an actual application of a database system of interest. Define the requirements of the different levels of users in terms of data needed types of queries and transactions to be processed. Discuss the characteristics that a data model for conceptual schema design should possess. Compare and contrast the two main approaches to conceptual schema design. Discuss the strategies for designing a single conceptual schema from its requirements. What are the steps of the view integration approach to conceptual schema design What are the difficulties during each step How would a view integration tool work Design a sample modular architecture for such a tool. What are the different strategies for view integration Discuss the factors that influence the choice of a DBMS package for the information system of an organization. What is system independent data model mapping How is it different from system dependent data model mapping What are the important factors that influence physical database design Discuss the decisions made during physical database design. Discuss the macro and micro life cycles of an information system. Discuss the guidelines for physical database design in RDBMSs. Discuss the types of modifications that may be applied to the logical database design of a relational database. What functions do the typical database design tools provide What type of functionality would be desirable in automated tools to support optimal design of large databases Review Questions Chapter Practical Database Design Methodology and Use of UML Diagrams What are the current relational DBMSs that dominate the market Choose one that you are familiar with and show how it measures up based on the criteria laid out in Section A possible DDL corresponding to Figure follows CREATE TABLE STUDENT Discuss the following detailed design decisions a. The choice of requiring Name to be NON NULL b. Selection of Ssn as the PRIMARY KEY c. Choice of field sizes and precision d. Any modification of the fields defined in this database e. Any constraints on individual fields What naming conventions can you develop to help identify foreign keys more efficiently What functions do the typical database design tools provide Selected Bibliography There is a vast amount of literature on database design. First we list some of the books that address database design. Batini et al. is a comprehensive treatment of conceptual and logical database design. Wiederhold covers all phases of database design with an emphasis on physical design. O’Neil has a detailed discussion of physical design and transaction issues in reference to commercial RDBMSs. A large body of work on conceptual modeling and design was done in the Brodie et al. gives a collection of chapters on conceptual modeling constraint specification and analysis and transaction design. Yao is a collection of works ranging from requirements specification techniques to schema restructuring. Teorey emphasizes EER modeling and discusses various aspects of conceptual and logical database design. Hoffer et al. is a good introduction to the business applications issues of database management. Navathe and Kerschberg discuss all phases of database design and point out the role of data dictionaries. Goldfine and Konig and ANSI discuss the role of data dictionaries in database design. Rozen and Shasha and Carlis and March present different models for the problem of physical database design. Object oriented analysis and design is discussed in Schlaer and Mellor Selected Bibliography Rumbaugh et al. Martin and Odell and Jacobson et al. Recent books by Blaha and Rumbaugh and Martin and Odell consolidate the existing techniques in object oriented analysis and design using UML. Fowler and Scott is a quick introduction to UML. For a comprehensive treatment of UML and its use in the software development process consult Jacobson et al. and Rumbaugh et al. Requirements collection and analysis is a heavily researched topic. Chatzoglu et al. and Lubars et al. present surveys of current practices in requirements capture modeling and analysis. Carroll provides a set of readings on the use of scenarios for requirements gathering in early stages of system development. Wood and Silver gives a good overview of the official Joint Application Design process. Potter et al. describes the Z notation and methodology for formal specification of software. Zave has classified the research efforts in requirements engineering. A large body of work has been produced on the problems of schema and view integration which is becoming particularly relevant now because of the need to integrate a variety of existing databases. Navathe and Gadgil defined approaches to view integration. Schema integration methodologies are compared in Batini et al. Detailed work on n ary view integration can be found in Navathe et al. Elmasri et al. and Larson et al. An integration tool based on Elmasri et al. is described in Sheth et al. Another view integration system is discussed in Hayne and Ram Casanova et al. describes a tool for modular database design. Motro discusses integration with respect to preexisting databases. The binary balanced strategy to view integration is discussed in Teorey and Fry A formal approach to view integration which uses inclusion dependencies is given in Casanova and Vidal Ramesh and Ram describe a methodology for integration of relationships in schemas utilizing the knowledge of integrity constraints this extends the previous work of Navathe et al. Sheth at al. describe the issues of building global schemas by reasoning about attribute relationships and entity equivalences. Navathe and Savasere describe a practical approach to building global schemas based on operators applied to schema components. Santucci provides a detailed treatment of refinement of EER schemas for integration. Castano et al. present a comprehensive survey of conceptual schema analysis techniques. Transaction design is a relatively less thoroughly researched topic. Mylopoulos et al. proposed the TAXIS language and Albano et al. developed the GALILEO system both of which are comprehensive systems for specifying transactions. The GORDAS language for the ECR model and the database systems are referred to as object data management systems . Traditional data models and systems such as relational network and hierarchical have been quite successful in developing the database technologies required for many traditional business database applications. However they have certain shortcomings when more complex database applications must be designed and implemented for example databases for engineering design and manufacturing vendors have also recognized the need for incorporating features that were proposed for object databases and newer versions of relational systems have incorporated many of these features. This has led to database systems that are characterized as object relational or ORDBMSs. The latest version of the SQL standard for RDBMSs includes many of these features which were originally known as SQL Object and they have now been merged into the main SQL specification known as SQL Foundation. Although many experimental prototypes and commercial object oriented database systems have been created they have not found widespread use because of the popularity of relational and object relational systems. The experimental prototypes included the Orion system developed at OpenOODB at Texas Instruments the Iris system at Hewlett Packard laboratories the Ode system at AT&T Bell and the ENCORE ObServer project at Brown University. Commercially available systems included GemStone Object Server of GemStone Systems ONTOS DB of Ontos Objectivity DB of Objectivity Versant Object Database and FastObjects by Versant Corporation ObjectStore of Object Design and Ardent Database of These represent only a partial list of the experimental prototypes and commercial object oriented database systems that were created. As commercial object DBMSs became available the need for a standard model and language was recognized. Because the formal procedure for approval of standards normally takes a number of years a consortium of object DBMS vendors and users called proposed a standard whose current specification is known as the ODMG standard. Object oriented databases have adopted many of the concepts that were developed originally for object oriented programming In Section we describe the key concepts utilized in many object database systems and that were later incorporated into object relational systems and the SQL standard. These include object identity object structure and type constructors encapsulation of operations and the definition of methods as part of class declarations mechanisms for storing objects in and Computer Technology Corporation Austin Texas. called Lucent Technologies. of Technology. Data Management Group. concepts were also developed in the fields of semantic data modeling and knowledge representation. Overview of Object Database Concepts a database by making them persistent and type and class hierarchies and inheritance. Then in Section we see how these concepts have been incorporated into the latest SQL standards leading to object relational databases. Object features were originally introduced in and then updated in the latest version of the standard. In Section we turn our attention to “pure” object database standards by presenting features of the object database standard ODMG and the object definition language ODL. Section presents an overview of the database design process for object databases. Section discusses the object query language which is part of the ODMG standard. In Section we discuss programming language bindings which specify how to extend objectoriented programming languages to include the features of the object database standard. Section summarizes the chapter. Sections and may be left out if a less thorough introduction to object databases is desired. Overview of Object Database Concepts Introduction to Object Oriented Concepts and Features The term object oriented abbreviated OO or O O has its origins in OO programming languages or OOPLs. Today OO concepts are applied in the areas of databases software engineering knowledge bases artificial intelligence and computer systems in general. OOPLs have their roots in the SIMULA language which was proposed in the late The programming language Smalltalk developed at Xerox in the was one of the first languages to explicitly incorporate additional OO concepts such as message passing and inheritance. It is known as a pure OO programming language meaning that it was explicitly designed to be object oriented. This contrasts with hybrid OO programming languages which incorporate OO concepts into an already existing language. An example of the latter is C++ which incorporates OO concepts into the popular C programming language. An object typically has two components state and behavior . It can have a complex data structure as well as specific operations defined by the Objects in an OOPL exist only during program execution therefore they are called transient objects. An OO database can extend the existence of objects so that they are stored permanently in a database and hence the objects become persistent objects that exist beyond program termination and can be retrieved later and shared by other programs. In other words OO databases store persistent objects permanently in secondary storage and allow the sharing of these objects among multiple programs and applications. This requires the incorporation of other well known features of database management systems such as indexing mechanisms to efficiently locate the objects concurrency control to allow object Alto Research Center Palo Alto California. have many other characteristics as we discuss in the rest of this chapter. Chapter Object and Object Relational Databases sharing among concurrent programs and recovery from failures. An OO database system will typically interface with one or more OO programming languages to provide persistent and shared object capabilities. The internal structure of an object in OOPLs includes the specification of instance variables which hold the values that define the internal state of the object. An instance variable is similar to the concept of an attribute in the relational model except that instance variables may be encapsulated within the object and thus are not necessarily visible to external users. Instance variables may also be of arbitrarily complex data types. Object oriented systems allow definition of the operations or functions that can be applied to objects of a particular type. In fact some OO models insist that all operations a user can apply to an object must be predefined. This forces a complete encapsulation of objects. This rigid approach has been relaxed in most OO data models for two reasons. First database users often need to know the attribute names so they can specify selection conditions on the attributes to retrieve specific objects. Second complete encapsulation implies that any simple retrieval requires a predefined operation thus making ad hoc queries difficult to specify on the fly. To encourage encapsulation an operation is defined in two parts. The first part called the signature or interface of the operation specifies the operation name and arguments . The second part called the method or body specifies the implementation of the operation usually written in some general purpose programming language. Operations can be invoked by passing a message to an object which includes the operation name and the parameters. The object then executes the method for that operation. This encapsulation permits modification of the internal structure of an object as well as the implementation of its operations without the need to disturb the external programs that invoke these operations. Hence encapsulation provides a form of data and operation independence depending on whether the object is of type triangle circle or rectangle. This may require the use of late binding of the operation name to the appropriate method at runtime when the type of object to which the operation is applied becomes known. In the next several sections we discuss in some detail the main characteristics of object databases. Section discusses object identity Section shows how the types for complex structured objects are specified via type constructors Section discusses encapsulation and persistence and Section presents inheritance concepts. Section discusses some additional OO concepts and Section gives a summary of all the OO concepts that we introduced. In Section we show how some of these concepts have been incorporated into the standard for relational databases. Then in Section we show how these concepts are realized in the ODMG object database standard. Object Identity and Objects versus Literals One goal of an ODMS is to maintain a direct correspondence between real world and database objects so that objects do not lose their integrity and identity and can easily be identified and operated upon. Hence an ODMS provides a unique identity to each independent object stored in the database. This unique identity is typically implemented via a unique system generated object identifier . The value of an OID is not visible to the external user but is used internally by the system to identify each object uniquely and to create and manage inter object references. The OID can be assigned to program variables of the appropriate type when needed. The main property required of an OID is that it be immutable that is the OID value of a particular object should not change. This preserves the identity of the real world object being represented. Hence an ODMS must have some mechanism for generating OIDs and preserving the immutability property. It is also desirable that each OID be used only once that is even if an object is removed from the database its OID should not be assigned to another object. These two properties imply that the OID should not depend on any attribute values of the object since the value of an attribute may be changed or corrected. We can compare this with the relational model where each relation must have a primary key attribute whose value identifies each tuple uniquely. In the relational model if the value of the primary key is changed the tuple will have a new identity even though it may still represent the same real world object. Alternatively a real world object may have different names for key attributes in different relations making it difficult to ascertain that the keys represent the same real world object . It is inappropriate to base the OID on the physical address of the object in storage since the physical address can change after a physical reorganization of the database. However some early ODMSs have used the physical address as the OID to increase Chapter Object and Object Relational Databases the efficiency of object retrieval. If the physical address of the object changes an indirect pointer can be placed at the former address which gives the new physical location of the object. It is more common to use long integers as OIDs and then to use some form of hash table to map the OID value to the current physical address of the object in storage. Some early OO data models required that everything from a simple value to a complex object was represented as an object hence every basic value such as an integer string or Boolean value has an OID. This allows two identical basic values to have different OIDs which can be useful in some cases. For example the integer value can sometimes be used to mean a weight in kilograms and at other times to mean the age of a person. Then two basic objects with distinct OIDs could be created but both objects would represent the integer value Although useful as a theoretical model this is not very practical since it leads to the generation of too many OIDs. Hence most OO database systems allow for the representation of both objects and literals . Every object must have an immutable OID whereas a literal value has no OID and its value just stands for itself. Thus a literal value is typically stored within an object and cannot be referenced from other objects. In many systems complex structured literal values can also be created without having a corresponding OID if needed. Complex Type Structures for Objects and Literals Another feature of an ODMS is that objects and literals may have a type structure of arbitrary complexity in order to contain all of the necessary information that describes the object or literal. In contrast in traditional database systems information about a complex object is often scattered over many relations or records leading to loss of direct correspondence between a real world object and its database representation. In ODBs a complex type may be constructed from other types by nesting of type constructors. The three most basic constructors are atom struct and collection. One type constructor has been called the atom constructor although this term is not used in the latest object standard. This includes the basic built in data types of the object model which are similar to the basic types in many programming languages integers strings floating point numbers enumerated types Booleans and so on. They are called single valued or atomic types since each value of the type is considered an atomic single value. A second type constructor is referred to as the struct constructor. This can create standard structured types such as the tuples in the basic relational model. A structured type is made up of several components and is also sometimes referred to as a compound or composite type. More accurately the struct constructor is not considered to be a type but rather a type generator because many different structured types can be created. For example two different structured types that can be created are struct Name FirstName string MiddleInitial char LastName string and Overview of Object Database Concepts struct CollegeDegree Major string Degree string Year date . To create complex nested type structures in the object model the collection type constructors are needed which we discuss next. Notice that the type constructors atom and struct are the only ones available in the original relational model. Collection type constructors include the set list bag array and dictionary type constructors. These allow part of an object or literal value to include a collection of other objects or values when needed. These constructors are also considered to be type generators because many different types can be created. For example set set and set are three different types that can be created from the set type constructor. All the elements in a particular collection value must be of the same type. For example all values in a collection of type set must be string values. The atom constructor is used to represent all basic atomic values such as integers real numbers character strings Booleans and any other basic data types that the system supports directly. The tuple constructor can create structured values and objects of the form an i n where each aj is an attribute and each i j is a value or an OID. The other commonly used constructors are collectively referred to as collection types but have individual differences among them. The set constructor will create objects or literals that are a set of distinct elements i i i n all of the same type. The bag constructor is similar to a set except that the elements in a bag need not be distinct. The list constructor will create an ordered list [i i i n] of OIDs or values of the same type. A list is similar to a bag except that the elements in a list are ordered and hence we can refer to the first second or jth element. The array constructor creates a single dimensional array of elements of the same type. The main difference between array and list is that a list can have an arbitrary number of elements whereas an array typically has a maximum size. Finally the dictionary constructor creates a collection of two tuples where the value of a key K can be used to retrieve the corresponding value V. The main characteristic of a collection type is that its objects or values will be a collection of objects or values of the same type that may be unordered or ordered . The tuple type constructor is often called a structured type since it corresponds to the struct construct in the C and C++ programming languages. An object definition language that incorporates the preceding type constructors can be used to define the object types for a particular database application. In Section we will describe the standard ODL of ODMG but first we introduce called an instance variable name in OO terminology. corresponds to the DDL of the database system . The value of such an attribute would be an OID for a specific DEPARTMENT object. A binary relationship can be represented in one direction or it can have an inverse reference. The latter representation makes it easy to traverse the relationship in both directions. For example in Figure the attribute Employees of DEPARTMENT has as its value a set of references to objects of type EMPLOYEE these are the employees who work for the DEPARTMENT. The inverse is the reference attribute Dept of EMPLOYEE. We will see in Section how the ODMG standard allows inverses to be explicitly declared as relationship attributes to ensure that inverse references are consistent. define type EMPLOYEE tuple define type DEPARTMENT tuple Locations set Employees set Projects set Overview of Object Database Concepts Encapsulation of Operations and Persistence of Objects Encapsulation of Operations. The concept of encapsulation is one of the main characteristics of OO languages and systems. It is also related to the concepts of abstract data types and information hiding in programming languages. In traditional database models and systems this concept was not applied since it is customary to make the structure of database objects visible to users and external programs. In these traditional models a number of generic database operations are applicable to objects of all types. For example in the relational model the operations for selecting inserting deleting and modifying tuples are generic and may be applied to any relation in the database. The relation and its attributes are visible to users and to external programs that access the relation by using these operations. The concepts of encapsulation is applied to database objects in ODBs by defining the behavior of a type of object based on the operations that can be externally applied to objects of that type. Some operations may be used to create or destroy objects other operations may update the object state and others may be used to retrieve parts of the object state or to apply some calculations. Still other operations may perform a combination of retrieval calculation and update. In general the implementation of an operation can be specified in a general purpose programming language that provides flexibility and power in defining the operations. The external users of the object are only made aware of the interface of the operations which defines the name and arguments of each operation. The implementation is hidden from the external users it includes the definition of any hidden internal data structures of the object and the implementation of the operations that access these structures. The interface part of an operation is sometimes called the signature and the operation implementation is sometimes called the method. For database applications the requirement that all objects be completely encapsulated is too stringent. One way to relax this requirement is to divide the structure of an object into visible and hidden attributes . Visible attributes can be seen by and are directly accessible to the database users and programmers via the query language. The hidden attributes of an object are completely encapsulated and can be accessed only through predefined operations. Most ODMSs employ high level query languages for accessing visible attributes. In Section we will describe the OQL query language that is proposed as a standard query language for ODBs. The term class is often used to refer to a type definition along with the definitions of the operations for that Figure shows how the type definitions in Figure can be extended with operations to define classes. A number of operations are definition of class is similar to how it is used in the popular C++ programming language. The ODMG standard uses the word interface in addition to class of each operation is included in the class definition. A method for each operation must be defined elsewhere using a programming language. Typical operations include the object constructor operation which is used to create a new object and the destructor operation which is used to destroy an object. A number of object modifier operations can also be declared to modify the states of various attributes of an object. Additional operations can retrieve information about the object. An operation is typically applied to an object by using the dot notation. For example if d is a reference to a DEPARTMENT object we can invoke an operation such as noofemps by writing . Similarly by writing the object define class EMPLOYEE type tuple operations age integer createemp EMPLOYEE destroyemp boolean end EMPLOYEE define class DEPARTMENT type tuple Locations set Employees set Projects set operations noofemps integer createdept DEPARTMENT destroydept boolean assignemp boolean removeemp boolean end DEPARTMENT Overview of Object Database Concepts referenced by d is destroyed . The only exception is the constructor operation which returns a reference to a new DEPARTMENT object. Hence it is customary in some OO models to have a default name for the constructor operation that is the name of the class itself although this was not used in Figure The dot notation is also used to refer to attributes of an object for example by writing or . Specifying Object Persistence via Naming and Reachability. An ODBS is often closely coupled with an object oriented programming language . The OOPL is used to specify the method implementations as well as other application code. Not all objects are meant to be stored permanently in the database. Transient objects exist in the executing program and disappear once the program terminates. Persistent objects are stored in the database and persist after program termination. The typical mechanisms for making an object persistent are naming and reachability. The naming mechanism involves giving an object a unique persistent name within a particular database. This persistent object name can be given via a specific statement or operation in the program as shown in Figure The named persistent objects are used as entry points to the database through which users and applications can start their database access. Obviously it is not practical to give names to all objects in a large database that includes thousands of objects so most objects are made persistent by using the second mechanism called reachability. The reachability mechanism works by making the object reachable from some other persistent object. An object B is said to be reachable from an object A if a sequence of references in the database lead from object A to object B. If we first create a named persistent object N whose state is a set of objects of some class C we can make objects of C persistent by adding them to the set thus making them reachable from N. Hence N is a named object that defines a persistent collection of objects of class C. In the object model standard N is called the extent of C . Chapter Object and Object Relational Databases Figure Creating persistent objects by naming and reachability. define class DEPARTMENTSET type set operations adddept boolean removedept boolean createdeptset DEPARTMENTSET destroydeptset boolean end DEPARTMENTSET persistent name ALLDEPARTMENTS DEPARTMENTSET d createdept b In traditional database models such as the relational model all objects are assumed to be persistent. Hence when a table such as EMPLOYEE is created in a relational database it represents both the type declaration for EMPLOYEE and a persistent set of all EMPLOYEE records . In the OO approach a class declaration of EMPLOYEE specifies only the type and operations for a class of objects. The user must separately define a persistent object of type set or bag whose value is the collection of references to all persistent EMPLOYEE objects if this is desired as shown in Figure This allows transient and persistent objects to follow the same type and class declarations of the ODL and the OOPL. In general it is possible to define several persistent collections for the same class definition if desired. Type Hierarchies and Inheritance Simplified Model for Inheritance. Another main characteristic of ODBs is that they allow type hierarchies and inheritance. We use a simple OO model in this section a model in which attributes and operations are treated uniformly since both attributes and operations can be inherited. In Section we will discuss the inheritance model of the ODMG standard which differs from the model discussed here because it distinguishes between two types of inheritance. Inheritance allows the definition of new types based on other predefined types leading to a type hierarchy. systems such as POET automatically create the extent for a class. Overview of Object Database Concepts A type is defined by assigning it a type name and then defining a number of attributes and operations for the In the simplified model we use in this section the attributes and operations are together called functions since attributes resemble functions with zero arguments. A function name can be used to refer to the value of an attribute or to refer to the resulting value of an operation . We use the term function to refer to both attributes and operations since they are treated similarly in a basic introduction to A type in its simplest form has a type name and a list of visible functions. When specifying a type in this section we use the following format which does not specify arguments of functions to simplify the discussion TYPENAME function function function For example a type that describes characteristics of a PERSON may be defined as follows PERSON Name Address Birthdate Age Ssn In the PERSON type the Name Address Ssn and Birthdate functions can be implemented as stored attributes whereas the Age function can be implemented as an operation that calculates the Age from the value of the Birthdate attribute and the current date. The concept of subtype is useful when the designer or user must create a new type that is similar but not identical to an already defined type. The subtype then inherits all the functions of the predefined type which is referred to as the supertype. For example suppose that we want to define two new types EMPLOYEE and STUDENT as follows EMPLOYEE Name Address Birthdate Age Ssn Salary Hiredate Seniority STUDENT Name Address Birthdate Age Ssn Major Gpa Since both STUDENT and EMPLOYEE include all the functions defined for PERSON plus some additional functions of their own we can declare them to be subtypes of PERSON. Each will inherit the previously defined functions of PERSON namely Name Address Birthdate Age and Ssn. For STUDENT it is only necessary to define the new functions Major and Gpa which are not inherited. Presumably Major can be defined as a stored attribute whereas Gpa may be implemented as an operation that calculates the student’s grade point average by accessing the Grade values that are internally stored within each STUDENT object as hidden attributes. For EMPLOYEE the Salary and Hiredate functions may be stored attributes whereas Seniority may be an operation that calculates Seniority from the value of Hiredate. this section we will use the terms type and class as meaning the same thing namely the attributes and operations of some type of object. will see in Section that types with functions are similar to the concept of interfaces as used in ODMG ODL. Chapter Object and Object Relational Databases Therefore we can declare EMPLOYEE and STUDENT as follows EMPLOYEE subtype of PERSON Salary Hiredate Seniority STUDENT subtype of PERSON Major Gpa In general a subtype includes all of the functions that are defined for its supertype plus some additional functions that are specific only to the subtype. Hence it is possible to generate a type hierarchy to show the supertype subtype relationships among all the types declared in the system. As another example consider a type that describes objects in plane geometry which may be defined as follows GEOMETRYOBJECT Shape Area Referencepoint For the GEOMETRYOBJECT type Shape is implemented as an attribute and Area is a method that is applied to calculate the area. Referencepoint specifies the coordinates of a point that determines the object location. Now suppose that we want to define a number of subtypes for the GEOMETRYOBJECT type as follows RECTANGLE subtype of GEOMETRYOBJECT Width Height TRIANGLE S subtype of GEOMETRYOBJECT Angle CIRCLE subtype of GEOMETRYOBJECT Radius Notice that the Area operation may be implemented by a different method for each subtype since the procedure for area calculation is different for rectangles triangles and circles. Similarly the attribute Referencepoint may have a different meaning for each subtype it might be the center point for RECTANGLE and CIRCLE objects and the vertex point between the two given sides for a TRIANGLE object. Notice that type definitions describe objects but do not generate objects on their own. When an object is created typically it belongs to one or more of these types that have been declared. For example a circle object is of type CIRCLE and GEOMETRYOBJECT . Each object also becomes a member of one or more persistent collections of objects which are used to group together collections of objects that are persistently stored in the database. Constraints on Extents Corresponding to a Type Hierarchy. In most ODBs an extent is defined to store the collection of persistent objects for each type or subtype. In this case the constraint is that every object in an extent that corresponds to a subtype must also be a member of the extent that corresponds to its supertype. Some OO database systems have a predefined system type whose extent contains all the objects in the Classification then proceeds by assigning objects into additional subtypes that are meaningful to the application creating a type hierarchy for the system. All extents for system and user defined classes are subsets of the extent is called OBJECT in the ODMG model depending on the application. An extent is a named persistent object whose value is a persistent collection that holds a collection of objects of the same type that are stored permanently in the database. The objects can be accessed and shared by multiple programs. It is also possible to create a transient collection which exists temporarily during the execution of a program but is not kept when the program terminates. For example a transient collection may be created in a program to hold the result of a query that selects some objects from a persistent collection and copies those objects into the transient collection. The program can then manipulate the objects in the transient collection and once the program terminates the transient collection ceases to exist. In general numerous collections transient or persistent may contain objects of the same type. The inheritance model discussed in this section is very simple. As we will see in Section the ODMG model distinguishes between type inheritance called interface inheritance and denoted by a colon and the extent inheritance constraint denoted by the keyword EXTEND. Other Object Oriented Concepts Polymorphism of Operations . Another characteristic of OO systems in general is that they provide for polymorphism of operations which is also known as operator overloading. This concept allows the same operator name or symbol to be bound to two or more different implementations of the operator depending on the type of objects to which the operator is applied. A simple example from programming languages can illustrate this concept. In some languages the operator symbol “+” can mean different things when applied to operands of different types. If the operands of “+” are of type integer the operation invoked is integer addition. If the operands of “+” are of type floating point the operation invoked is floating point addition. If the operands of “+” are of type set the operation invoked is set union. The compiler can determine which operation to execute based on the types of operands supplied. In OO databases a similar situation may occur. We can use the GEOMETRYOBJECT example presented in Section to illustrate operation in ODB. In this example the function Area is declared for all objects of type GEOMETRYOBJECT. However the implementation of the method for Area may differ for each subtype of GEOMETRYOBJECT. One possibility is to have a general implementation for calculating the area of a generalized GEOMETRYOBJECT and then to rewrite more efficient algorithms to calculate the areas of specific types of geometric objects such as a circle a rectangle a triangle and so on. In this case the Area function is overloaded by different implementations. The ODMS must now select the appropriate method for the Area function based on the type of geometric object to which it is applied. In strongly typed systems this can be done at compile time since the object types must be known. This is termed early binding. However in systems with weak typing or no typing the type of the object to which a function is applied may not be known until runtime. In this case the function must check the type of object at runtime and then invoke the appropriate method. This is often referred to as late binding. Multiple Inheritance and Selective Inheritance. Multiple inheritance occurs when a certain subtype T is a subtype of two types and hence inherits the functions of both supertypes. For example we may create a subtype ENGINEERINGMANAGER that is a subtype of both MANAGER and ENGINEER. This leads to the creation of a type lattice rather than a type hierarchy. One problem that can occur with multiple inheritance is that the supertypes from which the subtype inherits may have distinct functions of the same name creating an ambiguity. For example both MANAGER and ENGINEER may have a function called Salary. If the Salary function is implemented by different methods in the MANAGER and ENGINEER supertypes an ambiguity exists as to which of the two is inherited by the subtype ENGINEERINGMANAGER. It is possible however that both ENGINEER and MANAGER inherit Salary from the same supertype higher up in the lattice. The general rule is that if a function is inherited from some common supertype then it is inherited only once. In such a case there is no ambiguity the problem only arises if the functions are distinct in the two supertypes. There are several techniques for dealing with ambiguity in multiple inheritance. One solution is to have the system check for ambiguity when the subtype is created and to let the user explicitly choose which function is to be inherited at this time. A second solution is to use some system default. A third solution is to disallow multiple inheritance altogether if name ambiguity occurs instead forcing the user to change the name of one of the functions in one of the supertypes. Indeed some OO systems do not permit multiple inheritance at all. In the object database standard or by explicit naming. Type hierarchies and inheritance. Object types can be specified by using a type hierarchy which allows the inheritance of both attributes and methods of previously defined types. Multiple inheritance is allowed in some models. Extents. All persistent objects of a particular type can be stored in an extent. Extents corresponding to a type hierarchy have set subset constraints enforced on their collections of persistent objects. Polymorphism and operator overloading. Operations and method names can be overloaded to apply to different object types with different implementations. In the following sections we show how these concepts are realized in the SQL standard constructor. An array type for specifying collections is also provided. Other collection type constructors such as set list and bag constructors were not part of the original SQL Object specifications but were later included in the standard. A mechanism for specifying object identity through the use of reference type is included. Encapsulation of operations is provided through the mechanism of userdefined types that may include operations as part of their declaration. These are somewhat similar to the concept of abstract data types that were developed in programming languages. In addition the concept of userdefined routines allows the definition of general methods . Inheritance mechanisms are provided using the keyword UNDER. We now discuss each of these concepts in more detail. In our discussion we will refer to the example in Figure User Defined Types and Complex Structures for Objects To allow the creation of complex structured objects and to separate the declaration of a type from the creation of a table SQL now provides user defined types . In addition four collection types have been included to allow for multivalued types and attributes in order to specify complex structured objects rather than just simple records. The user will create the UDTs for a particular application as part of the database schema. A UDT may be specified in its simplest form using the following syntax CREATE TYPE TYPENAME AS Figure illustrates some of the object concepts in SQL. We will explain the examples in this figure gradually as we explain the concepts. First a UDT can be used as either the type for an attribute or as the type for a table. By using a UDT as the type for an attribute within another UDT a complex structure for objects in a table can be created much like that achieved by nesting type constructors. This is similar to using the struct type constructor of Section For example in Figure the UDT STREETADDRTYPE is used as the type for the STREETADDR attribute in the UDT USAADDRTYPE. Similarly the UDT USAADDRTYPE is in turn used as the type for the ADDR attribute in the UDT PERSONTYPE in Figure If a UDT does not have any operations as in the examples in Figure it is possible to use the concept of ROW TYPE to directly create a structured attribute Object Relational Features Object Database Extensions to SQL Figure Illustrating some of the object features of SQL. Using UDTs as types for attributes such as Address and Phone Specifying UDT for PERSONTYPE Specifying UDTs for STUDENTTYPE and EMPLOYEETYPE as two subtypes of PERSONTYPE CREATE TYPE STREETADDRTYPE AS CREATE TYPE USAADDRTYPE AS CREATE TYPE USAPHONETYPE AS CREATE TYPE PERSONTYPE AS RETURNS INTEGER CREATE INSTANCE METHOD AGE RETURNS INTEGER FOR PERSONTYPE BEGIN RETURN CODE TO CALCULATE A PERSON’S AGE FROM TODAY’S DATE AND END CREATE TYPE GRADETYPE AS CREATE TYPE STUDENTTYPE UNDER PERSONTYPE AS INSTANTIABLE NOT FINAL INSTANCE METHOD GPA RETURNS FLOAT CREATE INSTANCE METHOD GPA RETURNS FLOAT FOR STUDENTTYPE BEGIN RETURN CODE TO CALCULATE A STUDENT’S GPA FROM END CREATE TYPE EMPLOYEETYPE UNDER PERSONTYPE AS CREATE TYPE MANAGERTYPE UNDER EMPLOYEETYPE AS CREATE TABLE PERSON OF PERSONTYPE REF IS PERSONID SYSTEM GENERATED CREATE TABLE EMPLOYEE OF EMPLOYEETYPE UNDER PERSON CREATE TABLE MANAGER OF MANAGERTYPE UNDER EMPLOYEE CREATE TABLE STUDENT OF STUDENTTYPE UNDER PERSON CREATE TYPE COMPANYTYPE AS SCOPE Company REF SCOPE CREATE TABLE COMPANY OF COMPANYTYPE CREATE TABLE EMPLOYMENT OF EMPLOYMENTTYPE Chapter Object and Object Relational Databases Figure Illustrating some of the object features of SQL. Specifying UDTs for STUDENTTYPE and EMPLOYEETYPE as two subtypes of PERSONTYPE Creating tables based on some of the UDTs and illustrating table inheritance Specifying relationships using REF and SCOPE. Object Relational Features Object Database Extensions to SQL by using the keyword ROW. For example we could use the following instead of declaring STREETADDRTYPE as a separate type as in Figure CREATE TYPE USAADDRTYPE AS CITY VARCHAR ZIP VARCHAR To allow for collection types in order to create complex structured objects four constructors are now included in SQL ARRAY MULTISET LIST and SET. These are similar to the type constructors discussed in Section In the initial specification of SQL Object only the ARRAY type was specified since it can be used to simulate the other types but the three additional collection types were included in the latest version of the SQL standard. In Figure the PHONES attribute of PERSONTYPE has as its type an array whose elements are of the previously defined UDT USAPHONETYPE. This array has a maximum of four elements meaning that we can store up to four phone numbers per person. An array can also have no maximum number of elements if desired. An array type can have its elements referenced using the common notation of square brackets. For example refers to the first location value in a PHONES attribute . For example PHONES[CARDINALITY ] refers to the last element in the array. The commonly used dot notation is used to refer to components of a ROW TYPE or a UDT. For example refers to the CITY component of an ADDR attribute is created and inserted in the table. Encapsulation of Operations In SQL a user defined type can have its own behavioral specification by specifying methods in addition to the attributes. The general form of a UDT specification with methods is as follows CREATE TYPE TYPE NAME For example in Figure we declared a method Age that calculates the age of an individual object of type PERSONTYPE. The code for implementing the method still has to be written. We can refer to the method implementation by specifying the file that contains the code for the method or we can write the actual code within the type declaration itself returns a new object of that type. In the new UDT object every attribute is initialized to its default value. An observer function A is implicitly created for each attribute A to read its value. Hence A or returns the value of attribute A of TYPET if X is of type TYPET. A mutator function for updating an attribute sets the value of the attribute to a new value. SQL allows these functions to be blocked from public use an EXECUTE privilege is needed to have access to these functions. In general a UDT can have a number of user defined functions associated with it. The syntax is INSTANCE METHOD NAME RETURNS RETURNTYPE Object Relational Features Object Database Extensions to SQL Two types of functions can be defined internal SQL and external. Internal functions are written in the extended PSM language of SQL appearing in the UDT definition. An external function definition can be declared as follows DECLARE EXTERNAL FUNCTIONNAME SIGNATURE LANGUAGE LANGUAGENAME Attributes and functions in UDTs are divided into three categories PUBLIC PRIVATE PROTECTED It is also possible to define virtual attributes as part of UDTs which are computed and updated using functions. Specifying Inheritance and Overloading of Functions Recall that we already discussed many of the principles of inheritance in Section SQL has rules for dealing with type inheritance . In general both attributes and instance methods are inherited. The phrase NOT FINAL must be included in a UDT if subtypes are allowed to be created under that UDT where PERSONTYPE STUDENTTYPE and EMPLOYEETYPE are declared to be NOT FINAL . Associated with type inheritance are the rules for overloading of function implementations and for resolution of function names. These inheritance rules can be summarized as follows All attributes are inherited. The order of supertypes in the UNDER clause determines the inheritance hierarchy. An instance of a subtype can be used in every context in which a supertype instance is used. A subtype can redefine any function that is defined in its supertype with the restriction that the signature be the same. When a function is called the best match is selected based on the types of all arguments. For dynamic linking the runtime types of parameters is considered. Consider the following examples to illustrate type inheritance which are illustrated in Figure Suppose that we want to create two subtypes of PERSONTYPE EMPLOYEETYPE and STUDENTTYPE. In addition we also create a subtype MANAGERTYPE that inherits all the attributes of EMPLOYEETYPE but has an additional attribute DEPTMANAGED. These subtypes are shown in Figure In general we specify the local attributes and any additional specific methods for the subtype which inherits the attributes and operations of its supertype. Chapter Object and Object Relational Databases Another facility in SQL is table inheritance via the supertable subtable facility. This is also specified using the keyword UNDER to a tuple of another table. An example is shown in Figure The keyword SCOPE specifies the name of the table whose tuples can be referenced by the reference attribute. Notice that this is similar to a foreign key except that the system generated value is used rather than the primary key value. SQL uses a dot notation to build path expressions that refer to the component attributes of tuples and row types. However for an attribute whose type is REF the dereferencing symbol – is used. For example the query below retrieves employees working in the company named ‘ABCXYZ’ by querying the EMPLOYMENT table SELECT – NAME FROM EMPLOYMENT AS E WHERE – COMPNAME ‘ABCXYZ’ In SQL – is used for dereferencing and has the same meaning assigned to it in the C programming language. Thus if r is a reference to a tuple and a is a component attribute in that tuple then r – a is the value of attribute a in that tuple. If several relations of the same type exist SQL provides the SCOPE keyword by which a reference attribute may be made to point to a tuple within a specific table of that type. The ODMG Object Model and the Object Definition Language ODL As we discussed in the introduction to Chapter one of the reasons for the success of commercial relational DBMSs is the SQL standard. The lack of a standard for ODMSs for several years may have caused some potential users to shy away from converting to this new technology. Subsequently a consortium of ODMS vendors and users called ODMG proposed a standard that is known as the or ODMG standard. This was revised into ODMG and later to ODMG The standard is made up of several parts including the object model the object definition language the object query language and the bindings to object oriented programming languages. In this section we describe the ODMG object model and the ODL. In Section we discuss how to design an ODB from an EER conceptual schema. We will give an The ODMG Object Model and the Object Definition Language ODL overview of OQL in Section and the C++ language binding in Section Examples of how to use ODL OQL and the C++ language binding will use the UNIVERSITY database example introduced in Chapter In our description we will follow the ODMG object model as described in Cattell et al. It is important to note that many of the ideas embodied in the ODMG object model are based on two decades of research into conceptual modeling and object databases by many researchers. The incorporation of object concepts into the SQL relational database standard leading to object relational technology was presented in Section Overview of the Object Model of ODMG The ODMG object model is the data model upon which the object definition language and object query language are based. It is meant to provide a standard data model for object databases just as SQL describes a standard data model for relational databases. It also provides a standard terminology in a field where the same terms were sometimes used to describe different concepts. We will try to adhere to the ODMG terminology in this chapter. Many of the concepts in the ODMG model have already been discussed in Section and we assume the reader has read this section. We will point out whenever the ODMG terminology differs from that used in Section Objects and Literals. Objects and literals are the basic building blocks of the object model. The main difference between the two is that an object has both an object identifier and a state whereas a literal has a value but no object In either case the value can have a complex structure. The object state can change over time by modifying the object value. A literal is basically a constant value possibly having a complex structure but it does not change. An object has five aspects identifier name lifetime structure and creation. The object identifier is a unique system wide identifier or transient object . Lifetimes are independent of types that is some objects of a particular type may be transient whereas others may be persistent. The structure of an object specifies how the object is constructed by using the type constructors. The structure specifies whether an object is atomic or not. An atomic object refers to a single object that follows a user defined type such as Employee or Department. If an object is not atomic then it will be composed of other objects. For example a collection object is not an atomic object since its state will be a collection of other The term atomic object is different from how we defined the atom constructor in Section which referred to all values of built in data types. In the ODMG model an atomic object is any individual user defined object. All values of the basic built in data types are considered to be literals. Object creation refers to the manner in which an object can be created. This is typically accomplished via an operation new for a special ObjectFactory interface. We shall describe this in more detail later in this section. In the object model a literal is a value that does not have an object identifier. However the value may have a simple or complex structure. There are three types of literals atomic structured and collection. Atomic correspond to the values of basic data types and are predefined. The basic data types of the object model include long short and unsigned integer numbers regular and double precision floating point numbers Boolean values single characters character strings and enumeration types among others. Structured literals correspond roughly to values that are constructed using the tuple constructor described in Section The built in structured literals include Date Interval Time and Timestamp are considered literals. use of the word atomic in atomic literal corresponds to the way we used atom constructor in Section structures for Date Interval Time and Timestamp can be used to create either literal values or objects with identifiers. Figure Overview of the interface definitions for part of the ODMG object model. The basic Object interface inherited by all objects Some standard interfaces for structured literals The ODMG Object Model and the Object Definition Language ODL interface Object boolean sameas object copy void delete Class Date Object enum Weekday Sunday Monday Tuesday Wednesday Thursday Friday Saturday enum Month January February March April May June July August September October November December unsigned short year unsigned short month unsigned short day boolean isequal boolean isgreater Class Time Object unsigned short hour unsigned short minute unsigned short second unsigned short millisecond boolean isequal boolean isgreater Time addinterval Time subtractinterval Interval subtracttime class Timestamp Object unsigned short year unsigned short month unsigned short day unsigned short hour unsigned short minute unsigned short second unsigned short millisecond Timestamp plus Timestamp minus boolean isequal boolean isgreater class Interval Object unsigned short day unsigned short hour unsigned short minute unsigned short second unsigned short millisecond Interval plus Interval minus Interval product Interval quotient boolean isequal boolean isgreater interface Collection Object exception ElementNotFound Object element unsigned long cardinality boolean isempty boolean containselement void insertelement void removeelement raises iterator createiterator interface Iterator exception NoMoreElements boolean atend void reset Object getelement raises void nextposition raises interface set Collection set createunion boolean issubsetof interface bag Collection unsigned long occurrencesof Chapter Object and Object Relational Databases Figure Overview of the interface definitions for part of the ODMG object model. Some standard interfaces for structured literals Interfaces for collections and iterators. bag createunion interface list Collection exception lnvalidlndex unsignedlong index void removeelementat raises Object retrieveelementat raises void replaceelementat raises void insertelementafter raises void insertelementfirst void removefirstelement raises Object retrievefirstelement raises list concat void append interface array Collection exception lnvalidlndex unsignedlong index exception lnvalidSize unsignedlong size void removeelementat raises Object retrieveelementat raises void replaceelementat raises void resize raises struct association Object key Object value interface dictionary Collection exception DuplicateName string key exception KeyNotFound Object key void bind raises void unbind raises Object lookup raises boolean containskey The ODMG Object Model and the Object Definition Language ODL Chapter Object and Object Relational Databases Collection literals specify a literal value that is a collection of objects or values but the collection itself does not have an Objectid. The collections in the object model can be defined by the type generators set T bag T list T and array T where T is the type of objects or values in the Another collection type is dictionary K V which is a collection of associations K V where each K is a key associated with a value V this can be used to create an index on a collection of values V. Figure gives a simplified view of the basic types and type generators of the object model. The notation of ODMG uses three concepts interface literal and class. Following the ODMG terminology we use the word behavior to refer to operations and state to refer to properties . An interface specifies only behavior of an object type and is typically noninstantiable . Although an interface may have state properties as part of its specifications these cannot be inherited from the interface. Hence an interface serves to define operations that can be inherited by other interfaces as well as by classes that define the user defined objects for a particular application. A class specifies both state and behavior of an object type and is instantiable. Hence database and application objects are typically created based on the user specified class declarations that form a database schema. Finally a literal declaration specifies state but no behavior. Thus a literal instance holds a simple or complex structured value but has neither an object identifier nor encapsulated operations. Figure is a simplified version of the object model. For the full specifications see Cattell et al. We will describe some of the constructs shown in Figure as we describe the object model. In the object model all objects inherit the basic interface operations of Object shown in Figure these include operations such as copy delete and sameas The result returned by this operation is Boolean and would be true if the identity of P is the same as that of O and false otherwise. Similarly to create a copy P of object O we write P An alternative to the dot notation is the arrow notation O– sameas or O– copy. are similar to the corresponding type constructors described in Section operations are defined on objects for locking purposes which are not shown in Figure We discuss locking concepts for databases in Chapter The ODMG Object Model and the Object Definition Language ODL Inheritance in the Object Model of ODMG In the ODMG object model two types of inheritance relationships exist behavioronly inheritance and state plus behavior inheritance. Behavior inheritance is also known as ISA or interface inheritance and is specified by the colon Hence in the ODMG object model behavior inheritance requires the supertype to be an interface whereas the subtype could be either a class or another interface. The other inheritance relationship called EXTENDS inheritance is specified by the keyword extends. It is used to inherit both state and behavior strictly among classes so both the supertype and the subtype must be classes. Multiple inheritance via extends is not permitted. However multiple inheritance is allowed for behavior inheritance via the colon notation. Hence an interface may inherit behavior from several other interfaces. A class may also inherit behavior from several interfaces via colon notation in addition to inheriting behavior and state from at most one other class via extends. In Section we will give examples of how these two inheritance relationships “ ” and extends may be used. Built in Interfaces and Classes in the Object Model Figure shows the built in interfaces and classes of the object model. All interfaces such as Collection Date and Time inherit the basic Object interface. In the object model there is a distinction between collection objects whose state contains multiple objects or literals versus atomic objects whose state is an individual object or literal. Collection objects inherit the basic Collection interface shown in Figure which shows the operations for all collection objects. Given a collection object O the operation returns the number of elements in the collection. The operation returns true if the collection O is empty and returns false otherwise. The operations and insert or remove an element E from the collection O. Finally the operation returns true if the collection O includes element E and returns false otherwise. The operation I creates an iterator object I for the collection object O which can iterate over each element in the collection. The interface for iterator objects is also shown in Figure The operation sets the iterator at the first element in a collection and sets the iterator to the next element. The retrieves the current element which is the element at which the iterator is currently positioned. The ODMG object model uses exceptions for reporting errors or particular conditions. For example the ElementNotFound exception in the Collection interface would ODMG report also calls interface inheritance as type subtype is a and generalization specialization relationships although in the literature these terms have been used to describe inheritance of both state and operations operation if E is not an element in the collection O. The NoMoreElements exception in the iterator interface would be raised by the operation if the iterator is currently positioned at the last element in the collection and hence no more elements exist for the iterator to point to. Collection objects are further specialized into set list bag array and dictionary which inherit the operations of the Collection interface. A set T type generator can be used to create objects such that the value of object O is a set whose elements are of type T. The Set interface includes the additional operation P and createdifference. Operations for set comparison include the operation which returns true if the set object O is a subset of some other set object S and returns false otherwise. Similar operations issupersetof and ispropersupersetof. The bag T type generator allows duplicate elements in the collection and also inherits the Collection interface. It has three operations createunion createintersection and createdifference that all return a new object of type bag T . A list T object type inherits the Collection operations and can be used to create collections where the order of the elements is important. The value of each such object O is an ordered list whose elements are of type T. Hence we can refer to the first last and ith element in the list. Also when we add an element to the list we must specify the position in the list where the element is inserted. Some of the list operations are shown in Figure If O is an object of type list T the operation inserts the element E before the first element in the list O so that E becomes the first element in the list. A similar operation is . The operation in Figure inserts the element E after the ith element in the list O and will raise the exception InvalidIndex if no ith element exists in O. A similar operation is . To remove elements from the list the operations are E E and E at these operations remove the indicated element from the list and return the element as the operation’s result. Other operations retrieve an element without removing it from the list. These are E E lastelement and E . Also two operations to manipulate lists are defined. They are P which creates a new list P that is the concatenation of lists O and I and which appends the elements of list I to the end of list O . The array T object type also inherits the Collection operations and is similar to list. Specific operations for an array object O are which replaces the array element at position I with element E E which retrieves the ith element and replaces it with a NULL value and The ODMG Object Model and the Object Definition Language ODL E which simply retrieves the ith element of the array. Any of these operations can raise the exception InvalidIndex if I is greater than the array’s size. The operation changes the number of array elements to N. The last type of collection objects are of type dictionary K V . This allows the creation of a collection of association pairs K V where all K values are unique. This allows for associative retrieval of a particular pair given its key value . If O is a collection object of type dictionary K V then binds value V to the key K as an association K V in the collection whereas removes the association with key K from O and V returns the value V associated with key K in O. The latter two operations can raise the exception KeyNotFound. Finally returns true if key K exists in O and returns false otherwise. Figure is a diagram that illustrates the inheritance hierarchy of the built in constructs of the object model. Operations are inherited from the supertype to the subtype. The collection interfaces described above are not directly instantiable that is one cannot directly create objects based on these interfaces. Rather the interfaces can be used to generate user defined collection types of type set bag list array or dictionary for a particular database application. If an attribute or class has a collection type say a set then it will inherit the operations of the set interface. For example in a UNIVERSITY database application the user can specify a type for set STUDENT whose state would be sets of STUDENT objects. The programmer can then use the operations for set T to manipulate an instance of type set STUDENT . Creating application classes is typically done by utilizing the object definition language ODL . Collection Object Iterator Date Interval Time set list bag dictionary Timestamp array Figure Inheritance hierarchy for the built in interfaces of the object model. Chapter Object and Object Relational Databases Atomic Objects The previous section described the built in collection types of the object model. Now we discuss how object types for atomic objects can be constructed. These are specified using the keyword class in ODL. In the object model any user defined object that is not a collection object is called an atomic object. For example in a UNIVERSITY database application the user can specify an object type for STUDENT objects. Most such objects will be structured objects for example a STUDENT object will have a complex structure with many attributes relationships and operations but it is still considered atomic because it is not a collection. Such a user defined atomic object type is defined as a class by specifying its properties and operations. The properties define the state of the object and are further distinguished into attributes and relationships. In this subsection we elaborate on the three types of components attributes relationships and operations that a user defined object type for atomic objects can include. We illustrate our discussion with the two classes EMPLOYEE and DEPARTMENT shown in Figure An attribute is a property that describes some aspect of an object. Attributes have values that are stored within the object. However attribute values can also be Objectids of other objects. Attribute values can even be specified via methods that are used to calculate the attribute value. In Figure the attributes for EMPLOYEE are Name Ssn Birthdate Sex and Age and those for DEPARTMENT are Dname Dnumber Mgr Locations and Projs. The Mgr and Projs attributes of DEPARTMENT have complex structure and are defined via struct which corresponds to the tuple constructor of Section Hence the value of Mgr in each DEPARTMENT object will have two components Manager whose value is an Objectid that references the EMPLOYEE object that manages the DEPARTMENT and Startdate whose value is a date. The locations attribute of DEPARTMENT is defined via the set constructor since each DEPARTMENT object can have a set of locations. A relationship is a property that specifies that two objects in the database are related. In the object model of ODMG only binary relationships notation in Figure which will be discussed in more detail in Section discusses how a relationship can be represented by two attributes in inverse directions. The ODMG Object Model and the Object Definition Language ODL Figure The attributes relationships and operations in a class definition. By specifying inverses the database system can maintain the referential integrity of the relationship automatically. That is if the value of Worksfor for a particular EMPLOYEE E refers to DEPARTMENT D then the value of Hasemps for DEPARTMENT D must include a reference to E in its set of EMPLOYEE references. If the database designer desires to have a relationship to be represented in only one direction then it has to be modeled as an attribute . An example is the Manager component of the Mgr attribute in DEPARTMENT. In addition to attributes and relationships the designer can include operations in object type specifications. Each object type can have a number of operation signatures which specify the operation name its argument types and its returned value if applicable. Operation names are unique within each object type but they can be overloaded by having the same operation name appear in distinct object types. The operation signature can also specify the names of exceptions that can class EMPLOYEE attribute string Name attribute string Ssn attribute date Birthdate attribute enum Gender M F Sex attribute short Age relationship DEPARTMENT Worksfor inverse DEPARTMENT Hasemps void reassignemp raises class DEPARTMENT attribute string Dname attribute short Dnumber attribute struct Deptmgr EMPLOYEE Manager date Startdate Mgr attribute set string Locations attribute struct Projs string Projname time Weeklyhours Projs relationship set EMPLOYEE Hasemps inverse EMPLOYEE Worksfor void addemp raises void changemanager occur during operation execution. The implementation of the operation will include the code to raise these exceptions. In Figure the EMPLOYEE class has one operation reassignemp and the DEPARTMENT class has two operations addemp and changemanager. Extents Keys and Factory Objects In the ODMG object model the database designer can declare an extent for any object type that is defined via a class declaration. The extent is given a name and it will contain all persistent objects of that class. Hence the extent behaves as a set object that holds all persistent objects of the class. In Figure the EMPLOYEE and DEPARTMENT classes have extents called ALLEMPLOYEES and ALLDEPARTMENTS respectively. This is similar to creating two objects one of type set EMPLOYEE and the second of type set DEPARTMENT and making them persistent by naming them ALLEMPLOYEES and ALLDEPARTMENTS. Extents are also used to automatically enforce the set subset relationship between the extents of a supertype and its subtype. If two classes A and B have extents ALLA and ALLB and class B is a subtype of class A then the collection of objects in ALLB must be a subset of those in ALLA at any point. This constraint is automatically enforced by the database system. A class with an extent can have one or more keys. A key consists of one or more properties whose values are constrained to be unique for each object in the extent. For example in Figure the EMPLOYEE class has the Ssn attribute as key and the DEPARTMENT class has two distinct keys Dname and Dnumber . For a composite that is made of several properties the properties that form the key are contained in parentheses. For example if a class VEHICLE with an extent ALLVEHICLES has a key made up of a combination of two attributes State and Licensenumber they would be placed in parentheses as in the key declaration. Next we present the concept of factory object an object that can be used to generate or create individual objects via its operations. Some of the interfaces of factory objects that are part of the ODMG object model are shown in Figure The interface ObjectFactory has a single operation new which returns a new object with an Objectid. By inheriting this interface users can create their own factory interfaces for each user defined object type and the programmer can implement the operation new differently for each type of object. Figure also shows a DateFactory interface which has additional operations for creating a new calendardate and for creating an object whose value is the currentdate among other operations interface SetFactory ObjectFactory Set newofsize interface ListFactory ObjectFactory List newofsize interface ArrayFactory ObjectFactory Array newofsize interface DictionaryFactory ObjectFactory Dictionary newofsize interface DateFactory ObjectFactory exception InvalidDate Date calendardate raises Date current interface DatabaseFactory Database new interface Database void open raises void close raises void bind raises Object unbind raises Object Iookup raises Chapter Object and Object Relational Databases Finally we discuss the concept of a database. Because an ODBMS can create many different databases each with its own schema the ODMG object model has interfaces for DatabaseFactory and Database objects as shown in Figure Each database has its own database name and the bind operation can be used to assign individual unique names to persistent objects in a particular database. The lookup operation returns an object from the database that has the specified objectname and the unbind operation removes the name of a persistent named object from the database. The Object Definition Language ODL After our overview of the ODMG object model in the previous section we now show how these concepts can be utilized to create an object database schema using the object definition language The ODL is designed to support the semantic constructs of the ODMG object model and is independent of any particular programming language. Its main use is to create object specifications that is classes and interfaces. Hence ODL is not a full programming language. A user can specify a database schema in ODL independently of any programming language and then use the specific language bindings to specify how ODL constructs can be mapped to constructs in specific programming languages such as C++ Smalltalk and Java. We will give an overview of the C++ binding in Section Figure shows a possible object schema for part of the UNIVERSITY database which was presented in Chapter We will describe the concepts of ODL using this example and the one in Figure The graphical notation for Figure is shown in Figure and can be considered as a variation of EER diagrams and attributes of relationships. Figure shows one possible set of ODL class definitions for the UNIVERSITY database. In general there may be several possible mappings from an object schema diagram into ODL classes. We will discuss these options further in Section Figure shows the straightforward way of mapping part of the UNIVERSITY database from Chapter Entity types are mapped into ODL classes and inheritance is done using extends. However there is no direct way to map categories or to do multiple inheritance. In Figure the classes PERSON FACULTY STUDENT and GRADSTUDENT have the extents PERSONS FACULTY STUDENTS and GRADSTUDENTS respectively. Both FACULTY and STUDENT extends PERSON and GRADSTUDENT extends STUDENT. Hence the collection of STUDENTS will be constrained to be a subset of the ODL syntax and data types are meant to be compatible with the Interface Definition language of CORBA with extensions for relationships and other database concepts. Interface Person IF Class STUDENT PERSON Worksin Hasfaculty Hasmajors DEPARTMENT GRADSTUDENT Registeredin FACULTYSTUDENT Advisor Committee Advises COURSE Offeredby Majorsin Completedsections Hassections Students Ofcourse Offers SECTION Registeredstudents Oncommitteeof CURRSECTION Relationships M N Inheritance Interface inheritance using “ ” Class inheritance using extends The ODMG Object Model and the Object Definition Language ODL Figure An example of a database schema. Graphical notation for representing ODL schemas. A graphical object database schema for part of the UNIVERSITY database . collection of PERSONs at any time. Similarly the collection of GRADSTUDENTs will be a subset of STUDENTs. At the same time individual STUDENT and FACULTY objects will inherit the properties and operations of PERSON and individual GRADSTUDENT objects will inherit those of STUDENT. The classes DEPARTMENT COURSE SECTION and CURRSECTION in Figure are straightforward mappings of the corresponding entity types in Figure Chapter Object and Object Relational Databases Figure Possible ODL schema for the UNIVERSITY database in Figure class PERSON attribute struct Pname string Fname string Mname string Lname Name attribute string Ssn attribute date Birthdate attribute enum Gender M F Sex attribute struct Address short No string Street short Aptno string City string State short Zip Address short Age class FACULTY extends PERSON attribute string Rank attribute float Salary attribute string Office attribute string Phone relationship DEPARTMENT Worksin inverse DEPARTMENT Has faculty relationship set GRADSTUDENT Advises inverse GRADSTUDENT Advisor relationship set GRADSTUDENT Oncommitteeof inverse GRADSTUDENT Committee void giveraise void promote class GRADE attribute enum GradeValues A B C D F l P Grade relationship SECTION Section inverse SECTION Students relationship STUDENT Student inverse STUDENT Completedsections class STUDENT extends PERSON attribute string Class attribute DEPARTMENT Minorsin relationship DEPARTMENT Majorsin inverse DEPARTMENT Hasmajors relationship set GRADE Completedsections inverse GRADE Student relationship set CURRSECTION Registeredin INVERSE CURRSECTION Registeredstudents void changemajor raises float gpa void register raises void assigngrade raises class DEGREE attribute string College attribute string Degree attribute string Year class GRADSTUDENT extends STUDENT attribute set DEGREE Degrees relationship FACULTY Advisor inverse FACULTY Advises relationship set FACULTY Committee inverse FACULTY Oncommitteeof void assignadvisor raises void assigncommitteemember raises class DEPARTMENT attribute string Dname attribute string Dphone attribute string Doffice attribute string College attribute FACULTY Chair relationship set FACULTY Hasfaculty inverse FACULTY Worksin relationship set STUDENT Hasmajors inverse STUDENT Majorsin relationship set COURSE Offers inverse COURSE Offeredby class COURSE attribute string Cname attribute string Cno attribute string Description relationship set SECTION Hassections inverse SECTION Ofcourse relationship DEPARTMENT Offeredby inverse DEPARTMENT Offers class SECTION attribute short Secno attribute string Year attribute enum Quarter Fall Winter Spring Summer Qtr relationship set GRADE Students inverse GRADE Section relationship course Ofcourse inverse COURSE Hassections class CURRSECTION extends SECTION relationship set STUDENT Registeredstudents inverse STUDENT Registeredin void registerstudent raises The ODMG Object Model and the Object Definition Language ODL Figure An illustration of interface inheritance via “ ”. Graphical schema representation Corresponding interface and class definitions in ODL. interface GeometryObject attribute enum Shape RECTANGLE TRIANGLE CIRCLE Shape attribute struct Point short x short y Referencepoint float perimeter float area void translate void rotate class RECTANGLE GeometryObject attribute struct Point short x short y Referencepoint attribute short Length attribute short Height attribute float Orientationangle class TRIANGLE GeometryObject attribute struct Point short x short y Referencepoint attribute short attribute short attribute float attribute float class CIRCLE GeometryObject attribute struct Point short x short y Referencepoint attribute short Radius TRIANGLE GeometryObject R ECTANG LE CIRCLE . . . However the class GRADE requires some explanation. The GRADE class corresponds to the M N relationship between STUDENT and SECTION in Figure The reason it was made into a separate class is because it includes the relationship attribute Grade. Hence the M N relationship is mapped to the class GRADE and a pair of relationships one between STUDENT and GRADE and the other between SECTION and Chapter Object and Object Relational Databases will discuss alternative mappings for attributes of relationships in Section Object Database Conceptual Design These relationships are represented by the following relationship properties Completedsections of STUDENT Section and Student of GRADE and Students of SECTION inheritance. Figure is part of a database schema for storing geometric objects. An interface GeometryObject is specified with operations to calculate the perimeter and area of a geometric object plus operations to translate and rotate an object. Several classes inherit the GeometryObject interface. Since GeometryObject is an interface it is noninstantiable that is no objects can be created based on this interface directly. However objects of type RECTANGLE TRIANGLE CIRCLE can be created and these objects inherit all the operations of the GeometryObject interface. Note that with interface inheritance only operations are inherited not properties . Hence if a property is needed in the inheriting class it must be repeated in the class definition as with the Referencepoint attribute in Figure Notice that the inherited operations can have different implementations in each class. For example the implementations of the area and perimeter operations may be different for RECTANGLE TRIANGLE and CIRCLE. Multiple inheritance of interfaces by a class is allowed as is multiple inheritance of interfaces by another interface. However with the extends inheritance multiple inheritance is not permitted. Hence a class can inherit via extends from at most one class . Object Database Conceptual Design Section discusses how object database design differs from relational database design. Section outlines a mapping algorithm that can be used to create an ODB schema made of ODMG ODL class definitions from a conceptual EER schema. Differences between Conceptual Design of ODB and RDB One of the main differences between ODB and RDB design is how relationships are handled. In ODB relationships are typically handled by having relationship properties or reference attributes that include OID of the related objects. These can be considered as OID references to the related objects. Both single references and collections of references are allowed. References for a binary relationship can be declared is similar to how an M N relationship is mapped in the relational model . in a single direction or in both directions depending on the types of access expected. If declared in both directions they may be specified as inverses of one another thus enforcing the ODB equivalent of the relational referential integrity constraint. In RDB relationships among tuples are specified by attributes with matching values. These can be considered as value references and are specified via foreign keys which are values of primary key attributes repeated in tuples of the referencing relation. These are limited to being single valued in each record because multivalued attributes are not permitted in the basic relational model. Thus M N relationships must be represented not directly but as a separate relation as discussed in Section Mapping binary relationships that contain attributes is not straightforward in ODBs since the designer must choose in which direction the attributes should be included. If the attributes are included in both directions then redundancy in storage will exist and may lead to inconsistent data. Hence it is sometimes preferable to use the relational approach of creating a separate table by creating a separate class to represent the relationship. This approach can also be used for n ary relationships with degree n Another major area of difference between ODB and RDB design is how inheritance is handled. In ODB these structures are built into the model so the mapping is achieved by using the inheritance constructs such as derived and extends. In relational design as we discussed in Section there are several options to choose from since no built in construct exists for inheritance in the basic relational model. It is important to note though that object relational and extended relational systems are adding features to model these constructs directly as well as to include operation specifications in abstract data types . Declare an extent for each class and specify any key attributes as keys of the extent. Step Add relationship properties or reference attributes for each binary relationship into the ODL classes that participate in the relationship. These may be created in one or both directions. If a binary relationship is represented by references in both directions declare the references to be relationship properties that are inverses of one another if such a facility If a binary relationship is represented by a reference in only one direction declare the reference to be an attribute in the referencing class whose type is the referenced class name. Depending on the cardinality ratio of the binary relationship the relationship properties or reference attributes may be single valued or collection types. They will be single valued for binary relationships in the or directions they are collection types can be used to create a structure of the form reference relationship attributes which may be included instead of the reference attribute. However this does not allow the use of the inverse constraint. Additionally if this choice is represented in both directions the attribute values will be represented twice creating redundancy. implicitly uses a tuple constructor at the top level of the type declaration but in general the tuple constructor is not explicitly shown in the ODL class declarations. analysis of the application domain is needed to decide which constructor to use because this information is not available from the EER schema. ODL standard provides for the explicit definition of inverse relationships. Some ODBMS products may not provide this support in such cases programmers must maintain every relationship explicitly by coding the methods that update the objects appropriately. decision whether to use set or list is not available from the EER schema and must be determined from the requirements. Chapter Object and Object Relational Databases Step Include appropriate operations for each class. These are not available from the EER schema and must be added to the database design by referring to the original requirements. A constructor method should include program code that checks any constraints that must hold when a new object is created. A destructor method should check any constraints that may be violated when an object is deleted. Other methods should include any further constraint checks that are relevant. Step An ODL class that corresponds to a subclass in the EER schema inherits the type and methods of its superclass in the ODL schema. Its specific attributes relationship references and operations are specified as discussed in steps and Step Weak entity types can be mapped in the same way as regular entity types. An alternative mapping is possible for weak entity types that do not participate in any relationships except their identifying relationship these can be mapped as though they were composite multivalued attributes of the owner entity type by using the or constructors. The attributes of the weak entity are included in the construct which corresponds to a tuple constructor. Attributes are mapped as discussed in steps and Step Categories in an EER schema are difficult to map to ODL. It is possible to create a mapping similar to the EER to relational mapping of objects from the class. Looking at the extent names in Figure the named object DEPARTMENTS is of type set DEPARTMENT PERSONS is of type set PERSON FACULTY is of type set FACULTY and so on. The use of an extent name DEPARTMENTS in an entry point refers to a persistent collection of objects. Whenever a collection is referenced in an OQL query we should define an iterator in ranges over each object in the collection. In many cases as in the query will select certain objects from the collection based on the conditions specified in the where clause. In only persistent objects D in the collection of DEPARTMENTS that satisfy the condition ‘Engineering’ are selected for the query result. For each selected object D the value of is retrieved in the query result. Hence the type of the result for is bag string because the type of each Dname value is string . In general the result of a query would be of type bag for select from and of type set for select distinct from as in SQL . Using the example in there are three syntactic options for specifying iterator variables D in DEPARTMENTS DEPARTMENTS D DEPARTMENTS AS D is similar to the tuple variables that range over tuples in SQL queries. Chapter Object and Object Relational Databases We will use the first construct in our The named objects used as database entry points for OQL queries are not limited to the names of extents. Any named persistent object whether it refers to an atomic object or to a collection object can be used as a database entry point. Query Results and Path Expressions In general the result of a query can be of any type that can be expressed in the ODMG object model. A query does not have to follow the select from where structure in the simplest case any persistent name on its own is a query whose result is a reference to that persistent object. For example the query DEPARTMENTS returns a reference to the collection of all persistent DEPARTMENT objects whose type is set DEPARTMENT . Similarly suppose we had given then the query CSDEPARTMENT returns a reference to that individual object of type DEPARTMENT. Once an entry point is specified the concept of a path expression can be used to specify a path to related attributes and objects. A path expression typically starts at a persistent object name or at the iterator variable that ranges over individual objects in a collection. This name will be followed by zero or more relationship names or attribute names connected using the dot notation. For example referring to the UNIVERSITY database in Figure the following are examples of path expressions which are also valid queries in OQL .Rank The first expression returns an object of type FACULTY because that is the type of the attribute Chair of the DEPARTMENT class. This will be a reference to the FACULTY object that is related to the DEPARTMENT object whose persistent name is CSDEPARTMENT via the attribute Chair that is a reference to the FACULTY object who is chairperson of the Computer Science department. The second expression is similar except that it returns the Rank of this FACULTY object rather than the object reference hence the type returned by is string which is the data type for the Rank attribute of the FACULTY class. Path expressions and return single values because the attributes Chair and Rank are both single valued and they are applied to a single object. The third expression is different it returns an object of type set FACULTY even when applied to a single object because that is the type of the that the latter two options are similar to the syntax for specifying tuple variables in SQL queries. The Object Query Language OQL relationship Hasfaculty of the DEPARTMENT class. The collection returned will include references to all FACULTY objects that are related to the DEPARTMENT object whose persistent name is CSDEPARTMENT via the relationship Hasfaculty that is references to all FACULTY objects who are working in the Computer Science department. Now to return the ranks of Computer Science faculty we cannot write .Rank because it is not clear whether the object returned would be of type set string or bag string . Because of this type of ambiguity problem OQL does not allow expressions such as Rather one must use an iterator variable over any collections as in or below select from F in select distinct from F in Here returns bag string whereas returns set string . Both and illustrate how an iterator variable can be defined in the from clause to range over a restricted collection specified in the query. The variable F in and ranges over the elements of the collection which is of type set FACULTY and includes only those faculty who are members of the Computer Science department. In general an OQL query can return a result with a complex structure specified in the query itself by utilizing the struct keyword. Consider the following examples .Advises select struct degrees from D in from S in .Advises Here is straightforward returning an object of type set GRADSTUDENT as its result this is the collection of graduate students who are advised by the chair of the Computer Science department. Now suppose that a query is needed to retrieve the last and first names of these graduate students plus the list of previous degrees of each. This can be written as in where the variable S ranges over the collection of graduate students advised by the chairperson and the variable D ranges over the degrees of each such student S. The type of the result of is a collection of structs where each struct has two components name and degrees. mentioned earlier struct corresponds to the tuple constructor discussed in Section Chapter Object and Object Relational Databases The name component is a further struct made up of lastname and firstname each being a single string. The degrees component is defined by an embedded query and is itself a collection of further structs each with three string components deg yr and college. Note that OQL is orthogonal with respect to specifying path expressions. That is attributes relationships and operation names can be used interchangeably within the path expressions as long as the type system of OQL is not compromised. For example one can write the following queries to retrieve the grade point average of all senior students majoring in Computer Science with the result ordered by GPA and within that by last and first name select struct from S in where ‘senior’ order by gpa desc lastname asc firstname asc select struct from S in STUDENTS where .Dname ‘Computer Science’ and ‘senior’ order by gpa desc lastname asc firstname asc used the named entry point CSDEPARTMENT to directly locate the reference to the Computer Science department and then locate the students via the relationship Hasmajors whereas searches the STUDENTS extent to locate all students majoring in that department. Notice how attribute names relationship names and operation names are all used interchangeably in the path expressions gpa is an operation Majorsin and Hasmajors are relationships and Class Name Dname Lname and Fname are attributes. The implementation of the gpa operation computes the grade point average and returns its value as a float type for each selected STUDENT. The order by clause is similar to the corresponding SQL construct and specifies in which order the query result is to be displayed. Hence the collection returned by a query with an order by clause is of type list. Other Features of OQL Specifying Views as Named Queries. The view mechanism in OQL uses the concept of a named query. The define keyword is used to specify an identifier of the named query which must be a unique name among all named objects class names method names and function names in the schema. If the identifier has the same name as an existing named query then the new definition replaces the previous definition. Once defined a query definition is persistent until it is redefined or deleted. A view can also have parameters in its definition. The Object Query Language OQL For example the following view defines a named query Hasminors to retrieve the set of objects for students minoring in a given department define Hasminors as select S from S in STUDENTS where .Dname Deptname Because the ODL schema in Figure only provided a unidirectional Minorsin attribute for a STUDENT we can use the above view to represent its inverse without having to explicitly define a relationship. This type of view can be used to represent inverse relationships that are not expected to be used frequently. The user can now utilize the above view to write queries such as Hasminors which would return a bag of students minoring in the Computer Science department. Note that in Figure we defined Hasmajors as an explicit relationship presumably because it is expected to be used more often. Extracting Single Elements from Singleton Collections. An OQL query will in general return a collection as its result such as a bag set or list . If the user requires that a query only return a single element there is an element operator in OQL that is guaranteed to return a single element E from a singleton collection C that contains only one element. If C contains more than one element or if C is empty then the element operator raises an exception. For example returns the single object reference to the Computer Science department element Since a department name is unique across all departments the result should be one department. The type of the result is D DEPARTMENT. Collection Operators . Because many query expressions specify collections as their result a number of operators have been defined that are applied to such collections. These include aggregate operators as well as membership and quantification over a collection. The aggregate operators operate over a The operator count returns an integer type. The remaining aggregate operators return the same type as the type of the operand collection. Two examples follow. The query returns the number of students minoring in Computer Science and returns the average GPA of all seniors majoring in Computer Science. correspond to aggregate functions in SQL. Chapter Object and Object Relational Databases count avg Notice that aggregate operations can be applied to any collection of the appropriate type and can be used in any part of a query. For example the query to retrieve all department names that have more than majors can be written as in select from D in DEPARTMENTS where count The membership and quantification expressions return a Boolean type that is true or false. Let V be a variable C a collection expression B an expression of type Boolean and E an element of the type of elements in collection C. Then returns true if element E is a member of collection C. returns true if all the elements of collection C satisfy B. returns true if there is at least one element in C satisfying B. To illustrate the membership condition suppose we want to retrieve the names of all students who completed the course called ‘Database Systems I’. This can be written as in where the nested query returns the collection of course names that each STUDENT S has completed and the membership condition returns true if ‘Database Systems I’ is in the collection for a particular STUDENT S select .Lname .Fname from S in STUDENTS where ‘Database Systems I’ in also illustrates a simpler way to specify the select clause of queries that return a collection of structs the type returned by is bag struct . One can also write queries that return true false results. As an example let us assume that there is a named object called JEREMY of type STUDENT. Then query answers the following question Is Jeremy a Computer Science minor Similarly answers the question Are all Computer Science graduate students advised by Computer Science faculty Both and return true or false which are interpreted as yes or no answers to the above questions JEREMY in Hasminors for all G in in The Object Query Language OQL Note that query also illustrates how attribute relationship and operation inheritance applies to queries. Although S is an iterator that ranges over the extent GRADSTUDENTS we can write because the Majorsin relationship is inherited by GRADSTUDENT from STUDENT via extends Ordered Collection Expressions. As we discussed in Section collections that are lists and arrays have additional operations such as retrieving the ith first and last elements. Additionally operations exist for extracting a subcollection and concatenating two lists. Hence query expressions that involve lists or arrays can invoke these operations. We will illustrate a few of these operations using sample queries. retrieves the last name of the faculty member who earns the highest salary first from F in FACULTY order by salary desc illustrates the use of the first operator on a list collection that contains the salaries of faculty members sorted in descending order by salary. Thus the first element in this sorted list contains the faculty member with the highest salary. This query assumes that only one faculty member earns the maximum salary. The next query retrieves the top three Computer Science majors based on GPA. from S in order by gpa desc The select from order by query returns a list of Computer Science students ordered by GPA in descending order. The first element of an ordered collection has an index position of so the expression returns a list containing the first second and third elements of the select from order by result. The Grouping Operator. The group by clause in OQL although similar to the corresponding clause in SQL provides explicit reference to the collection of objects within each group or partition. First we give an example and then we describe the general form of these queries. retrieves the number of majors in each department. In this query the students are grouped into the same partition if they have the same major that is the Chapter Object and Object Relational Databases same value for .Dname from S in STUDENTS group by deptname .Dname The result of the grouping specification is of type set struct which contains a struct for each group that has two components the grouping attribute value and the bag of the STUDENT objects in the group . The select clause returns the grouping attribute and a count of the number of elements in each partition where partition is the keyword used to refer to each partition. The result type of the select clause is set struct . In general the syntax for the group by clause is group by Fk Ek where Fk Ek is a list of partitioning attributes and each partitioning attribute specification Fi Ei defines an attribute name Fi and an expression Ei . The result of applying the grouping is a set of structures Fk Tk partition bag B where Ti is the type returned by the expression Ei partition is a distinguished field name and B is a structure whose fields are the iterator variables . In the previous query is modified to illustrate the having clause . retrieves for each department having more than majors the average GPA of its majors. The having clause in selects only those partitions that have more than elements . select deptname avggpa avg from S in STUDENTS group by deptname .Dname having count Note that the select clause of returns the average GPA of the students in the partition. The expression select from P in partition returns a bag of student GPAs for that partition. The from clause declares an iterator variable P over the partition collection which is of type bag struct . Then the path expression is used to access the GPA of each student in the partition. Overview of the C++ Language Binding in the ODMG Standard Overview of the C++ Language Binding in the ODMG Standard The C++ language binding specifies how ODL constructs are mapped to C++ constructs. This is done via a C++ class library that provides classes and operations that implement the ODL constructs. An object manipulation language is needed to specify how database objects are retrieved and manipulated within a C++ program and this is based on the C++ programming language syntax and semantics. In addition to the ODL OML bindings a set of constructs called physical pragmas are defined to allow the programmer some control over physical storage issues such as clustering of objects utilizing indexes and memory management. The class library added to C++ for the ODMG standard uses the prefix d for class declarations that deal with database The goal is that the programmer should think that only one language is being used not two separate languages. For the programmer to refer to database objects in a program a class DRef T is defined for each database class T in the schema. Hence program variables of type DRef T can refer to both persistent and transient objects of class T. In order to utilize the various built in types in the ODMG object model such as collection types various template classes are specified in the library. For example an abstract class DObject T specifies the operations to be inherited by all objects. Similarly an abstract class DCollection T specifies the operations of collections. These classes are not instantiable but only specify the operations that can be inherited by all objects and by collection objects respectively. A template class is specified for each type of collection these include DSet T DList T DBag T DVarray T and DDictionary T and correspond to the collection types in the object model dUshort dLong and dFloat are provided. In addition to the basic data types several structured literal types are provided to correspond to the structured literal types of the ODMG object model. These include dString dInterval dDate dTime and dTimestamp STUDENT the programmer creates a named persistent object of type STUDENT in database with persistent name JohnSmith. Another operation deleteobject can be used to delete objects. Object modification is done by the operations defined in each class by the programmer. The C++ binding also allows the creation of extents by using the library class dExtent. For example by writing DExtent PERSON the programmer would create a named collection object ALLPERSONS whose type would be DSet PERSON in the database that would hold persistent objects of type PERSON. However key constraints are not supported in the C++ binding and any key checks must be programmed in the class Also the C++ binding does not support persistence via reachability the object must be statically declared to be persistent at the time it is created. Summary In this chapter we started in Section with an overview of the concepts utilized in object databases and discussed how these concepts were derived from general object oriented principles. The main concepts we discussed were object identity and identifiers encapsulation of operations inheritance complex structure of objects through nesting of type constructors and how objects are made persistent. Then in Section we showed how many of these concepts were incorporated into the relational model and the SQL standard leading to expanded relational database functionality. These systems have been called object relational databases. have only provided a brief overview of the C++ binding. For full details see Cattell and Barry eds. Ch. Review Questions We then discussed the ODMG standard for object databases. We started by describing the various constructs of the object model in Sction The various built in types such as Object Collection Iterator set list and so on were described by their interfaces which specify the built in operations of each type. These built in types are the foundation upon which the object definition language and object query language are based. We also described the difference between objects which have an ObjectId and literals which are values with no OID. Users can declare classes for their application that inherit operations from the appropriate built in interfaces. Two types of properties can be specified in a user defined class attributes and relationships in addition to the operations that can be applied to objects of the class. The ODL allows users to specify both interfaces and classes and permits two different types of inheritance interface inheritance via “ ” and class inheritance via extends. A class can have an extent and keys. A description of ODL followed and an example database schema for the UNIVERSITY database was used to illustrate the ODL constructs. Following the description of the ODMG object model we described a general technique for designing object database schemas in Section We discussed how object databases differ from relational databases in three main areas references to represent relationships inclusion of operations and inheritance. Finally we showed how to map a conceptual database design in the EER model to the constructs of object databases. In Section we presented an overview of the object query language . The OQL follows the concept of orthogonality in constructing queries meaning that an operation can be applied to the result of another operation as long as the type of the result is of the correct input type for the operation. The OQL syntax follows many of the constructs of SQL but includes additional concepts such as path expressions inheritance methods relationships and collections. Examples of how to use OQL over the UNIVERSITY database were given. Next we gave an overview of the C++ language binding in Section which extends C++ class declarations with the ODL type constructors but permits seamless integration of C++ with the ODBMS. In Sun endorsed the ODMG API . technologies was the first corporation to deliver an ODMG compliant DBMS. Many ODBMS vendors including Object Design Gemstone Systems POET Software and Versant Object Technology have endorsed the ODMG standard. Review Questions What are the origins of the object oriented approach What primary characteristics should an OID possess Discuss the various type constructors. How are they used to create complex object structures Chapter Object and Object Relational Databases Discuss the concept of encapsulation and tell how it is used to create abstract data types. Explain what the following terms mean in object oriented database terminology method signature message collection extent. What is the relationship between a type and its subtype in a type hierarchy What is the constraint that is enforced on extents corresponding to types in the type hierarchy What is the difference between persistent and transient objects How is persistence handled in typical OO database systems How do regular inheritance multiple inheritance and selective inheritance differ Discuss the concept of polymorphism operator overloading. Discuss how each of the following features is realized in SQL object type inheritance encapsulation of operations and complex object structures. In the traditional relational model creating a table defined both the table type and the table itself . How can these two concepts be separated in SQL Describe the rules of inheritance in SQL What are the differences and similarities between objects and literals in the ODMG object model List the basic operations of the following built in interfaces of the ODMG object model Object Collection Iterator Set List Bag Array and Dictionary. Describe the built in structured literals of the ODMG object model and the operations of each. What are the differences and similarities of attribute and relationship properties of a user defined class What are the differences and similarities of class inhertance via extends and interface inheritance via “ ”in the ODMG object model Discuss how persistence is specified in the ODMG object model in the C++ binding. Why are the concepts of extents and keys important in database applications Describe the following OQL concepts database entry points path expressions iterator variables named queries aggregate functions grouping and quantifiers. Exercises What is meant by the type orthogonality of OQL Discuss the general principles behind the C++ binding of the ODMG standard. What are the main differences between designing a relational database and an object database Describe the steps of the algorithm for object database design by EER to OO mapping. Exercises Convert the example of GEOMETRYOBJECTs given in Section from the functional notation to the notation given in Figure that distinguishes between attributes and operations. Use the keyword INHERIT to show that one class inherits from another class. Compare inheritance in the EER model and other Internet applications provide Web interfaces to access information stored in one or more databases. These databases are often referred to as data sources. It is common to use two tier and three tier client server architectures for Internet applications . Although HTML is widely used for formatting and structuring Web documents it is not suitable for specifying structured data that is extracted from databases. A new language namely XML has emerged as the standard for structuring and exchanging data over the Web. XML can be used to provide information about the structure and meaning of the data in the Web pages rather than just specifying how the Web pages are formatted for display on the screen. The formatting aspects are specified separately for example by using a formatting language such as XSL or a transformation language such as XSLT . Recently XML has also been proposed as a possible model for data storage and retrieval although only a few experimental database systems based on XML have been developed so far. Basic HTML is useful for generating static Web pages with fixed text and other objects but most e commerce applications require Web pages that provide interactive features with the user. For example consider the case of an airline customer who wants to check the arrival time and gate information of a particular flight. The user may enter information such as a date and flight number in certain form fields chapter Chapter XML Extensible Markup Language of the Web page. The Web program must first submit a query to the airline database to retrieve this information and then display it. Such Web pages where part of the information is extracted from databases or other data sources are called dynamic Web pages because the data extracted and displayed each time will be for different flights and dates. In this chapter we will focus on describing the XML data model and its associated languages and how data extracted from relational databases can be formatted as XML documents to be exchanged over the Web. Section discusses the difference between structured semistructured and unstructured data. Section presents the XML data model which is based on tree structures as compared to the flat relational data model structures. In Section we focus on the structure of XML documents and the languages for specifying the structure of these documents such as DTD and XML Schema. Section shows the relationship between XML and relational databases. Section describes some of the languages associated with XML such as XPath and XQuery. Section discusses how data extracted from relational databases can be formatted as XML documents. Finally Section is the chapter summary. Structured Semistructured and Unstructured Data The information stored in databases is known as structured data because it is represented in a strict format. For example each record in a relational database table such as each of the tables in the COMPANY database in Figure the same format as the other records in that table. For structured data it is common to carefully design the database schema using techniques such as those described in Chapters and in order to define the database structure. The DBMS then checks to ensure that all data follows the structures and constraints specified in the schema. However not all data is collected and inserted into carefully designed structured databases. In some applications data is collected in an ad hoc manner before it is known how it will be stored and managed. This data may have a certain structure but not all the information collected will have the identical structure. Some attributes may be shared among the various entities but other attributes may exist only in a few entities. Moreover additional attributes can be introduced in some of the newer data items at any time and there is no predefined schema. This type of data is known as semistructured data. A number of data models have been introduced for representing semistructured data often based on using tree or graph data structures rather than the flat relational model structures. A key difference between structured and semistructured data concerns how the schema constructs are handled. In semistructured data the schema information is mixed in with the data values since each data object can have different attributes that are not known in advance. Hence this type of data is sometimes referred to as self describing data. Consider the following example. We want to collect a list of bibliographic references Structured Semistructured and Unstructured Data Number Location Project Project Company projects Name ‘Product X’ ‘Bellaire’ WorkerWorker Last Hours name Ssn Hours First name Ssn ‘Smith’ ‘Joyce’ Figure Representing semistructured data as a graph. related to a certain research project. Some of these may be books or technical reports others may be research articles in journals or conference proceedings and still others may refer to complete journal issues or conference proceedings. Clearly each of these may have different attributes and different types of information. Even for the same type of reference say conference articles we may have different information. For example one article citation may be quite complete with full information about author names title proceedings page numbers and so on whereas another citation may not have all the information available. New types of bibliographic sources may appear in the future for instance references to Web pages or to conference tutorials and these may have new attributes that describe them. Semistructured data may be displayed as a directed graph as shown in Figure The information shown in Figure corresponds to some of the structured data shown in Figure As we can see this model somewhat resembles the object model and relationships. The internal nodes represent individual objects or composite attributes. The leaf nodes represent actual data values of simple attributes. There are two main differences between the semistructured model and the object model that we discussed in Chapter The schema information names of attributes relationships and classes in the semistructured model is intermixed with the objects and their data values in the same data structure. In the semistructured model there is no requirement for a predefined schema to which the data objects must conform although it is possible to define a schema if necessary. Chapter XML Extensible Markup Language Figure Part of an HTML document representing unstructured data. HTML HEAD HEAD BODY of company projects and the employees in each ProductX TABLE TR TD face “Arial” John Smith FONT TD hours per week TD TR TR TD face “Arial” Joyce English FONT TD hours per week TD TR TABLE ProductY TABLE TR TD face “Arial” John Smith FONT TD hours per week TD TR TR TD face “Arial” Joyce English FONT TD hours per week TD TR TR TD width face “Arial” Franklin Wong FONT TD hours per week TD TR TABLE BODY HTML In addition to structured and semistructured data a third category exists known as unstructured data because there is very limited indication of the type of data. A typical example is a text document that contains information embedded within it. Web pages in HTML that contain some data are considered to be unstructured data. Consider part of an HTML file shown in Figure Text that appears between angled brackets is an HTML tag. A tag with a slash indicates an end tag which represents the ending of the effect of a matching start tag. The tags mark Structured Semistructured and Unstructured Data up the in order to instruct an HTML processor how to display the text between a start tag and a matching end tag. Hence the tags specify document formatting rather than the meaning of the various data elements in the document. HTML tags specify information such as font size and style color heading levels in documents and so on. Some tags provide text structuring in documents such as specifying a numbered or unnumbered list or a table. Even these structuring tags specify that the embedded textual data is to be displayed in a certain manner rather than indicating the type of data represented in the table. HTML uses a large number of predefined tags which are used to specify a variety of commands for formatting Web documents for display. The start and end tags specify the range of text to be formatted by each command. A few examples of the tags shown in Figure follow The HTML HTML tags specify the boundaries of the document. The document header information within the HEAD HEAD tags specifies various commands that will be used elsewhere in the document. For example it may specify various script functions in a language such as JavaScript or PERL or certain formatting styles that can be used in the document. It can also specify a title to indicate what the HTML file is for and other similar information that will not be displayed as part of the document. The body of the document specified within the BODY BODY tags includes the document text and the markup tags that specify how the text is to be formatted and displayed. It can also include references to other objects such as images videos voice messages and other documents. The tags specify that the text is to be displayed as a level heading. There are many heading levels and so on each displaying text in a less prominent heading format. The TABLE TABLE tags specify that the following text is to be displayed as a table. Each table row in the table is enclosed within TR TR tags and the individual table data elements in a row are displayed within TD TD Some tags may have attributes which appear within the start tag and describe additional properties of the In Figure the TABLE start tag has four attributes describing various characteristics of the table. The following TD and FONT start tags have one and two attributes respectively. HTML has a very large number of predefined tags and whole books are devoted to describing how to use these tags. If designed properly HTML documents can be is why it is known as HyperText Markup Language. stands for table row and TD stands for table data. is how the term attribute is used in document markup languages which differs from how it is used in database models. Chapter XML Extensible Markup Language formatted so that humans are able to easily understand the document contents and are able to navigate through the resulting Web documents. However the source HTML text documents are very difficult to interpret automatically by computer programs because they do not include schema information about the type of data in the documents. As e commerce and other Internet applications become increasingly automated it is becoming crucial to be able to exchange Web documents among various computer sites and to interpret their contents automatically. This need was one of the reasons that led to the development of XML. In addition an extendible version of HTML called XHTML was developed that allows users to extend the tags of HTML for different applications and allows an XHTML file to be interpreted by standard XML processing programs. Our discussion will focus on XML only. The example in Figure illustrates a static HTML page since all the information to be displayed is explicitly spelled out as fixed text in the HTML file. In many cases some of the information to be displayed may be extracted from a database. For example the project names and the employees working on each project may be extracted from the database in Figure through the appropriate SQL query. We may want to use the same HTML formatting tags for displaying each project and the employees who work on it but we may want to change the particular projects being displayed. For example we may want to see a Web page displaying the information for ProjectX and then later a page displaying the information for ProjectY. Although both pages are displayed using the same HTML formatting tags the actual data items displayed will be different. Such Web pages are called dynamic since the data parts of the page may be different each time it is displayed even though the display appearance is the same. XML Hierarchical Data Model We now introduce the data model used in XML. The basic object in XML is the XML document. Two main structuring concepts are used to construct an XML document elements and attributes. It is important to note that the term attribute in XML is not used in the same manner as is customary in database terminology but rather as it is used in document description languages such as HTML and Attributes in XML provide additional information that describes elements as we will see. There are additional concepts in XML such as entities identifiers and references but first we concentrate on describing elements and attributes to show the essence of the XML model. Figure shows an example of an XML element called Projects . As in HTML elements are identified in a document by their start tag and end tag. The tag names are enclosed between angled brackets and end tags are further identified by a slash is a more general language for describing documents and provides capabilities for specifying new tags. However it is more complex than HTML and XML. left and right angled bracket characters are reserved characters as are the ampersand apostrophe and single quotation mark . To include them within the text of a document they must be encoded with escapes as &lt &gt &amp &apos and &quot respectively. XML Hierarchical Data Model Figure A complex XML element called Projects . xml version standalone “yes” Projects Project Name ProductX Name Location Bellaire Location Worker Lastname Smith Lastname Worker Worker Firstname Joyce Firstname Worker Project Project Name ProductY Name Location Sugarland Location Worker Worker Worker Worker Worker Worker Project Projects Complex elements are constructed from other elements hierarchically whereas simple elements contain data values. A major difference between XML and HTML is that XML tag names are defined to describe the meaning of the data elements in the document rather than to describe how the text is to be displayed. This makes it possible to process the data elements in the XML document automatically by computer programs. Also the XML tag names can be defined in another document known as the schema document to give a semantic meaning to the tag names Chapter XML Extensible Markup Language that can be exchanged among multiple users. In HTML all tag names are predefined and fixed that is why they are not extendible. It is straightforward to see the correspondence between the XML textual representation shown in Figure and the tree structure shown in Figure In the tree representation internal nodes represent complex elements whereas leaf nodes represent simple elements. That is why the XML model is called a tree model or a hierarchical model. In Figure the simple elements are the ones with the tag names Name Number Location Deptno Ssn Lastname Firstname and Hours . The complex elements are the ones with the tag names Projects Project and Worker . In general there is no limit on the levels of nesting of elements. It is possible to characterize three main types of XML documents Data centric XML documents. These documents have many small data items that follow a specific structure and hence may be extracted from a structured database. They are formatted as XML documents in order to exchange them over or display them on the Web. These usually follow a predefined schema that defines the tag names. Document centric XML documents. These are documents with large amounts of text such as news articles or books. There are few or no structured data elements in these documents. Hybrid XML documents. These documents may have parts that contain structured data and other parts that are predominantly textual or unstructured. They may or may not have a predefined schema. XML documents that do not follow a predefined schema of element names and corresponding tree structure are known as schemaless XML documents. It is important to note that data centric XML documents can be considered either as semistructured data or as structured data as defined in Section If an XML document conforms to a predefined XML schema or DTD within which they appear. It is also possible to use XML attributes to hold the values of simple data elements however this is generally not recommended. An exception to this rule is in cases that need to reference another element in another part of the XML document. To do this it is common to use attribute values in one element as the references. This resembles the concept of foreign keys in relational databases and is a way to get around the strict hierarchical model that the XML tree model implies. We discuss XML attributes further in Section when we discuss XML schema and DTD. XML Documents DTD and XML Schema XML Documents DTD and XML Schema Well Formed and Valid XML Documents and XML DTD In Figure we saw what a simple XML document may look like. An XML document is well formed if it follows a few conditions. In particular it must start with an XML declaration to indicate the version of XML being used as well as any other relevant attributes as shown in the first line in Figure It must also follow the syntactic guidelines of the tree data model. This means that there should be a single root element and every element must include a matching pair of start and end tags within the start and end tags of the parent element. This ensures that the nested elements specify a well formed tree structure. A well formed XML document is syntactically correct. This allows it to be processed by generic processors that traverse the document and create an internal tree representation. A standard model with an associated set of API functions called DOM allows programs to manipulate the resulting tree representation corresponding to a well formed XML document. However the whole document must be parsed beforehand when using DOM in order to convert the document to that standard DOM internal data structure representation. Another API called SAX allows processing of XML documents on the fly by notifying the processing program through callbacks whenever a start or end tag is encountered. This makes it easier to process large documents and allows for processing of so called streaming XML documents where the processing program can process the tags as they are encountered. This is also known as event based processing. A well formed XML document can be schemaless that is it can have any tag names for the elements within the document. In this case there is no predefined set of elements that a program processing the document knows to expect. This gives the document creator the freedom to specify new elements but limits the possibilities for automatically interpreting the meaning or semantics of the elements within the document. A stronger criterion is for an XML document to be valid. In this case the document must be well formed and it must follow a particular schema. That is the element names used in the start and end tag pairs must follow the structure specified in a separate XML DTD file or XML schema file. We first discuss XML DTD here and then we give an overview of XML schema in Section Figure shows a simple XML DTD file which specifies the elements and their nested structures. Any valid documents conforming to this DTD should follow the specified structure. A special syntax exists for specifying DTD files as illustrated in Figure First a name is given to the root tag of the document which is called Projects in the first line in Figure Then the elements and their nested structure are specified. Chapter XML Extensible Markup Language Figure An XML DTD file called Projects. When specifying elements the following notation is used A following the element name means that the element can be repeated zero or more times in the document. This kind of element is known as an optional multivalued element. A + following the element name means that the element can be repeated one or more times in the document. This kind of element is a required multivalued element. A following the element name means that the element can be repeated zero or one times. This kind is an optional single valued element. An element appearing without any of the preceding three symbols must appear exactly once in the document. This kind is a required single valued element. The type of the element is specified via parentheses following the element. If the parentheses include names of other elements these latter elements are the children of the element in the tree structure. If the parentheses include the keyword #PCDATA or one of the other data types available in XML DTD the element is a leaf node. PCDATA stands for parsed character data which is roughly similar to a string data type. The list of attributes that can appear within an element can also be specified via the keyword !ATTLIST. In Figure the Project element has an attribute ProjId. If the type of an attribute is ID then it can be referenced from another attribute whose type is IDREF within another element. Notice that attributes can also be used to hold the values of simple data elements of type #PCDATA. Parentheses can be nested when specifying elements. A bar symbol specifies that either or can appear in the document. !DOCTYPE Projects [ !ELEMENT Projects !ELEMENT Project !ATTLIST Project ProjId ID #REQUIRED !ELEMENT Name !ELEMENT Number !ELEMENT Location !ELEMENT Deptno !ELEMENT Workers !ELEMENT Worker !ELEMENT Ssn !ELEMENT Lastname !ELEMENT Firstname !ELEMENT Hours ] XML Documents DTD and XML Schema We can see that the tree structure in Figure and the XML document in Figure conform to the XML DTD in Figure To require that an XML document be checked for conformance to a DTD we must specify this in the declaration of the document. For example we could change the first line in Figure to the following xml standalone “no” !DOCTYPE Projects SYSTEM “” When the value of the standalone attribute in an XML document is “no” the document needs to be checked against a separate DTD document or XML schema document . The DTD file shown in Figure should be stored in the same file system as the XML document and should be given the file name . Alternatively we could include the DTD document text at the beginning of the XML document itself to allow the checking. Although XML DTD is quite adequate for specifying tree structures with required optional and repeating elements and with various types of attributes it has several limitations. First the data types in DTD are not very general. Second DTD has its own special syntax and thus requires specialized processors. It would be advantageous to specify XML schema documents using the syntax rules of XML itself so that the same processors used for XML documents could process XML schema descriptions. Third all DTD elements are always forced to follow the specified ordering of the document so unordered elements are not permitted. These drawbacks led to the development of XML schema a more general but also more complex language for specifying the structure and elements of XML documents. XML Schema The XML schema language is a standard for specifying the structure of XML documents. It uses the same syntax rules as regular XML documents so that the same processors can be used on both. To distinguish the two types of documents we will use the term XML instance document or XML document for a regular XML document and XML schema document for a document that specifies an XML schema. Figure shows an XML schema document corresponding to the COMPANY database shown in Figures and Although it is unlikely that we would want to display the whole database as a single document there have been proposals to store data in native XML format as an alternative to storing the data in relational databases. The schema in Figure would serve the purpose of specifying the structure of the COMPANY database if it were stored in a native XML system. We discuss this topic further in Section As with XML DTD XML schema is based on the tree data model with elements and attributes as the main structuring concepts. However it borrows additional concepts from database and object models such as keys references and identifiers. Here we describe the features of XML schema in a step by step manner referring to the sample XML schema document in Figure for illustration. We introduce and describe some of the schema concepts in the order in which they are used in Figure Chapter XML Extensible Markup Language Figure An XML schema file called company. xml xsd schema xsd annotation xsd documentation xml lang “en” Company Schema Prepared by Babak Hojabri xsd documentation xsd annotation xsd element name “company” xsd complexType xsd sequence xsd element name “department” type “Department” maxOccurs “unbounded” xsd element name “employee” type “Employee” maxOccurs “unbounded” xsd unique name “dependentNameUnique” xsd selector xpath “employeeDependent” xsd field xpath “dependentName” xsd unique xsd element xsd element name “project” type “Project” maxOccurs “unbounded” xsd sequence xsd complexType xsd unique name “departmentNameUnique” xsd selector xpath “department” xsd field xpath “departmentName” xsd unique xsd unique name “projectNameUnique” xsd selector xpath “project” xsd field xpath “projectName” xsd unique xsd key name “projectNumberKey” xsd selector xpath “project” xsd field xpath “projectNumber” xsd key xsd key name “departmentNumberKey” xsd selector xpath “department” xsd field xpath “departmentNumber” xsd key xsd key name “employeeSSNKey” xsd selector xpath “employee” xsd field xpath “employeeSSN” xsd key xsd keyref name “departmentManagerSSNKeyRef” refer “employeeSSNKey” xsd selector xpath “department” xsd field xpath “departmentManagerSSN” xsd keyref xsd keyref name “employeeDepartmentNumberKeyRef” refer “departmentNumberKey” xsd selector xpath “employee” xsd field xpath “employeeDepartmentNumber” xsd keyref xsd keyref name “employeeSupervisorSSNKeyRef” refer “employeeSSNKey” xsd selector xpath “employee” xsd field xpath “employeeSupervisorSSN” xsd keyref xsd keyref name “projectDepartmentNumberKeyRef” refer “departmentNumberKey” xsd selector xpath “project” xsd field xpath “projectDepartmentNumber” xsd keyref xsd keyref name “projectWorkerSSNKeyRef” refer “employeeSSNKey” xsd selector xpath “project projectWorker” xsd field xpath “SSN” xsd keyref xsd keyref name “employeeWorksOnProjectNumberKeyRef” refer “projectNumberKey” xsd selector xpath “employee employeeWorksOn” xsd field xpath “projectNumber” xsd keyref xsd element xsd complexType name “Department” xsd sequence xsd element name “departmentName” type “xsd string” xsd element name “departmentNumber” type “xsd string” xsd element name “departmentManagerSSN” type “xsd string” xsd element name “departmentManagerStartDate” type “xsd date” xsd element name “departmentLocation” type “xsd string” maxOccurs “unbounded” xsd sequence xsd complexType xsd complexType name “Employee” xsd sequence xsd element name “employeeName” type “Name” xsd element name “employeeSSN” type “xsd string” xsd element name “employeeSex” type “xsd string” xsd element name “employeeSalary” type “xsd unsignedInt” xsd element name “employeeBirthDate” type “xsd date” xsd element name “employeeDepartmentNumber” type “xsd string” xsd element name “employeeSupervisorSSN” type “xsd string” xsd element name “employeeAddress” type “Address” xsd element name “employeeWorksOn” type “WorksOn” maxOccurs “unbounded” xsd element name “employeeDependent” type “Dependent” maxOccurs “unbounded” xsd sequence xsd complexType xsd complexType name “Project” xsd sequence xsd element name “projectName” type “xsd string” xsd element name “projectNumber” type “xsd string” xsd element name “projectLocation” type “xsd string” XML Documents DTD and XML Schema Figure An XML schema called company. xsd element name “projectDepartmentNumber” type “xsd string” xsd element name “projectWorker” type “Worker” maxOccurs “unbounded” xsd sequence xsd complexType xsd complexType name “Dependent” xsd sequence xsd element name “dependentName” type “xsd string” xsd element name “dependentSex” type “xsd string” xsd element name “dependentBirthDate” type “xsd date” xsd element name “dependentRelationship” type “xsd string” xsd sequence xsd complexType xsd complexType name “Address” xsd sequence xsd element name “number” type “xsd string” xsd element name “street” type “xsd string” xsd element name “city” type “xsd string” xsd element name “state” type “xsd string” xsd sequence xsd complexType xsd complexType name “Name” xsd sequence xsd element name “firstName” type “xsd string” xsd element name “middleName” type “xsd string” xsd element name “lastName” type “xsd string” xsd sequence xsd complexType xsd complexType name “Worker” xsd sequence xsd element name “SSN” type “xsd string” xsd element name “hours” type “xsd float” xsd sequence xsd complexType xsd complexType name “WorksOn” xsd sequence xsd element name “projectNumber” type “xsd string” xsd element name “hours” type “xsd float” xsd sequence xsd complexType xsd schema Chapter XML Extensible Markup Language Schema descriptions and XML namespaces. It is necessary to identify the specific set of XML schema language elements being used by specifying a file stored at a Web site location. The second line in Figure specifies XML Documents DTD and XML Schema the file used in this example which is This is a commonly used standard for XML schema commands. Each such definition is called an XML namespace because it defines the set of commands that can be used. The file name is assigned to the variable xsd using the attribute xmlns and this variable is used as a prefix to all XML schema commands . For example in Figure when we write xsd element or xsd sequence we are referring to the definitions of the element and sequence tags as defined in the file Annotations documentation and language used. The next couple of lines in Figure illustrate the XML schema elements xsd annotation and xsd documentation which are used for providing comments and other descriptions in the XML document. The attribute xml lang of the xsd documentation element specifies the language being used where en stands for the English language. Elements and types. Next we specify the root element of our XML schema. In XML schema the name attribute of the xsd element tag specifies the element name which is called company for the root element in our example directly instead of having a separate matching end tag. These are called empty elements examples are the xsd element elements named department and project in Figure Specifying element type and minimum and maximum occurrences. In XML schema the attributes type minOccurs and maxOccurs in the xsd element tag specify the type and multiplicity of each element in any document that conforms to the schema specifications. If we specify a type attribute in an xsd element the structure of the element must be described separately typically using the xsd complexType element of XML schema. This is illustrated by the employee department and project elements in Figure On the other hand if no type attribute is specified the element structure can be defined directly following the tag as illustrated by the company root element in Figure The minOccurs and maxOccurs tags are used for specifying lower and upper bounds on the number of occurrences of an element in Chapter XML Extensible Markup Language any XML document that conforms to the schema specifications. If they are not specified the default is exactly one occurrence. These serve a similar role to the + and symbols of XML DTD. Specifying keys. In XML schema it is possible to specify constraints that correspond to unique and primary key constraints in a relational database constraints database attributes where null is allowed we need to specify minOccurs whereas for multivalued database attributes we need to specify maxOccurs “unbounded” on the corresponding element. Notice that if we were not going to specify any key constraints we could have embedded the subelements within the parent element definitions directly without having to specify complex types. However when unique primary key and foreign key constraints need to be specified we must define complex types to specify the element structures. Composite attributes. Composite attributes from Figure are also specified as complex types in Figure as illustrated by the Address Name Worker and WorksOn complex types. These could have been directly embedded within their parent elements. This example illustrates some of the main features of XML schema. There are other features but they are beyond the scope of our presentation. In the next section we discuss the different approaches to creating XML documents from relational databases and storing XML documents. Storing and Extracting XML Documents from Databases Storing and Extracting XML Documents from Databases Several approaches to organizing the contents of XML documents to facilitate their subsequent querying and retrieval have been proposed. The following are the most common approaches Using a DBMS to store the documents as text. A relational or object DBMS can be used to store whole XML documents as text fields within the DBMS records or objects. This approach can be used if the DBMS has a special module for document processing and would work for storing schemaless and document centric XML documents. Using a DBMS to store the document contents as data elements. This approach would work for storing a collection of documents that follow a specific XML DTD or XML schema. Because all the documents have the same structure one can design a relational database to store the leaf level data elements within the XML documents. This approach would require mapping algorithms to design a database schema that is compatible with the XML document structure as specified in the XML schema or DTD and to recreate the XML documents from the stored data. These algorithms can be implemented either as an internal DBMS module or as separate middleware that is not part of the DBMS. Designing a specialized system for storing native XML data. A new type of database system based on the hierarchical model could be designed and implemented. Such systems are being called Native XML DBMSs. The system would include specialized indexing and querying techniques and would work for all types of XML documents. It could also include data compression techniques to reduce the size of the documents for storage. Tamino by Software AG and the Dynamic Application Platform of eXcelon are two popular products that offer native XML DBMS capability. Oracle also offers a native XML storage option. Creating or publishing customized XML documents from preexisting relational databases. Because there are enormous amounts of data already stored in relational databases parts of this data may need to be formatted as documents for exchanging or displaying over the Web. This approach would use a separate middleware software layer to handle the conversions needed between the XML documents and the relational database. Section discusses this approach in which data centric XML documents are extracted from existing databases in more detail. In particular we show how tree structured documents can be created from graph structured databases. Section discusses the problem of cycles and how to deal with it. All of these approaches have received considerable attention. We focus on the fourth approach in Section because it gives a good conceptual understanding of the differences between the XML tree data model and the traditional database models Chapter XML Extensible Markup Language based on flat files and graph representations . But first we give an overview of XML query languages in Section XML Languages There have been several proposals for XML query languages and two query language standards have emerged. The first is XPath which provides language constructs for specifying path expressions to identify certain nodes or attributes within an XML document that match specific patterns. The second is XQuery which is a more general query language. XQuery uses XPath expressions but has additional constructs. We give an overview of each of these languages in this section. Then we discuss some additional languages related to HTML in Section XPath Specifying Path Expressions in XML An XPath expression generally returns a sequence of items that satisfy a certain pattern as specified by the expression. These items are either values or elements or attributes. The most common type of XPath expression returns a collection of element or attribute nodes that satisfy certain patterns specified in the expression. The names in the XPath expression are node names in the XML document tree that are either tag names or attribute names possibly with additional qualifier conditions to further restrict the nodes that satisfy the pattern. Two main separators are used when specifying a path single slash and double slash . A single slash before a tag specifies that the tag must appear as a direct child of the previous tag whereas a double slash specifies that the tag can appear as a descendant of the previous tag at any level. Let us look at some examples of XPath as shown in Figure The first XPath expression in Figure returns the company root node and all its descendant nodes which means that it returns the whole XML document. We should note that it is customary to include the file name in the XPath query. This allows us to specify any local file name or even any path name that specifies a file on the Web. For example if the COMPANY XML document is stored at the location .com then the first XPath expression in Figure can be written as doc company This prefix would also be included in the other examples of XPath expressions. Figure Some examples of XPath expressions on XML documents that follow the XML schema file company in Figure company company department employee [employeeSalary gt company employee [employeeSalary gt company project projectWorker [hours ge XML Languages The second example in Figure returns all department nodes and their descendant subtrees. Note that the nodes in an XML document are ordered so the XPath result that returns multiple nodes will do so in the same order in which the nodes are ordered in the document tree. The third XPath expression in Figure illustrates the use of which is convenient to use if we do not know the full path name we are searching for but do know the name of some tags of interest within the XML document. This is particularly useful for schemaless XML documents or for documents with many nested levels of The expression returns all employeeName nodes that are direct children of an employee node such that the employee node has another child element employeeSalary whose value is greater than This illustrates the use of qualifier conditions which restrict the nodes selected by the XPath expression to those that satisfy the condition. XPath has a number of comparison operations for use in qualifier conditions including standard arithmetic string and set comparison operations. The fourth XPath expression in Figure should return the same result as the previous one except that we specified the full path name in this example. The fifth expression in Figure returns all projectWorker nodes and their descendant nodes that are children under a path company project and have a child node hours with a value greater than hours. When we need to include attributes in an XPath expression the attribute name is prefixed by the @ symbol to distinguish it from element names. It is also possible to use the wildcard symbol which stands for any element as in the following example which retrieves all elements that are child elements of the root regardless of their element type. When wildcards are used the result can be a sequence of different types of items. company The examples above illustrate simple XPath expressions where we can only move down in the tree structure from a given node. A more general model for path expressions has been proposed. In this model it is possible to move in multiple directions from the current node in the path expression. These are known as the axes of an XPath expression. Our examples above used only three of these axes child of the current node descendent or self at any level of the current node and attribute of the current node . Other axes include parent ancestor previous sibling and next sibling . These axes allow for more complex path expressions. The main restriction of XPath path expressions is that the path that specifies the pattern also specifies the items to be retrieved. Hence it is difficult to specify certain conditions on the pattern while separately specifying which result items should be use the terms node tag and element interchangeably here. Chapter XML Extensible Markup Language retrieved. The XQuery language separates these two concerns and provides more powerful constructs for specifying queries. XQuery Specifying Queries in XML XPath allows us to write expressions that select items from a tree structured XML document. XQuery permits the specification of more general queries on one or more XML documents. The typical form of a query in XQuery is known as a FLWR expression which stands for the four main clauses of XQuery and has the following form FOR variable bindings to individual nodes LET variable bindings to collections of nodes WHERE qualifier conditions RETURN query result specification There can be zero or more instances of the FOR clause as well as of the LET clause in a single XQuery. The WHERE clause is optional but can appear at most once and the RETURN clause must appear exactly once. Let us illustrate these clauses with the following simple example of an XQuery. LET $d doc FOR $x IN $d company project[projectNumber $y IN $d company employee WHERE $x hours gt AND $ $ RETURN res $y employeeName firstName $y employeeName lastName $x hours res Variables are prefixed with the $ sign. In the above example $d $x and $y are variables. The LET clause assigns a variable to a particular expression for the rest of the query. In this example $d is assigned to the document file name. It is possible to have a query that refers to multiple documents by assigning multiple variables in this way. The FOR clause assigns a variable to range over each of the individual items in a sequence. In our example the sequences are specified by path expressions. The $x variable ranges over elements that satisfy the path expression $d company project[projectNumber The $y variable ranges over elements that satisfy the path expression $d company employee. Hence $x ranges over projectWorker elements whereas $y ranges over employee elements. The WHERE clause specifies additional conditions on the selection of items. In this example the first condition selects only those projectWorker elements that satisfy the condition employee [employeeSalary gt RETURN res $x firstName $x lastName res FOR $x IN doc company employee WHERE $x employeeSalary gt RETURN res $x employeeName firstName $x employeeName lastName res FOR $x IN doc company project[projectNumber $y IN doc company employee WHERE $x hours gt AND $ $ RETURN res $y employeeName firstName $y employeeName lastName $x hours res Chapter XML Extensible Markup Language Other Languages and Protocols Related to XML There are several other languages and protocols related to XML technology. The long term goal of these and other languages and protocols is to provide the technology for realization of the Semantic Web where all information in the Web can be intelligently located and processed. The Extensible Stylesheet Language can be used to define how a document should be rendered for display by a Web browser. The Extensible Stylesheet Language for Transformations can be used to transform one structure into a different structure. Hence it can convert documents from one form to another. The Web Services Description Language allows for the description of Web Services in XML. This makes the Web Service available to users and programs over the Web. The Simple Object Access Protocol is a platform independent and programming language independent protocol for messaging and remote procedure calls. The Resource Description Framework provides languages and tools for exchanging and processing of meta data descriptions and specifications over the Web. Extracting XML Documents from Relational Databases Creating Hierarchical XML Views over Flat or Graph Based Data This section discusses the representational issues that arise when converting data from a database system into XML documents. As we have discussed XML uses a hierarchical model to represent documents. The database systems with the most widespread use follow the flat relational data model. When we add referential integrity constraints a relational schema can be considered to be a graph structure view with COURSE as the root. Figure XML schema document with course as the root. xsd element name “root” xsd sequence xsd element name “course” maxOccurs “unbounded” xsd sequence xsd element name “cname” type “xsd string” xsd element name “cnumber” type “xsd unsignedInt” xsd element name “section” maxOccurs “unbounded” xsd sequence xsd element name “secnumber” type “xsd unsignedInt” xsd element name “year” type “xsd string” xsd element name “quarter” type “xsd string” S D Ssn Name Class STUDENT Sections completed M NN Number Year Qtr SECTION Number Name S D COURSE Students attended Course Sections Grade Figure Subset of the UNIVERSITY database schema needed for XML document extraction. Extracting XML Documents from Relational Databases Ssn Sections completed STUDENT Name Number Qtr Year SECTION N Grade Class COURSE Coursenumber Coursename Figure Hierarchical view with STUDENT as the root. Figure XML schema document with course as the root. xsd element name “student” maxOccurs “unbounded” xsd sequence xsd element name “ssn” type “xsd string” xsd element name “sname” type “xsd string” xsd element name “class” type “xsd string” xsd element name “grade” type “xsd string” xsd sequence xsd element xsd sequence xsd element xsd sequence xsd element xsd sequence xsd element In the second hierarchical document view we can choose STUDENT as root view with SECTION as the root. xsd element name ”root” xsd sequence xsd element name ”student” maxOccurs ”unbounded” xsd sequence xsd element name ”ssn” type ”xsd string” xsd element name ”sname” type ”xsd string” xsd element name ”class” type ”xsd string” xsd element name ”section” maxOccurs ”unbounded” xsd sequence xsd element name ”secnumber” type ”xsd unsignedInt” xsd element name ”year” type ”xsd string” xsd element name ”quarter” type ”xsd string” xsd element name ”cnumber” type ”xsd unsignedInt” xsd element name ”cname” type ”xsd string” xsd element name ”grade” type ”xsd string” xsd sequence xsd element xsd sequence xsd element xsd sequence xsd element The third possible way is to choose SECTION as the root as shown in Figure Similar to the second hierarchical view the COURSE information can be merged into the SECTION element. The GRADE database attribute can be migrated to the STUDENT element. As we can see even in this simple example there can be numerous hierarchical document views each corresponding to a different root and a different XML document structure. Extracting XML Documents from Relational Databases COURSE INSTRUCTOR N N N STUDENT SECTION DEPARTMENT COURSE INSTRUCTOR STUDENT SECTION DEPARTMENT STUDENT SECTION DEPARTMENT INSTRUCTOR COURSE M N N Figure Converting a graph with cycles into a hierarchical structure. Breaking Cycles to Convert Graphs into Trees In the previous examples the subset of the database of interest had no cycles. It is possible to have a more complex subset with one or more cycles indicating multiple relationships among the entities. In this case it is more difficult to decide how to create the document hierarchies. Additional duplication of entities may be needed to represent the multiple relationships. We will illustrate this with an example using the ER schema in Figure Suppose that we need the information in all the entity types and relationships in Figure for a particular XML document with STUDENT as the root element. Figure illustrates how a possible hierarchical tree structure can be created for this document. First we get a lattice with STUDENT as the root as shown in Figure This is not a tree structure because of the cycles. One way to break the cycles is to replicate the entity types involved in the cycles. First we replicate INSTRUCTOR as shown in Figure calling the replica to the right The INSTRUCTOR replica on the left represents the relationship between instructors and the sections they teach whereas the replica on the right represents the relationship between instructors and the department each works in. After this we still have the cycle involving COURSE so we can replicate COURSE in a similar manner leading to the hierarchy shown in Figure The replica to the left represents the relationship between courses and their sections whereas the COURSE replica to the right represents the relationship between courses and the department that offers each course. In Figure we have converted the initial graph to a hierarchy. We can do further merging if desired before creating the final hierarchy and the corresponding XML schema structure. Chapter XML Extensible Markup Language Other Steps for Extracting XML Documents from Databases In addition to creating the appropriate XML hierarchy and corresponding XML schema document several other steps are needed to extract a particular XML document from a database It is necessary to create the correct query in SQL to extract the desired information for the XML document. Once the query is executed its result must be restructured from the flat relational form to the XML tree structure. The query can be customized to select either a single object or multiple objects into the document. For example in the view in Figure the query can select a single student entity and create a document corresponding to that single student or it may select several or even all of the students and create a document with multiple students. Summary This chapter provided an overview of the XML standard for representing and exchanging data over the Internet. First we discussed some of the differences between various types of data classifying three main types structured semi structured and unstructured. Structured data is stored in traditional databases. Semistructured data mixes data types names and data values but the data does not all have to follow a fixed predefined structure. Unstructured data refers to information displayed on the Web specified via HTML where information on the types of data items is missing. We described the XML standard and its tree structured data model and discussed XML documents and the languages for specifying the structure of these documents namely XML DTD and XML schema. We gave an overview of the various approaches for storing XML documents whether in their native format in a compressed form or in relational and other types of databases. Finally we gave an overview of the XPath and XQuery languages proposed for querying XML data and discussed the mapping issues that arise when it is necessary to convert data stored in traditional relational databases into XML documents. Review Questions What are the differences between structured semistructured and unstructured data Under which of the categories in do XML documents fall What about self describing data What are the differences between the use of tags in XML versus HTML What is the difference between data centric and document centric XML documents What is the difference between attributes and elements in XML List some of the important attributes used to specify elements in XML schema. What is the difference between XML schema and XML DTD Exercises Create part of an XML instance document to correspond to the data stored in the relational database shown in Figure such that the XML document conforms to the XML schema document in Figure Create XML schema documents and XML DTDs to correspond to the hierarchies shown in Figures and Consider the LIBRARY relational database schema in Figure Create an XML schema document that corresponds to this database schema. Specify the following views as queries in XQuery on the company XML schema shown in Figure a. A view that has the department name manager name and manager salary for every department. b. A view that has the employee name supervisor name and employee salary for each employee who works in the Research department. c. A view that has the project name controlling department name number of employees and total hours worked per week on the project for each project. d. A view that has the project name controlling department name number of employees and total hours worked per week on the project for each project with more than one employee working on it. Selected Bibliography There are so many articles and books on various aspects of XML that it would be impossible to make even a modest list. We will mention one book Chaudhri Rashid and Zicari eds. This book discusses various aspects of XML and contains a list of some references to XML research and practice. Selected Bibliography This page intentionally left blank Database Programming Techniques This page intentionally left blank Introduction to SQL Programming Techniques I n Chapters and we described several aspects of the SQL language which is the standard for relational databases. We described the SQL statements for data definition schema modification queries views and updates. We also described how various constraints on the database contents such as key and referential integrity constraints are specified. In this chapter and the next we discuss some of the methods that have been developed for accessing databases from programs. Most database access in practical applications is accomplished through software programs that implement database applications. This software is usually developed in a general purpose programming language such as Java C C++ C# COBOL or some other programming language. In addition many scripting languages such as PHP and JavaScript are also being used for programming of database access within Web applications. In this chapter we focus on how databases can be accessed from the traditional programming languages C C++ and Java whereas in the next chapter we introduce how databases are accessed from scripting languages such as PHP and JavaScript. Recall from Section that when database statements are included in a program the general purpose programming language is called the host language whereas the database language SQL in our case is called the data sublanguage. In some cases special database programming languages are developed specifically for writing database applications. Although many of these were developed as research prototypes some notable database programming languages have widespread use such as Oracle’s PL SQL . It is important to note that database programming is a very broad topic. There are whole textbooks devoted to each database programming technique and how that technique is realized in a specific system. New techniques are developed all the time chapter Chapter Introduction to SQL Programming Techniques and changes to existing techniques are incorporated into newer system versions and languages. An additional difficulty in presenting this topic is that although there are SQL standards these standards themselves are continually evolving and each DBMS vendor may have some variations from the standard. Because of this we have chosen to give an introduction to some of the main types of database programming techniques and to compare these techniques rather than study one particular method or system in detail. The examples we give serve to illustrate the main differences that a programmer would face when using each of these database programming techniques. We will try to use the SQL standards in our examples rather than describe a specific system. When using a specific system the materials in this chapter can serve as an introduction but should be augmented with the system manuals or with books describing the specific system. We start our presentation of database programming in Section with an overview of the different techniques developed for accessing a database from programs. Then in Section we discuss the rules for embedding SQL statements into a general purpose programming language generally known as embedded SQL. This section also briefly discusses dynamic SQL in which queries can be dynamically constructed at runtime and presents the basics of the SQLJ variation of embedded SQL that was developed specifically for the programming language Java. In Section we discuss the technique known as SQL CLI in which a library of procedures and functions is provided for accessing the database. Various sets of library functions have been proposed. The SQL CLI set of functions is the one given in the SQL standard. Another library of functions is ODBC . We do not describe ODBC because it is considered to be the predecessor to SQL CLI. A third library of functions which we do describe is JDBC this was developed specifically for accessing databases from Java. In Section we discuss SQL PSM which is a part of the SQL standard that allows program modules procedures and functions to be stored by the DBMS and accessed through SQL. We briefly compare the three approaches to database programming in Section and provide a chapter summary in Section Database Programming Techniques and Issues We now turn our attention to the techniques that have been developed for accessing databases from programs and in particular to the issue of how to access SQL databases from application programs. Our presentation of SQL in Chapters and focused on the language constructs for various database operations from schema definition and constraint specification to querying updating and specifying views. Most database systems have an interactive interface where these SQL commands can be typed directly into a monitor for execution by the database system. For example in a computer system where the Oracle RDBMS is installed the command SQLPLUS starts the interactive interface. The user can type SQL commands or queries directly over several lines ended by a semicolon and the Enter key . Alternatively a file of commands can be created and executed through the interactive interface by typing @ filename . The system will execute the commands written in the file and display the results if any. The interactive interface is quite convenient for schema and constraint creation or for occasional ad hoc queries. However in practice the majority of database interactions are executed through programs that have been carefully designed and tested. These programs are generally known as application programs or database applications and are used as canned transactions by the end users as discussed in Section Another common use of database programming is to access a database through an application program that implements a Web interface for example when making airline reservations or online purchases. In fact the vast majority of Web electronic commerce applications include some database access commands. Chapter gives an overview of Web database programming using PHP a scripting language that has recently become widely used. In this section first we give an overview of the main approaches to database programming. Then we discuss some of the problems that occur when trying to access a database from a general purpose programming language and the typical sequence of commands for interacting with a database from a software program. Approaches to Database Programming Several techniques exist for including database interactions in application programs. The main approaches for database programming are the following Embedding database commands in a general purpose programming language. In this approach database statements are embedded into the host programming language but they are identified by a special prefix. For example the prefix for embedded SQL is the string EXEC SQL which precedes all SQL commands in a host language A precompiler or preproccessor scans the source program code to identify database statements and extract them for processing by the DBMS. They are replaced in the program by function calls to the DBMS generated code. This technique is generally referred to as embedded SQL. Using a library of database functions. A library of functions is made available to the host programming language for database calls. For example there could be functions to connect to a database execute a query execute an update and so on. The actual database query and update commands and any other necessary information are included as parameters in the function calls. This approach provides what is known as an application programming interface for accessing a database from application programs. Designing a brand new language. A database programming language is designed from scratch to be compatible with the database model and query language. Additional programming structures such as loops and conditional prefixes are sometimes used but this is the most common. Chapter Introduction to SQL Programming Techniques statements are added to the database language to convert it into a fullfledged programming language. An example of this approach is Oracle’s PL SQL. In practice the first two approaches are more common since many applications are already written in general purpose programming languages but require some database access. The third approach is more appropriate for applications that have intensive database interaction. One of the main problems with the first two approaches is impedance mismatch which does not occur in the third approach. Impedance Mismatch Impedance mismatch is the term used to refer to the problems that occur because of differences between the database model and the programming language model. For example the practical relational model has three main constructs columns and their data types rows and tables . The first problem that may occur is that the data types of the programming language differ from the attribute data types that are available in the data model. Hence it is necessary to have a binding for each host programming language that specifies for each attribute type the compatible programming language types. A different binding is needed for each programming language because different languages have different data types. For example the data types available in C C++ and Java are different and both differ from the SQL data types which are the standard data types for relational databases. Another problem occurs because the results of most queries are sets or multisets of tuples and each tuple is formed of a sequence of attribute values. In the program it is often necessary to access the individual data values within individual tuples for printing or processing. Hence a binding is needed to map the query result data structure which is a table to an appropriate data structure in the programming language. A mechanism is needed to loop over the tuples in a query result in order to access a single tuple at a time and to extract individual values from the tuple. The extracted attribute values are typically copied to appropriate program variables for further processing by the program. A cursor or iterator variable is typically used to loop over the tuples in a query result. Individual values within each tuple are then extracted into distinct program variables of the appropriate type. Impedance mismatch is less of a problem when a special database programming language is designed that uses the same data model and data types as the database model. One example of such a language is Oracle’s PL SQL. The SQL standard also has a proposal for such a database programming language known as SQL PSM. For object databases the object data model . Embedded SQL Dynamic SQL and SQLJ Typical Sequence of Interaction in Database Programming When a programmer or software engineer writes a program that requires access to a database it is quite common for the program to be running on one computer system while the database is installed on another. Recall from Section that a common architecture for database access is the client server model where a client program handles the logic of a software application but includes some calls to one or more database servers to access or update the When writing such a program a common sequence of interaction is the following When the client program requires access to a particular database the program must first establish or open a connection to the database server. Typically this involves specifying the Internet address of the machine where the database server is located plus providing a login account name and password for database access. Once the connection is established the program can interact with the database by submitting queries updates and other database commands. In general most types of SQL statements can be included in an application program. When the program no longer needs access to a particular database it should terminate or close the connection to the database. A program can access multiple databases if needed. In some database programming approaches only one connection can be active at a time whereas in other approaches multiple connections can be established simultaneously. In the next three sections we discuss examples of each of the three main approaches to database programming. Section describes how SQL is embedded into a programming language. Section discusses how function calls are used to access the database and Section discusses an extension to SQL called SQL PSM that allows general purpose programming constructs for defining modules that are stored within the database Section compares these approaches. Embedded SQL Dynamic SQL and SQLJ In this section we give an overview of the technique for how SQL statements can be embedded in a general purpose programming language. We focus on two languages C and Java. The examples used with the C language known as embedded we discussed in Section there are two tier and three tier architectures to keep our discussion simple we will assume a two tier client server architecture here. illustrates how typical general purpose programming language constructs such as loops and conditional structures can be incorporated into SQL. Chapter Introduction to SQL Programming Techniques SQL are presented in Sections through and can be adapted to other programming languages. The examples using Java known as SQLJ are presented in Sections and In this embedded approach the programming language is called the host language. Most SQL statements including data or constraint definitions queries updates or view definitions can be embedded in a host language program. Retrieving Single Tuples with Embedded SQL To illustrate the concepts of embedded SQL we will use C as the host programming When using C as the host language an embedded SQL statement is distinguished from programming language statements by prefixing it with the keywords EXEC SQL so that a preprocessor can separate embedded SQL statements from the host language code. The SQL statements within a program are terminated by a matching END EXEC or by a semicolon . Similar rules apply to embedding SQL in other programming languages. Within an embedded SQL command we may refer to specially declared C program variables. These are called shared variables because they are used in both the C program and the embedded SQL statements. Shared variables are prefixed by a colon when they appear in an SQL statement. This distinguishes program variable names from the names of database schema constructs such as attributes and relations . It also allows program variables to have the same names as attribute names since they are distinguishable by the colon prefix in the SQL statement. Names of database schema constructs such as attributes and relations can only be used within the SQL commands but shared program variables can be used elsewhere in the C program without the colon prefix. Suppose that we want to write C programs to process the COMPANY database in Figure We need to declare program variables to match the types of the database attributes that the program will process. The programmer can choose the names of the program variables they may or may not have names that are identical to their corresponding database attributes. We will use the C program variables declared in Figure for all our examples and show C program segments without variable declarations. Shared variables are declared within a declare section in the program as shown in Figure in SQL can be mapped to arrays of characters . Lines through are regular C program declarations. The C program variables declared in lines through correspond to the attributes of the EMPLOYEE and DEPARTMENT tables from the COMPANY database in Figure that was declared by the SQL DDL in Figure The variables declared in line and SQLSTATE are used to communicate errors and exception conditions between the database system and the executing program. Line shows a program variable loop that will not be used in any embedded SQL statement so it is declared outside the SQL declare section. Connecting to the Database. The SQL command for establishing a connection to a database has the following form CONNECT TO server name AS connection name AUTHORIZATION user account name and password In general since a user or program can access several database servers several connections can be established but only one connection can be active at any point in time. The programmer or user can use the connection name to change from the currently active connection to a different one by using the following command SET CONNECTION connection name Once a connection is no longer needed it can be terminated by the following command DISCONNECT connection name In the examples in this chapter we assume that the appropriate connection has already been established to the COMPANY database and that it is the currently active connection. strings can also be mapped to char types in C. Chapter Introduction to SQL Programming Techniques Communicating between the Program and the DBMS Using SQLCODE and SQLSTATE. The two special communication variables that are used by the DBMS to communicate exception or error conditions to the program are SQLCODE and SQLSTATE. The SQLCODE variable shown in Figure is an integer variable. After each database command is executed the DBMS returns a value in SQLCODE. A value of indicates that the statement was executed successfully by the DBMS. If SQLCODE are available in a query result. If SQLCODE this indicates some error has occurred. In some systems for example in the Oracle RDBMS SQLCODE is a field in a record structure called SQLCA so it is referenced as . In this case the definition of SQLCA must be included in the C program by including the following line EXEC SQL include SQLCA In later versions of the SQL standard a communication variable called SQLSTATE was added which is a string of five characters. A value of in SQLSTATE indicates no error or exception other values indicate various errors or exceptions. For example indicates ‘no more data’ when using SQLSTATE. Currently both SQLSTATE and SQLCODE are available in the SQL standard. Many of the error and exception codes returned in SQLSTATE are supposed to be standardized for all SQL vendors and whereas the codes returned in SQLCODE are not standardized but are defined by the DBMS vendor. Hence it is generally better to use SQLSTATE because this makes error handling in the application programs independent of a particular DBMS. As an exercise the reader should rewrite the examples given later in this chapter using SQLSTATE instead of SQLCODE. Example of Embedded SQL Programming. Our first example to illustrate embedded SQL programming is a repeating program segment that takes as input a Social Security number of an employee and prints some information from the corresponding EMPLOYEE record in the database. The C program code is shown as program segment in Figure The program reads an Ssn value and then retrieves the EMPLOYEE tuple with that Ssn from the database via the embedded SQL command. The INTO clause as we discussed earlier. The INTO clause can be used in this way only when the query result is a single record if multiple records are retrieved an error will be generated. We will see how multiple records are handled in Section Line in illustrates the communication between the database and the program through the special variable SQLCODE. If the value returned by the DBMS in SQLCODE is the previous statement was executed without errors or exception conditions. Line checks this and assumes that if an error occurred it was because particular SQLSTATE codes starting with the characters through or A through H are supposed to be standardized whereas other values can be implementation defined. Embedded SQL Dynamic SQL and SQLJ Figure Program segment a C program segment with embedded SQL. Program Segment loop while prompt EXEC SQL select Fname Minit Lname Address Salary into fname minit lname address salary from EMPLOYEE where Ssn ssn if else printf prompt " loop no EMPLOYEE tuple existed with the given Ssn therefore it outputs a message to that effect . When a single record is retrieved the programmer can assign its attribute values directly to C program variables in the INTO clause as in line In general an SQL query can retrieve many tuples. In that case the C program will typically go through the retrieved tuples and process them one at a time. The concept of a cursor is used to allow tuple at a time processing of a query result by the host language program. We describe cursors next. Retrieving Multiple Tuples with Embedded SQL Using Cursors We can think of a cursor as a pointer that points to a single tuple from the result of a query that retrieves multiple tuples. The cursor is declared when the SQL query command is declared in the program. Later in the program an OPEN CURSOR command fetches the query result from the database and sets the cursor to a position before the first row in the result of the query. This becomes the current row for the cursor. Subsequently FETCH commands are issued in the program each FETCH moves the cursor to the next row in the result of the query making it the current row and copying its attribute values into the C program variables specified in the FETCH command by an INTO clause. The cursor variable is basically an iterator that iterates over the tuples in the query result one tuple at a time. To determine when all the tuples in the result of the query have been processed the communication variable SQLCODE is checked. If a FETCH command is issued that results in moving the cursor past the last tuple in the result of the query a positive value was found . The programmer uses this to terminate a loop over the tuples in the query result. In general numerous cursors can be opened at the same time. A Chapter Introduction to SQL Programming Techniques CLOSE CURSOR command is issued to indicate that we are done with processing the result of the query associated with that cursor. An example of using cursors to process a query result with multiple records is shown in Figure where a cursor called EMP is declared in line The EMP cursor is associated with the SQL query declared in lines through but the query is not executed until the OPEN EMP command by subsequent FETCH cursor name commands a department name a raise amount for that employee we must add Figure Program segment a C program segment that uses cursors with embedded SQL for update purposes. Program Segment prompt EXEC SQL select Dnumber into dnumber from DEPARTMENT where Dname dname EXEC SQL DECLARE EMP CURSOR FOR select Ssn Fname Minit Lname Salary from EMPLOYEE where Dno dnumber FOR UPDATE OF Salary EXEC SQL OPEN EMP EXEC SQL FETCH from EMP into ssn fname minit lname salary while prompt EXEC SQL update EMPLOYEE set Salary Salary + raise where CURRENT OF EMP EXEC SQL FETCH from EMP into ssn fname minit lname salary EXEC SQL CLOSE EMP Embedded SQL Dynamic SQL and SQLJ the clause FOR UPDATE OF in the cursor declaration and list the names of any attributes that will be updated by the program. This is illustrated in line of code segment If rows are to be deleted the keywords FOR UPDATE must be added without specifying any attributes. In the embedded UPDATE command the condition WHERE CURRENT OF cursor name specifies that the current tuple referenced by the cursor is the one to be updated as in line of Notice that declaring a cursor and associating it with a query . General Options for a Cursor Declaration. Several options can be specified when declaring a cursor. The general form of a cursor declaration is as follows DECLARE cursor name [ INSENSITIVE ] [ SCROLL ] CURSOR [ WITH HOLD ] FOR query specification [ ORDER BY ordering specification ] [ FOR READ ONLY | FOR UPDATE [ OF attribute list ] ] We already briefly discussed the options listed in the last line. The default is that the query is for retrieval purposes . If some of the tuples in the query result are to be updated we need to specify FOR UPDATE OF attribute list and list the attributes that may be updated. If some tuples are to be deleted we need to specify FOR UPDATE without any attributes listed. When the optional keyword SCROLL is specified in a cursor declaration it is possible to position the cursor in other ways than for purely sequential access. A fetch orientation can be added to the FETCH command whose value can be one of NEXT PRIOR FIRST LAST ABSOLUTE i and RELATIVE i. In the latter two commands i must evaluate to an integer value that specifies an absolute tuple position within the query result or a tuple position relative to the current cursor position . The default fetch orientation which we used in our examples is NEXT. The fetch orientation allows the programmer to move the cursor around the tuples in the query result with greater flexibility providing random access by position or access in reverse order. When SCROLL is specified on the cursor the general form of a FETCH command is as follows with the parts in square brackets being optional FETCH [ [ fetch orientation ] FROM ] cursor name INTO fetch target list The ORDER BY clause orders the tuples so that the FETCH command will fetch them in the specified order. It is specified in a similar manner to the corresponding clause for SQL queries refer to transaction characteristics of database programs which we will discuss in Chapter Chapter Introduction to SQL Programming Techniques Figure Program segment a C program segment that uses dynamic SQL for updating a table. Program Segment EXEC SQL BEGIN DECLARE SECTION varchar sqlupdatestring EXEC SQL END DECLARE SECTION prompt EXEC SQL PREPARE sqlcommand FROM sqlupdatestring EXEC SQL EXECUTE sqlcommand Specifying Queries at Runtime Using Dynamic SQL In the previous examples the embedded SQL queries were written as part of the host program source code. Hence any time we want to write a different query we must modify the program code and go through all the steps involved . In some cases it is convenient to write a program that can execute different SQL queries or updates dynamically at runtime. For example we may want to write a program that accepts an SQL query typed from the monitor executes it and displays its result such as the interactive interfaces available for most relational DBMSs. Another example is when a userfriendly interface generates SQL queries dynamically for the user based on pointand click operations on a graphical schema . In this section we give a brief overview of dynamic SQL which is one technique for writing this type of database program by giving a simple example to illustrate how dynamic SQL can work. In Section we will describe another approach for dealing with dynamic queries. Program segment in Figure reads a string that is input by the user into the string program variable sqlupdatestring in line It then prepares this as an SQL command in line by associating it with the SQL variable sqlcommand. Line then executes the command. Notice that in this case no syntax check or other types of checks on the command are possible at compile time since the SQL command is not available until runtime. This contrasts with our previous examples of embedded SQL where the query could be checked at compile time because its text was in the program source code. Although including a dynamic update command is relatively straightforward in dynamic SQL a dynamic query is much more complicated. This is because usually we do not know the types or the number of attributes to be retrieved by the SQL query when we are writing the program. A complex data structure is sometimes needed to allow for different numbers and types of attributes in the query result if no prior information is known about the dynamic query. Techniques similar to those that we discuss in Section can be used to assign query results to host program variables. In the reason for separating PREPARE and EXECUTE is that if the command is to be executed multiple times in a program it can be prepared only once. Preparing the command generally involves syntax and other types of checks by the system as Embedded SQL Dynamic SQL and SQLJ well as generating the code for executing it. It is possible to combine the PREPARE and EXECUTE commands section assumes familiarity with object oriented concepts throws SQLException For example we can write the statements in lines through in Figure to connect to an Oracle database located at the url url name using the login of user name and password with automatic commitment of each and then set this connection as the default context for subsequent commands. In the following examples we will not show complete Java classes or programs since it is not our intention to teach Java. Rather we will show program segments that illustrate the use of SQLJ. Figure shows the Java program variables used in our examples. Program segment in Figure reads an employee’s Ssn and prints some of the employee’s information from the database. Notice that because Java already uses the concept of exceptions for error handling a special exception called SQLException is used to return errors or exception conditions after executing an SQL database command. This plays a similar role to SQLCODE and SQLSTATE in embedded SQL. Java has many types of predefined exceptions. Each Java operation must specify the exceptions that can be thrown that is the exception conditions that may occur while executing the Java code of that operation. If a defined exception occurs the system transfers control to the Java code specified for exception handling. In exception handling for an SQLException is specified in lines and In Java the following structure try operation catch exception handling code continuation code is used to deal with exceptions that occur during the execution of operation . If no exception occurs the continuation code is processed directly. Exceptions default context when set applies to subsequent commands in the program until it is changed. commitment roughly means that each command is applied to the database after it is executed. The alternative is that the programmer wants to execute several related database commands and then commit them together. We discuss commit concepts in Chapter when we describe database transactions. Embedded SQL Dynamic SQL and SQLJ Figure Program segment a Java program segment with SQLJ. Program Segment ssn readEntry try #sql select Fname Minit Lname Address Salary into fname minit lname address salary from EMPLOYEE where Ssn ssn catch .println Return .println that can be thrown by the code in a particular operation should be specified as part of the operation declaration or interface for example in the following format operation return type operation name throws SQLException IOException In SQLJ the embedded SQL commands within a Java program are preceded by #sql as illustrated in line so that they can be identified by the preprocessor. The #sql is used instead of the keywords EXEC SQL that are used in embedded SQL with the C programming language in the SQL statement as in embedded SQL. In a single tuple is retrieved by the embedded SQLJ query that is why we are able to assign its attribute values directly to Java program variables in the INTO clause in line in Figure For queries that retrieve many tuples SQLJ uses the concept of an iterator which is similar to a cursor in embedded SQL. Retrieving Multiple Tuples in SQLJ Using Iterators In SQLJ an iterator is a type of object associated with a collection of records in a query The iterator is associated with the tuples and attributes that appear in a query result. There are two types of iterators A named iterator is associated with a query result by listing the attribute names and types that appear in the query result. The attribute names must correspond to appropriately declared Java program variables as shown in Figure A positional iterator lists only the attribute types that appear in the query result. discussed iterators in more detail in Chapter when we presented object database concepts. Chapter Introduction to SQL Programming Techniques Figure Program segment a Java program segment that uses a named iterator to print employee information in a particular department. Program Segment dname readEntry try #sql select Dnumber into dnumber from DEPARTMENT where Dname dname catch .println Return .printline #sql iterator Emp Emp e null #sql e select ssn fname minit lname salary from EMPLOYEE where Dno dnumber while .printline In both cases the list should be in the same order as the attributes that are listed in the SELECT clause of the query. However looping over a query result is different for the two types of iterators as we shall see. First we show an example of using a named iterator in Figure program segment Line in Figure shows how a named iterator type Emp is declared. Notice that the names of the attributes in a named iterator type must match the names of the attributes in the SQL query result. Line shows how an iterator object e of type Emp is created in the program and then associated with a query in line performs two functions It gets the next tuple in the query result and controls the while loop. Once the program is done with processing the query result the command in the positional iterator only attribute types. This can provide more flexibility but makes the processing of the query result slightly more complex. The attribute types must still must be compatible with the attribute types in the SQL query result and in the same order. Line shows how a positional iterator object e of type Emppos is created in the program and then associated with a query is used. This function is set to a value of TRUE when the iterator is initially associated with an SQL query try #sql select Dnumber into dnumber from DEPARTMENT where Dname dname catch .println Return .printline #sql iterator Emppos Emppos e null #sql e select ssn fname minit lname salary from EMPLOYEE where Dno dnumber #sql fetch e into ssn fn mi ln sal while .printline #sql fetch e into ssn fn mi ln sal Chapter Introduction to SQL Programming Techniques each time a fetch command returns a valid tuple from the query result. It is set to TRUE again when a fetch command does not find any more tuples. Line shows how the looping is controlled by negation. Database Programming with Function Calls SQL CLI and JDBC Embedded SQL is used to access the database. Although this provides more flexibility because no preprocessor is needed one drawback is that syntax and other checks on SQL commands have to be done at runtime. Another drawback is that it sometimes requires more complex programming to access query results because the types and numbers of attributes in a query result may not be known in advance. In this section we give an overview of two function call interfaces. We first discuss the SQL Call Level Interface which is part of the SQL standard. This was developed as a follow up to the earlier technique known as ODBC . We use C as the host language in our SQL CLI examples. Then we give an overview of JDBC which is the call function interface for accessing databases from Java. Although it is commonly assumed that JDBC stands for Java Database Connectivity JDBC is just a registered trademark of Sun Microsystems not an acronym. The main advantage of using a function call interface is that it makes it easier to access multiple databases within the same application program even if they are stored under different DBMS packages. We discuss this further in Section when we discuss Java database programming with JDBC although this advantage also applies to database programming with SQL CLI and ODBC SQLHSTMT SQLHDBC SQLHENV SQLRETURN SQLAllocHandle else exit if SQLAllocHandle prompt SQLCHAR &ssn if SQLCHAR &lname SQLFLOAT &salary if printf else printf Chapter Introduction to SQL Programming Techniques In this function the parameters are as follows handletype indicates the type of record being created. The possible values for this parameter are the keywords SQLHANDLEENV SQLHANDLEDBC SQLHANDLESTMT or SQLHANDLEDESC for an environment connection statement or description record respectively. indicates the container within which the new handle is being created. For example for a connection record this would be the environment within which the connection is being created and for a statement record this would be the connection for that statement. is the pointer to the newly created record of type handletype . When writing a C program that will include database calls through SQL CLI the following are the typical steps that are taken. We illustrate the steps by referring to the example in Figure which reads a Social Security number of an employee and prints the employee’s last name and salary. The library of functions comprising SQL CLI must be included in the C program. This is called and is included using line in Figure Declare handle variables of types SQLHSTMT SQLHDBC SQLHENV and SQLHDESC for the statements connections environments and descriptions needed in the program respectively indicates successful execution of the function call. An environment record must be set up in the program using SQLAllocHandle. The function to do this is shown in line Because an environment record is not contained in any other record the parameter is the NULL handle SQLNULLHANDLE when creating an environment. The handle to the newly created environment record is returned in variable in line A connection record is set up in the program using SQLAllocHandle. In line the connection record created has the handle and is contained in the environment A connection is then established in to a particular server database using the SQLConnect function of SQL CLI to the keep our presentation simple we will not show description records here. Database Programming with Function Calls SQL CLI and JDBC statement handle The question mark symbol in line represents a statement parameter which is a value to be determined at runtime typically by binding it to a C program variable. In general there could be several parameters in a statement string. They are distinguished by the order of appearance of the question marks in the statement string . The last parameter in SQLPrepare should give the length of the SQL statement string in bytes but if we enter the keyword SQLNTS this indicates that the string holding the query is a NULL terminated string so that SQL can calculate the string length automatically. This use of SQLNTS also applies to other string parameters in the function calls in our examples. Before executing the query any parameters in the query string should be bound to program variables using the SQL CLI function SQLBindParameter. In Figure the parameter to the prepared query referenced by is bound to the C program variable ssn in line If there are n parameters in the SQL statement we should have n SQLBindParameter function calls each with a different parameter position n . Following these preparations we can now execute the SQL statement referenced by the handle using the function SQLExecute are bound to the C program variables lname and salary Finally in order to retrieve the column values into the C program variables the function SQLFetch is used code if there are no more tuples in the query alternative technique known as unbound columns uses different SQL CLI functions namely SQLGetCol or SQLGetData to retrieve columns from the query result without previously binding them these are applied after the SQLFetch command in line unbound program variables are used SQLFetch returns the tuple into a temporary program area. Each subsequent SQLGetCol returns one attribute value in order. Basically for each row in the query result the program should iterate over the attribute values in that row. This is useful if the number of columns in the query result is variable. Chapter Introduction to SQL Programming Techniques Figure Program segment a C program segment that uses SQL CLI for a query with a collection of tuples in its result. Program Segment #include void printDepartmentEmps SQLHSTMT SQLHDBC SQLHENV SQLRETURN SQLAllocHandle else exit if SQLAllocHandle prompt SQLINTEGER &dno if SQLCHAR &lname SQLFLOAT &salary while printf As we can see using dynamic function calls requires a lot of preparation to set up the SQL statements and to bind statement parameters and query results to the appropriate program variables. In a single tuple is selected by the SQL query. Figure shows an example of retrieving multiple tuples. We assume that appropriate C program variables have been declared as in Figure The program segment in reads a department number and then retrieves the employees who work in that department. A loop then iterates over each employee record one at a time and prints the employee’s last name and salary. Database Programming with Function Calls SQL CLI and JDBC JDBC SQL Function Calls for Java Programming We now turn our attention to how SQL can be called from the Java object oriented programming The function libraries for this access are known as JDBC. The Java programming language was designed to be platform independent that is a program should be able to run on any type of computer system that has a Java interpreter installed. Because of this portability many RDBMS vendors provide JDBC drivers so that it is possible to access their systems via Java programs. A JDBC driver is basically an implementation of the function calls specified in the JDBC application programming interface for a particular vendor’s RDBMS. Hence a Java program with JDBC function calls can access any RDBMS that has a JDBC driver available. Because Java is object oriented its function libraries are implemented as classes. Before being able to process JDBC function calls with Java it is necessary to import the JDBC class libraries which are called . . These can be downloaded and installed via the JDBC is designed to allow a single Java program to connect to several different databases. These are sometimes called the data sources accessed by the Java program. These data sources could be stored using RDBMSs from different vendors and could reside on different machines. Hence different data source accesses within the same Java program may require JDBC drivers from different vendors. To achieve this flexibility a special JDBC class called the driver manager class is employed which keeps track of the installed drivers. A driver should be registered with the driver manager before it is used. The operations of the driver manager class include getDriver registerDriver and deregisterDriver. These can be used to add and remove drivers dynamically. Other functions set up and close connections to data sources as we will see. To load a JDBC driver explicitly the generic Java function for loading a class can be used. For example to load the JDBC driver for the Oracle RDBMS the following command can be used This will register the driver with the driver manager and make it available to the program. It is also possible to load and register the driver needed in the command line that runs the program for example by including the following in the command line .driver section assumes familiarity with object oriented concepts throws SQLException IOException try catch .println String dbacct passwrd ssn lname Double salary dbacct readentry passwrd readentry Connection conn + dbacct + " " + passwrd String "select Lname Salary from EMPLOYEE where Ssn " PreparedStatement p ssn readentry ssn ResultSet r while lname salary .printline The following are typical steps that are taken when writing a Java application program with database access through JDBC function calls. We illustrate the steps by referring to the example in Figure which reads a Social Security number of an employee and prints the employee’s last name and salary. The JDBC library of classes must be imported into the Java program. These classes are called . and can be imported using line in Figure Any additional Java class libraries needed by the program must also be imported. Load the JDBC driver as discussed previously where urlstring has the form jdbc oracle driverType dbaccount password An alternative form is getConnection Various properties can be set for a connection object but they are mainly related to transactional properties which we discuss in Chapter The Statement object. A statement object is created in the program. In JDBC there is a basic statement class Statement with two specialized subclasses PreparedStatement and CallableStatement. The example in Figure illustrates how PreparedStatement objects are created and used. The next example throws SQLException IOException try catch .println String dbacct passwrd lname Double salary Integer dno dbacct readentry passwrd readentry Connection conn + dbacct + " " + passwrd dno readentry String q "select Lname Salary from EMPLOYEE where Dno " + Statement s ResultSet r while lname salary .printline Chapter Introduction to SQL Programming Techniques Statement objects. In line in Figure a query string with a single parameter indicated by the symbol is created in the string variable In line an object p of type PreparedStatement is created based on the query string in and using the connection object conn. In general the programmer should use PreparedStatement objects if a query is to be executed multiple times since it would be prepared checked and compiled only once thus saving this cost for the additional executions of the query. Setting the statement parameters. The question mark symbol in line represents a statement parameter which is a value to be determined at runtime typically by binding it to a Java program variable. In general there could be several parameters distinguished by the order of appearance of the question marks within the statement string as we discussed previously. Before executing a PreparedStatement query any parameters should be bound to program variables. Depending on the type of the parameter different functions such as setString setInteger setDouble and so on are applied to the PreparedStatement object to set its parameters. The appropriate function should be used to correspond to the data type of the parameter being set. In Figure the parameter in object p is bound to the Java program variable ssn in line The function setString is used because ssn is a string variable. If there are n parameters in the SQL statement we should have n functions each with a different parameter position n . Generally it is advisable to clear all parameters before setting any new values function in the ResultSet object and returns NULL if there are no more objects. This is used to control the looping. The programmer can refer to the attributes in the current tuple using various functions that depend on the type of each attribute . The programmer can either use the attribute positions or the actual attribute names Database Stored Procedures and SQL PSM with the functions. In our examples we used the positional notation in lines and In general the programmer can check for SQL exceptions after each JDBC function call. We did not do this to simplify the examples. Notice that JDBC does not distinguish between queries that return single tuples and those that return multiple tuples unlike some of the other techniques. This is justifiable because a single tuple result set is just a special case. In example a single tuple is selected by the SQL query so the loop in lines to is executed at most once. The example shown in Figure illustrates the retrieval of multiple tuples. The program segment in reads a department number and then retrieves the employees who work in that department. A loop then iterates over each employee record one at a time and prints the employee’s last name and salary. This example also illustrates how we can execute a query directly without having to prepare it as in the previous example. This technique is preferred for queries that will be executed only once since it is simpler to program. In line of Figure the programmer creates a Statement object without associating it with a particular query string. The query string q is passed to the statement object s when it is executed in line This concludes our brief introduction to JDBC. The interested reader is referred to the Web site which contains many further details about JDBC. Database Stored Procedures and SQL PSM This section introduces two additional topics related to database programming. In Section we discuss the concept of stored procedures which are program modules that are stored by the DBMS at the database server. Then in Section we discuss the extensions to SQL that are specified in the standard to include general purpose programming constructs in SQL. These extensions are known as SQL PSM and can be used to write stored procedures. SQL PSM also serves as an example of a database programming language that extends a database model and language namely SQL with some programming constructs such as conditional statements and loops. Database Stored Procedures and Functions In our presentation of database programming techniques so far there was an implicit assumption that the database application program was running on a client machine or more likely at the application server computer in the middle tier of a three tier client server architecture local declarations procedure body The parameters and local declarations are optional and are specified only if needed. For declaring a function a return type is necessary so the declaration form is CREATE FUNCTION function name RETURNS return type local declarations function body If the procedure is written in a general purpose programming language it is typical to specify the language as well as a file name where the program code is stored. For example the following format can be used CREATE PROCEDURE procedure name LANGUAGE programming language name EXTERNAL NAME file path name In general each parameter should have a parameter type that is one of the SQL data types. Each parameter should also have a parameter mode which is one of IN OUT or INOUT. These correspond to parameters whose values are input only output only or both input and output respectively. Database Stored Procedures and SQL PSM Because the procedures and functions are stored persistently by the DBMS it should be possible to call them from the various SQL interfaces and programming techniques. The CALL statement in the SQL standard can be used to invoke a stored procedure either from an interactive interface or from embedded SQL or SQLJ. The format of the statement is as follows CALL procedure or function name If this statement is called from JDBC it should be assigned to a statement object of type CallableStatement of stored procedures and functions. In this section we discuss the SQL PSM constructs for conditional statements and for looping statements. These will give a flavor of the type of constructs that SQL PSM has then we give an example to illustrate how these constructs can be used. The conditional branching statement in SQL PSM has the following form IF condition THEN statement list ELSEIF condition THEN statement list ELSEIF condition THEN statement list ELSE statement list END IF Consider the example in Figure which illustrates how the conditional branch structure can be used in an SQL PSM function. The function returns a string value RETURNS VARCHAR DECLARE Noofemps INTEGER SELECT COUNT INTO Noofemps FROM EMPLOYEE WHERE Dno deptno IF Noofemps THEN RETURN "HUGE" ELSEIF Noofemps THEN RETURN "LARGE" ELSEIF Noofemps THEN RETURN "MEDIUM" ELSE RETURN "SMALL" END IF WHILE condition DO statement list END WHILE REPEAT statement list UNTIL condition END REPEAT There is also a cursor based looping structure. The statement list in such a loop is executed once for each tuple in the query result. This has the following form FOR loop name AS cursor name CURSOR FOR query DO statement list END FOR Loops can have names and there is a LEAVE loop name statement to break a loop when a condition is satisfied. SQL PSM has many other features but they are outside the scope of our presentation. Comparing the Three Approaches In this section we briefly compare the three approaches for database programming and discuss the advantages and disadvantages of each approach. Embedded SQL Approach. The main advantage of this approach is that the query text is part of the program source code itself and hence can be checked for syntax errors and validated against the database schema at compile time. This also makes the program quite readable as the queries are readily visible in the source code. The main disadvantages are the loss of flexibility in changing the query at runtime and the fact that all changes to queries must go through the whole recompilation process. In addition because the queries are known beforehand the choice of program variables to hold the query results is a simple task and so the programming of the application is generally easier. However for complex applications where Summary queries have to be generated at runtime the function call approach will be more suitable. Library of Function Calls Approach. This approach provides more flexibility in that queries can be generated at runtime if needed. However this leads to more complex programming as program variables that match the columns in the query result may not be known in advance. Because queries are passed as statement strings within the function calls no checking can be done at compile time. All syntax checking and query validation has to be done at runtime and the programmer must check and account for possible additional runtime errors within the program code. Database Programming Language Approach. This approach does not suffer from the impedance mismatch problem as the programming language data types are the same as the database data types. However programmers must learn a new programming language rather than use a language they are already familiar with. In addition some database programming languages are vendor specific whereas general purpose programming languages can easily work with systems from multiple vendors. Summary In this chapter we presented additional features of the SQL database language. In particular we presented an overview of the most important techniques for database programming in Section Then we discussed the various approaches to database application programming in Sections to In Section we discussed the general technique known as embedded SQL where the queries are part of the program source code. A precompiler is typically used to extract SQL commands from the program for processing by the DBMS and replacing them with function calls to the DBMS compiled code. We presented an overview of embedded SQL using the C programming language as host language in our examples. We also discussed the SQLJ technique for embedding SQL in Java programs. The concepts of cursor and iterator were presented and illustrated by examples to show how they are used for looping over the tuples in a query result and extracting the attribute value into program variables for further processing. In Section we discussed how function call libraries can be used to access SQL databases. This technique is more dynamic than embedding SQL but requires more complex programming because the attribute types and number in a query result may be determined at runtime. An overview of the SQL CLI standard was presented with examples using C as the host language. We discussed some of the functions in the SQL CLI library how queries are passed as strings how query parameters are assigned at runtime and how results are returned to program variables. We then gave an overview of the JDBC class library which is used with Java and discussed some of its classes and operations. In particular the ResultSet class is used to create objects that hold the query results which can then be iterated over Chapter Introduction to SQL Programming Techniques by the next operation. The get and set functions for retrieving attribute values and setting parameter values were also discussed. In Section we gave a brief overview of stored procedures and discussed SQL PSM as an example of a database programming language. Finally we briefly compared the three approaches in Section It is important to note that we chose to give a comparative overview of the three main approaches to database programming since studying a particular approach in depth is a topic that is worthy of its own textbook. Review Questions What is ODBC How is it related to SQL CLI What is JDBC Is it an example of embedded SQL or of using function calls List the three main approaches to database programming. What are the advantages and disadvantages of each approach What is the impedance mismatch problem Which of the three programming approaches minimizes this problem Describe the concept of a cursor and how it is used in embedded SQL. What is SQLJ used for Describe the two types of iterators available in SQLJ. Exercises Consider the database shown in Figure whose schema is shown in Figure Write a program segment to read a student’s name and print his or her grade point average assuming that and points. Use embedded SQL with C as the host language. Repeat Exercise but use SQLJ with Java as the host language. Consider the library relational database schema in Figure Write a program segment that retrieves the list of books that became overdue yesterday and that prints the book title and borrower name for each. Use embedded SQL with C as the host language. Repeat Exercise but use SQLJ with Java as the host language. Repeat Exercises and but use SQL CLI with C as the host language. Repeat Exercises and but use JDBC with Java as the host language. Repeat Exercise but write a function in SQL PSM. Create a function in PSM that computes the median salary for the EMPLOYEE table shown in Figure Selected Bibliography Selected Bibliography There are many books that describe various aspects of SQL database programming. For example Sunderraman describes programming on the Oracle DBMS and Reese focuses on JDBC and Java programming. Many Web resources are also available. This page intentionally left blank Web Database Programming Using PHP I n the previous chapter we gave an overview of database programming techniques using traditional programming languages and we used the Java and C programming languages in our examples. We now turn our attention to how databases are accessed from scripting languages. Many electronic commerce and other Internet applications that provide Web interfaces to access information stored in one or more databases use scripting languages. These languages are often used to generate HTML documents which are then displayed by the Web browser for interaction with the user. In Chapter we gave an overview of the XML language for data representation and exchange on the Web and discussed some of the ways in which it can be used. We introduced HTML and discussed how it differs from XML. Basic HTML is useful for generating static Web pages with fixed text and other objects but most ecommerce applications require Web pages that provide interactive features with the user. For example consider the case of an airline customer who wants to check the arrival time and gate information of a particular flight. The user may enter information such as a date and flight number in certain form fields of the Web page. The Web program must first submit a query to the airline database to retrieve this information and then display it. Such Web pages where part of the information is extracted from databases or other data sources are called dynamic Web pages. The data extracted and displayed each time will be for different flights and dates. There are various techniques for programming dynamic features into Web pages. We will focus on one technique here which is based on using the PHP open source scripting language. PHP has recently experienced widespread use. The interpreters for PHP are provided free of charge and are written in the C language so they are chapter Chapter Web Database Programming Using PHP available on most computer platforms. A PHP interpreter provides a Hypertext Preprocessor which will execute PHP commands in a text file and create the desired HTML file. To access databases a library of PHP functions needs to be included in the PHP interpreter as we will discuss in Section PHP programs are executed on the Web server computer. This is in contrast to some scripting languages such as JavaScript that are executed on the client computer. This chapter is organized as follows. Section gives a simple example to illustrate how PHP can be used. Section gives a general overview of the PHP language and how it is used to program some basic functions for interactive Web pages. Section focuses on using PHP to interact with SQL databases through a library of functions known as PEAR DB. Finally Section contains a chapter summary. A Simple PHP Example PHP is an open source general purpose scripting language. The interpreter engine for PHP is written in the C programming language so it can be used on nearly all types of computers and operating systems. PHP usually comes installed with the UNIX operating system. For computer platforms with other operating systems such as Windows Linux or Mac OS the PHP interpreter can be downloaded from As with other scripting languages PHP is particularly suited for manipulation of text pages and in particular for manipulating dynamic HTML pages at the Web server computer. This is in contrast to JavaScript which is downloaded with the Web pages to execute on the client computer. PHP has libraries of functions for accessing databases stored under various types of relational database systems such as Oracle MySQL SQLServer and any system that supports the ODBC standard John Smith Welcome John Smith A Simple PHP Example Figure PHP program segment for entering a greeting Initial form displayed by PHP program segment User enters name John Smith Form prints welcome message for John Smith. Program Segment php Printing a welcome message if the user submitted their name through the HTML form if print print else Printing the form to enter the user name since no name has been entered yet print HTML FORM method "post" action "$SERVER['PHPSELF']" Enter your name input type "text" name "username" BR INPUT type "submit" value "SUBMIT NAME" FORM HTML printed as is. This allows PHP code segments to be included within a larger HTML file. Only the sections in the file between php and are processed by the PHP preprocessor. Line shows one way of posting comments in a PHP program on a single line started by . Single line comments can also be started with # and end at the end of the line in which they are entered. Multiple line comments start with and end with . The auto global predefined PHP variable $POST are numbered or they Chapter Web Database Programming Using PHP can be associative arrays whose indexes can be any string values. For example an associative array indexed based on color can have the indexes “red” “blue” “green” . In this example $POST is associatively indexed by the name of the posted value username that is specified in the name attribute of the input tag on line Thus $POST['username'] will contain the value typed in by the user. We will discuss PHP arrays further in Section When the Web page at is first opened the if condition in line will evaluate to false because there is no value yet in $POST['username']. Hence the PHP interpreter will process lines through which create the text for an HTML file that displays the form shown in Figure This is then displayed at the client side by the Web browser. Line shows one way of creating long text strings in an HTML file. We will discuss other ways to specify strings later in this section. All text between an opening HTML and a closing HTML is printed into the HTML file as is. The closing HTML must be alone on a separate line. Thus the text added to the HTML file sent to the client will be the text between lines and This includes HTML tags to create the form shown in Figure PHP variable names start with a $ sign and can include characters numbers and the underscore character . The PHP auto global variable $SERVER . No other special characters are permitted. Variable names are case sensitive and the first character cannot be a number. Variables are not typed. The values assigned to the variables determine their type. In fact the same variable can change its type once a new value is assigned to it. Assignment is via the operator. Since PHP is directed toward text processing there are several different types of string values. There are also many functions available for processing strings. We only discuss some basic properties of string values and variables here. Figure illustrates some string values. There are three main ways to express strings and text Single quoted strings. Enclose the string between single quotes as in lines and If a single quote is needed within the string use the escape character print strtolower print ucwords print 'abc' . 'efg' print "send your email reply to $emailaddress" print FORMHTML FORM method "post" action "$SERVER['PHPSELF']" Enter your name input type "text" name "username" FORMHTML Chapter Web Database Programming Using PHP Here documents. Enclose a part of a document between a DOCNAME and end it with a single line containing the document name DOCNAME. DOCNAME can be any string as long as it used both to start and end the here document. This is illustrated in lines through in Figure Variables are also interpolated by replacing them with their string values if they appear inside here documents. This feature is used in a similar way to double quoted strings but it is more convenient for multiple line text. Single and double quotes. Single and double quotes used by PHP to enclose strings should be straight quotes on both sides of the string. The text editor that creates these quotes should not produce curly opening and closing quotes around the string. There is also a string concatenate operator specified by the period symbol as illustrated in line of Figure There are many string functions. We only illustrate a couple of them here. The function strtolower changes the alphabetic characters in the string to all lowercase while the function ucwords capitalizes all the words in a string. These are illustrated in lines and in Figure The general rule is to use single quoted strings for literal strings that contain no PHP program variables and the other two types when the values from variables need to be interpolated into the string. For large blocks of multiline text the program should use the here documents style for strings. PHP also has numeric data types for integers and floating points and generally follows the rules of the C programming language for processing these types. Numbers can be formatted for printing into strings by specifying the number of digits that follow the decimal point. A variation of the print function called printf allows formatting of numbers within a string as illustrated in line of Figure There are the standard programming language constructs of for loops while loops and conditional if statements. They are generally similar to their C language counterparts. We will not discuss them here. Similarly any value evaluates to true if used as a Boolean expression except for numeric zero and blank string which evaluate to false. There are also literal true and false values that can be assigned. The comparison operators also generally follow C language rules. They are ! and . PHP Arrays Arrays are very important in PHP since they allow lists of elements. They are used frequently in forms that employ pull down menus. A single dimensional array is used to hold the list of choices in the pull down menu. For database query results two dimensional arrays are used with the first dimension representing rows of a table and the second dimension representing columns within a row. Overview of Basic Features of PHP Figure Illustrating basic PHP array processing. $teaching array $teaching['Graphics'] 'Benson' $teaching['Data Mining'] 'Kam' sort foreach print " $key $value\n" $courses array $altrowcolor array for i $num $i++ print ' TR bgcolor "' . $altrowcolor[$i % . '" ' print " TD Course $i is TD TD $course[$i] TD TR \n" There are two main types of arrays numeric and associative. We discuss each of these in the context of single dimensional arrays next. A numeric array associates a numeric index with each element in the array. Indexes are integer numbers that start at zero and grow incrementally. An element in the array is referenced through its index. An associative array provides pairs of elements. The value of an element is referenced through its key and all key values in a particular array must be unique. The element values can be strings or integers or they can be arrays themselves thus leading to higher dimensional arrays. Figure gives two examples of array variables $teaching and $courses. The first array $teaching is associative with the name of the course instructor . There are three elements in this array. Line shows how the array may be updated. The first command in line assigns a new instructor to the course ‘Graphics’ by updating its value. Since the key value ‘Graphics’ already exists in the array no new element is created but the existing value is updated. The second command creates a new element since the key value ‘Data Mining’ did not exist in the array before. New elements are added at the end of the array. If we only provide values as array elements the keys are automatically numeric and numbered This is illustrated in line of Figure by the $courses array. Both associative and numeric arrays have no size limits. If some value of another data type say an integer is assigned to a PHP variable that was holding an array the variable now holds the integer value and the array contents are lost. Basically most variables can be assigned to values of any data type at any time. There are several different techniques for looping through arrays in PHP. We illustrate two of these techniques in Figure Lines and show one method of looping through all the elements in an array using the foreach construct and Chapter Web Database Programming Using PHP printing the key and value of each element on a separate line. Lines through show how a traditional for loop construct can be used. A built in function count tag. The count function . For associative arrays each key remains associated with the same element value after sorting. This does not occur when sorting numeric arrays. There are many other functions that can be applied to PHP arrays but a full discussion is outside the scope of our presentation. PHP Functions As with other programming languages functions can be defined in PHP to better structure a complex program and to share common sections of code that can be reused by multiple applications. The newer version of PHP also has objectoriented features but we will not discuss these here as we are focusing on the basics of PHP. Basic PHP functions can have arguments that are passed by value. Global variables can be accessed within functions. Standard scope rules apply to variables that appear within a function and within the code that calls the function. We now give two simple examples to illustrate basic PHP functions. In Figure we show how we could rewrite the code segment from Figure using functions. The code segment in Figure has two functions displaywelcome in lines to in Figure has two arguments $course and $teachingassignments print print function displayemptyform print HTML FORM method "post" action "$SERVER['PHPSELF']" Enter your name INPUT type "text" name "username" BR INPUT type "submit" value "Submit name" FORM HTML if displaywelcome else displayemptyform Figure Illustrating a function with arguments and return value. function courseinstructor if $instructor $teachingassignments[$course] RETURN "$instructor is teaching $course" else RETURN "there is no $course course" $teaching array $teaching['Graphics'] 'Benson' $teaching['Data Mining'] 'Kam' $x courseinstructor print $x courseinstructor print Chapter Web Database Programming Using PHP ‘Computer Architecture’. A few comments about this example and about PHP functions in general The built in PHP array function arraykeyexists returns true if the value in variable $k exists as a key in the associative array in the variable $a. In our example it checks whether the $course value provided exists as a key in the array $teachingassignments to the function arguments when the function is called. Return values of a function are placed after the RETURN keyword. A function can return any type. In this example it returns a string type. Two different strings can be returned in our example depending on whether the $course key value provided exists in the array or not. Scope rules for variable names apply as in other programming languages. Global variables outside of the function cannot be used unless they are referred to using the built in PHP array $GLOBALS. Basically $GLOBALS['abc'] will access the value in a global variable $abc defined outside the function. Otherwise variables appearing inside a function are local even if there is a global variable with the same name. The previous discussion gives a brief overview of PHP functions. Many details are not discussed since it is not our goal to present PHP in detail. PHP Server Variables and Forms There are a number of built in entries in a PHP auto global built in array variable called $SERVER that can provide the programmer with useful information about the server where the PHP interpreter is running as well as other information. These may be needed when constructing the text in an HTML document address of the client user computer that is accessing the server for example $SERVER['REMOTEHOST']. This is the Web site name of the client user computer for example .edu. In this case the server will need to translate the name into an IP address to access the client. $SERVER['PATHINFO']. This is the part of the URL address that comes after a backslash at the end of the URL. Overview of PHP Database Programming $SERVER['QUERYSTRING']. This provides the string that holds parameters in a URL after a question mark at the end of the URL. This can hold search parameters for example. $SERVER['DOCUMENTROOT']. This is the root directory that holds the files on the Web server that are accessible to client users. These and other entries in the $SERVER array are usually needed when creating the HTML file to be sent for display. Another important PHP auto global built in array variable is called $POST. This provides the programmer with input values submitted by the user through HTML forms specified in the HTML INPUT tag and other similar tags. For example in Figure line the variable $POST['username'] provides the programmer with the value typed in by the user in the HTML form specified via the INPUT tag on line The keys to this array are the names of the various input parameters provided via the form for example by using the name attribute of the HTML INPUT tag as on line When users enter data through forms the data values can be stored in this array. Overview of PHP Database Programming There are various techniques for accessing a database through a programming language. We discussed some of the techniques in Chapter in the overviews of how to access an SQL database using the C and Java programming languages. In particular we discussed embedded SQL JDBC SQL CLI and SQLJ. In this section we give an overview of how to access the database using the script language PHP which is quite suitable for creating Web interfaces for searching and updating databases as well as dynamic Web pages. There is a PHP database access function library that is part of PHP Extension and Application Repository which is a collection of several libraries of functions for enhancing PHP. The PEAR DB library provides functions for database access. Many database systems can be accessed from this library including Oracle MySQL SQLite and Microsoft SQLServer among others. We will discuss several functions that are part of PEAR DB in the context of some examples. Section shows how to connect to a database using PHP. Section discusses how data collected from HTML forms can be used to insert a new record in a database table . Section shows how retrieval queries can be executed and have their results displayed within a dynamic Web page. Connecting to a Database To use the database functions in a PHP program the PEAR DB library module called must be loaded. In Figure this is done in line of the example. The DB library functions can now be accessed using DB functionname . The function for connecting to a database is called DB connect where the Chapter Web Database Programming Using PHP Figure Connecting to a database creating a table and inserting a record. require '' $d if die $q $d query" if die $d setErrorHandling $eid $d nextID $q $d query" $eid $d nextID $q $d query' array string argument specifies the database information. The format for 'string' is DBMS software user account password @ database server In Figure line connects to the database that is stored using Oracle . In general the PHP programmer can check after every database call to determine whether the last database operation was successful or not and terminate the program if it was not successful. An error message is also returned from the database via the operation $d getmessage. This can also be displayed as shown in line of Figure In general most SQL commands can be sent to the database once a connection is established via the query function. The function $d query takes an SQL command as its string argument and sends it to the database server for execution. In Figure lines to send a CREATE TABLE command to create a table called EMPLOYEE with four attributes. Whenever a query is executed the result of the query is assigned to a query variable which is called $q in our example. Line checks whether the query was executed successfully or not. The PHP PEAR DB library offers an alternative to having to check for errors after every database command. The function $d– setErrorHandling will terminate the program and print the default error messages if any subsequent errors occur when accessing the database through connection $d . An example is illustrated in lines to where another record is to be inserted. In this form of the $d query function there are two arguments. The first argument is the SQL statement with one or more symbols . The second argument is an array whose element values will be used to replace the placeholders in the order they are specified. Retrieval Queries from Database Tables We now give three examples of retrieval queries through PHP shown in Figure The first few lines to establish a database connection $d and set the error handling to the default as we discussed in the previous section. The first query in line serves to retrieve the next record in the query result and to control the loop. The looping starts at the first record. The second query example is shown in lines to and illustrates a dynamic query. In this query the conditions for selection of rows are based on values input by the user. Here we want to retrieve the names of employees who have a specific job and work for a particular department. The particular job and department number are entered through a form in the array variables $POST['empjob'] and would be similar to the system generated OID discussed in Chapter for object and objectrelational database systems. Overview of PHP Database Programming Figure Illustrating database retrieval queries. require '' $d if die $d setErrorHandling $q $d query while print "employee works for department \n" $q $d query print "employees in dept $POST['empdno'] whose job is $POST['empjob'] \n" while print "employee \n" $allresult $d getAll foreach print "employee has job and works for department \n" $POST['empdno']. If the user had entered ‘Engineer’ for the job and for the department number the query would select the names of all engineers who worked in department As we can see this is a dynamic query whose results differ depending on the choices that the user enters as input. We used two placeholders in this example as discussed at the end of Section The last query data model. These models make the designer identify entity types and relationship types and their respective attributes which leads to a natural and logical grouping of the attributes into relations when the mapping procedures discussed in Chapter are followed. However we still need some formal way of analyzing why one grouping of attributes into a relation schema may be better than another. While discussing database design in Chapters through we did not develop any measure of appropriateness or goodness to measure the quality of the design other than the intuition of the designer. In this chapter we discuss some of the theory that has been developed with the goal of evaluating relational schemas for design quality that is to measure formally why one set of groupings of attributes into relation schemas is better than another. There are two levels at which we can discuss the goodness of relation schemas. The first is the logical level how users interpret the relation schemas and the meaning of their attributes. Having good relation schemas at this level enables users to understand clearly the meaning of the data in the relations and hence to formulate their queries correctly. The second is the implementation level how the tuples in a base relation are stored and updated. This level applies only to schemas of base relations which will be physically stored as files whereas at the logical level we are interested in schemas of both base relations and views . The relational database design theory developed in this chapter applies mainly to base relations although some criteria of appropriateness also apply to views as shown in Section As with many design problems database design may be performed using two approaches bottom up or top down. A bottom up design methodology considers the basic relationships among individual attributes as the starting point and uses those to construct relation schemas. This approach is not very popular in because it suffers from the problem of having to collect a large number of binary relationships among attributes as the starting point. For practical situations it is next to impossible to capture binary relationships among all such pairs of attributes. In contrast a top down design methodology starts with a number of groupings of attributes into relations that exist together naturally for example on an invoice a form or a report. The relations are then analyzed individually and collectively leading to further decomposition until all desirable properties are met. The theory described in this chapter is applicable to both the top down and bottom up design approaches but is more appropriate when used with the top down approach. Relational database design ultimately produces a set of relations. The implicit goals of the design activity are information preservation and minimum redundancy. Information is very hard to quantify hence we consider information preservation in terms of maintaining all concepts including attribute types entity types and relationship types as well as generalization specialization relationships which are described using a model such as the EER model. Thus the relational design must preserve all of these concepts which are originally captured in the conceptual design after the conceptual to logical design mapping. Minimizing redundancy implies minimizing redundant storage of the same information and reducing the need for multiple updates to maintain consistency across multiple copies of the same information in response to real world events that require making an update. We start this chapter by informally discussing some criteria for good and bad relation schemas in Section In Section we define the concept of functional dependency a formal constraint among attributes that is the main tool for formally measuring the appropriateness of attribute groupings into relation schemas. In Section we discuss normal forms and the process of normalization using functional dependencies. Successive normal forms are defined to meet a set of desirable constraints expressed using functional dependencies. The normalization procedure consists of applying a series of tests to relations to meet these increasingly stringent requirements and decompose the relations when necessary. In Section we exception in which this approach is used in practice is based on a model called the binary relational model. An example is the NIAM methodology followed by the join dependency in Section Section summarizes the chapter. Chapter continues the development of the theory related to the design of good relational schemas. We discuss desirable properties of relational decomposition nonadditive join property and functional dependency preservation property. A general algorithm that tests whether or not a decomposition has the nonadditive join property . We then discuss properties of functional dependencies and the concept of a minimal cover of dependencies. We consider the bottom up approach to database design consisting of a set of algorithms to design relations in a desired normal form. These algorithms assume as input a given set of functional dependencies and achieve a relational design in a target normal form while adhering to the above desirable properties. In Chapter we also define additional types of dependencies that further enhance the evaluation of the goodness of relation schemas. If Chapter is not covered in a course we recommend a quick introduction to the desirable properties of decomposition and the discussion of Property NJB in Section Informal Design Guidelines for Relation Schemas Before discussing the formal theory of relational database design we discuss four informal guidelines that may be used as measures to determine the quality of relation schema design Making sure that the semantics of the attributes is clear in the schema Reducing the redundant information in tuples Reducing the NULL values in tuples Disallowing the possibility of generating spurious tuples These measures are not always independent of one another as we will see. Imparting Clear Semantics to Attributes in Relations Whenever we group attributes to form a relation schema we assume that attributes belonging to one relation have certain real world meaning and a proper interpretation associated with them. The semantics of a relation refers to its meaning resulting from the interpretation of attribute values in a tuple. In Chapter we discussed how a relation can be interpreted as a set of facts. If the conceptual design described in Chapters and is done carefully and the mapping procedure in Chapter is followed systematically the relational schema design should have a clear meaning. Chapter Basics of Functional Dependencies and Normalization for Relational Databases DEPARTMENT Dname Dnumber Ename Bdate Address Dnumber EMPLOYEE PK PK FK Pname Pnumber Plocat on Dnum PROJECT FK FK DEPTLOCATIONS Dnumber Dlocat on PK PK Pnumber Hours WORKSON FK FK PK FK Ssn Dmgrssn Ssn Figure A simplified COMPANY relational database schema. In general the easier it is to explain the semantics of the relation the better the relation schema design will be. To illustrate this consider Figure a simplified version of the COMPANY relational database schema in Figure and Figure which presents an example of populated relation states of this schema. The meaning of the EMPLOYEE relation schema is quite simple Each tuple represents an employee with values for the employee’s name Social Security number birth date and address and the number of the department that the employee works for . The Dnumber attribute is a foreign key that represents an implicit relationship between EMPLOYEE and DEPARTMENT. The semantics of the DEPARTMENT and PROJECT schemas are also straightforward Each DEPARTMENT tuple represents a department entity and each PROJECT tuple represents a project entity. The attribute Dmgrssn of DEPARTMENT relates a department to the employee who is its manager while Dnum of PROJECT relates a project to its controlling department both are foreign key attributes. The ease with which the meaning of a relation’s attributes can be explained is an informal measure of how well the relation is designed. Informal Design Guidelines for Relation Schemas Ename EMPLOYEE Sm th John B Wong Frankl n T Zelaya Al c a J Wallace Jenn fer S Narayan Ramesh K Engl sh Joyce A Jabbar Ahmad V Borg James E Null Bdate Castle Spr ng TX Fondren Houston TX Voss Houston TX R ce Houston TX Dallas Houston TX Stone Houston TX Bella re TX F re Oak Humble TX Address Dnumber Dname DEPARTMENT Research Adm strat on Headquarters Dnumber DEPTLOCATIONS Dnumber Houston Dlocat on Bella re Stafford Houston Sugarland PROJECT ProductX ProductY ProductZ Pname Pnumber Plocat on Dnum Reorgan zat on Bella re Houston Sugarland Houston Stafford Newbenef ts Stafford Computer zat on WORKSON Pnumber Hours Ssn Dmgrssn Ssn Figure Sample database state for the relational database schema in Figure Chapter Basics of Functional Dependencies and Normalization for Relational Databases Ssn EMPPROJ Pnumber Hours Ename Pname Plocat on Ename Ssn EMPDEPT Bdate Address Dnumber Dname Dmgrssn Figure Two relation schemas suffering from update anomalies. EMPDEPT and EMPPROJ. The semantics of the other two relation schemas in Figure are slightly more complex. Each tuple in DEPTLOCATIONS gives a department number and one of the locations of the department . Each tuple in WORKSON gives an employee Social Security number the project number of one of the projects that the employee works on and the number of hours per week that the employee works on that project . However both schemas have a well defined and unambiguous interpretation. The schema DEPTLOCATIONS represents a multivalued attribute of DEPARTMENT whereas WORKSON represents an M N relationship between EMPLOYEE and PROJECT. Hence all the relation schemas in Figure may be considered as easy to explain and therefore good from the standpoint of having clear semantics. We can thus formulate the following informal design guideline. Guideline Design a relation schema so that it is easy to explain its meaning. Do not combine attributes from multiple entity types and relationship types into a single relation. Intuitively if a relation schema corresponds to one entity type or one relationship type it is straightforward to interpret and to explain its meaning. Otherwise if the relation corresponds to a mixture of multiple entities and relationships semantic ambiguities will result and the relation cannot be easily explained. Examples of Violating Guideline The relation schemas in Figures and also have clear semantics. of the department for which the employee works and the Social Security number of the department manager. For the EMPPROJ relation in Figure each tuple relates an employee to a project but also includes Informal Design Guidelines for Relation Schemas the employee name project name and project location . Although there is nothing wrong logically with these two relations they violate Guideline by mixing attributes from distinct real world entities EMPDEPT mixes attributes of employees and departments and EMPPROJ mixes attributes of employees and projects and the WORKSON relationship. Hence they fare poorly against the above measure of design quality. They may be used as views but they cause problems when used as base relations as we discuss in the following section. Redundant Information in Tuples and Update Anomalies One goal of schema design is to minimize the storage space used by the base relations . Grouping attributes into relation schemas has a significant effect on storage space. For example compare the space used by the two base relations EMPLOYEE and DEPARTMENT in Figure with that for an EMPDEPT base relation in Figure which is the result of applying the NATURAL JOIN operation to EMPLOYEE and DEPARTMENT. In EMPDEPT the attribute values pertaining to a particular department are repeated for every employee who works for that department. In contrast each department’s information appears only once in the DEPARTMENT relation in Figure Only the department number is repeated in the EMPLOYEE relation for each employee who works in that department as a foreign key. Similar comments apply to the EMPPROJ relation . For example to insert a new tuple for an employee who works in department number we must enter all the attribute values of department correctly so that they are consistent with the corresponding values for department in other tuples in EMPDEPT. In the design of Figure we do not have to worry about this consistency problem because we enter only the department number in the employee tuple all other attribute values of department are recorded only once in the database as a single tuple in the DEPARTMENT relation. It is difficult to insert a new department that has no employees as yet in the EMPDEPT relation. The only way to do this is to place NULL values in the anomalies were identified by Codd to justify the need for normalization of relations as we shall discuss in Section Chapter Basics of Functional Dependencies and Normalization for Relational Databases Ename EMPDEPT Sm th John B Wong Frankl n T Zelaya Al a J Wallace Jenn fer S Narayan Ramesh K Engl sh Joyce A Jabbar Ahmad V Borg James E Ssn Bdate Castle Spr ng TX Fondren Houston TX Voss Houston TX R ce Houston TX Dallas Houston TX Stone Houston TX Berry Bella re TX F eOak Humble TX Address Adm strat on Research Research Research Adm strat on Headquarters Adm strat on Research Dnumber Dname Dmgrssn Ssn EMPPROJ Pnumber Null Hours Narayan Ramesh K Sm th John B Sm th John B Wong Frankl n T Wong Frankl n T Wong Frankl n T Wong Frankl n T Engl sh Joyce A Engl sh Joyce A Zelaya Al a J Jabbar Ahmad V Zelaya Al a J Jabbar Ahmad V Wallace Jenn fer S Wallace Jenn fer S Borg James E Ename ProductZ ProductX ProductY ProductY ProductZ Reorgan zat on ProductX ProductY Newbenef ts Newbenef ts Computer zat on Computer zat on Newbenef ts Reorgan zat on Reorgan zat on Houston Bella re Sugarland Sugarland Houston Stafford Houston Bella re Sugarland Stafford Stafford Stafford Stafford Stafford Houston Houston Pname Plocat on Computer zat on Redundancy Redundancy Redundancy Figure Sample states for EMPDEPT and EMPPROJ resulting from applying NATURAL JOIN to the relations in Figure These may be stored as base relations for performance reasons. attributes for employee. This violates the entity integrity for EMPDEPT because Ssn is its primary key. Moreover when the first employee is assigned to that department we do not need this tuple with NULL values any more. This problem does not occur in the design of Figure because a department is entered in the DEPARTMENT relation whether or not any employees work for it and whenever an employee is assigned to that department a corresponding tuple is inserted in EMPLOYEE. Informal Design Guidelines for Relation Schemas Deletion Anomalies. The problem of deletion anomalies is related to the second insertion anomaly situation just discussed. If we delete from EMPDEPT an employee tuple that happens to represent the last employee working for a particular department the information concerning that department is lost from the database. This problem does not occur in the database of Figure because DEPARTMENT tuples are stored separately. Modification Anomalies. In EMPDEPT if we change the value of one of the attributes of a particular department say the manager of department must update the tuples of all employees who work in that department otherwise the database will become inconsistent. If we fail to update some tuples the same department will be shown to have two different values for manager in different employee tuples which would be It is easy to see that these three anomalies are undesirable and cause difficulties to maintain consistency of data as well as require unnecessary updates that can be avoided hence we can state the next guideline as follows. Guideline Design the base relation schemas so that no insertion deletion or modification anomalies are present in the relations. If any anomalies are note them clearly and make sure that the programs that update the database will operate correctly. The second guideline is consistent with and in a way a restatement of the first guideline. We can also see the need for a more formal approach to evaluating whether a design meets these guidelines. Sections through provide these needed formal concepts. It is important to note that these guidelines may sometimes have to be violated in order to improve the performance of certain queries. If EMPDEPT is used as a stored relation in addition to the base relations of EMPLOYEE and DEPARTMENT the anomalies in EMPDEPT must be noted and accounted for . This way whenever the base relation is updated we do not end up with inconsistencies. In general it is advisable to use anomaly free base relations and to specify views that include the joins for placing together the attributes frequently referenced in important queries. NULL Values in Tuples In some schema designs we may group many attributes together into a “fat” relation. If many of the attributes do not apply to all tuples in the relation we end up with many NULLs in those tuples. This can waste space at the storage level and may is not as serious as the other problems because all tuples can be updated by a single SQL query. application considerations may dictate and make certain anomalies unavoidable. For example the EMPDEPT relation may correspond to a query or a report that is frequently required. Chapter Basics of Functional Dependencies and Normalization for Relational Databases also lead to problems with understanding the meaning of the attributes and with specifying JOIN operations at the logical Another problem with NULLs is how to account for them when aggregate operations such as COUNT or SUM are applied. SELECT and JOIN operations involve comparisons if NULL values are present the results may become Moreover NULLs can have multiple interpretations such as the following The attribute does not apply to this tuple. For example Visastatus may not apply to . students. The attribute value for this tuple is unknown. For example the Dateofbirth may be unknown for an employee. The value is known but absent that is it has not been recorded yet. For example the HomePhoneNumber for an employee may exist but may not be available and recorded yet. Having the same representation for all NULLs compromises the different meanings they may have. Therefore we may state another guideline. Guideline As far as possible avoid placing attributes in a base relation whose values may frequently be NULL. If NULLs are unavoidable make sure that they apply in exceptional cases only and do not apply to a majority of tuples in the relation. Using space efficiently and avoiding joins with NULL values are the two overriding criteria that determine whether to include the columns that may have NULLs in a relation or to have a separate relation for those columns . For example if only percent of employees have individual offices there is little justification for including an attribute Officenumber in the EMPLOYEE relation rather a relation EMPOFFICES can be created to include tuples for only the employees with individual offices. Generation of Spurious Tuples Consider the two relation schemas EMPLOCS and in Figure which can be used instead of the single EMPPROJ relation in Figure A tuple in EMPLOCS means that the employee whose name is Ename works on some project whose location is Plocation. A tuple in refers to the fact that the employee whose Social Security number is Ssn works Hours per week on the project whose name number and location are Pname Pnumber and Plocation. Figure shows relation states of EMPLOCS and corresponding to the is because inner and outer joins produce different results when NULLs are involved in joins. The users must thus be aware of the different meanings of the various types of joins. Although this is reasonable for sophisticated users it may be difficult for others. Section we presented comparisons involving NULL values where the outcome are TRUE FALSE and UNKNOWN. Informal Design Guidelines for Relation Schemas Ssn Pnumber Hours Pname Plocat on Ename PK Plocat on PK EMPLOCS Ename Sm th John B Sm th John B Narayan Ramesh K Engl sh Joyce A Engl sh Joyce A Wong Frankl n T Wong Frankl n T Wong Frankl n T Zelaya Al a J Jabbar Ahmad V Wallace Jenn fer S Wallace Jenn fer S Borg James E Houston Bella re Sugarland Sugarland Bella re Sugarland Stafford Houston Stafford Houston Houston Stafford Stafford Plocat on Ssn Pnumber NULL ProductZ ProductX ProductY ProductY ProductZ Computer zat on Reorgan zat on ProductX ProductY Newbenef ts Newbenef ts Computer zat on Computer zat on Newbenef ts Reorgan zat on Reorgan zat on Houston Bella re Sugarland Sugarland Houston Stafford Houston Bella re Sugarland Stafford Stafford Stafford Stafford Stafford Houston Houston Hours Pname Plocat on EMPLOCS Figure Particularly poor design for the EMPPROJ relation in Figure The two relation schemas EMPLOCS and The result of projecting the extension of EMPPROJ from Figure onto the relations EMPLOCS and EMPPROJ relation in Figure which are obtained by applying the appropriate PROJECT operations to EMPPROJ . Suppose that we used and EMPLOCS as the base relations instead of EMPPROJ. This produces a particularly bad schema design because we cannot recover the information that was originally in EMPPROJ from and EMPLOCS. If we attempt a NATURAL JOIN operation on and EMPLOCS the result produces many more tuples than the original set of tuples in EMPPROJ. In Figure the result of applying the join to only the tuples above the dashed lines in Figure is shown . Additional tuples that were not in EMPPROJ are called spurious tuples Chapter Basics of Functional Dependencies and Normalization for Relational Databases Ssn Pnumber Hours ProductY ProductX ProductX ProductZ ProductZ ProductX ProductX ProductY ProductY ProductY ProductY ProductY ProductY ProductY ProductY ProductZ Pname Sugarland Bella re Bella re Houston Houston Bella re Bella re Sugarland Sugarland Sugarland Sugarland Sugarland Sugarland Sugarland Sugarland Houston ProductZ Houston Computer zat on Stafford Reorgan zat on Houston Reorgan zat on Houston Sm th John B Sm th John B Engl sh Joyce A Narayan Ramesh K Wong Frankl n T Sm th John B Engl sh Joyce A Engl sh Joyce A Wong Frankl n T Sm th John B Sm th John B Engl sh Joyce A Wong Frankl n T Engl sh Joyce A Wong Frankl n T Narayan Ramesh K Wong Frankl n T Wong Frankl n T Narayan Ramesh K Wong Frankl n T Plocat on Ename Figure Result of applying NATURAL JOIN to the tuples above the dashed lines in and EMPLOCS of Figure Generated spurious tuples are marked by asterisks. because they represent spurious information that is not valid. The spurious tuples are marked by asterisks in Figure Decomposing EMPPROJ into EMPLOCS and is undesirable because when we JOIN them back using NATURAL JOIN we do not get the correct original information. This is because in this case Plocation is the attribute that relates EMPLOCS and and Plocation is neither a primary key nor a foreign key in either EMPLOCS or We can now informally state another design guideline. Guideline Design relation schemas so that they can be joined with equality conditions on attributes that are appropriately related pairs in a way that guarantees that no spurious tuples are generated. Avoid relations that contain Functional Dependencies matching attributes that are not combinations because joining on such attributes may produce spurious tuples. This informal guideline obviously needs to be stated more formally. In Section we discuss a formal condition called the nonadditive join property that guarantees that certain joins do not produce spurious tuples. Summary and Discussion of Design Guidelines In Sections through we informally discussed situations that lead to problematic relation schemas and we proposed informal guidelines for a good relational design. The problems we pointed out which can be detected without additional tools of analysis are as follows Anomalies that cause redundant work to be done during insertion into and modification of a relation and that may cause accidental loss of information during a deletion from a relation Waste of storage space due to NULLs and the difficulty of performing selections aggregation operations and joins due to NULL values Generation of invalid and spurious data during joins on base relations with matched attributes that may not represent a proper relationship In the rest of this chapter we present formal concepts and theory that may be used to define the goodness and badness of individual relation schemas more precisely. First we discuss functional dependency as a tool for analysis. Then we specify the three normal forms and Boyce Codd normal form for relation schemas. The strategy for achieving a good design is to decompose a badly designed relation appropriately. We also briefly introduce additional normal forms that deal with additional dependencies. In Chapter we discuss the properties of decomposition in detail and provide algorithms that design relations bottom up by using the functional dependencies as a starting point. Functional Dependencies So far we have dealt with the informal measures of database design. We now introduce a formal tool for analysis of relational schemas that enables us to detect and describe some of the above mentioned problems in precise terms. The single most important concept in relational schema design theory is that of a functional dependency. In this section we formally define the concept and in Section we see how it can be used to define normal forms for relation schemas. Definition of Functional Dependency A functional dependency is a constraint between two sets of attributes from the database. Suppose that our relational database schema has n attributes An let us think of the whole database as being described by a single universal Chapter Basics of Functional Dependencies and Normalization for Relational Databases relation schema R We do not imply that we will actually store the database as a single universal table we use this concept only in developing the formal theory of data Definition. A functional dependency denoted by X → Y between two sets of attributes X and Y that are subsets of R specifies a constraint on the possible tuples that can form a relation state r of R. The constraint is that for any two tuples t and t in r that have t t they must also have t t This means that the values of the Y component of a tuple in r depend on or are determined by the values of the X component alternatively the values of the X component of a tuple uniquely determine the values of the Y component. We also say that there is a functional dependency from X to Y or that Y is functionally dependent on X. The abbreviation for functional dependency is FD or . The set of attributes X is called the left hand side of the FD and Y is called the right hand side. Thus X functionally determines Y in a relation schema R if and only if whenever two tuples of r agree on their X value they must necessarily agree on their Yvalue. Note the following If a constraint on R states that there cannot be more than one tuple with a given X value in any relation instance r that is X is a candidate key of R this implies that X → Y for any subset of attributes Y of R will have the same value of X . If X is a candidate key of R then X → R. If X → Y in R this does not say whether or not Y → X in R. A functional dependency is a property of the semantics or meaning of the attributes. The database designers will use their understanding of the semantics of the attributes of R that is how they relate to one another to specify the functional dependencies that should hold on all relation states r of R. Whenever the semantics of two sets of attributes in R indicate that a functional dependency should hold we specify the dependency as a constraint. Relation extensions r that satisfy the functional dependency constraints are called legal relation states of R. Hence the main use of functional dependencies is to describe further a relation schema R by specifying constraints on its attributes that must hold at all times. Certain FDs can be specified without referring to a specific relation but as a property of those attributes given their commonly understood meaning. For example State Driverlicensenumber → Ssn should hold for any adult in the United States and hence should hold whenever these attributes appear in a relation. It is also possible that certain functional dependencies may cease to concept of a universal relation is important when we discuss the algorithms for relational database design in Chapter assumption implies that every attribute in the database should have a distinct name. In Chapter we prefixed attribute names by relation names to achieve uniqueness whenever attributes in distinct relations had the same name. Functional Dependencies TEACH Teacher Sm th Sm th Hall Brown Bartram Mart n Hoffman Horow tz Comp lers Data Structures Data Management Data Structures Course Text Figure A relation state of TEACH with a possible functional dependency TEXT → COURSE. However TEACHER → COURSE is ruled out. exist in the real world if the relationship changes. For example the FD Zipcode → Areacode used to exist as a relationship between postal codes and telephone number codes in the United States but with the proliferation of telephone area codes it is no longer true. Consider the relation schema EMPPROJ in Figure from the semantics of the attributes and the relation we know that the following functional dependencies should hold a. Ssn → Ename b. Pnumber → Pname Plocation c. Ssn Pnumber → Hours These functional dependencies specify that the value of an employee’s Social Security number uniquely determines the employee name the value of a project’s number uniquely determines the project name and location and a combination of Ssn and Pnumber values uniquely determines the number of hours the employee currently works on the project per week . Alternatively we say that Ename is functionally determined by Ssn or given a value of Ssn we know the value of Ename and so on. A functional dependency is a property of the relation schema R not of a particular legal relation state r of R. Therefore an FD cannot be inferred automatically from a given relation extension r but must be defined explicitly by someone who knows the semantics of the attributes of R. For example Figure shows a particular state of the TEACH relation schema. Although at first glance we may think that Text → Course we cannot confirm this unless we know that it is true for all possible legal states of TEACH. It is however sufficient to demonstrate a single counterexample to disprove a functional dependency. For example because ‘Smith’ teaches both ‘Data Structures’ and ‘Data Management ’ we can conclude that Teacher does not functionally determine Course. Given a populated relation one cannot determine which FDs hold and which do not unless the meaning of and the relationships among the attributes are known. All one can say is that a certain FD may exist if it holds in that particular extension. One cannot guarantee its existence until the meaning of the corresponding attributes is clearly understood. One can however emphatically state that a certain FD does not Chapter Basics of Functional Dependencies and Normalization for Relational Databases Figure A relation R with its extension. A B C D hold if there are tuples that show the violation of such an FD. See the illustrative example relation in Figure Here the following FDs may hold because the four tuples in the current extension have no violation of these constraints B → C C → B A B → C A B → D and C D → B. However the following do not hold because we already have violations of them in the given extension A → B B → A D → C . Figure introduces a diagrammatic notation for displaying FDs Each FD is displayed as a horizontal line. The left hand side attributes of the FD are connected by vertical lines to the line representing the FD while the right hand side attributes are connected by the lines with arrows pointing toward the attributes. We denote by F the set of functional dependencies that are specified on relation schema R. Typically the schema designer specifies the functional dependencies that are semantically obvious usually however numerous other functional dependencies hold in all legal relation instances among sets of attributes that can be derived from and satisfy the dependencies in F. Those other dependencies can be inferred or deduced from the FDs in F. We defer the details of inference rules and properties of functional dependencies to Chapter Normal Forms Based on Primary Keys Having introduced functional dependencies we are now ready to use them to specify some aspects of the semantics of relation schemas. We assume that a set of functional dependencies is given for each relation and that each relation has a designated primary key this information combined with the tests for normal forms drives the normalization process for relational schema design. Most practical relational design projects take one of the following two approaches Perform a conceptual schema design using a conceptual model such as ER or EER and map the conceptual design into a set of relations Design the relations based on external knowledge derived from an existing implementation of files or forms or reports Following either of these approaches it is then useful to evaluate the relations for goodness and decompose them further as needed to achieve higher normal forms using the normalization theory presented in this chapter and the next. We focus in Normal Forms Based on Primary Keys this section on the first three normal forms for relation schemas and the intuition behind them and discuss how they were developed historically. More general definitions of these normal forms which take into account all candidate keys of a relation rather than just the primary key are deferred to Section We start by informally discussing normal forms and the motivation behind their development as well as reviewing some definitions from Chapter that are needed here. Then we discuss the first normal form in Section and present the definitions of second normal form and third normal form which are based on primary keys in Sections and respectively. Normalization of Relations The normalization process as first proposed by Codd takes a relation schema through a series of tests to certify whether it satisfies a certain normal form. The process which proceeds in a top down fashion by evaluating each relation against the criteria for normal forms and decomposing relations as necessary can thus be considered as relational design by analysis. Initially Codd proposed three normal forms which he called first second and third normal form. A stronger definition of Boyce Codd normal form was proposed later by Boyce and Codd. All these normal forms are based on a single analytical tool the functional dependencies among the attributes of a relation. Later a fourth normal form and a fifth normal form were proposed based on the concepts of multivalued dependencies and join dependencies respectively these are briefly discussed in Sections and Normalization of data can be considered a process of analyzing the given relation schemas based on their FDs and primary keys to achieve the desirable properties of minimizing redundancy and minimizing the insertion deletion and update anomalies discussed in Section It can be considered as a “filtering” or “purification” process to make the design have successively better quality. Unsatisfactory relation schemas that do not meet certain conditions the normal form tests are decomposed into smaller relation schemas that meet the tests and hence possess the desirable properties. Thus the normalization procedure provides database designers with the following A formal framework for analyzing relation schemas based on their keys and on the functional dependencies among their attributes A series of normal form tests that can be carried out on individual relation schemas so that the relational database can be normalized to any desired degree Definition. The normal form of a relation refers to the highest normal form condition that it meets and hence indicates the degree to which it has been normalized. Normal forms when considered in isolation from other factors do not guarantee a good database design. It is generally not sufficient to check separately that each Chapter Basics of Functional Dependencies and Normalization for Relational Databases relation schema in the database is say in BCNF or Rather the process of normalization through decomposition must also confirm the existence of additional properties that the relational schemas taken together should possess. These would include two properties The nonadditive join or lossless join property which guarantees that the spurious tuple generation problem discussed in Section does not occur with respect to the relation schemas created after decomposition. The dependency preservation property which ensures that each functional dependency is represented in some individual relation resulting after decomposition. The nonadditive join property is extremely critical and must be achieved at any cost whereas the dependency preservation property although desirable is sometimes sacrificed as we discuss in Section We defer the presentation of the formal concepts and techniques that guarantee the above two properties to Chapter Practical Use of Normal Forms Most practical design projects acquire existing designs of databases from previous designs designs in legacy models or from existing files. Normalization is carried out in practice so that the resulting designs are of high quality and meet the desirable properties stated previously. Although several higher normal forms have been defined such as the and that we discuss in Sections and the practical utility of these normal forms becomes questionable when the constraints on which they are based are rare and hard to understand or to detect by the database designers and users who must discover these constraints. Thus database design as practiced in industry today pays particular attention to normalization only up to BCNF or at most Another point worth noting is that the database designers need not normalize to the highest possible normal form. Relations may be left in a lower normalization status such as for performance reasons such as those discussed at the end of Section Doing so incurs the corresponding penalties of dealing with the anomalies. Definition. Denormalization is the process of storing the join of higher normal form relations as a base relation which is in a lower normal form. Definitions of Keys and Attributes Participating in Keys Before proceeding further let’s look again at the definitions of keys of a relation schema from Chapter Definition. A superkey of a relation schema R An is a set of attributes S ⊆ R with the property that no two tuples t and t in any legal relation state r of R will have t t A key K is a superkey with the additional property that removal of any attribute from K will cause K not to be a superkey any more. Normal Forms Based on Primary Keys The difference between a key and a superkey is that a key has to be minimal that is if we have a key K Ak of R then K – Ai is not a key of R for any Ai ≤ i ≤ k. In Figure Ssn is a key for EMPLOYEE whereas Ssn Ssn Ename Ssn Ename Bdate and any set of attributes that includes Ssn are all superkeys. If a relation schema has more than one key each is called a candidate key. One of the candidate keys is arbitrarily designated to be the primary key and the others are called secondary keys. In a practical relational database each relation schema must have a primary key. If no candidate key is known for a relation the entire relation can be treated as a default superkey. In Figure Ssn is the only candidate key for EMPLOYEE so it is also the primary key. Definition. An attribute of relation schema R is called a prime attribute of R if it is a member of some candidate key of R. An attribute is called nonprime if it is not a prime attribute that is if it is not a member of any candidate key. In Figure both Ssn and Pnumber are prime attributes of WORKSON whereas other attributes of WORKSON are nonprime. We now present the first three normal forms and These were proposed by Codd as a sequence to achieve the desirable state of relations by progressing through the intermediate states of and if needed. As we shall see and attack different problems. However for historical reasons it is customary to follow them in that sequence hence by definition a relation already satisfies First Normal Form First normal form is now considered to be part of the formal definition of a relation in the basic relational model historically it was defined to disallow multivalued attributes composite attributes and their combinations. It states that the domain of an attribute must include only atomic values and that the value of any attribute in a tuple must be a single value from the domain of that attribute. Hence disallows having a set of values a tuple of values or a combination of both as an attribute value for a single tuple. In other words disallows relations within relations or relations as attribute values within tuples. The only attribute values permitted by are single atomic values. Consider the DEPARTMENT relation schema shown in Figure whose primary key is Dnumber and suppose that we extend it by including the Dlocations attribute as shown in Figure We assume that each department can have a number of locations. The DEPARTMENT schema and a sample relation state are shown in Figure As we can see this is not in because Dlocations is not an atomic attribute as illustrated by the first tuple in Figure There are two ways we can look at the Dlocations attribute The domain of Dlocations contains atomic values but some tuples can have a set of these values. In this case Dlocations is not functionally dependent on the primary key Dnumber. Chapter Basics of Functional Dependencies and Normalization for Relational Databases Dname DEPARTMENT DEPARTMENT DEPARTMENT Dnumber Dmgrssn Dlocat ons Dname Research Adm n strat on Headquarters Dnumber Dmgrssn Houston Bella re Sugarland Houston Stafford Dlocat ons Dname Research Research Research Adm n strat on Headquarters Bella re Sugarland Houston Stafford Houston Dnumber Dmgrssn Dlocat on Figure Normalization into A relation schema that is not in Sample state of relation DEPARTMENT. version of the same relation with redundancy. The domain of Dlocations contains sets of values and hence is nonatomic. In this case Dnumber → Dlocations because each set is considered a single member of the attribute In either case the DEPARTMENT relation in Figure is not in in fact it does not even qualify as a relation according to our definition of relation in Section There are three main techniques to achieve first normal form for such a relation Remove the attribute Dlocations that violates and place it in a separate relation DEPTLOCATIONS along with the primary key Dnumber of DEPARTMENT. The primary key of this relation is the combination Dnumber Dlocation as shown in Figure A distinct tuple in DEPTLOCATIONS exists for each location of a department. This decomposes the relation into two relations. this case we can consider the domain of Dlocations to be the power set of the set of single locations that is the domain is made up of all possible subsets of the set of single locations. Normal Forms Based on Primary Keys Expand the key so that there will be a separate tuple in the original DEPARTMENT relation for each location of a DEPARTMENT as shown in Figure In this case the primary key becomes the combination Dnumber Dlocation . This solution has the disadvantage of introducing redundancy in the relation. If a maximum number of values is known for the attribute for example if it is known that at most three locations can exist for a department replace the Dlocations attribute by three atomic attributes and This solution has the disadvantage of introducing NULL values if most departments have fewer than three locations. It further introduces spurious semantics about the ordering among the location values that is not originally intended. Querying on this attribute becomes more difficult for example consider how you would write the query List the departments that have ‘Bellaire’ as one of their locations in this design. Of the three solutions above the first is generally considered best because it does not suffer from redundancy and it is completely general having no limit placed on a maximum number of values. In fact if we choose the second solution it will be decomposed further during subsequent normalization steps into the first solution. First normal form also disallows multivalued attributes that are themselves composite. These are called nested relations because each tuple can have a relation within it. Figure shows how the EMPPROJ relation could appear if nesting is allowed. Each tuple represents an employee entity and a relation PROJS within each tuple represents the employee’s projects and the hours per week that employee works on each project. The schema of this EMPPROJ relation can be represented as follows EMPPROJ The set braces identify the attribute PROJS as multivalued and we list the component attributes that form PROJS between parentheses . Interestingly recent trends for supporting complex objects while Pnumber is the partial key of the nested relation that is within each tuple the nested relation must have unique values of Pnumber. To normalize this into we remove the nested relation attributes into a new relation and propagate the primary key into it the primary key of the new relation will combine the partial key with the primary key of the original relation. Decomposition and primary key propagation yield the schemas and as shown in Figure This procedure can be applied recursively to a relation with multiple level nesting to unnest the relation into a set of relations. This is useful in converting an unnormalized relation schema with many levels of nesting into relations. The Chapter Basics of Functional Dependencies and Normalization for Relational Databases EMPPROJ Pro s Ssn Ename Pnumber Hours Ssn Ename Ssn Pnumber Hours EMPPROJ Ssn Zelaya Al c a J Jabbar Ahmad V Wallace Jenn fer S Borg James E NULL Engl sh Joyce A Narayan Ramesh K Sm th John B Wong Frankl n T Ename Pnumber Hours Figure Normalizing nested relations into Schema of the EMPPROJ relation with a nested relation attribute PROJS. Sample extension of the EMPPROJ relation showing nested relations within each tuple. Decomposition of EMPPROJ into relations and by propagating the primary key. existence of more than one multivalued attribute in one relation must be handled carefully. As an example consider the following relation PERSON This relation represents the fact that a person has multiple cars and multiple phones. If strategy above is followed it results in an all key relation Normal Forms Based on Primary Keys To avoid introducing any extraneous relationship between Carlic# and Phone# all possible combinations of values are represented for every Ss# giving rise to redundancy. This leads to the problems handled by multivalued dependencies and which we will discuss in Section The right way to deal with the two multivalued attributes in PERSON shown previously is to decompose it into two separate relations using strategy discussed above Carlic# and Phone# . Second Normal Form Second normal form is based on the concept of full functional dependency. A functional dependency X → Y is a full functional dependency if removal of any attribute A from X means that the dependency does not hold any more that is for any attribute A ε X does not functionally determine Y. A functional dependency X → Y is a partial dependency if some attribute A ε X can be removed from X and the dependency still holds that is for some A ε X → Y. In Figure Ssn Pnumber → Hours is a full dependency . However the dependency Ssn Pnumber → Ename is partial because Ssn → Ename holds. Definition. A relation schema R is in if every nonprime attribute A in R is fully functionally dependent on the primary key of R. The test for involves testing for functional dependencies whose left hand side attributes are part of the primary key. If the primary key contains a single attribute the test need not be applied at all. The EMPPROJ relation in Figure is in but is not in The nonprime attribute Ename violates because of as do the nonprime attributes Pname and Plocation because of The functional dependencies and make Ename Pname and Plocation partially dependent on the primary key Ssn Pnumber of EMPPROJ thus violating the test. If a relation schema is not in it can be second normalized or normalized into a number of relations in which nonprime attributes are associated only with the part of the primary key on which they are fully functionally dependent. Therefore the functional dependencies and in Figure lead to the decomposition of EMPPROJ into the three relation schemas and shown in Figure each of which is in Third Normal Form Third normal form is based on the concept of transitive dependency. A functional dependency X → Y in a relation schema R is a transitive dependency if there exists a set of attributes Z in R that is neither a candidate key nor a subset of any key of R and both X → Z and Z → Y hold. The dependency Ssn → Dmgrssn is transitive through Dnumber in EMPDEPT in Figure because both the is the general definition of transitive dependency. Because we are concerned only with primary keys in this section we allow transitive dependencies where X is the primary key but Z may be a candidate key. Chapter Basics of Functional Dependencies and Normalization for Relational Databases Ssn EMPPROJ Normalization Pnumber Hours Ename Pname Plocat on Ssn Pnumber Hours Ename Ssn Bdate Address Dnumber Ssn Ename Pnumber Pname Plocat on Ename Ssn EMPDEPT Bdate Address Dnumber Dname Dmgrssn Dnumber Dname Dmgrssn Normalization Figure Normalizing into and Normalizing EMPPROJ into relations. Normalizing EMPDEPT into relations. dependencies Ssn → Dnumber and Dnumber → Dmgrssn hold and Dnumber is neither a key itself nor a subset of the key of EMPDEPT. Intuitively we can see that the dependency of Dmgrssn on Dnumber is undesirable in EMPDEPT since Dnumber is not a key of EMPDEPT. Definition. According to Codd’s original definition a relation schema R is in if it satisfies and no nonprime attribute of R is transitively dependent on the primary key. The relation schema EMPDEPT in Figure is in since no partial dependencies on a key exist. However EMPDEPT is not in because of the transitive dependency of Dmgrssn on Ssn via Dnumber. We can normalize General Definitions of Second and Third Normal Forms Table Summary of Normal Forms Based on Primary Keys and Corresponding Normalization Normal Form Test Remedy First Relation should have no multivalued attributes or nested relations. Form new relations for each multivalued attribute or nested relation. Second For relations where primary key contains multiple attributes no nonkey attribute should be functionally dependent on a part of the primary key. Decompose and set up a new relation for each partial key with its dependent attribute. Make sure to keep a relation with the original primary key and any attributes that are fully functionally dependent on it. Third Relation should not have a nonkey attribute functionally determined by another nonkey attribute . That is there should be no transitive dependency of a nonkey attribute on the primary key. Decompose and set up a relation that includes the nonkey attribute that functionally determine other nonkey attribute. EMPDEPT by decomposing it into the two relation schemas and shown in Figure Intuitively we see that and represent independent entity facts about employees and departments. A NATURAL JOIN operation on and will recover the original relation EMPDEPT without generating spurious tuples. Intuitively we can see that any functional dependency in which the left hand side is part of the primary key or any functional dependency in which the left hand side is a nonkey attribute is a problematic FD. and normalization remove these problem FDs by decomposing the original relation into new relations. In terms of the normalization process it is not necessary to remove the partial dependencies before the transitive dependencies but historically has been defined with the assumption that a relation is tested for first before it is tested for Table informally summarizes the three normal forms based on primary keys the tests used in each case and the corresponding remedy or normalization performed to achieve the normal form. General Definitions of Second and Third Normal Forms In general we want to design our relation schemas so that they have neither partial nor transitive dependencies because these types of dependencies cause the update anomalies discussed in Section The steps for normalization into relations that we have discussed so far disallow partial and transitive dependencies on the primary key. The normalization procedure described so far is useful for analysis in practical situations for a given database where primary keys have already been defined. These definitions however do not take other candidate keys of a relation if Chapter Basics of Functional Dependencies and Normalization for Relational Databases any into account. In this section we give the more general definitions of and that take all candidate keys of a relation into account. Notice that this does not affect the definition of since it is independent of keys and functional dependencies. As a general definition of prime attribute an attribute that is part of any candidate key will be considered as prime. Partial and full functional dependencies and transitive dependencies will now be considered with respect to all candidate keys of a relation. General Definition of Second Normal Form Definition. A relation schema R is in second normal form if every nonprime attribute A in R is not partially dependent on any key of R. The test for involves testing for functional dependencies whose left hand side attributes are part of the primary key. If the primary key contains a single attribute the test need not be applied at all. Consider the relation schema LOTS shown in Figure which describes parcels of land for sale in various counties of a state. Suppose that there are two candidate keys Propertyid# and Countyname Lot# that is lot numbers are unique only within each county but Propertyid# numbers are unique across counties for the entire state. Based on the two candidate keys Propertyid# and Countyname Lot# the functional dependencies and in Figure hold. We choose Propertyid# as the primary key so it is underlined in Figure but no special consideration will be given to this key over the other candidate key. Suppose that the following two additional functional dependencies hold in LOTS Countyname → Taxrate Area → Price In words the dependency says that the tax rate is fixed for a given county while says that the price of a lot is determined by its area regardless of which county it is in. The LOTS relation schema violates the general definition of because Taxrate is partially dependent on the candidate key Countyname Lot# due to To normalize LOTS into we decompose it into the two relations and shown in Figure We construct by removing the attribute Taxrate that violates from LOTS and placing it with Countyname into another relation Both and are in Notice that does not violate and is carried over to definition can be restated as follows A relation schema R is in if every nonprime attribute A in R is fully functionally dependent on every key of R. General Definitions of Second and Third Normal Forms Property d# LOTS Countyname Lot# Area Pr ce Taxrate Property d# Countyname Lot# Area Pr ce Property d# Countyname Lot# Area Countyname Taxrate Area Pr ce LOTS Cand date Key Figure Normalization into and The LOTS relation with its functional dependencies through Decomposing into the relations and Decomposing into the relations and Summary of the progressive normalization of LOTS. Chapter Basics of Functional Dependencies and Normalization for Relational Databases General Definition of Third Normal Form Definition. A relation schema R is in third normal form if whenever a nontrivial functional dependency X → A holds in R either X is a superkey of R or A is a prime attribute of R. According to this definition into another relation Both and are in Two points are worth noting about this example and the general definition of violates because Price is transitively dependent on each of the candidate keys of via the nonprime attribute Area. This general definition can be applied directly to test whether a relation schema is in it does not have to go through first. If we apply the above definition to LOTS with the dependencies through we find that both and violate Therefore we could decompose LOTS into and directly. Hence the transitive and partial dependencies that violate can be removed in any order. Interpreting the General Definition of Third Normal Form A relation schema R violates the general definition of if a functional dependency X → A holds in R that does not meet either condition meaning that it violates both conditions and of This can occur due to two types of problematic functional dependencies A nonprime attribute determines another nonprime attribute. Here we typically have a transitive dependency that violates A proper subset of a key of R functionally determines a nonprime attribute. Here we have a partial dependency that violates BCNF Normalization Countyname Lot# Area Property d# Area Lot# A R B C Area Countyname Figure Boyce Codd normal form. BCNF normalization of with the functional dependency being lost in the decomposition. A schematic relation with FDs it is in but not in BCNF. Boyce Codd Normal Form Boyce Codd normal form was proposed as a simpler form of but it was found to be stricter than That is every relation in BCNF is also in however a relation in is not necessarily in BCNF. Intuitively we can see the need for a stronger normal form than by going back to the LOTS relation schema in Figure with its four functional dependencies through Suppose that we have thousands of lots in the relation but the lots are from only two counties DeKalb and Fulton. Suppose also that lot sizes in DeKalb County are only and acres whereas lot sizes in Fulton County are restricted to and acres. In such a situation we would have the additional functional dependency Area → Countyname. If we add this to the other dependencies the relation schema still is in because Countyname is a prime attribute. The area of a lot that determines the county as specified by can be represented by tuples in a separate relation R since there are only possible Area values of which allows A to be prime is absent from BCNF. That makes BCNF a stronger normal form compared to In our example violates BCNF in because AREA is not a superkey of Note that satisfies in because Countyname is a prime attribute but this condition does not exist in the definition of BCNF. We can decompose into two BCNF relations and shown in Figure This decomposition loses the functional dependency because its attributes no longer coexist in the same relation after decomposition. In practice most relation schemas that are in are also in BCNF. Only if X → A holds in a relation schema R with X not being a superkey and A being a prime attribute will R be in but not in BCNF. The relation schema R shown in Figure illustrates the general case of such a relation. Ideally relational database design should strive to achieve BCNF or for every relation schema. Achieving the normalization status of just or is not considered adequate since they were developed historically as stepping stones to and BCNF. As another example consider Figure which shows a relation TEACH with the following dependencies Student Course → Instructor Instructor → Course Note that Student Course is a candidate key for this relation and that the dependencies shown follow the pattern in Figure with Student as A Course as B and Instructor as C. Hence this relation is in but not BCNF. Decomposition of this relation schema into two schemas is not straightforward because it may be dependency means that each instructor teaches one course is a constraint for this application. Multivalued Dependency and Fourth Normal Form decomposed into one of the three following possible pairs Student Instructor and Student Course . Course Instructor and Course Student . Instructor Course and Instructor Student . All three decompositions lose the functional dependency The desirable decomposition of those just shown is because it will not generate spurious tuples after a join. A test to determine whether a decomposition is nonadditive is discussed in Section under Property NJB. In general a relation not in BCNF should be decomposed so as to meet this property. We make sure that we meet this property because nonadditive decomposition is a must during normalization. We may have to possibly forgo the preservation of all functional dependencies in the decomposed relations as is the case in this example. Algorithm does that and could be used above to give decomposition for TEACH which yields two relations in BCNF as and Note that if we designate as a primary key of the relation TEACH the FD Instructor → Course causes a partial dependency of Course on a part of this key. This FD may be removed as a part of second normalization yielding exactly the same two relations in the result. This is an example of a case where we may reach the same ultimate BCNF design via alternate paths of normalization. Multivalued Dependency and Fourth Normal Form So far we have discussed the concept of functional dependency which is by far the most important type of dependency in relational database design theory and normal forms based on functional dependencies. However in many cases relations have constraints that cannot be specified as functional dependencies. In this section we discuss the concept of multivalued dependency and define fourth normal form which is based on this dependency. A more formal discussion of MVDs and their properties is deferred to Chapter Multivalued dependencies are a consequence of first normal form EMP Ename Sm th Sm th Sm th Sm th John Anna Anna John X Y X Y Pname Dname EMPPROJECTS Ename Sm th Sm th X Y Pname EMPDEPENDENTS Ename Sm th Sm th John Anna Dname S U P P LY Sname Sm th Sm th Adamsky Walton Adamsky Adamsky Sm th Bolt Bolt Nut Bolt Nut Na Bolt Pro Y Pro X Pro Y Pro X Pro Z Pro X Pro Y Partname Pro name R Sname Sm th Sm th Adamsky Walton Adamsky Bolt Bolt Nut Nut Na Bolt Bolt Nut Nut Na Partname R Sname Sm th Sm th Adamsky Walton Adamsky Pro name Pro Y Pro X Pro Y Pro Z Pro X R Partname Pro name Pro Y Pro X Pro Y Pro Z Pro X Figure Fourth and fifth normal forms. The EMP relation with two MVDs Ename →→ Pname and Ename →→ Dname. Decomposing the EMP relation into two relations EMPPROJECTS and EMPDEPENDENTS. The relation SUPPLY with no MVDs is in but not in if it has the Decomposing the relation SUPPLY into the relations For example consider the relation EMP shown in Figure A tuple in this EMP relation represents the fact that an employee whose name is Ename works on the project whose name is Pname and has a dependent whose name is Dname. An employee may work on several projects and may have several dependents and the employee’s projects and dependents are independent of one To keep the relation state consistent and to avoid any spurious relationship between the two independent attributes we must have a separate tuple to represent every combination of an employee’s dependent and an employee’s project. This constraint is an ER diagram each would be represented as a multivalued attribute or as a weak entity type an MVD may Formal Definition of Multivalued Dependency Definition. A multivalued dependency X →→ Y specified on relation schema R where X and Y are both subsets of R specifies the following constraint on any relation state r of R If two tuples t and t exist in r such that t t then two tuples t and t should also exist in r with the following where we use Z to denote Y is a subset of X or X ∪ Y R. For example the relation EMPPROJECTS in Figure has the trivial MVD Ename →→ Pname. An MVD that satisfies neither nor is called a nontrivial MVD. A trivial MVD will hold in any relation state r of R it is called trivial because it does not specify any significant or meaningful constraint on R. If we have a nontrivial MVD in a relation we may have to repeat values redundantly in the tuples. In the EMP relation of Figure the values ‘X’ and ‘Y’ of Pname are repeated with each value of Dname . This redundancy is clearly undesirable. However the EMP schema is in BCNF because no functional dependencies hold in EMP. Therefore we need to define a fourth normal form that is stronger than BCNF and disallows relation schemas such as EMP. Notice that relations containing nontrivial MVDs tend to be all key relations that is their key is all their attributes taken together. Furthermore it is rare that such all key relations with a combinatorial occurrence of repeated values would be designed in practice. However recognition of MVDs as a potential problematic dependency is essential in relational design. We now present the definition of fourth normal form which is violated when a relation has undesirable multivalued dependencies and hence can be used to identify and decompose such relations. MVD is denoted as A →→ B|C. tuples t t t and t are not necessarily distinct. is shorthand for the attributes in R after the attributes in are removed from R. Chapter Basics of Functional Dependencies and Normalization for Relational Databases Definition. A relation schema R is in with respect to a set of dependencies F if for every nontrivial multivalued dependency X →→ Y in X is a superkey for R. We can state the following points An all key relation is always in BCNF since it has no FDs. An all key relation such as the EMP relation in Figure which has no FDs but has the MVD Ename →→ Pname | Dname is not in A relation that is not in due to a nontrivial MVD must be decomposed to convert it into a set of relations in The decomposition removes the redundancy caused by the MVD. The process of normalizing a relation involving the nontrivial MVDs that is not in consists of decomposing it so that each MVD is represented by a separate relation where it becomes a trivial MVD. Consider the EMP relation in Figure EMP is not in because in the nontrivial MVDs Ename →→ Pname and Ename →→ Dname and Ename is not a superkey of EMP. We decompose EMP into EMPPROJECTS and EMPDEPENDENTS shown in Figure Both EMPPROJECTS and EMPDEPENDENTS are in because the MVDs Ename →→ Pname in EMPPROJECTS and Ename →→ Dname in EMPDEPENDENTS are trivial MVDs. No other nontrivial MVDs hold in either EMPPROJECTS or EMPDEPENDENTS. No FDs hold in these relation schemas either. Join Dependencies and Fifth Normal Form In our discussion so far we have pointed out the problematic functional dependencies and showed how they were eliminated by a process of repeated binary decomposition to remove them during the process of normalization to achieve and BCNF. These binary decompositions must obey the NJB property from Section that we referenced while discussing the decomposition to achieve BCNF. Achieving typically involves eliminating MVDs by repeated binary decompositions as well. However in some cases there may be no nonadditive join decomposition of R into two relation schemas but there may be a nonadditive join decomposition into more than two relation schemas. Moreover there may be no functional dependency in R that violates any normal form up to BCNF and there may be no nontrivial MVD present in R either that violates We then resort to another dependency called the join dependency and if it is present carry out a multiway decomposition into fifth normal form It is important to note that such a dependency is a very peculiar semantic constraint that is very difficult to detect in practice therefore normalization into is very rarely done in practice. refers to the cover of functional dependencies F or all dependencies that are implied by F. This is defined in Section Summary Definition. A join dependency denoted by Rn specified on relation schema R specifies a constraint on the states r of R. The constraint states that every legal state r of R should have a nonadditive join decomposition into Rn. Hence for every such r we have ∗ πRn r Notice that an MVD is a special case of a JD where n That is a JD denoted as implies an MVD ∩ →→ – specified on relation schema R is a trivial JD if one of the relation schemas Ri in Rn is equal to R. Such a dependency is called trivial because it has the nonadditive join property for any relation state r of R and thus does not specify any constraint on R. We can now define fifth normal form which is also called project join normal form. Definition. A relation schema R is in fifth normal form with respect to a set F of functional multivalued and join dependencies if for every nontrivial join dependency Rn in F+ Projname and Projname of SUPPLY. If this constraint holds the tuples below the dashed line in Figure must exist in any legal state of the SUPPLY relation that also contains the tuples above the dashed line. Figure shows how the SUPPLY relation with the join dependency is decomposed into three relations and that are each in Notice that applying a natural join to any two of these relations produces spurious tuples but applying a natural join to all three together does not. The reader should verify this on the sample relation in Figure and its projections in Figure This is because only the JD exists but no MVDs are specified. Notice too that the is specified on all legal relation states not just on the one shown in Figure Discovering JDs in practical databases with hundreds of attributes is next to impossible. It can be done only with a great degree of intuition about the data on the part of the designer. Therefore the current practice of database design pays scant attention to them. Summary In this chapter we discussed several pitfalls in relational database design using intuitive arguments. We identified informally some of the measures for indicating F+ refers to the cover of functional dependencies F or all dependencies that are implied by F. This is defined in Section Chapter Basics of Functional Dependencies and Normalization for Relational Databases whether a relation schema is good or bad and provided informal guidelines for a good design. These guidelines are based on doing a careful conceptual design in the ER and EER model following the mapping procedure in Chapter correctly to map entities and relationships into relations. Proper enforcement of these guidelines and lack of redundancy will avoid the insertion deletion update anomalies and generation of spurious data. We recommended limiting NULL values which cause problems during SELECT JOIN and aggregation operations. Then we presented some formal concepts that allow us to do relational design in a top down fashion by analyzing relations individually. We defined this process of design by analysis and decomposition by introducing the process of normalization. We defined the concept of functional dependency which is the basic tool for analyzing relational schemas and discussed some of its properties. Functional dependencies specify semantic constraints among the attributes of a relation schema. Next we described the normalization process for achieving good designs by testing relations for undesirable types of problematic functional dependencies. We provided a treatment of successive normalization based on a predefined primary key in each relation and then relaxed this requirement and provided more general definitions of second normal form and third normal form that take all candidate keys of a relation into account. We presented examples to illustrate how by using the general definition of a given relation may be analyzed and decomposed to eventually yield a set of relations in We presented Boyce Codd normal form and discussed how it is a stronger form of We also illustrated how the decomposition of a non BCNF relation must be done by considering the nonadditive decomposition requirement. Then we introduced the fourth normal form based on multivalued dependencies that typically arise due to mixing independent multivalued attributes into a single relation. Finally we introduced the fifth normal form which is based on join dependency and which identifies a peculiar constraint that causes a relation to be decomposed into several components so that they always yield the original relation back after a join. In practice most commercial designs have followed the normal forms up to BCNF. Need for decomposing into rarely arises in practice and join dependencies are difficult to detect for most practical situations making more of theoretical value. Chapter presents synthesis as well as decomposition algorithms for relational database design based on functional dependencies. Related to decomposition we discuss the concepts of nonadditive join and dependency preservation which are enforced by some of these algorithms. Other topics in Chapter include a more detailed treatment of functional and multivalued dependencies and other types of dependencies. Review Questions Discuss attribute semantics as an informal measure of goodness for a relation schema. Exercises Discuss insertion deletion and modification anomalies. Why are they considered bad Illustrate with examples. Why should NULLs in a relation be avoided as much as possible Discuss the problem of spurious tuples and how we may prevent it. State the informal guidelines for relation schema design that we discussed. Illustrate how violation of these guidelines may be harmful. What is a functional dependency What are the possible sources of the information that defines the functional dependencies that hold among the attributes of a relation schema Why can we not infer a functional dependency automatically from a particular relation state What does the term unnormalized relation refer to How did the normal forms develop historically from first normal form up to Boyce Codd normal form Define first second and third normal forms when only primary keys are considered. How do the general definitions of and which consider all keys of a relation differ from those that consider only primary keys What undesirable dependencies are avoided when a relation is in What undesirable dependencies are avoided when a relation is in In what way do the generalized definitions of and extend the definitions beyond primary keys Define Boyce Codd normal form. How does it differ from Why is it considered a stronger form of What is multivalued dependency When does it arise Does a relation with two or more columns always have an MVD Show with an example. Define fourth normal form. When is it violated When is it typically applicable Define join dependency and fifth normal form. Why is also called project join normal form Why do practical database designs typically aim for BCNF and not aim for higher normal forms Exercises Suppose that we have the following requirements for a university database that is used to keep track of students’ transcripts a. The university keeps track of each student’s name student number Social Security number current address and Chapter Basics of Functional Dependencies and Normalization for Relational Databases phone permanent address and phone birth date sex class major department minor department and degree program . Both Ssn and student number have unique values for each student. b. Each department is described by a name department code office number office phone and college . Both name and code have unique values for each department. c. Each course has a course name description course number number of semester hours level and offering department . The course number is unique for each course. d. Each section has an instructor semester year course and section number . The section number distinguishes different sections of the same course that are taught during the same semester year its values are up to the total number of sections taught during each semester. e. A grade record refers to a student a particular section and a grade . Design a relational database schema for this database application. First show all the functional dependencies that should hold among the attributes. Then design relation schemas for the database that are each in or BCNF. Specify the key attributes of each relation. Note any unspecified requirements and make appropriate assumptions to render the specification complete. What update anomalies occur in the EMPPROJ and EMPDEPT relations of Figures and In what normal form is the LOTS relation schema in Figure with respect to the restrictive interpretations of normal form that take only the primary key into account Would it be in the same normal form if the general definitions of normal form were used Prove that any relation schema with two attributes is in BCNF. Why do spurious tuples occur in the result of joining the and EMP LOCS relations in Figure which of the following dependencies may hold in the above relation If the dependency cannot hold explain why by specifying the tuples that cause the violation. i. A → B ii. B → C iii. C → B iv. B → A v. C → A b. Does the above relation have a potential candidate key If it does what is it If it does not why not Consider a relation R with the following dependencies AB → C CD → E DE → B Is AB a candidate key of this relation If not is ABD Explain your answer. Consider the relation R which has attributes that hold schedules of courses and sections at a university R Courseno Secno Offeringdept Credithours Courselevel Instructorssn Semester Year Dayshours Roomno Noofstudents . Suppose that the following functional dependencies hold on R Courseno → Offeringdept Credithours Courselevel Courseno Secno Semester Year → Dayshours Roomno Noofstudents Instructorssn Roomno Dayshours Semester Year → Instructorssn Courseno Secno Try to determine which sets of attributes form keys of R. How would you normalize this relation Consider the following relations for an order processing application database at ABC Inc. ORDER ORDERITEM Assume that each item has a different discount. The Totalprice refers to one item Odate is the date on which the order was placed and the Totalamount is the amount of the order. If we apply a natural join on the relations ORDERITEM and ORDER in this database what does the resulting relation schema look like What will be its key Show the FDs in this resulting relation. Is it in Is it in Why or why not Chapter Basics of Functional Dependencies and Normalization for Relational Databases Consider the following relation CARSALE Assume that a car may be sold by multiple salespeople and hence Car# Salesperson# is the primary key. Additional dependencies are Datesold → Discountamt and Salesperson# → Commission% Based on the given primary key is this relation in or Why or why not How would you successively normalize it completely Consider the following relation for published books BOOK Authoraffil refers to the affiliation of author. Suppose the following dependencies exist Booktitle → Publisher Booktype Booktype → Listprice Authorname → Authoraffil a. What normal form is the relation in Explain your answer. b. Apply normalization until you cannot decompose the relations further. State the reasons behind each decomposition. This exercise asks you to convert business statements into dependencies. Consider the relation DISKDRIVE . Each tuple in the relation DISKDRIVE contains information about a disk drive with a unique Serialnumber made by a manufacturer with a particular model number released in a certain batch which has a certain storage capacity and is sold by a certain retailer. For example the tuple Diskdrive ‘WesternDigital’ ‘CompUSA’ specifies that WesternDigital made a disk drive with serial number and model number released in batch it is and sold by CompUSA. Write each of the following dependencies as an FD a. The manufacturer and serial number uniquely identifies the drive. b. A model number is registered by a manufacturer and therefore can’t be used by another manufacturer. c. All disk drives in a particular batch are the same model. d. All disk drives of a certain model of a particular manufacturer have exactly the same capacity. Consider the following relation R Exercises In the above relation a tuple describes a visit of a patient to a doctor along with a treatment code and daily charge. Assume that diagnosis is determined for each patient by a doctor. Assume that each treatment code has a fixed charge . Is this relation in Justify your answer and decompose if necessary. Then argue whether further normalization to is necessary and if so perform it. Consider the following relation CARSALE This relation refers to options installed in cars that were sold at a dealership and the list and discounted prices of the options. If CarID → Saledate and Optiontype → Optionlistprice and CarID Optiontype → Optiondiscountedprice argue using the generalized definition of the that this relation is not in Then argue from your knowledge of why it is not even in Consider the relation BOOK with the data a. Based on a common sense understanding of the above data what are the possible candidate keys of this relation b. Justify that this relation has the MVD Book →→ Author | Edition Year . c. What would be the decomposition of this relation based on the above MVD Evaluate each resulting relation for the highest normal form it possesses. Consider the following relation TRIP This relation refers to business trips made by company salespeople. Suppose the TRIP has a single Startdate but involves many Cities and salespeople may use multiple credit cards on the trip. Make up a mock up population of the table. a. Discuss what FDs and or MVDs exist in this relation. b. Show how you will go about normalizing it. BookName Author Edition CopyrightYear DBfundamentals Navathe DBfundamentals Elmasri DBfundamentals Elmasri DBfundamentals Navathe Chapter Basics of Functional Dependencies and Normalization for Relational Databases Laboratory Exercise Note The following exercise use the DBD system that is described in the laboratory manual. The relational schema R and set of functional dependencies F need to be coded as lists. As an example R and F for this problem is coded as R [a b c d e f g h i j] F [[[a b] [c]] [[a] [d e]] [[b] [f]] [[f] [g h]] [[d] [i j]]] Since DBD is implemented in Prolog use of uppercase terms is reserved for variables in the language and therefore lowercase constants are used to code the attributes. For further details on using the DBD system please refer to the laboratory manual. Using the DBD system verify your answers to the following exercises a. only b. c. d. Selected Bibliography Functional dependencies were originally introduced by Codd The original definitions of first second and third normal form were also defined in Codd where a discussion on update anomalies can be found. Boyce Codd normal form was defined in Codd The alternative definition of third normal form is given in Ullman as is the definition of BCNF that we give here. Ullman Maier and Atzeni and De Antonellis contain many of the theorems and proofs concerning functional dependencies. Additional references to relational design theory are given in Chapter Relational Database Design Algorithms and Further Dependencies Chapter presented a top down relational design technique and related concepts used extensively in commercial database design projects today. The procedure involves designing an ER or EER conceptual schema then mapping it to the relational model by a procedure such as the one described in Chapter Primary keys are assigned to each relation based on known functional dependencies. In the subsequent process which may be called relational design by analysis initially designed relations from the above procedure or those inherited from previous files forms and other sources are analyzed to detect undesirable functional dependencies. These dependencies are removed by the successive normalization procedure that we described in Section along with definitions of related normal forms which are successively better states of design of individual relations. In Section we assumed that primary keys were assigned to individual relations in Section a more general treatment of normalization was presented where all candidate keys are considered for each relation and Section discussed a further normal form called BCNF. Then in Sections and we discussed two more types of dependencies multivalued dependencies and join dependencies that can also cause redundancies and showed how they can be eliminated with further normalization. In this chapter we use the theory of normal forms and functional multivalued and join dependencies developed in the last chapter and build upon it while maintaining three different thrusts. First we discuss the concept of inferring new functional dependencies from a given set and discuss notions including cover minimal cover and equivalence. Conceptually we need to capture the semantics of attibutes within chapter Chapter Relational Database Design Algorithms and Further Dependencies a relation completely and succinctly and the minimal cover allows us to do it. Second we discuss the desirable properties of nonadditive joins and preservation of functional dependencies. A general algorithm to test for nonadditivity of joins among a set of relations is presented. Third we present an approach to relational design by synthesis of functional dependencies. This is a bottom up approach to design that presupposes that the known functional dependencies among sets of attributes in the Universe of Discourse have been given as input. We present algorithms to achieve the desirable normal forms namely and BCNF and achieve one or both of the desirable properties of nonadditivity of joins and functional dependency preservation. Although the synthesis approach is theoretically appealing as a formal approach it has not been used in practice for large database design projects because of the difficulty of providing all possible functional dependencies up front before the design can be attempted. Alternately with the approach presented in Chapter successive decompositions and ongoing refinements to design become more manageable and may evolve over time. The final goal of this chapter is to discuss further the multivalued dependency concept we introduced in Chapter and briefly point out other types of dependencies that have been identified. In Section we discuss the rules of inference for functional dependencies and use them to define the concepts of a cover equivalence and minimal cover among functional dependencies. In Section first we describe the two desirable properties of decompositions namely the dependency preservation property and the nonadditive join property which are both used by the design algorithms to achieve desirable decompositions. It is important to note that it is insufficient to test the relation schemas independently of one another for compliance with higher normal forms like and BCNF. The resulting relations must collectively satisfy these two additional properties to qualify as a good design. Section is devoted to the development of relational design algorithms that start off with one giant relation schema called the universal relation which is a hypothetical relation containing all the attributes. This relation is decomposed into relations that satisfy a certain normal form like or BCNF and also meet one or both of the desirable properties. In Section we discuss the multivalued dependency concept further by applying the notions of inference and equivalence to MVDs. Finally in Section we complete the discussion on dependencies among data by introducing inclusion dependencies and template dependencies. Inclusion dependencies can represent referential integrity constraints and class subclass constraints across relations. Template dependencies are a way of representing any generalized constraint on attributes. We also describe some situations where a procedure or function is needed to state and verify a functional dependency among attributes. Then we briefly discuss domain key normal form which is considered the most general normal form. Section summarizes this chapter. It is possible to skip some or all of Sections and in an introductory database course. Further Topics in Functional Dependencies Inference Rules Equivalence and Minimal Cover Further Topics in Functional Dependencies Inference Rules Equivalence and Minimal Cover We introduced the concept of functional dependencies in Section illustrated it with some examples and developed a notation to denote multiple FDs over a single relation. We identified and discussed problematic functional dependencies in Sections and and showed how they can be eliminated by a proper decomposition of a relation. This process was described as normalization and we showed how to achieve the first through third normal forms through given primary keys in Section In Sections and we provided generalized tests for and BCNF given any number of candidate keys in a relation and showed how to achieve them. Now we return to the study of functional dependencies and show how new dependencies can be inferred from a given set and discuss the concepts of closure equivalence and minimal cover that we will need when we later consider a synthesis approach to design of relations given a set of FDs. Inference Rules for Functional Dependencies We denote by F the set of functional dependencies that are specified on relation schema R. Typically the schema designer specifies the functional dependencies that are semantically obvious usually however numerous other functional dependencies hold in all legal relation instances among sets of attributes that can be derived from and satisfy the dependencies in F. Those other dependencies can be inferred or deduced from the FDs in F. In real life it is impossible to specify all possible functional dependencies for a given situation. For example if each department has one manager so that Deptno uniquely determines Mgrssn and a manager has a unique phone number called Mgrphone then these two dependencies together imply that Deptno → Mgrphone. This is an inferred FD and need not be explicitly stated in addition to the two given FDs. Therefore it is useful to define a concept called closure formally that includes all possible dependencies that can be inferred from the given set F. Definition. Formally the set of all dependencies that include F as well as all dependencies that can be inferred from F is called the closure of F it is denoted by F+. For example suppose that we specify the following set F of obvious functional dependencies on the relation schema in Figure F Ssn → Ename Bdate Address Dnumber Dnumber → Dname Dmgrssn Some of the additional functional dependencies that we can infer from F are the following Ssn → Dname Dmgrssn Ssn → Ssn Dnumber → Dname Chapter Relational Database Design Algorithms and Further Dependencies An FD X → Y is inferred from a set of dependencies F specified on R if X → Y holds in every legal relation state r of R that is whenever r satisfies all the dependencies in F X → Y also holds in r. The closure F+ of F is the set of all functional dependencies that can be inferred from F. To determine a systematic way to infer dependencies we must discover a set of inference rules that can be used to infer new dependencies from a given set of dependencies. We consider some of these inference rules next. We use the notation F | X → Y to denote that the functional dependency X → Y is inferred from the set of functional dependencies F. In the following discussion we use an abbreviated notation when discussing functional dependencies. We concatenate attribute variables and drop the commas for convenience. Hence the FD X Y → Z is abbreviated to XY → Z and the FD X Y Z → U V is abbreviated to XYZ → UV. The following six rules through are well known inference rules for functional dependencies X → Y Y → Z | X → Z. X → YZ | X → Y. X → Y X → Z | X → YZ. X → Y WY → Z | WX → Z. The reflexive rule states that a set of attributes always determines itself or any of its subsets which is obvious. Because generates dependencies that are always true such dependencies are called trivial. Formally a functional dependency X → Y is trivial if X ⊇ Y otherwise it is nontrivial. The augmentation rule says that adding the same set of attributes to both the left and right hand sides of a dependency results in another valid dependency. According to functional dependencies are transitive. The decomposition rule says that we can remove attributes from the right hand side of a dependency applying this rule repeatedly can decompose the FD X → An into the set of dependencies X → X → X → An . The union rule allows us to do the opposite we can combine a set of dependencies X → X → X → An into the single FD X → An . The pseudotransitive rule allows us to replace a set of attributes Y on the left hand side of a dependency with another set X that functionally determines Y and can be derived from and if we augment the first functional dependency X → Y with W and then apply the transitive rule. One cautionary note regarding the use of these rules. Although X → A and X → B implies X → AB by the union rule stated above X → A and Y → B does imply that XY → AB. Also XY → A does not necessarily imply either X → A or Y → A. reflexive rule can also be stated as X → X that is any set of attributes functionally determines itself. augmentation rule can also be stated as X → Y | XZ → Y that is augmenting the left hand side attributes of an FD produces another valid FD. Further Topics in Functional Dependencies Inference Rules Equivalence and Minimal Cover Each of the preceding inference rules can be proved from the definition of functional dependency either by direct proof or by contradiction. A proof by contradiction assumes that the rule does not hold and shows that this is not possible. We now prove that the first three rules through are valid. The second proof is by contradiction. Proof of Suppose that X ⊇ Y and that two tuples t and t exist in some relation instance r of R such that t [X] t [X]. Then t t because X ⊇ Y hence X → Y must hold in r. Proof of . Assume that X → Y holds in a relation instance r of R but that XZ → YZ does not hold. Then there must exist two tuples t and t in r such that t [X] t [X] t [Y] t [Y] t [XZ] t [XZ] and t [YZ] ≠ t [YZ]. This is not possible because from and we deduce t [Z] t [Z] and from and we deduce t [YZ] t [YZ] contradicting Proof of Assume that X → Y and Y → Z both hold in a relation r. Then for any two tuples t and t in r such that t [X] t [X] we must have t [Y] t [Y] from assumption hence we must also have t [Z] t [Z] from and assumption thus X → Z must hold in r. Using similar proof arguments we can prove the inference rules to and any additional valid inference rules. However a simpler way to prove that an inference rule for functional dependencies is valid is to prove it by using inference rules that have already been shown to be valid. For example we can prove through by using through as follows. Proof of . YZ → Y . X → Y . X → Z . X → XY . XY → YZ . X → YZ . WY → Z . WX → WY . WX → Z Algorithm starts by setting X+ to all the attributes in X. By we know that all these attributes are functionally dependent on X. Using inference rules and we add attributes to X+ using each functional dependency in F. We keep going through all the dependencies in F until no more attributes are added to X+ during a complete cycle through the dependencies in F. For example consider the relation schema EMPPROJ in Figure from the semantics of the attributes we specify the following set F of functional dependencies that should hold on EMPPROJ F Ssn → Ename Pnumber → Pname Plocation Ssn Pnumber → Hours are actually known as Armstrong’s axioms. In the strict mathematical sense the axioms are the functional dependencies in F since we assume that they are correct whereas through are the inference rules for inferring new functional dependencies . Further Topics in Functional Dependencies Inference Rules Equivalence and Minimal Cover Using Algorithm we calculate the following closure sets with respect to F Ssn + Ssn Ename Pnumber + Pnumber Pname Plocation Ssn Pnumber + Ssn Pnumber Ename Pname Plocation Hours Intuitively the set of attributes in the right hand side in each line above represents all those attributes that are functionally dependent on the set of attributes in the left hand side based on the given set F. Equivalence of Sets of Functional Dependencies In this section we discuss the equivalence of two sets of functional dependencies. First we give some preliminary definitions. Definition. A set of functional dependencies F is said to cover another set of functional dependencies E if every FD in E is also in F+ that is if every dependency in E can be inferred from F alternatively we can say that E is covered by F. Definition. Two sets of functional dependencies E and F are equivalent if E+ F+. Therefore equivalence means that every FD in E can be inferred from F and every FD in F can be inferred from E that is E is equivalent to F if both the conditions E covers F and F covers E hold. We can determine whether F covers E by calculating X+ with respect to F for each FD X → Y in E and then checking whether this X+ includes the attributes in Y. If this is the case for every FD in E then F covers E. We determine whether E and F are equivalent by checking that E covers F and F covers E. It is left to the reader as an exercise to show that the following two sets of FDs are equivalent F A → C AC → D E → AD E → H and G A → CD E → AH . Minimal Sets of Functional Dependencies Informally a minimal cover of a set of functional dependencies E is a set of functional dependencies F that satisfies the property that every dependency in E is in the closure F+ of F. In addition this property is lost if any dependency from the set F is removed F must have no redundancies in it and the dependencies in F are in a standard form. To satisfy these properties we can formally define a set of functional dependencies F to be minimal if it satisfies the following conditions Every dependency in F has a single attribute for its right hand side. We cannot replace any dependency X → A in F with a dependency Y → A where Y is a proper subset of X and still have a set of dependencies that is equivalent to F. We cannot remove any dependency from F and still have a set of dependencies that is equivalent to F. We can think of a minimal set of dependencies as being a set of dependencies in a standard or canonical form and with no redundancies. Condition just represents Chapter Relational Database Design Algorithms and Further Dependencies every dependency in a canonical form with a single attribute on the right hand Conditions and ensure that there are no redundancies in the dependencies either by having redundant attributes on the left hand side of a dependency that is equivalent to E. We can always find at least one minimal cover F for any set of dependencies E using Algorithm If several sets of FDs qualify as minimal covers of E by the definition above it is customary to use additional criteria for minimality. For example we can choose the minimal set with the smallest number of dependencies or with the smallest total length . Algorithm Finding a Minimal Cover F for a Set of Functional Dependencies E Input A set of functional dependencies E. Set F E. Replace each functional dependency X → An in F by the n functional dependencies X X X → An. For each functional dependency X → A in F for each attribute B that is an element of X if F – X → A ∪ → A is equivalent to F then replace X → A with → A in F. For each remaining functional dependency X → A in F if F – X → A is equivalent to F then remove X → A from F. We illustrate the above algorithm with the following Let the given set of FDs be E B → A D → A AB → D . We have to find the minimal cover of E. All above dependencies are in canonical form so we have completed step of Algorithm and can proceed to step In step we need to determine if AB → D has any redundant attribute on the left hand side that is can it be replaced by B → D or A → D is a standard form to simplify the conditions and algorithms that ensure no redundancy exists in F. By using the inference rule we can convert a single dependency with multiple attributes on the right hand side into a set of dependencies with single attributes on the right hand side. Properties of Relational Decompositions Since B → A by augmenting with B on both sides we have BB → AB or B → AB . However AB → D as given . Hence by the transitive rule we get from and B → D. Thus AB → D may be replaced by B → D. We now have a set equivalent to original E say E B → A D → A B → D . No further reduction is possible in step since all FDs have a single attribute on the left hand side. In step we look for a redundant FD in E. By using the transitive rule on B → D and D → A we derive B → A. Hence B → A is redundant in E and can be eliminated. Therefore the minimal cover of E is B → D D → A . In Section we will see how relations can be synthesized from a given set of dependencies E by first finding the minimal cover F for E. Next we provide a simple algorithm to determine the key of a relation Algorithm Finding a Key K for R Given a set F of Functional Dependencies Input A relation R and a set of functional dependencies F on the attributes of R. Set K R. For each attribute A in K compute + with respect to F if + contains all the attributes in R then set K K – A In Algoritm we start by setting K to all the attributes of R we then remove one attribute at a time and check whether the remaining attributes still form a superkey. Notice too that Algorithm determines only one key out of the possible candidate keys for R the key returned depends on the order in which attributes are removed from R in step Properties of Relational Decompositions We now turn our attention to the process of decomposition that we used throughout Chapter to decompose relations in order to get rid of unwanted dependencies and achieve higher normal forms. In Section we give examples to show that looking at an individual relation to test whether it is in a higher normal form does not on its own guarantee a good design rather a set of relations that together form the relational database schema must possess certain additional properties to ensure a good design. In Sections and we discuss two of these properties the dependency preservation property and the nonadditive join property. Section discusses binary decompositions and Section discusses successive nonadditive join decompositions. Chapter Relational Database Design Algorithms and Further Dependencies Relation Decomposition and Insufficiency of Normal Forms The relational database design algorithms that we present in Section start from a single universal relation schema R An that includes all the attributes of the database. We implicitly make the universal relation assumption which states that every attribute name is unique. The set F of functional dependencies that should hold on the attributes of R is specified by the database designers and is made available to the design algorithms. Using the functional dependencies the algorithms decompose the universal relation schema R into a set of relation schemas D Rm that will become the relational database schema D is called a decomposition of R. We must make sure that each attribute in R will appear in at least one relation schema Ri in the decomposition so that no attributes are lost formally we have This is called the attribute preservation condition of a decomposition. Another goal is to have each individual relation Ri in the decomposition D be in BCNF or However this condition is not sufficient to guarantee a good database design on its own. We must consider the decomposition of the universal relation as a whole in addition to looking at the individual relations. To illustrate this point consider the EMPLOCS relation in Figure which is in and also in BCNF. In fact any relation schema with only two attributes is automatically in Although EMPLOCS is in BCNF it still gives rise to spurious tuples when joined with EMPPROJ which is not in BCNF in Figure is in BCNF using Plocation as a joining attribute also gives rise to spurious tuples. This underscores the need for other criteria that together with the conditions of or BCNF prevent such bad designs. In the next three subsections we discuss such additional conditions that should hold on a decomposition D as a whole. Dependency Preservation Property of a Decomposition It would be useful if each functional dependency X→Y specified in F either appeared directly in one of the relation schemas Ri in the decomposition D or could be inferred from the dependencies that appear in some Ri . Informally this is the dependency preservation condition. We want to preserve the dependencies because R R i i m ∪ an exercise the reader should prove that this statement is true. Properties of Relational Decompositions each dependency in F represents a constraint on the database. If one of the dependencies is not represented in some individual relation Ri of the decomposition we cannot enforce this constraint by dealing with an individual relation. We may have to join multiple relations so as to include all attributes involved in that dependency. It is not necessary that the exact dependencies specified in F appear themselves in individual relations of the decomposition D. It is sufficient that the union of the dependencies that hold on the individual relations in D be equivalent to F. We now define these concepts more formally. Definition. Given a set of dependencies F on R the projection of F on Ri denoted by πRi where Ri is a subset of R is the set of dependencies X → Y in F+ such that the attributes in X ∪ Y are all contained in Ri . Hence the projection of F on each relation schema Ri in the decomposition D is the set of functional dependencies in F+ the closure of F such that all their left and right hand side attributes are in Ri . We say that a decomposition D Rm of R is dependency preserving with respect to F if the union of the projections of F on each Ri in D is equivalent to F that is ∪ ∪ + F+ . If a decomposition is not dependency preserving some dependency is lost in the decomposition. To check that a lost dependency holds we must take the JOIN of two or more relations in the decomposition to get a relation that includes all leftand right hand side attributes of the lost dependency and then check that the dependency holds on the result of the JOIN an option that is not practical. An example of a decomposition that does not preserve dependencies is shown in Figure in which the functional dependency is lost when is decomposed into The decompositions in Figure however are dependency preserving. Similarly for the example in Figure no matter what decomposition is chosen for the relation TEACH from the three provided in the text one or both of the dependencies originally present are bound to be lost. We state a claim below related to this property without providing any proof. Claim It is always possible to find a dependency preserving decomposition D with respect to F such that each relation Ri in D is in In Section we describe Algorithm which creates a dependencypreserving decomposition D Rm of a universal relation R based on a set of functional dependencies F such that each Ri in D is in Nonadditive Join Property of a Decomposition Another property that a decomposition D should possess is the nonadditive join property which ensures that no spurious tuples are generated when a NATURAL JOIN operation is applied to the relations resulting from the decomposition. We already illustrated this problem in Section with the example in Figures Chapter Relational Database Design Algorithms and Further Dependencies and Because this is a property of a decomposition of relation schemas the condition of no spurious tuples should hold on every legal relation state that is every relation state that satisfies the functional dependencies in F. Hence the lossless join property is always defined with respect to a specific set F of dependencies. Definition. Formally a decomposition D Rm of R has the lossless join property with respect to the set of dependencies F on R if for every relation state r of R that satisfies F the following holds where is the NATURAL JOIN of all the relations in D πRm r. The word loss in lossless refers to loss of information not to loss of tuples. If a decomposition does not have the lossless join property we may get additional spurious tuples after the PROJECT and NATURAL JOIN operations are applied these additional tuples represent erroneous or invalid information. We prefer the term nonadditive join because it describes the situation more accurately. Although the term lossless join has been popular in the literature we will henceforth use the term nonadditive join which is self explanatory and unambiguous. The nonadditive join property ensures that no spurious tuples result after the application of PROJECT and JOIN operations. We may however sometimes use the term lossy design to refer to a design that represents a loss of information in Figure into EMPLOCS and Pnumber Hours Pname Plocation in Figure obviously does not have the nonadditive join property as illustrated by Figure We will use a general procedure for testing whether any decomposition D of a relation into n relations is nonadditive with respect to a set of given functional dependencies F in the relation it is presented as Algorithm below. It is possible to apply a simpler test to check if the decomposition is nonadditive for binary decompositions that test is described in Section Algorithm Testing for Nonadditive Join Property Input A universal relation R a decomposition D Rm of R and a set F of functional dependencies. Note Explanatory comments are given at the end of some of the steps. They follow the format . Create an initial matrix S with one row i for each relation Ri in D and one column j for each attribute Aj in R. Set S bij for all matrix entries. . For each row i representing relation schema Ri for each column j representing attribute Aj if then set S aj . Properties of Relational Decompositions Repeat the following loop until a complete loop execution results in no changes to S for each functional dependency X → Y in F for all rows in S that have the same symbols in the columns corresponding to attributes in X make the symbols in each column that correspond to an attribute in Y be the same in all these rows as follows If any of the rows has an a symbol for the column set the other rows to that same a symbol in the column. If no a symbol exists for the attribute in any of the rows choose one of the b symbols that appears in one of the rows for the attribute and set the other rows to that same b symbol in the column If a row is made up entirely of a symbols then the decomposition has the nonadditive join property otherwise it does not. Given a relation R that is decomposed into a number of relations Rm Algorithm begins the matrix S that we consider to be some relation state r of R. Row i in S represents a tuple t i that has a symbols in the columns that correspond to the attributes of Ri and b symbols in the remaining columns. The algorithm then transforms the rows of this matrix that does have the nonadditive join property and Figure shows how we apply the algorithm to that decomposition. Once a row consists only of a symbols we conclude that the decomposition has the nonadditive join property and we can stop applying the functional dependencies to the matrix S. Chapter Relational Database Design Algorithms and Further Dependencies Pnumber PROJECT Pname Plocation Ssn R R R R R D R R Ename Pnumber Pname Hours Plocation Ssn EMP R Ssn Ename Pnumber Pname Plocation Hours R EMPLOCS Ename Plocation R Ssn Pnumber Hours Pname Plocation Ename Ssn WORKSON Pnumber Hours Ssn R R R Ename Pnumber Pname Hours Plocation Ssn Ename Pnumber Pname Hours Plocation F Ssn Ename Pnumber Pname Plocation Ssn Pnumber Hours R Ssn Ename Pnumber Pname Plocation Hours D R R R R EMP Ssn Ename R PROJ Pnumber Pname Plocation R WORKSON Ssn Pnumber Hours F Ssn Ename Pnumber Pname Plocation Ssn Pnumber Hours Figure Nonadditive join test for n ary decompositions. Case Decomposition of EMPPROJ into and EMPLOCS fails test. A decomposition of EMPPROJ that has the lossless join property. Case Decomposition of EMPPROJ into EMP PROJECT and WORKSON satisfies test. Algorithms for Relational Database Schema Design Testing Binary Decompositions for the Nonadditive Join Property Algorithm allows us to test whether a particular decomposition D into n relations obeys the nonadditive join property with respect to a set of functional dependencies F. There is a special case of a decomposition called a binary decomposition decomposition of a relation R into two relations. We give an easier test to apply than Algorithm but while it is very handy to use it is limited to binary decompositions only. Property NJB . A decomposition D of R has the lossless join property with respect to a set of functional dependencies F on R if and only if either The FD ∩ → – is in F+ or The FD ∩ → – is in F+ You should verify that this property holds with respect to our informal successive normalization examples in Sections and In Section we decomposed into two BCNF relations and and decomposed the TEACH relation in Figure into the two relations Instructor Course and Instructor Student . These are valid decompositions because they are nonadditive per the above test. Successive Nonadditive Join Decompositions We saw the successive decomposition of relations during the process of second and third normalization in Sections and To verify that these decompositions are nonadditive we need to ensure another property as set forth in Claim Claim . If a decomposition D Rm of R has the nonadditive join property with respect to a set of functional dependencies F on R and if a decomposition Di Qk of Ri has the nonadditive join property with respect to the projection of F on Ri then the decomposition Qk Rm of R has the nonadditive join property with respect to F. Algorithms for Relational Database Schema Design We now give three algorithms for creating a relational decomposition from a universal relation. Each algorithm has specific properties as we discuss next. Chapter Relational Database Design Algorithms and Further Dependencies Dependency Preserving Decomposition into Schemas Algorithm creates a dependency preserving decomposition D Rm of a universal relation R based on a set of functional dependencies F such that each Ri in D is in It guarantees only the dependency preserving property it does not guarantee the nonadditive join property. The first step of Algorithm is to find a minimal cover G for F Algorithm can be used for this step. Note that multiple minimal covers may exist for a given set F Place any remaining attributes in a single relation schema to ensure the attribute preservation property. Example of Algorithm Consider the following universal relation U Empssn Esal Ephone refer to the Social Security number salary and phone number of the employee. Pno Pname and Plocation refer to the number name and location of the project. Dno is department number. The following dependencies are present Empssn → Esal Ephone Dno Pno → Pname Plocation Empssn Pno → Esal Ephone Dno Pname Plocation By virtue of the attribute set Empssn Pno represents a key of the universal relation. Hence F the set of given FDs includes Empssn → Esal Ephone Dno Pno → Pname Plocation Empssn Pno → Esal Ephone Dno Pname Plocation . By applying the minimal cover Algorithm in step we see that Pno is a redundant attribute in Empssn Pno → Esal Ephone Dno. Moreover Empssn is redundant in Empssn Pno → Pname Plocation. Hence the minimal cover consists of and only being completely redundant as follows Minimal cover G Empssn → Esal Ephone Dno Pno → Pname Plocation Algorithms for Relational Database Schema Design Maier or Ullman for a proof. By applying Algorithm to the above Minimal cover G we get a design consisting of two relations with keys Empssn and Pno as follows An observant reader would notice easily that these two relations have lost the original information contained in the key of the universal relation U . Thus while the algorithm does preserve the original dependencies it makes no guarantee of preserving all of the information. Hence the resulting design is a lossy design. Claim Every relation schema created by Algorithm is in It is obvious that all the dependencies in G are preserved by the algorithm because each dependency appears in one of the relations Ri in the decomposition D. Since G is equivalent to F all the dependencies in F are either preserved directly in the decomposition or are derivable using the inference rules from Section from those in the resulting relations thus ensuring the dependency preservation property. Algorithm is called a relational synthesis algorithm because each relation schema Ri in the decomposition is synthesized from the set of functional dependencies in G with the same left hand side X. Nonadditive Join Decomposition into BCNF Schemas The next algorithm decomposes a universal relation schema R An into a decomposition D Rm such that each Ri is in BCNF and the decomposition D has the lossless join property with respect to F. Algorithm utilizes Property NJB and Claim to create a nonadditive join decomposition D Rm of a universal relation R based on a set of functional dependencies F such that each Ri in D is in BCNF. Algorithm Relational Decomposition into BCNF with Nonadditive Join Property Input A universal relation R and a set of functional dependencies F on the attributes of R. Set D R While there is a relation schema Q in D that is not in BCNF do choose a relation schema Q in D that is not in BCNF find a functional dependency X → Y in Q that violates BCNF replace Q in D by two relation schemas and Chapter Relational Database Design Algorithms and Further Dependencies Each time through the loop in Algorithm we decompose one relation schema Q that is not in BCNF into two relation schemas. According to Property NJB for binary decompositions and Claim the decomposition D has the nonadditive join property. At the end of the algorithm all relation schemas in D will be in BCNF. The reader can check that the normalization example in Figures and basically follows this algorithm. The functional dependencies and later violate BCNF so the LOTS relation is decomposed appropriately into BCNF relations and the decomposition then satisfies the nonadditive join property. Similarly if we apply the algorithm to the TEACH relation schema from Figure it is decomposed into Student and Course because the dependency Instructor → Course violates BCNF. In step of Algorithm it is necessary to determine whether a relation schema Q is in BCNF or not. One method for doing this is to test for each functional dependency X → Y in Q whether X+ fails to include all the attributes in Q thereby determining whether or not X is a key in Q. Another technique is based on an observation that whenever a relation schema Q has a BCNF violation there exists a pair of attributes A and B in Q such that Q – A B → A by computing the closure Q – A B + for each pair of attributes A B of Q and checking whether the closure includes A we can determine whether Q is in BCNF. Dependency Preserving and Nonadditive Join Decomposition into Schemas So far in Algorithm we showed how to achieve a design with the potential for loss of information and in Algorithm we showed how to achieve BCNF design with the potential loss of certain functional dependencies. By now we know that it is not possible to have all three of the following guaranteed nonlossy design guaranteed dependency preservation and all relations in BCNF. As we have said before the first condition is a must and cannot be compromised. The second condition is desirable but not a must and may have to be relaxed if we insist on achieving BCNF. Now we give an alternative algorithm where we achieve conditions and and only guarantee A simple modification to Algorithm shown as Algorithm yields a decomposition D of R that does the following Preserves dependencies Has the nonadditive join property Is such that each resulting relation schema in the decomposition is in Because the Algorithm achieves both the desirable properties rather than only functional dependency preservation as guaranteed by Algorithm it is preferred over Algorithm Algorithm Relational Synthesis into with Dependency Preservation and Nonadditive Join Property Input A universal relation R and a set of functional dependencies F on the attributes of R. Algorithms for Relational Database Schema Design of Algorithm is not needed in Algorithm to preserve attributes because the key will include any unplaced attributes these are the attributes that do not participate in any functional dependency. that there is an additional type of dependency R is a projection of the join of two or more relations in the schema. This type of redundancy is considered join dependency as we discussed in Section Hence technically it may continue to exist without disturbing the status for the schema. Find a minimal cover G for F . If none of the relation schemas in D contains a key of R then create one more relation schema in D that contains attributes that form a key of R. Eliminate redundant relations from the resulting set of relations in the relational database schema. A relation R is considered redundant if R is a projection of another relation S in the schema alternately R is subsumed by S. Step of Algorithm involves identifying a key K of R. Algorithm can be used to identify a key K of R based on the set of given functional dependencies F. Notice that the set of functional dependencies used to determine a key in Algorithm could be either F or G since they are equivalent. Example of Algorithm Let us revisit the example given earlier at the end of Algorithm The minimal cover G holds as before. The second step produces relations and as before. However now in step we will generate a relation corresponding to the key Empssn Pno . Hence the resulting design contains This design achieves both the desirable properties of dependency preservation and nonadditive join. Example of Algorithm . Consider the relation schema shown in Figure Assume that this relation is given as a universal relation with the following functional dependencies Propertyid → Lot# County Area Lot# County → Area Propertyid Area → County These were called and in Figure The meanings of the above attributes and the implication of the above functional dependencies were explained Chapter Relational Database Design Algorithms and Further Dependencies in Section For ease of reference let us abbreviate the above attributes with the first letter for each and represent the functional dependencies as the set F P → LCA LC → AP A → C . If we apply the minimal cover Algorithm to F using the above minimal cover as Design X and . In step of the algorithm we find that is subsumed by Design X . or in other words it is identical to the relation that we had determined to be in in Section Example of Algorithm . Starting with as the universal relation and with the same given set of functional dependencies the second step of the minimal cover Algorithm produces as before F P → C P → A P → L LC → A LC → P A → C . The FD LC → A may be considered redundant because LC → P and P → A implies LC → A by transitivity. Also P → C may be considered to be redundant because P → A and A → C implies P → C by transitivity. This gives a different minimal cover as Minimal cover GY P → LA LC → P A → C . The alternative design Y produced by the algorithm now is Design Y and . Note that this design has three relations none of which can be considered as redundant by the condition in step All FDs in the original set F are preserved. The reader will notice that out of the above three relations relations and were produced as the BCNF design by the procedure given in Section . Now suppose that we want to retrieve a list of values for all the employees. If we apply the NATURAL JOIN operation on EMPLOYEE and DEPARTMENT Ename EMPLOYEE Ssn Bdate Address Dnum Smith John B. Wong Franklin T. Zelaya Alicia J. Wallace Jennifer S. Narayan Ramesh K. English Joyce A. Jabbar Ahmad V. Borg James E. Dallas Houston TX Stone Houston TX Fondren Houston TX Voss Houston TX Castle Spring TX Berry Bellaire TX Fire Oak Humble TX Rice Houston TX Berger Anders C. Braes Bellaire TX NULL Benitez Carlos M. Beech Houston TX NULL Dname DEPARTMENT Dnum Dmgrssn Research Administration Headquarters Ename Smith John B. Wong Franklin T. Zelaya Alicia J. Wallace Jennifer S. Narayan Ramesh K. English Joyce A. Jabbar Ahmad V. Borg James E. Ssn Bdate Castle Spring TX Fondren Houston TX Voss Houston TX Rice Houston TX Dallas Houston TX Stone Houston TX Berry Bellaire TX Fire Oak Humble TX Address Administration Research Research Research Administration Headquarters Administration Research Dnum Dname Dmgrssn Ename Smith John B. Wong Franklin T. Zelaya Alicia J. Wallace Jennifer S. Narayan Ramesh K. English Joyce A. Jabbar Ahmad V. Borg James E. Bdate Castle Spring TX Fondren Houston TX Voss Houston TX Rice Houston TX Dallas Houston TX Stone Houston TX Berry Bellaire TX Fire Oak Humble TX Address Administration Research Research Research Administration Headquarters Administration Research Berger Anders C. Benitez Carlos M. Braes Bellaire TX Beech Houston TX NULL NULL NULL NULL NULL NULL Ssn Dnum Dname Dmgrssn Figure Issues with NULL value joins. Some EMPLOYEE tuples have NULL for the join attribute Dnum. Result of applying NATURAL JOIN to the EMPLOYEE and DEPARTMENT relations. Result of applying LEFT OUTER JOIN to EMPLOYEE and DEPARTMENT. Ename Ssn Bdate Address Smith John B. Wong Franklin T. Zelaya Alicia J. Wallace Jennifer S. Narayan Ramesh K. English Joyce A. Jabbar Ahmad V. Borg James E. Dallas Houston TX Stone Houston TX Fondren Houston TX Voss Houston TX Castle Spring TX Berry Bellaire TX Fire Oak Humble TX Rice Houston TX Berger Anders C. Benitez Carlos M. Braes Bellaire TX Beech Houston TX Ssn NULL NULL Dnum Ssn Dnum About Nulls Dangling Tuples and Alternative Relational Designs Figure The dangling tuple problem. The relation . The relation . The relation . in if the employee has not been assigned a department JOIN operation. Discussion of Normalization Algorithms and Alternative Relational Designs One of the problems with the normalization algorithms we described is that the database designer must first specify all the relevant functional dependencies among Chapter Relational Database Design Algorithms and Further Dependencies the database attributes. This is not a simple task for a large database with hundreds of attributes. Failure to specify one or two important dependencies may result in an undesirable design. Another problem is that these algorithms are not deterministic in general. For example the synthesis algorithms . To illustrate the above points let us revisit the relation in Figure It is a relation in which is not in BCNF as was shown in Section We also showed that starting with the functional dependencies and in Figure using the bottom up approach to design and applying Algorithm it is possible to either come up with the relation as the design or an alternate design Y which consists of three relations each of which is a relation. Note that if we test design Y further for BCNF each of and turn out to be individually in BCNF. The design X however when tested for BCNF fails the test. It yields the two relations and by applying Algorithm . Thus the bottom up design procedure of applying Algorithm to design relations to achieve both properties and then applying Algorithm to achieve BCNF with the nonadditive join property yields as the final BCNF design by one route and by the other route . This happens due to the multiple minimal covers for the original set of functional dependencies. Note that is a redundant relation in the Y design however it does not violate the nonadditive join constraint. It is easy to see that is a valid and meaningful relation that has the two candidate keys and P placed side byside. Table summarizes the properties of the algorithms discussed in this chapter so far. Futher Discussion of Multivalued Dependencies and Table Summary of the Algorithms Discussed in This Chapter Algorithm Input Output Properties Purpose Remarks An attribute or a set of attributes X and a set of FDs F A set of attrbutes in the closure of X with respect to F Determine all the attributes that can be functionally determined from X The closure of a key is the entire relation A set of functional dependencies F The minimal cover of functional dependencies To determine the minimal cover of a set of dependencies F Multiple minimal covers may exist depends on the order of selecting functional dependencies Relation schema R with a set of functional dependencies F Key K of R To find a key K The entire relation R is always a default superkey A decomposition D of R and a set F of functional dependencies Boolean result yes or no for nonadditive join property Testing for nonadditive join decomposition See a simpler test NJB in Section for binary decompositions A relation R and a set of functional dependencies F A set of relations in Dependency preservation No guarantee of satisfying lossless join property A relation R and a set of functional dependencies F A set of relations in BCNF Nonadditive join decomposition No guarantee of dependency preservation A relation R and a set of functional dependencies F A set of relations in Nonadditive join and dependencypreserving decomposition May not achieve BCNF but achieves all desirable properties and A relation R and a set of functional and multivalued dependencies A set of relations in Nonadditive join decomposition No guarantee of dependency preservation Further Discussion of Multivalued Dependencies and We introduced and defined the concept of multivalued dependencies and used it to define the fourth normal form in Section Now we revisit MVDs to make our treatment complete by stating the rules of inference on MVDs. Chapter Relational Database Design Algorithms and Further Dependencies Inference Rules for Functional and Multivalued Dependencies As with functional dependencies inference rules for multivalued dependencies have been developed. It is better though to develop a unified framework that includes both FDs and MVDs so that both types of constraints can be considered together. The following inference rules through form a sound and complete set for inferring functional and multivalued dependencies from a given set of dependencies. Assume that all attributes are included in a universal relation schema R An and that X Y Z and W are subsets of R. If X ⊇ Y then X → Y. X → Y | XZ → YZ. X → Y Y → Z | X → Z. X →→ Y | X →→ . If X →→ Y and W ⊇ Z then WX→→ YZ. X →→ Y Y→→ Z | X →→ . X → Y | X→→ Y. If X →→ Y and there exists W with the properties that W ∩ Y is empty W → Z and Y ⊇ Z then X → Z. through are Armstrong’s inference rules for FDs alone. through are inference rules pertaining to MVDs only. and relate FDs and MVDs. In particular says that a functional dependency is a special case of a multivalued dependency that is every FD is also an MVD because it satisfies the formal definition of an MVD. However this equivalence has a catch An FD X → Y is an MVD X →→ Y with the additional implicit restriction that at most one value of Y is associated with each value of Given a set F of functional and multivalued dependencies specified on R An we can use through to infer the set of all dependencies F+ that will hold in every relation state r of R that satisfies F. We again call F+ the closure of F. Fourth Normal Form Revisited We restate the definition of fourth normal form from Section Definition. A relation schema R is in with respect to a set of dependencies F if for every nontrivial multivalued dependency X→→ Y in F+ X is a superkey for R. is the set of values of Y determined by a value of X is restricted to being a singleton set with only one value. Hence in practice we never view an FD as an MVD. EMP Ename Smith Smith Smith Smith Brown Brown Brown Brown Brown Brown Brown Brown Brown Brown Brown Brown John Anna Anna John Jim Jim Jim Jim Joan Joan Joan Joan Bob Bob Bob Bob X Y X Y Y Z W X Y Z W X Y Z W X Pname Dname EMPPROJECTS Ename Smith Smith Brown Brown Brown Brown W X Y Z X Y Pname EMPDEPENDENTS Ename Smith Smith Brown Brown Brown Jim Joan Bob Anna John Dname Futher Discussion of Multivalued Dependencies and Figure Decomposing a relation state of EMP that is not in EMP relation with additional tuples. Two corresponding relations EMPPROJECTS and EMPDEPENDENTS. To illustrate the importance of Figure shows the EMP relation in Figure with an additional employee ‘Brown’ who has three dependents and works on four different projects . There are tuples in EMP in Figure If we decompose EMP into EMPPROJECTS and EMPDEPENDENTS as shown in Figure we need to store a total of only tuples in both relations. Not only would the decomposition save on storage but the update anomalies associated with multivalued dependencies would also be avoided. For example if ‘Brown’ starts working on a new additional project ‘P ’ we must insert three tuples in EMP one for each dependent. If we forget to insert any one of those the relation violates the MVD and becomes inconsistent in that it incorrectly implies a relationship between project and dependent. If the relation has nontrivial MVDs then insert delete and update operations on single tuples may cause additional tuples to be modified besides the one in question. If the update is handled incorrectly the meaning of the relation may change. However after normalization into these update anomalies disappear. For Chapter Relational Database Design Algorithms and Further Dependencies example to add the information that ‘Brown’ will be assigned to project ‘P’ only a single tuple need be inserted in the relation EMPPROJECTS. The EMP relation in Figure is not in because it represents two independent relationships one between employees and the projects they work on and the other between employees and their dependents. We sometimes have a relationship among three entities that depends on all three participating entities such as the SUPPLY relation shown in Figure In this case a tuple represents a supplier supplying a specific part to a particular project so there are no nontrivial MVDs. Hence the SUPPLY all key relation is already in and should not be decomposed. Nonadditive Join Decomposition into Relations Whenever we decompose a relation schema R into and based on an MVD X →→ Y that holds in R the decomposition has the nonadditive join property. It can be shown that this is a necessary and sufficient condition for decomposing a schema into two schemas that have the nonadditive join property as given by Property NJBthat is a further generalization of Property NJB given earlier. Property NJB dealt with FDs only whereas NJB deals with both FDs and MVDs . Property NJB. The relation schemas and form a nonadditive join decomposition of R with respect to a set F of functional and multivalued dependencies if and only if ∩ →→ – or by symmetry if and only if ∩ →→ – We can use a slight modification of Algorithm to develop Algorithm which creates a nonadditive join decomposition into relation schemas that are in . As with Algorithm Algorithm does not necessarily produce a decomposition that preserves FDs. Algorithm Relational Decomposition into Relations with Nonadditive Join Property Input A universal relation R and a set of functional and multivalued dependencies F. Set D R While there is a relation schema Q in D that is not in do choose a relation schema Q in D that is not in find a nontrivial MVD X →→ Y in Q that violates replace Q in D by two relation schemas and Other Dependencies and Normal Forms Other Dependencies and Normal Forms We already introduced another type of dependency called join dependency in Section It arises when a relation is decomposable into a set of projected relations that can be joined back to yield the original relation. After defining JD we defined the fifth normal form based on it in Section In the present section we will introduce some other types of dependencies that have been identified. Inclusion Dependencies Inclusion dependencies were defined in order to formalize two types of interrelational constraints The foreign key constraint cannot be specified as a functional or multivalued dependency because it relates attributes across relations. The constraint between two relations that represent a class subclass relationship ⊆ πY The ⊆ relationship does not necessarily have to be a proper subset. Obviously the sets of attributes on which the inclusion dependency is specified X of R and Y of S must have the same number of attributes. In addition the domains for each pair of corresponding attributes should be compatible. For example if X An and Y Bn one possible correspondence is to have dom compatible with dom for ≤ i ≤ n. In this case we say that Ai corresponds to Bi . For example we can specify the following inclusion dependencies on the relational schema in Figure All the preceding inclusion dependencies represent referential integrity constraints. We can also use inclusion dependencies to represent class subclass Chapter Relational Database Design Algorithms and Further Dependencies relationships. For example in the relational schema of Figure we can specify the following inclusion dependencies As with other types of dependencies there are inclusion dependency inference rules . The following are three examples . If where X An and Y Bn and Ai corresponds to Bi then for ≤ i ≤ n. If and then . The preceding inference rules were shown to be sound and complete for inclusion dependencies. So far no normal forms have been developed based on inclusion dependencies. Template Dependencies Template dependencies provide a technique for representing constraints in relations that typically have no easy and formal definitions. No matter how many types of dependencies we develop some peculiar constraint may come up based on the semantics of attributes within relations that cannot be represented by any of them. The idea behind template dependencies is to specify a template or example that defines each constraint or dependency. There are two types of templates tuple generating templates and constraintgenerating templates. A template consists of a number of hypothesis tuples that are meant to show an example of the tuples that may appear in one or more relations. The other part of the template is the template conclusion. For tuple generating templates the conclusion is a set of tuples that must also exist in the relations if the hypothesis tuples are there. For constraint generating templates the template conclusion is a condition that must hold on the hypothesis tuples. Using constraintgenerating templates we are able to define semantic constraints those that are beyond the scope of the relational model in terms of its data definition language and notation. Figure shows how we may define functional multivalued and inclusion dependencies by templates. Figure shows how we may specify the constraint that an employee’s salary cannot be higher than the salary of his or her direct supervisor on the relation schema EMPLOYEE in Figure Other Dependencies and Normal Forms X A B Y C D Hypothesis Conclusion a b c c c and d d a b c R A B C D R A B C D X A B Y C X C D Y E F Hypothesis Conclusion Hypothesis S E F G Conclusion a b c d a b c d c d g a b c d a b c d a b c d R A B C D d d Figure Templates for some common type of dependencies. Template for functional dependency X → Y. Template for the multivalued dependency X →→ Y. Template for the inclusion dependency . EMPLOYEE Name Ssn . . . Salary Supervisorssn Hypothesis Conclusion ab c d e fg c f d Figure Templates for the constraint that an employee’s salary must be less than the supervisor’s salary. Chapter Relational Database Design Algorithms and Further Dependencies Functional Dependencies Based on Arithmetic Functions and Procedures Sometimes some attributes in a relation may be related via some arithmetic function or a more complicated functional relationship. As long as a unique value of Y is associated with every X we can still consider that the FD X → Y exists. For example in the relation ORDERLINE each tuple represents an item from an order with a particular quantity and the price per unit for that item. In this relation → Extendedprice by the formula Extendedprice Unitprice Quantity. Hence there is a unique value for Extendedprice for every pair and thus it conforms to the definition of functional dependency. Moreover there may be a procedure that takes into account the quantity discounts the type of item and so on and computes a discounted price for the total quantity ordered for that item. Therefore we can say → Discountedprice or → Discountedprice. To check the above FD a more complex procedure COMPUTETOTALPRICE may have to be called into play. Although the above kinds of FDs are technically present in most relations they are not given particular attention during normalization. Domain Key Normal Form There is no hard and fast rule about defining normal forms only up to Historically the process of normalization and the process of discovering undesirable dependencies were carried through but it has been possible to define stricter normal forms that take into account additional types of dependencies and constraints. The idea behind domain key normal form is to specify the ultimate normal form that takes into account all possible types of dependencies and constraints. A relation schema is said to be in DKNF if all constraints and dependencies that should hold on the valid relation states can be enforced simply by enforcing the domain constraints and key constraints on the relation. For a relation in DKNF it becomes very straightforward to enforce all database constraints by simply checking that each attribute value in a tuple is of the appropriate domain and that every key constraint is enforced. However because of the difficulty of including complex constraints in a DKNF relation its practical utility is limited since it may be quite difficult to specify general integrity constraints. For example consider a relation CAR and another relation MANUFACTURE . A general constraint may be of the following form If the Make is either ‘Toyota’ or ‘Lexus ’ then the first character of the Vin# is a ‘J’ if the country of manufacture is ‘Japan’ if the Make is ‘Honda’ or ‘Acura ’ the second character of the Vin# is a ‘J’ if the country of manufacture is There is no simplified way to represent such constraints short of writing a procedure to test them. The procedure COMPUTETOTALPRICE above is an example of such procedures needed to enforce an appropriate integrity constraint. Summary In this chapter we presented a further set of topics related to dependencies a discussion of decomposition and several algorithms related to them as well as to normalization. In Section we presented inference rules for functional dependencies the notion of closure of an attribute closure of a set of functional dependencies equivalence among sets of functional dependencies and algorithms for finding the closure of an attribute were described. We then discussed relational design by synthesis based on a set of given functional dependencies. The relational synthesis algorithms in Section which arise from an improper combination of two or more independent multivalued attributes in the same relation and that result in a combinational expansion of the tuples used to define fourth normal form We discussed inference rules applicable to MVDs and discussed the importance of Finally in Section we discussed inclusion dependencies which are used to specify referential integrity and class subclass constraints and template dependencies which can be used to specify Chapter Relational Database Design Algorithms and Further Dependencies arbitrary types of constraints. We pointed out the need for arithmetic functions or more complex procedures to enforce certain functional dependency constraints. We concluded with a brief discussion of the domain key normal form . Review Questions What is the role of Armstrong’s inference rules join property of a decomposition Why is it important Between the properties of dependency preservation and losslessness which one must definitely be satisfied Why Discuss the NULL value and dangling tuple problems. Illustrate how the process of creating first normal form relations may lead to multivalued dependencies. How should the first normalization be done properly so that MVDs are avoided What types of constraints are inclusion dependencies meant to represent How do template dependencies differ from the other types of dependencies we discussed Why is the domain key normal form known as the ultimate normal form Exercises Exercises Show that the relation schemas produced by Algorithm are in Show that if the matrix S resulting from Algorithm does not have a row that is all a symbols projecting S on the decomposition and joining it back will always produce at least one spurious tuple. Show that the relation schemas produced by Algorithm are in BCNF. Show that the relation schemas produced by Algorithm are in Specify a template dependency for join dependencies. Specify all the inclusion dependencies for the relational schema in Figure Prove that a functional dependency satisfies the formal definition of multivalued dependency. Consider the example of normalizing the LOTS relation in Sections and Determine whether the decomposition of LOTS into has the lossless join property by applying Algorithm and also by using the test under Property NJB. Show how the MVDs Ename →→ Pname and Ename →→ Dname in Figure may arise during normalization into of a relation where the attributes Pname and Dname are multivalued. Apply Algorithm to the relation in Exercise to determine a key for R. Create a minimal set of dependencies G that is equivalent to F and apply the synthesis algorithm which is abbreviated as REFRIG and the following set F of functional dependencies F M → MP M Y → P MP → C a. Evaluate each of the following as a candidate key for REFRIG giving reasons why it can or cannot be a key M M Y M C . b. Based on the above key determination state whether the relation REFRIG is in and in BCNF giving proper reasons. c. Consider the decomposition of REFRIG into D Y P MP C . Is this decomposition lossless Show why. system that is described in the laboratory manual. The relational schema R and set of functional dependencies F need to be coded as lists. As an example R and F for problem are coded as R [a b c d e f g h i j] F [[[a b] [c]] [[a] [d e]] [[b] [f]] [[f] [g h]] [[d] [i j]]] Since DBD is implemented in Prolog use of uppercase terms is reserved for variables in the language and therefore lowercase constants are used to code the attributes. For further details on using the DBD system please refer to the laboratory manual. Using the DBD system verify your answers to the following exercises a. b. c. d. e. f. and g. and Selected Bibliography Selected Bibliography The books by Maier and Atzeni and De Antonellis include a comprehensive discussion of relational dependency theory. The decomposition algorithm Disks a data storage system architecture that is commonly used in large organizations for better reliability and performance. Finally in Section we describe three developments in the storage systems area storage area networks networkchapter Chapter Disk Storage Basic File Structures and Hashing attached storage and iSCSI the latest technology which makes storage area networks more affordable without the use of the Fiber Channel infrastructure and hence is getting very wide acceptance in industry. Section summarizes the chapter. In Chapter we discuss techniques for creating auxiliary data structures called indexes which speed up the search for and retrieval of records. These techniques involve storage of auxiliary data called index files in addition to the file records themselves. Chapters and may be browsed through or even omitted by readers who have already studied file organizations and indexing in a separate course. The material covered here in particular Sections through is necessary for understanding Chapters and which deal with query processing and optimization and database tuning for improving performance of queries. Introduction The collection of data that makes up a computerized database must be stored physically on some computer storage medium. The DBMS software can then retrieve update and process this data as needed. Computer storage media form a storage hierarchy that includes two main categories Primary storage. This category includes storage media that can be operated on directly by the computer’s central processing unit such as the computer’s main memory and smaller but faster cache memories. Primary storage usually provides fast access to data but is of limited storage capacity. Although main memory capacities have been growing rapidly in recent years they are still more expensive and have less storage capacity than secondary and tertiary storage devices. Secondary and tertiary storage. This category includes magnetic disks optical disks and tapes. Hard disk drives are classified as secondary storage whereas removable media such as optical disks and tapes are considered tertiary storage. These devices usually have a larger capacity cost less and provide slower access to data than do primary storage devices. Data in secondary or tertiary storage cannot be processed directly by the CPU first it must be copied into primary storage and then processed by the CPU. We first give an overview of the various storage devices used for primary and secondary storage in Section and then discuss how databases are typically handled in the storage hierarchy in Section Memory Hierarchies and Storage Devices In a modern computer system data resides and is transported throughout a hierarchy of storage media. The highest speed memory is the most expensive and is therefore available with the least capacity. The lowest speed memory is offline tape storage which is essentially available in indefinite storage capacity. Introduction At the primary storage level the memory hierarchy includes at the most expensive end cache memory which is a static RAM . Cache memory is typically used by the CPU to speed up execution of program instructions using techniques such as prefetching and pipelining. The next level of primary storage is DRAM which provides the main work area for the CPU for keeping program instructions and data. It is popularly called main memory. The advantage of DRAM is its low cost which continues to decrease the drawback is its and lower speed compared with static RAM. At the secondary and tertiary storage level the hierarchy includes magnetic disks as well as mass storage in the form of CD ROM and DVD devices and finally tapes at the least expensive end of the hierarchy. The storage capacity is measured in kilobytes megabytes gigabytes and even terabytes GB . The word petabyte terabytes or bytes is now becoming relevant in the context of very large repositories of data in physics astronomy earth sciences and other scientific applications. Programs reside and execute in DRAM. Generally large permanent databases reside on secondary storage and portions of the database are read into and written from buffers in main memory as needed. Nowadays personal computers and workstations have large main memories of hundreds of megabytes of RAM and DRAM so it is becoming possible to load a large part of the database into main memory. Eight to GB of main memory on a single server is becoming commonplace. In some cases entire databases can be kept in main memory leading to main memory databases these are particularly useful in real time applications that require extremely fast response times. An example is telephone switching applications which store databases that contain routing and line information in main memory. Between DRAM and magnetic disk storage another form of memory flash memory is becoming common particularly because it is nonvolatile. Flash memories are high density high performance memories using EEPROM technology. The advantage of flash memory is the fast access speed the disadvantage is that an entire block must be erased and written over simultaneously. Flash memory cards are appearing as the data storage medium in appliances with capacities ranging from a few megabytes to a few gigabytes. These are appearing in cameras players cell phones PDAs and so on. USB flash drives have become the most portable medium for carrying data between personal computers they have a flash memory storage device integrated with a USB interface. CD ROM disks store data optically and are read by a laser. CD ROMs contain prerecorded data that cannot be overwritten. WORM disks are a form of optical storage used for memory typically loses its contents in case of a power outage whereas nonvolatile memory does not. Chapter Disk Storage Basic File Structures and Hashing archiving data they allow data to be written once and read any number of times without the possibility of erasing. They hold about half a gigabyte of data per disk and last much longer than magnetic Optical jukebox memories use an array of CD ROM platters which are loaded onto drives on demand. Although optical jukeboxes have capacities in the hundreds of gigabytes their retrieval times are in the hundreds of milliseconds quite a bit slower than magnetic disks. This type of storage is continuing to decline because of the rapid decrease in cost and increase in capacities of magnetic disks. The DVD is another standard for optical disks allowing to GB of storage per disk. Most personal computer disk drives now read CDROM and DVD disks. Typically drives are CD R that can create CD ROMs and audio CDs as well as record on DVDs. Finally magnetic tapes are used for archiving and backup storage of data. Tape jukeboxes which contain a bank of tapes that are catalogued and can be automatically loaded onto tape drives are becoming popular as tertiary storage to hold terabytes of data. For example NASA’s EOS system stores archived databases in this fashion. Many large organizations are already finding it normal to have terabyte sized databases. The term very large database can no longer be precisely defined because disk storage capacities are on the rise and costs are declining. Very soon the term may be reserved for databases containing tens of terabytes. Storage of Databases Databases typically store large amounts of data that must persist over long periods of time and hence is often referred to as persistent data. Parts of this data are accessed and processed repeatedly during this period. This contrasts with the notion of transient data that persist for only a limited time during program execution. Most databases are stored permanently on magnetic disk secondary storage for the following reasons Generally databases are too large to fit entirely in main memory. The circumstances that cause permanent loss of stored data arise less frequently for disk secondary storage than for primary storage. Hence we refer to disk and other secondary storage devices as nonvolatile storage whereas main memory is often called volatile storage. The cost of storage per unit of data is an order of magnitude less for disk secondary storage than for primary storage. Some of the newer technologies such as optical disks DVDs and tape jukeboxes are likely to provide viable alternatives to the use of magnetic disks. In the future databases may therefore reside at different levels of the memory hierarchy from those described in Section However it is anticipated that magnetic rotational speeds are lower giving higher latency delays and low transfer rates . Secondary Storage Devices disks will continue to be the primary medium of choice for large databases for years to come. Hence it is important to study and understand the properties and characteristics of magnetic disks and the way data files can be organized on disk in order to design effective databases with acceptable performance. Magnetic tapes are frequently used as a storage medium for backing up databases because storage on tape costs even less than storage on disk. However access to data on tape is quite slow. Data stored on tapes is offline that is some intervention by an operator or an automatic loading device to load a tape is needed before the data becomes available. In contrast disks are online devices that can be accessed directly at any time. The techniques used to store large amounts of structured data on disk are important for database designers the DBA and implementers of a DBMS. Database designers and the DBA must know the advantages and disadvantages of each storage technique when they design implement and operate a database on a specific DBMS. Usually the DBMS has several options available for organizing the data. The process of physical database design involves choosing the particular data organization techniques that best suit the given application requirements from among the options. DBMS system implementers must study data organization techniques so that they can implement them efficiently and thus provide the DBA and users of the DBMS with sufficient options. Typical database applications need only a small portion of the database at a time for processing. Whenever a certain portion of the data is needed it must be located on disk copied to main memory for processing and then rewritten to the disk if the data is changed. The data stored on disk is organized as files of records. Each record is a collection of data values that can be interpreted as facts about entities their attributes and their relationships. Records should be stored on disk in a manner that makes it possible to locate them efficiently when they are needed. There are several primary file organizations which determine how the file records are physically placed on the disk and hence how the records can be accessed. A heap file places the records on disk in no particular order by appending new records at the end of the file whereas a sorted file keeps the records ordered by the value of a particular field . A hashed file uses a hash function applied to a particular field to determine a record’s placement on disk. Other primary file organizations such as B trees use tree structures. We discuss primary file organizations in Sections through A secondary organization or auxiliary access structure allows efficient access to file records based on alternate fields than those that have been used for the primary file organization. Most of these exist as indexes and will be discussed in Chapter Secondary Storage Devices In this section we describe some characteristics of magnetic disk and magnetic tape storage devices. Readers who have already studied these devices may simply browse through this section. Chapter Disk Storage Basic File Structures and Hashing Actuator movement Track Actuator Arm Read write head Spindle Disk rotation Cylinder of tracks Figure A single sided disk with read write hardware. A disk pack with read write hardware. Hardware Description of Disk Devices Magnetic disks are used for storing large amounts of data. The most basic unit of data on the disk is a single bit of information. By magnetizing an area on disk in certain ways one can make it represent a bit value of either or . To code information bits are grouped into bytes . Byte sizes are typically to bits depending on the computer and the device. We assume that one character is stored in a single byte and we use the terms byte and character interchangeably. The capacity of a disk is the number of bytes it can store which is usually very large. Small floppy disks used with microcomputers typically hold from KB to MB they are rapidly going out of circulation. Hard disks for personal computers typically hold from several hundred MB up to tens of GB and large disk packs used with servers and mainframes have capacities of hundreds of GB. Disk capacities continue to grow as technology improves. Whatever their capacity all disks are made of magnetic material shaped as a thin circular disk as shown in Figure and protected by a plastic or acrylic cover. Secondary Storage Devices Track Sector Three sectors Two sectors One sector Figure Different sector organizations on disk. Sectors subtending a fixed angle. Sectors maintaining a uniform recording density. A disk is single sided if it stores information on one of its surfaces only and doublesided if both surfaces are used. To increase storage capacity disks are assembled into a disk pack as shown in Figure which may include many disks and therefore many surfaces. Information is stored on a disk surface in concentric circles of small each having a distinct diameter. Each circle is called a track. In disk packs tracks with the same diameter on the various surfaces are called a cylinder because of the shape they would form if connected in space. The concept of a cylinder is important because data stored on one cylinder can be retrieved much faster than if it were distributed among different cylinders. The number of tracks on a disk ranges from a few hundred to a few thousand and the capacity of each track typically ranges from tens of Kbytes to Kbytes. Because a track usually contains a large amount of information it is divided into smaller blocks or sectors. The division of a track into sectors is hard coded on the disk surface and cannot be changed. One type of sector organization as shown in Figure calls a portion of a track that subtends a fixed angle at the center a sector. Several other sector organizations are possible one of which is to have the sectors subtend smaller angles at the center as one moves away thus maintaining a uniform density of recording as shown in Figure A technique called ZBR allows a range of cylinders to have the same number of sectors per arc. For example cylinders may have one sector per track may have two per track and so on. Not all disks have their tracks divided into sectors. The division of a track into equal sized disk blocks is set by the operating system during disk formatting . Block size is fixed during initialization and cannot be changed dynamically. Typical disk block sizes range from to bytes. A disk with hard coded sectors often has the sectors subdivided into blocks during initialization. Blocks are separated by fixed size interblock gaps which include specially coded control information written during disk initialization. This information is used to determine which block on the track follows each some disks the circles are now connected into a kind of continuous spiral. Chapter Disk Storage Basic File Structures and Hashing Table Specifications of Typical High End Cheetah Disks from Seagate Description Cheetah Cheetah NS Model Number Height mm mm Width mm mm Length mm mm Weight kg kg Capacity Formatted Capacity Gbytes Gbytes Configuration Number of disks Number of heads Performance Transfer Rates Internal Transfer Rate Mb sec Internal Transfer Rate Mb sec Mb sec Mean Time Between Failure M hours Seek Times Avg. Seek Time ms ms Avg. Seek Time ms ms Track to track Seek Read ms ms Track to track Seek Write ms ms Average Latency ms msec Courtesy Seagate Technology interblock gap. Table illustrates the specifications of typical disks used on large servers in industry. The and prefixes on disk names refer to the rotational speeds in rpm . There is continuous improvement in the storage capacity and transfer rates associated with disks they are also progressively getting cheaper currently costing only a fraction of a dollar per megabyte of disk storage. Costs are going down so rapidly that costs as low cent MB which translates to and already here. A disk is a random access addressable device. Transfer of data between main memory and disk takes place in units of disk blocks. The hardware address of a block a combination of a cylinder number track number and block number is supplied to the disk I O hardware. In many modern disk drives a single number called LBA which is a number between and n is mapped automatically to the right block by the disk drive controller. The address of a buffer a contiguous reserved area in main storage that holds one disk block is also provided. For a read command the disk block is copied into the buffer whereas for a write command the contents of the buffer are copied into the disk block. Sometimes several contiguous blocks called a cluster may be transferred as a unit. In this case the buffer size is adjusted to match the number of bytes in the cluster. The actual hardware mechanism that reads or writes a block is the disk read write head which is part of a system called a disk drive. A disk or disk pack is mounted in the disk drive which includes a motor that rotates the disks. A read write head includes an electronic component attached to a mechanical arm. Disk packs with multiple surfaces are controlled by several read write heads one for each surface as shown in Figure All arms are connected to an actuator attached to another electrical motor which moves the read write heads in unison and positions them precisely over the cylinder of tracks specified in a block address. Disk drives for hard disks rotate the disk pack continuously at a constant speed . Once the read write head is positioned on the right track and the block specified in the block address moves under the read write head the electronic component of the read write head is activated to transfer the data. Some disk units have fixed read write heads with as many heads as there are tracks. These are called fixed head disks whereas disk units with an actuator are called movable head disks. For fixed head disks a track or cylinder is selected by electronically switching to the appropriate read write head rather than by actual mechanical movement consequently it is much faster. However the cost of the additional read write heads is quite high so fixed head disks are not commonly used. A disk controller typically embedded in the disk drive controls the disk drive and interfaces it to the computer system. One of the standard interfaces used today for disk drives on PCs and workstations is called SCSI . The controller accepts high level I O commands and takes appropriate action to position the arm and causes the read write action to take place. To transfer a disk block given its address the disk controller must first mechanically position the read write head on the correct track. The time required to do this is called the seek time. Typical seek times are to msec on desktops and to msecs on servers. Following that there is another delay called the rotational delay or latency while the beginning of the desired block rotates into position under the read write head. It depends on the rpm of the disk. For example at rpm the time per rotation is msec and the average rotational delay is the time per half revolution or msec. At rpm the average rotational delay increases to msec. Finally some additional time is needed to transfer the data this is called the block transfer time. Hence the total time needed to locate and transfer an arbitrary block given its address is the sum of the seek time rotational delay and block transfer time. The seek time and rotational delay are usually much larger than the block transfer time. To make the transfer of multiple blocks more efficient it is common to transfer several consecutive blocks on the same track or cylinder. This eliminates the seek time and rotational delay for all but the first block and can result Secondary Storage Devices Chapter Disk Storage Basic File Structures and Hashing in a substantial saving of time when numerous contiguous blocks are transferred. Usually the disk manufacturer provides a bulk transfer rate for calculating the time required to transfer consecutive blocks. Appendix B contains a discussion of these and other disk parameters. The time needed to locate and transfer a disk block is in the order of milliseconds usually ranging from to msec. For contiguous blocks locating the first block takes from to msec but transferring subsequent blocks may take only to msec each. Many search techniques take advantage of consecutive retrieval of blocks when searching for data on disk. In any case a transfer time in the order of milliseconds is considered quite high compared with the time required to process data in main memory by current CPUs. Hence locating data on disk is a major bottleneck in database applications. The file structures we discuss here and in Chapter attempt to minimize the number of block transfers needed to locate and transfer the required data from disk to main memory. Placing “related information” on contiguous blocks is the basic goal of any storage organization on disk. Magnetic Tape Storage Devices Disks are random access secondary storage devices because an arbitrary disk block may be accessed at random once we specify its address. Magnetic tapes are sequential access devices to access the nth block on tape first we must scan the preceding n – blocks. Data is stored on reels of high capacity magnetic tape somewhat similar to audiotapes or videotapes. A tape drive is required to read the data from or write the data to a tape reel. Usually each group of bits that forms a byte is stored across the tape and the bytes themselves are stored consecutively on the tape. A read write head is used to read or write data on tape. Data records on tape are also stored in blocks although the blocks may be substantially larger than those for disks and interblock gaps are also quite large. With typical tape densities of to bytes per inch a typical interblock of inch corresponds to to bytes of wasted storage space. It is customary to group many records together in one block for better space utilization. The main characteristic of a tape is its requirement that we access the data blocks in sequential order. To get to a block in the middle of a reel of tape the tape is mounted and then scanned until the required block gets under the read write head. For this reason tape access can be slow and tapes are not used to store online data except for some specialized applications. However tapes serve a very important function backing up the database. One reason for backup is to keep copies of disk files in case the data is lost due to a disk crash which can happen if the disk read write head touches the disk surface because of mechanical malfunction. For this reason disk files are copied periodically to tape. For many online critical applications such as airline reservation systems to avoid any downtime mirrored systems are used to keep three sets of identical disks two in online operation and one interrecord gaps in tape terminology. Buffering of Blocks as backup. Here offline disks become a backup device. The three are rotated so that they can be switched in case there is a failure on one of the live disk drives. Tapes can also be used to store excessively large database files. Database files that are seldom used or are outdated but required for historical record keeping can be archived on tape. Originally half inch reel tape drives were used for data storage employing the so called track tapes. Later smaller magnetic tapes that can store up to GB as well as helical scan data cartridges and writable CDs and DVDs became popular media for backing up data files from PCs and workstations. They are also used for storing images and system libraries. Backing up enterprise databases so that no transaction information is lost is a major undertaking. Currently tape libraries with slots for several hundred cartridges are used with Digital and Superdigital Linear Tapes having capacities in hundreds of gigabytes that record data on linear tracks. Robotic arms are used to write on multiple cartridges in parallel using multiple tape drives with automatic labeling software to identify the backup cartridges. An example of a giant library is the model of Sun Storage Technology that can store up to petabytes of data using up to drives with a maximum throughput rate of TB hour. We defer the discussion of disk storage technology called RAID and of storage area networks network attached storage and iSCSI storage systems to the end of the chapter. Buffering of Blocks When several blocks need to be transferred from disk to main memory and all the block addresses are known several buffers can be reserved in main memory to speed up the transfer. While one buffer is being read or written the CPU can process data in the other buffer because an independent disk I O processor exists that once started can proceed to transfer a data block between memory and disk independent of and in parallel to CPU processing. Figure illustrates how two processes can proceed in parallel. Processes A and B are running concurrently in an interleaved fashion whereas processes C and D are running concurrently in a parallel fashion. When a single CPU controls multiple processes parallel execution is not possible. However the processes can still run concurrently in an interleaved way. Buffering is most useful when processes can run concurrently in a parallel fashion either because a separate disk I O processor is available or because multiple CPU processors exist. Figure illustrates how reading and processing can proceed in parallel when the time required to process a disk block in memory is less than the time required to read the next block and fill a buffer. The CPU can start processing a block once its transfer to main memory is completed at the same time the disk I O processor can be reading and transferring the next block into a different buffer. This technique is called double buffering and can also be used to read a continuous stream of blocks from disk to memory. Double buffering permits continuous reading or writing of data on consecutive disk blocks which eliminates the seek time and rotational delay Chapter Disk Storage Basic File Structures and Hashing Interleaved concurrency of operations A and B Parallel execution of operations C and D A A B B Time Figure Interleaved concurrency versus parallel execution. + Process B + Fill A Time Process A + Fill B Disk Block I O Disk Block PROCESSING Fill A + Process A + Fill A + Process A + Process B + Fill A Figure Use of two buffers A and B for reading from disk. for all but the first block transfer. Moreover data is kept ready for processing thus reducing the waiting time in the programs. Placing File Records on Disk In this section we define the concepts of records record types and files. Then we discuss techniques for placing file records on disk. Records and Record Types Data is usually stored in the form of records. Each record consists of a collection of related data values or items where each value is formed of one or more bytes and corresponds to a particular field of the record. Records usually describe entities and their attributes. For example an EMPLOYEE record represents an employee entity and each field value in the record specifies some attribute of that employee such as Name Birthdate Salary or Supervisor. A collection of field names and their corre Placing File Records on Disk sponding data types constitutes a record type or record format definition. A data type associated with each field specifies the types of values a field can take. The data type of a field is usually one of the standard data types used in programming. These include numeric string of characters Boolean and sometimes specially coded date and time data types. The number of bytes required for each data type is fixed for a given computer system. An integer may require bytes a long integer bytes a real number bytes a Boolean byte a date bytes and a fixed length string of k characters k bytes. Variable length strings may require as many bytes as there are characters in each field value. For example an EMPLOYEE record type may be defined using the C programming language notation as the following structure struct employee char char int salary int jobcode char In some database applications the need may arise for storing data items that consist of large unstructured objects which represent images digitized video or audio streams or free text. These are referred to as BLOBs . A BLOB data item is typically stored separately from its record in a pool of disk blocks and a pointer to the BLOB is included in the record. Files Fixed Length Records and Variable Length Records A file is a sequence of records. In many cases all records in a file are of the same record type. If every record in the file has exactly the same size the file is said to be made up of fixed length records. If different records in the file have different sizes the file is said to be made up of variable length records. A file may have variable length records for several reasons The file records are of the same record type but one or more of the fields are of varying size . For example the Name field of EMPLOYEE can be a variable length field. The file records are of the same record type but one or more of the fields may have multiple values for individual records such a field is called a repeating field and a group of values for the field is often called a repeating group. The file records are of the same record type but one or more of the fields are optional that is they may have values for some but not all of the file records . Name Smith John Ssn DEPARTMENT Computer Smith John Name Name Ssn Salary Jobcode Department Hiredate Ssn Salary Jobcode Department XXXX XXXX Computer Separator Characters Separator Characters Separates field name from field value Separates fields Terminates record Chapter Disk Storage Basic File Structures and Hashing Figure Three record storage formats. A fixed length record with six fields and size of bytes. A record with two variable length fields and three fixed length fields. A variable field record with three types of separator characters. The file contains records of different record types and hence of varying size . This would occur if related records of different types were clustered on disk blocks for example the GRADEREPORT records of a particular student may be placed following that STUDENT’s record. The fixed length EMPLOYEE records in Figure have a record size of bytes. Every record has the same fields and field lengths are fixed so the system can identify the starting byte position of each field relative to the starting position of the record. This facilitates locating field values by programs that access such files. Notice that it is possible to represent a file that logically should have variable length records as a fixed length records file. For example in the case of optional fields we could have every field included in every file record but store a special NULL value if no value exists for that field. For a repeating field we could allocate as many spaces in each record as the maximum possible number of occurrences of the field. In either case space is wasted when certain records do not have values for all the physical spaces provided in each record. Now we consider other options for formatting records of a file of variable length records. Placing File Records on Disk For variable length fields each record has a value for each field but we do not know the exact length of some field values. To determine the bytes within a particular record that represent each field we can use special separator characters which do not appear in any field value to terminate variable length fields as shown in Figure or we can store the length in bytes of the field in the record preceding the field value. A file of records with optional fields can be formatted in different ways. If the total number of fields for the record type is large but the number of fields that actually appear in a typical record is small we can include in each record a sequence of field name field value pairs rather than just the field values. Three types of separator characters are used in Figure although we could use the same separator character for the first two purposes separating the field name from the field value and separating one field from the next field. A more practical option is to assign a short field type code say an integer number to each field and include in each record a sequence of field type field value pairs rather than field name field value pairs. A repeating field needs one separator character to separate the repeating values of the field and another separator character to indicate termination of the field. Finally for a file that includes records of different types each record is preceded by a record type indicator. Understandably programs that process files of variablelength records which are usually part of the file system and hence hidden from the typical programmers need to be more complex than those for fixed length records where the starting position and size of each field are known and Record Blocking and Spanned versus Unspanned Records The records of a file must be allocated to disk blocks because a block is the unit of data transfer between disk and memory. When the block size is larger than the record size each block will contain numerous records although some files may have unusually large records that cannot fit in one block. Suppose that the block size is B bytes. For a file of fixed length records of size R bytes with B ≥ R we can fit bfr ⎣B R⎦ records per block where the ⎣⎦ rounds down the number x to an integer. The value bfr is called the blocking factor for the file. In general R may not divide B exactly so we have some unused space in each block equal to B − bytes To utilize this unused space we can store part of a record on one block and the rest on another. A pointer at the end of the first block points to the block containing the remainder of the record in case it is not the next consecutive block on disk. This organization is called spanned because records can span more than one block. Whenever a record is larger than a block we must use a spanned organization. If records are not allowed to cross block boundaries the organization is called unspanned. This is used with fixed length records having B R because it makes schemes are also possible for representing variable length records. Chapter Disk Storage Basic File Structures and Hashing Block Record Record Record Record P Block + Record Record Record Record P Block Record Record Record Block + Record Record Record Figure Types of record organization. Unspanned. Spanned. each record start at a known location in the block simplifying record processing. For variable length records either a spanned or an unspanned organization can be used. If the average record is large it is advantageous to use spanning to reduce the lost space in each block. Figure illustrates spanned versus unspanned organization. For variable length records using spanned organization each block may store a different number of records. In this case the blocking factor bfr represents the average number of records per block for the file. We can use bfr to calculate the number of blocks b needed for a file of r records b ⎡⎤ blocks where the ⎡⎤ rounds the value x up to the next integer. Allocating File Blocks on Disk There are several standard techniques for allocating the blocks of a file on disk. In contiguous allocation the file blocks are allocated to consecutive disk blocks. This makes reading the whole file very fast using double buffering but it makes expanding the file difficult. In linked allocation each file block contains a pointer to the next file block. This makes it easy to expand the file but makes it slow to read the whole file. A combination of the two allocates clusters of consecutive disk blocks and the clusters are linked. Clusters are sometimes called file segments or extents. Another possibility is to use indexed allocation where one or more index blocks contain pointers to the actual file blocks. It is also common to use combinations of these techniques. File Headers A file header or file descriptor contains information about a file that is needed by the system programs that access the file records. The header includes information to determine the disk addresses of the file blocks as well as to record format descriptions which may include field lengths and the order of fields within a record for fixed length unspanned records and field type codes separator characters and record type codes for variable length records. To search for a record on disk one or more blocks are copied into main memory buffers. Programs then search for the desired record or records within the buffers using the information in the file header. If the address of the block that contains the desired record is not known the search programs must do a linear search through Operations on Files the file blocks. Each file block is copied into a buffer and searched until the record is located or all the file blocks have been searched unsuccessfully. This can be very time consuming for a large file. The goal of a good file organization is to locate the block that contains a desired record with a minimal number of block transfers. Operations on Files Operations on files are usually grouped into retrieval operations and update operations. The former do not change any data in the file but only locate certain records so that their field values can be examined and processed. The latter change the file by insertion or deletion of records or by modification of field values. In either case we may have to select one or more records for retrieval deletion or modification based on a selection condition which specifies criteria that the desired record or records must satisfy. Consider an EMPLOYEE file with fields Name Ssn Salary Jobcode and Department. A simple selection condition may involve an equality comparison on some field value for example . More complex conditions can involve other types of comparison operators such as or ≥ an example is to extract a simple condition that can be used to locate the records on disk. Each located record is then checked to determine whether it satisfies the full selection condition. For example we may extract the simple condition from the complex condition each record satisfying is located and then tested to see if it also satisfies to hold file blocks from disk and retrieves the file header. Sets the file pointer to the beginning of the file. Reset. Sets the file pointer of an open file to the beginning of the file. Find . Searches for the first record that satisfies a search condition. Transfers the block containing that record into a main memory buffer . The file pointer points to the record in the buffer Chapter Disk Storage Basic File Structures and Hashing and it becomes the current record. Sometimes different verbs are used to indicate whether the located record is to be retrieved or updated. Read . Copies the current record from the buffer to a program variable in the user program. This command may also advance the current record pointer to the next record in the file which may necessitate reading the next file block from disk. FindNext. Searches for the next record in the file that satisfies the search condition. Transfers the block containing that record into a main memory buffer . The record is located in the buffer and becomes the current record. Various forms of FindNext are available in legacy DBMSs based on the hierarchical and network models. Delete. Deletes the current record and updates the file on disk to reflect the deletion. Modify. Modifies some field values for the current record and updates the file on disk to reflect the modification. Insert. Inserts a new record in the file by locating the block where the record is to be inserted transferring that block into a main memory buffer writing the record into the buffer and writing the buffer to disk to reflect the insertion. Close. Completes the file access by releasing the buffers and performing any other needed cleanup operations. The preceding are called record at a time operations because each operation applies to a single record. It is possible to streamline the operations Find FindNext and Read into a single operation Scan whose description is as follows Scan. If the file has just been opened or reset Scan returns the first record otherwise it returns the next record. If a condition is specified with the operation the returned record is the first or next record satisfying the condition. In database systems additional set at a time higher level operations may be applied to a file. Examples of these are as follows FindAll. Locates all the records in the file that satisfy a search condition. Find n. Searches for the first record that satisfies a search condition and then continues to locate the next n – records satisfying the same condition. Transfers the blocks containing the n records to the main memory buffer . FindOrdered. Retrieves all the records in the file in some specified order. Reorganize. Starts the reorganization process. As we shall see some file organizations require periodic reorganization. An example is to reorder the file records by sorting them on a specified field. Files of Unordered Records At this point it is worthwhile to note the difference between the terms file organization and access method. A file organization refers to the organization of the data of a file into records blocks and access structures this includes the way records and blocks are placed on the storage medium and interlinked. An access method on the other hand provides a group of operations such as those listed earlier that can be applied to a file. In general it is possible to apply several access methods to a file organization. Some access methods though can be applied only to files organized in certain ways. For example we cannot apply an indexed access method to a file without an index delete records and modify records . Deleting or modifying a record requires a selection condition to identify a particular record or set of records. Retrieving one or more records also requires a selection condition. If users expect mainly to apply a search condition based on Ssn the designer must choose a file organization that facilitates locating a record given its Ssn value. This may involve physically ordering the records by Ssn value or defining an index on Ssn In this simplest and most basic type of organization records are placed in the file in the order in which they are inserted so new records are inserted at the end of the Chapter Disk Storage Basic File Structures and Hashing file. Such an organization is called a heap or pile file. This organization is often used with additional access paths such as the secondary indexes discussed in Chapter It is also used to collect and store data records for future use. Inserting a new record is very efficient. The last disk block of the file is copied into a buffer the new record is added and the block is then rewritten back to disk. The address of the last file block is kept in the file header. However searching for a record using any search condition involves a linear search through the file block by block an expensive procedure. If only one record satisfies the search condition then on the average a program will read into memory and search half the file blocks before it finds the record. For a file of b blocks this requires searching blocks on average. If no records or several records satisfy the search condition the program must read and search all b blocks in the file. To delete a record a program must first find its block copy the block into a buffer delete the record from the buffer and finally rewrite the block back to the disk. This leaves unused space in the disk block. Deleting a large number of records in this way results in wasted storage space. Another technique used for record deletion is to have an extra byte or bit called a deletion marker stored with each record. A record is deleted by setting the deletion marker to a certain value. A different value for the marker indicates a valid record. Search programs consider only valid records in a block when conducting their search. Both of these deletion techniques require periodic reorganization of the file to reclaim the unused space of deleted records. During reorganization the file blocks are accessed consecutively and records are packed by removing deleted records. After such a reorganization the blocks are filled to capacity once more. Another possibility is to use the space of deleted records when inserting new records although this requires extra bookkeeping to keep track of empty locations. We can use either spanned or unspanned organization for an unordered file and it may be used with either fixed length or variable length records. Modifying a variable length record may require deleting the old record and inserting a modified record because the modified record may not fit in its old space on disk. To read all records in order of the values of some field we create a sorted copy of the file. Sorting is an expensive operation for a large disk file and special techniques for external sorting are used ⎦ and is the th record in that block. Such a file is often called a relative or direct file because records can easily be accessed directly by their relative positions. Accessing a record by its position does not help locate a record based on a search condition however it facilitates the construction of access paths on the file such as the indexes discussed in Chapter this organization is called a sequential file. Files of Ordered Records Files of Ordered Records We can physically order the records of a file on disk based on the values of one of their fields called the ordering field. This leads to an ordered or sequential If the ordering field is also a key field of the file a field guaranteed to have a unique value in each record then the field is called the ordering key for the file. Figure shows an ordered file with Name as the ordering key field . Ordered records have some advantages over unordered files. First reading the records in order of the ordering key values becomes extremely efficient because no sorting is required. Second finding the next record from the current one in order of the ordering key usually requires no additional block accesses because the next record is in the same block as the current one . Third using a search condition based on the value of an ordering key field results in faster access when the binary search technique is used which constitutes an improvement over linear searches although it is not often used for disk files. Ordered files are blocked and stored on contiguous cylinders to minimize the seek time. A binary search for disk files can be done on the blocks rather than on the records. Suppose that the file has b blocks numbered b the records are ordered by ascending value of their ordering key field and we are searching for a record whose ordering key field value is K. Assuming that disk addresses of the file blocks are available in the file header the binary search can be described by Algorithm A binary search usually accesses blocks whether the record is found or not an improvement over linear searches where on the average blocks are accessed when the record is found and b blocks are accessed when the record is not found. Algorithm Binary Search on an Ordering Key of a Disk File l ← u ← b while do begin i ← div read block i of the file into the buffer if K then u ← i – else if K then l ← i + else if the record with ordering key field value K is in the buffer then goto found else goto notfound end goto notfound A search criterion involving the conditions ≥ and ≤ on the ordering field is quite efficient since the physical ordering of records means that all records term sequential file has also been used to refer to unordered files although it is more appropriate for ordered files. Chapter Disk Storage Basic File Structures and Hashing Name Aaron Ed Abbott Diane Block Acosta Marc Ssn Birthdate Job Salary Sex Adams John Adams Robin Block Akers Jan Alexander Ed Alfred Bob Block Allen Sam Allen Troy Anders Keith Block Anderson Rob Anderson Zach Angeli Joe Block Archer Sue Arnold Mack Arnold Steven Block Atkins Timothy Wong James Wood Donald Block Woods Manny Wright Pam Wyatt Charles Block n Zimmer Byron Figure Some blocks of an ordered file of EMPLOYEE records with Name as the ordering key field. satisfying the condition are contiguous in the file. For example referring to Figure if the search criterion is where means alphabetically before the records satisfying the search criterion are those from the beginning of the file up to the first record that has a Name value starting with the letter ‘G’. Files of Ordered Records Ordering does not provide any advantages for random or ordered access of the records based on values of the other nonordering fields of the file. In these cases we do a linear search for random access. To access the records in order based on a nonordering field it is necessary to create another sorted copy in a different order of the file. Inserting and deleting records are expensive operations for an ordered file because the records must remain physically ordered. To insert a record we must find its correct position in the file based on its ordering field value and then make space in the file to insert the record in that position. For a large file this can be very timeconsuming because on the average half the records of the file must be moved to make space for the new record. This means that half the file blocks must be read and rewritten after records are moved among them. For record deletion the problem is less severe if deletion markers and periodic reorganization are used. One option for making insertion more efficient is to keep some unused space in each block for new records. However once this space is used up the original problem resurfaces. Another frequently used method is to create a temporary unordered file called an overflow or transaction file. With this technique the actual ordered file is called the main or masterfile. New records are inserted at the end of the overflow file rather than in their correct position in the main file. Periodically the overflow file is sorted and merged with the master file during file reorganization. Insertion becomes very efficient but at the cost of increased complexity in the search algorithm. The overflow file must be searched using a linear search if after the binary search the record is not found in the main file. For applications that do not require the most upto date information overflow records can be ignored during a search. Modifying a field value of a record depends on two factors the search condition to locate the record and the field to be modified. If the search condition involves the ordering key field we can locate the record using a binary search otherwise we must do a linear search. A nonordering field can be modified by changing the record and rewriting it in the same physical location on disk assuming fixed length records. Modifying the ordering field means that the record can change its position in the file. This requires deletion of the old record followed by insertion of the modified record. Reading the file records in order of the ordering field is quite efficient if we ignore the records in overflow since the blocks can be read consecutively using double buffering. To include the records in overflow we must merge them in their correct positions in this case first we can reorganize the file and then read its blocks sequentially. To reorganize the file first we sort the records in the overflow file and then merge them with the master file. The records marked for deletion are removed during the reorganization. Table summarizes the average access time in block accesses to find a specific record in a file with b blocks. Ordered files are rarely used in database applications unless an additional access path called a primary index is used this results in an indexed sequential file. This Chapter Disk Storage Basic File Structures and Hashing Table Average Access Times for a File of b Blocks under Basic File Organizations Average Blocks to Access Type of Organization Access Search Method a Specific Record Heap Sequential scan Ordered Sequential scan Ordered Binary search b further improves the random access time on the ordering key field. K mod M function which returns the remainder of an integer hash field value K after division by M this value is then used for the record address. hash file has also been called a direct file. Hashing Techniques Noninteger hash field values can be transformed into integers before the mod function is applied. For character strings the numeric codes associated with characters can be used in the transformation for example by multiplying those code values. For a hash field whose data type is a string of characters Algorithm can be used to calculate the hash address. We assume that the code function returns the numeric code of a character and that we are given a hash field value K of type K array of char or char . M + M M – M – Data fields Overflow pointer Address space Overflow space M + M + M + M + – M + – null pointer overflow pointer refers to position of next record in linked list M – M M + M + M – Name Ssn Job Salary Figure Internal hashing data structures. Array of M positions for use in internal hashing. Collision resolution by chaining records. Chapter Disk Storage Basic File Structures and Hashing Algorithm Two simple hashing algorithms Applying the mod hash function to a character string K. Collision resolution by open addressing. temp ← for i ← to do temp ← temp code mod M hashaddress ← temp mod M i ← hashaddress a ← i if location i is occupied then begin i ← and location i is occupied do i ← then all positions are full else newhashaddress ← i end Other hashing functions can be used. One technique called folding involves applying an arithmetic function such as addition or a logical function such as exclusive or to different portions of the hash field value to calculate the hash address position is found. Algorithm may be used for this purpose. Chaining. For this method various overflow locations are kept usually by extending the array with a number of overflow positions. Additionally a pointer field is added to each record location. A collision is resolved by placing the new record in an unused overflow location and setting the pointer of the occupied hash address location to the address of that overflow location. detailed discussion of hashing functions is outside the scope of our presentation. Hashing Techniques A linked list of overflow records for each hash address is thus maintained as shown in Figure Multiple hashing. The program applies a second hash function if the first results in a collision. If another collision results the program uses open addressing or applies a third hash function and then uses open addressing if necessary. Each collision resolution method requires its own algorithms for insertion retrieval and deletion of records. The algorithms for chaining are the simplest. Deletion algorithms for open addressing are rather tricky. Data structures textbooks discuss internal hashing algorithms in more detail. The goal of a good hashing function is to distribute the records uniformly over the address space so as to minimize collisions while not leaving many unused locations. Simulation and analysis studies have shown that it is usually best to keep a hash table between and percent full so that the number of collisions remains low and we do not waste too much space. Hence if we expect to have r records to store in the table we should choose M locations for the address space such that is between and It may also be useful to choose a prime number for M since it has been demonstrated that this distributes the hash addresses better over the address space when the mod hashing function is used. Other hash functions may require M to be a power of External Hashing for Disk Files Hashing for disk files is called external hashing. To suit the characteristics of disk storage the target address space is made of buckets each of which holds multiple records. A bucket is either one disk block or a cluster of contiguous disk blocks. The hashing function maps a key into a relative bucket number rather than assigning an absolute block address to the bucket. A table maintained in the file header converts the bucket number into the corresponding disk block address as illustrated in Figure The collision problem is less severe with buckets because as many records as will fit in a bucket can hash to the same bucket without causing problems. However we must make provisions for the case where a bucket is filled to capacity and a new record being inserted hashes to that bucket. We can use a variation of chaining in which a pointer is maintained in each bucket to a linked list of overflow records for the bucket as shown in Figure The pointers in the linked list should be record pointers which include both a block address and a relative record position within the block. Hashing provides the fastest possible access for retrieving an arbitrary record given the value of its hash field. Although most good hash functions do not maintain records in order of hash field values some functions called order preserving do. A simple example of an order preserving hash function is to take the leftmost three digits of an invoice number field that yields a bucket address as the hash address and keep the records sorted by invoice number within each bucket. Another Chapter Disk Storage Basic File Structures and Hashing M – M – Bucket Number Block address on disk Figure Matching bucket numbers to disk block addresses. example is to use an integer hash key directly as an index to a relative file if the hash key values fill up a particular interval for example if employee numbers in a company are assigned as up to the total number of employees we can use the identity hash function that maintains order. Unfortunately this only works if keys are generated in order by some application. The hashing scheme described so far is called static hashing because a fixed number of buckets M is allocated. This can be a serious drawback for dynamic files. Suppose that we allocate M buckets for the address space and let m be the maximum number of records that can fit in one bucket then at most records will fit in the allocated space. If the number of records turns out to be substantially fewer than we are left with a lot of unused space. On the other hand if the number of records increases to substantially more than numerous collisions will result and retrieval will be slowed down because of the long lists of overflow records. In either case we may have to change the number of blocks M allocated and then use a new hashing function to redistribute the records. These reorganizations can be quite time consuming for large files. Newer dynamic file organizations based on hashing allow the number of buckets to vary dynamically with only localized reorganization Record pointer Record pointer Record pointer Record pointer Record pointer Bucket Record pointer Bucket Record pointer NULL Figure Handling overflow for buckets by chaining. Modifying a specific record’s field value depends on two factors the search condition to locate that specific record and the field to be modified. If the search condition is an equality comparison on the hash field we can locate the record efficiently by using the hashing function otherwise we must do a linear search. A nonhash field can be modified by changing the record and rewriting it in the same bucket. Modifying the hash field means that the record can move to another bucket which requires deletion of the old record followed by insertion of the modified record. Hashing Techniques That Allow Dynamic File Expansion A major drawback of the static hashing scheme just discussed is that the hash address space is fixed. Hence it is difficult to expand or shrink the file dynamically. The schemes described in this section attempt to remedy this situation. The first scheme extendible hashing stores an access structure in addition to the file and Chapter Disk Storage Basic File Structures and Hashing hence is somewhat similar to indexing d bits of a hash value is used as an index to the array to determine a directory entry and the address in that entry determines the bucket in which the corresponding records are stored. However there does not have to be a distinct bucket for each of the directory locations. Several directory locations with the same first d bits for their hash values may contain the same bucket address if all the records that hash to these locations fit in a single bucket. A local depth d stored with each bucket specifies the number of bits on which the bucket contents are based. Figure shows a directory with global depth d The value of d can be increased or decreased by one at a time thus doubling or halving the number of entries in the directory array. Doubling is needed if a bucket whose local depth dis equal to the global depth d overflows. Halving occurs if d dfor all the buckets after some deletions occur. Most record retrievals require two block accesses one to the directory and the other to the bucket. To illustrate bucket splitting suppose that a new inserted record causes overflow in the bucket whose hash values start with third bucket in Figure The records will be distributed between two buckets the first contains all records whose hash values start with and the second all those whose hash values start with Now the two directory locations for and point to the two new distinct buckets. Before the split they pointed to the same bucket. The local depth d of the two new buckets is which is one more than the local depth of the old bucket. If a bucket that overflows and is split used to have a local depth dequal to the global depth d of the directory then the size of the directory must now be doubled so that we can use an extra bit to distinguish the two new buckets. For example if the bucket for records whose hash values start with in Figure overflows the two new buckets need a directory with global depth d because the two buckets are now labeled and and hence their local depths are both The directory size is hence doubled and each of the other original locations in the directory Hashing Techniques Global depth d d Bucket for records whose hash values start with Directory Data file buckets Local depth of each bucket d Bucket for records whose hash values start with d Bucket for records whose hash values start with d Bucket for records whose hash values start with d´ Bucket for records whose hash values start with d´ Bucket for records whose hash values start with Figure Structure of the extendible hashing scheme. is also split into two locations both of which have the same pointer value as did the original location. The main advantage of extendible hashing that makes it attractive is that the performance of the file does not degrade as the file grows as opposed to static external hashing where collisions increase and the corresponding chaining effectively Chapter Disk Storage Basic File Structures and Hashing increases the average number of accesses per key. Additionally no space is allocated in extendible hashing for future growth but additional buckets can be allocated dynamically as needed. The space overhead for the directory table is negligible. The maximum directory size is where k is the number of bits in the hash value. Another advantage is that splitting causes minor reorganization in most cases since only the records in one bucket are redistributed to the two new buckets. The only time reorganization is more expensive is when the directory has to be doubled . A disadvantage is that the directory must be searched before accessing the buckets themselves resulting in two block accesses instead of one in static hashing. This performance penalty is considered minor and thus the scheme is considered quite desirable for dynamic files. Dynamic Hashing. A precursor to extendible hashing was dynamic hashing in which the addresses of the buckets were either the n high order bits or n − highorder bits depending on the total number of keys belonging to the respective bucket. The eventual storage of records in buckets for dynamic hashing is somewhat similar to extendible hashing. The major difference is in the organization of the directory. Whereas extendible hashing uses the notion of global depth for the flat directory and then combines adjacent collapsible buckets into a bucket of local depth d − dynamic hashing maintains a tree structured directory with two types of nodes Internal nodes that have two pointers the left pointer corresponding to the bit and a right pointer corresponding to the bit. Leaf nodes these hold a pointer to the actual bucket with records. An example of the dynamic hashing appears in Figure Four buckets are shown and with high order addresses are shown with high order addresses K mod M this hash function is called the initial hash function hi . Overflow because of collisions is still needed and can be handled by maintaining individual overflow chains for each bucket. However when a collision leads to an overflow record in any file bucket the first bucket in the file bucket split into two buckets the original bucket and a new bucket M at the end of the file. The records originally in bucket are distributed between the two buckets based on a different hashing function K mod A key property of the two hash Hashing Techniques Data File Buckets Bucket for records whose hash values start with Bucket for records whose hash values start with Bucket for records whose hash values start with Bucket for records whose hash values start with Bucket for records whose hash values start with Bucket for records whose hash values start with Directory internal directory node leaf directory node Figure Structure of the dynamic hashing scheme. functions hi and is that any records that hashed to bucket based on hi will hash to either bucket or bucket M based on this is necessary for linear hashing to work. As further collisions lead to overflow records additional buckets are split in the linear order If enough overflows occur all the original file buckets M − will have been split so the file now has instead of M buckets and all buckets use the hash function Hence the records in overflow are eventually redistributed into regular buckets using the function via a delayed split of their buckets. There is no directory only a value n which is initially set to and is incremented by whenever a split occurs is needed to determine which buckets have been split. To retrieve a record with hash key value K first apply the function hi to K if hi n then apply the function on K because the bucket is already split. Initially n indicating that the function hi applies to all buckets n grows linearly as buckets are split. Chapter Disk Storage Basic File Structures and Hashing When n M after being incremented this signifies that all the original buckets have been split and the hash function applies to all records in the file. At this point n is reset to and any new collisions that cause overflow lead to the use of a new hashing function K mod In general a sequence of hashing functions hi+j K mod M is used where j a new hashing function is needed whenever all the buckets M − have been split and n is reset to The search for a record with hash key value K is given by Algorithm Splitting can be controlled by monitoring the file load factor instead of by splitting whenever an overflow occurs. In general the file load factor l can be defined as l r where r is the current number of file records bfr is the maximum number of records that can fit in a bucket and N is the current number of file buckets. Buckets that have been split can also be recombined if the load factor of the file falls below a certain threshold. Blocks are combined linearly and N is decremented appropriately. The file load can be used to trigger both splits and combinations in this manner the file load can be kept within a desired range. Splits can be triggered when the load exceeds a certain threshold say combinations can be triggered when the load falls below another threshold say The main advantages of linear hashing are that it maintains the load factor fairly constantly while the file grows and shrinks and it does not require a Algorithm The Search Procedure for Linear Hashing if n then m ← hj else begin m ← hj if m n then m ← end search the bucket whose hash value is m Other Primary File Organizations Files of Mixed Records The file organizations we have studied so far assume that all records of a particular file are of the same record type. The records could be of EMPLOYEEs PROJECTs STUDENTs or DEPARTMENTs but each file contains records of only one type. In most database applications we encounter situations in which numerous types of entities are interrelated in various ways as we saw in Chapter Relationships among records in various files can be represented by connecting fields. For example a STUDENT record can have a connecting field Majordept whose value gives the details of insertion and deletion into Linear hashed files refer to Litwin and Salzberg concept of foreign keys in the relational data model of related records or by physical pointers. These file organizations typically assign an area of the disk to hold records of more than one type so that records of different types can be physically clustered on disk. If a particular relationship is expected to be used frequently implementing the relationship physically can increase the system’s efficiency at retrieving related records. For example if the query to retrieve a DEPARTMENT record and all records for STUDENTs majoring in that department is frequent it would be desirable to place each DEPARTMENT record and its cluster of STUDENT records contiguously on disk in a mixed file. The concept of physical clustering of object types is used in object DBMSs to store related objects together in a mixed file. To distinguish the records in a mixed file each record has in addition to its field values a record type field which specifies the type of record. This is typically the first field in each record and is used by the system software to determine the type of record it is about to process. Using the catalog information the DBMS can determine the fields of that record type and their sizes in order to interpret the data values in the record. B Trees and Other Data Structures as Primary Organization Other data structures can be used for primary file organizations. For example if both the record size and the number of records in a file are small some DBMSs offer the option of a B tree data structure as the primary file organization. We will describe Btrees in Section when we discuss the use of the B tree data structure for indexing. In general any data structure that can be adapted to the characteristics of disk devices can be used as a primary file organization for record placement on disk. Recently column based storage of data has been proposed as a primary method for storage of relations in relational databases. We will briefly introduce it in Chapter as a possible alternative storage scheme for relational databases. Parallelizing Disk Access Using RAID Technology With the exponential growth in the performance and capacity of semiconductor devices and memories faster microprocessors with larger and larger primary memories are continually becoming available. To match this growth it is natural to Chapter Disk Storage Basic File Structures and Hashing Disk A | A B | B Disk A | A B | B Disk A | A B | B Disk A | A B | B Disk A Disk A Disk A Disk A A | A | A | A | A | A | A | A B | B | B | B | B | B | B | B Data Block A File A Block A Block A Block A Figure Striping of data across multiple disks. Bit level striping across four disks. Block level striping across four disks. expect that secondary storage technology must also take steps to keep up with processor technology in performance and reliability. A major advance in secondary storage technology is represented by the development of RAID which originally stood for Redundant Arrays of Inexpensive Disks. More recently the I in RAID is said to stand for Independent. The RAID idea received a very positive industry endorsement and has been developed into an elaborate set of alternative RAID architectures with corresponding lack of fast access to large shared data sets. The natural solution is a large array of small independent disks acting as a single higher performance logical disk. A concept called data striping is used which utilizes parallelism to improve disk performance. Data striping distributes data transparently over multiple disks to make them appear as a single large fast disk. Figure shows a file distributed or striped over four disks. Striping improves overall I O performance by allowing multiple I Os to be serviced in parallel thus providing high overall transfer rates. Data striping also accomplishes load balancing among disks. Moreover by storing redundant information on disks using parity or some other error correction code reliability can be improved. In Sections and was predicted by Gordon Bell to be about percent every year between and and is now supposed to exceed percent per year. Parallelizing Disk Access Using RAID Technology we discuss how RAID achieves the two important objectives of improved reliability and higher performance. Section discusses RAID organizations and levels. Improving Reliability with RAID For an array of n disks the likelihood of failure is n times as much as that for one disk. Hence if the MTBF of a disk drive is assumed to be hours or about years the MTBF for a bank of disk drives becomes only hours or days . Keeping a single copy of data in such an array of disks will cause a significant loss of reliability. An obvious solution is to employ redundancy of data so that disk failures can be tolerated. The disadvantages are many additional I O operations for write extra computation to maintain redundancy and to do recovery from errors and additional disk capacity to store redundant information. One technique for introducing redundancy is called mirroring or shadowing. Data is written redundantly to two identical physical disks that are treated as one logical disk. When data is read it can be retrieved from the disk with shorter queuing seek and rotational delays. If a disk fails the other disk is used until the first is repaired. Suppose the mean time to repair is hours then the mean time to data loss of a mirrored disk system using disks with MTBF of hours each is hours which is Disk mirroring also doubles the rate at which read requests are handled since a read can go to either disk. The transfer rate of each read however remains the same as that for a single disk. Another solution to the problem of reliability is to store extra information that is not normally needed but that can be used to reconstruct the lost information in case of disk failure. The incorporation of redundancy must consider two problems selecting a technique for computing the redundant information and selecting a method of distributing the redundant information across the disk array. The first problem is addressed by using error correcting codes involving parity bits or specialized codes such as Hamming codes. Under the parity scheme a redundant disk may be considered as having the sum of all the data in the other disks. When a disk fails the missing information can be constructed by a process similar to subtraction. For the second problem the two major approaches are either to store the redundant information on a small number of disks or to distribute it uniformly across all disks. The latter results in better load balancing. The different levels of RAID choose a combination of these options to implement redundancy and improve reliability. Improving Performance with RAID The disk arrays employ the technique of data striping to achieve higher transfer rates. Note that data can be read or written only one block at a time so a typical transfer contains to bytes. Disk striping may be applied at a finer granularity by formulas for MTBF calculations appear in Chen et al. Chapter Disk Storage Basic File Structures and Hashing breaking up a byte of data into bits and spreading the bits to different disks. Thus bit level data striping consists of splitting a byte of data and writing bit j to the jth disk. With bytes eight physical disks may be considered as one logical disk with an eightfold increase in the data transfer rate. Each disk participates in each I O request and the total amount of data read per request is eight times as much. Bit level striping can be generalized to a number of disks that is either a multiple or a factor of eight. Thus in a four disk array bit n goes to the disk which is can be serviced in parallel by separate disks thus decreasing the queuing time of I O requests. Requests that access multiple blocks can be parallelized thus reducing their response time. In general the more the number of disks in an array the larger the potential performance benefit. However assuming independent failures the disk array of disks collectively has the reliability of a single disk. Thus redundancy via error correcting codes and disk mirroring is necessary to provide reliability along with high performance. RAID Organizations and Levels Different RAID organizations were defined based on different combinations of the two factors of granularity of data interleaving and pattern used to compute redundant information. In the initial proposal levels through of RAID were proposed and two additional and added later. RAID level uses data striping has no redundant data and hence has the best write performance since updates do not have to be duplicated. It splits data evenly across two or more disks. However its read performance is not as good as RAID level which uses mirrored disks. In the latter performance improvement is possible by scheduling a read request to the disk with shortest expected seek and rotational delay. RAID level uses memory style redundancy by using Hamming codes which contain parity bits for distinct overlapping subsets of components. Thus in one particular version of this level three redundant disks suffice for four original disks whereas with mirroring as in level would be required. Level includes both error detection and correction although detection is generally not required because broken disks identify themselves. RAID level uses a single parity disk relying on the disk controller to figure out which disk has failed. Levels and use block level data striping with level distributing data and parity information across all disks. Figure shows an illustration of RAID level where parity is shown with subscript p. If one disk fails the missing data is calculated based on the parity available from the remaining disks. Finally RAID level applies the so called P + Q redundancy scheme using Reed Soloman codes to protect against up to two disk failures by using just two redundant disks. New Storage Systems Disk Disk A B C Dp Cp D Bp Ap File A File B File C File D File A File B File C File D Figure Some popular levels of RAID. RAID level Mirroring of data on two disks. RAID level Striping of data with distributed parity across four disks. Rebuilding in case of disk failure is easiest for RAID level Other levels require the reconstruction of a failed disk by reading multiple disks. Level is used for critical applications such as storing logs of transactions. Levels and are preferred for large volume storage with level providing higher transfer rates. Most popular use of RAID technology currently uses level level and level with an extra drive for parity. A combination of multiple RAID levels are also used – for example combines striping and mirroring using a minimum of four disks. Other nonstandard RAID levels include RAID RAID RAID DP RAID S or Parity RAID Matrix RAID RAID K RAID Z RAIDn Linux MD RAID IBM ServeRAID and unRAID. A discussion of these nonstandard levels is beyond the scope of this book. Designers of a RAID setup for a given application mix have to confront many design decisions such as the level of RAID the number of disks the choice of parity schemes and grouping of disks for block level striping. Detailed performance studies on small reads and writes and large reads and writes have been performed. New Storage Systems In this section we describe three recent developments in storage systems that are becoming an integral part of most enterprise’s information system architectures. Storage Area Networks With the rapid growth of electronic commerce Enterprise Resource Planning systems that integrate application data across organizations and data warehouses that keep historical aggregate information . In a SAN online storage peripherals are configured as nodes on a high speed network and can be attached and detached from servers in a very flexible manner. Several companies have emerged as SAN providers and supply their own proprietary topologies. They allow storage systems to be placed at longer distances from the servers and provide different performance and connectivity options. Existing storage management applications can be ported into SAN configurations using Fiber Channel networks that encapsulate the legacy SCSI protocol. As a result the SAN attached devices appear as SCSI devices. Current architectural alternatives for SAN include the following point to point connections between servers and storage systems via fiber channel use of a fiber channel switch to connect multiple RAID systems tape libraries and so on to servers and the use of fiber channel hubs and switches to connect servers and storage systems in different configurations. Organizations can slowly move up from simpler topologies to more complex ones by adding servers and storage devices as needed. We do not provide further details here because they vary among SAN vendors. The main advantages claimed include Flexible many to many connectivity among servers and storage devices using fiber channel hubs and switches Up to km separation between a server and a storage system using appropriate fiber optic cables Better isolation capabilities allowing nondisruptive addition of new peripherals and servers SANs are growing very rapidly but are still faced with many problems such as combining storage options from multiple vendors and dealing with evolving standards of storage management software and hardware. Most major companies are evaluating SANs as a viable option for database storage. Network Attached Storage With the phenomenal growth in digital data particularly generated from multimedia and other enterprise applications the need for high performance storage solutions at low cost has become extremely important. Network attached storage devices are among the storage devices being used for this purpose. These devices are in fact servers that do not provide any of the common server services but simply allow the addition of storage for file sharing. NAS devices allow vast New Storage Systems amounts of hard disk storage space to be added to a network and can make that space available to multiple servers without shutting them down for maintenance and upgrades. NAS devices can reside anywhere on a local area network and may be combined in different configurations. A single hardware device often called the NAS box or NAS head acts as the interface between the NAS system and network clients. These NAS devices require no monitor keyboard or mouse. One or more disk or tape drives can be attached to many NAS systems to increase total capacity. Clients connect to the NAS head rather than to the individual storage devices. An NAS can store any data that appears in the form of files such as e mail boxes Web content remote system backups and so on. In that sense NAS devices are being deployed as a replacement for traditional file servers. NAS systems strive for reliable operation and easy administration. They include built in features such as secure authentication or the automatic sending of e mail alerts in case of error on the device. The NAS devices are being offered with a high degree of scalability reliability flexibility and performance. Such devices typically support RAID levels and Traditional storage area networks differ from NAS in several ways. Specifically SANs often utilize Fiber Channel rather than Ethernet and a SAN often incorporates multiple network devices or endpoints on a self contained or private LAN whereas NAS relies on individual devices connected directly to the existing public LAN. Whereas Windows UNIX and NetWare file servers each demand specific protocol support on the client side NAS systems claim greater operating system independence of clients. iSCSI Storage Systems A new protocol called iSCSI has been proposed recently. It allows clients to send SCSI commands to SCSI storage devices on remote channels. The main advantage of iSCSI is that it does not require the special cabling needed by Fiber Channel and it can run over longer distances using existing network infrastructure. By carrying SCSI commands over IP networks iSCSI facilitates data transfers over intranets and manages storage over long distances. It can transfer data over local area networks wide area networks or the Internet. iSCSI works as follows. When a DBMS needs to access data the operating system generates the appropriate SCSI commands and data request which then go through encapsulation and if necessary encryption procedures. A packet header is added before the resulting IP packets are transmitted over an Ethernet connection. When a packet is received it is decrypted and disassembled separating the SCSI commands and request. The SCSI commands go via the SCSI controller to the SCSI storage device. Because iSCSI is bidirectional the protocol can also be used to return data in response to the original request. Cisco and IBM have marketed switches and routers based on this technology. iSCSI storage has mainly impacted small and medium sized businesses because of its combination of simplicity low cost and the functionality of iSCSI devices. It allows them not to learn the ins and outs of Fiber Channel technology and Chapter Disk Storage Basic File Structures and Hashing instead benefit from their familiarity with the IP protocol and Ethernet hardware. iSCSI implementations in the data centers of very large enterprise businesses are slow in development due to their prior investment in Fiber Channel based SANs. iSCSI is one of two main approaches to storage data transmission over IP networks. The other method Fiber Channel over IP translates Fiber Channel control codes and data into IP packets for transmission between geographically distant Fiber Channel storage area networks. This protocol known also as Fiber Channel tunneling or storage tunneling can only be used in conjunction with Fiber Channel technology whereas iSCSI can run over existing Ethernet networks. The latest idea to enter the enterprise IP storage race is Fiber Channel over Ethernet which can be thought of as iSCSI without the IP. It uses many elements of SCSI and FC but it does not include TCP IP components. This promises excellent performance especially on Gigabit Ethernet and is relatively easy for vendors to add to their products. Summary We began this chapter by discussing the characteristics of memory hierarchies and then concentrated on secondary storage devices. In particular we focused on magnetic disks because they are used most often to store online database files. Data on disk is stored in blocks accessing a disk block is expensive because of the seek time rotational delay and block transfer time. To reduce the average block access time double buffering can be used when accessing consecutive disk blocks. We presented different ways of storing file records on disk. File records are grouped into disk blocks and can be fixed length or variable length spanned or unspanned and of the same record type or mixed types. We discussed the file header which describes the record formats and keeps track of the disk addresses of the file blocks. Information in the file header is used by system software accessing the file records. Then we presented a set of typical commands for accessing individual file records and discussed the concept of the current record of a file. We discussed how complex record search conditions are transformed into simple search conditions that are used to locate records in the file. Three primary file organizations were then discussed unordered ordered and hashed. Unordered files require a linear search to locate records but record insertion is very simple. We discussed the deletion problem and the use of deletion markers. Ordered files shorten the time required to read records in order of the ordering field. The time required to search for an arbitrary record given the value of its ordering key field is also reduced if a binary search is used. However maintaining the records in order makes insertion very expensive thus the technique of using an unordered overflow file to reduce the cost of record insertion was discussed. Overflow records are merged with the master file periodically during file reorganization. Review Questions Hashing provides very fast access to an arbitrary record of a file given the value of its hash key. The most suitable method for external hashing is the bucket technique with one or more contiguous blocks corresponding to each bucket. Collisions causing bucket overflow are handled by chaining. Access on any nonhash field is slow and so is ordered access of the records on any field. We discussed three hashing techniques for files that grow and shrink in the number of records dynamically extendible dynamic and linear hashing. The first two use the higher order bits of the hash address to organize a directory. Linear hashing is geared to keep the load factor of the file within a given range and adds new buckets linearly. We briefly discussed other possibilities for primary file organizations such as Btrees and files of mixed records which implement relationships among records of different types physically as part of the storage structure. We reviewed the recent advances in disk technology represented by RAID Disks which has become a standard technique in large enterprises to provide better reliability and fault tolerance features in storage. Finally we reviewed three currently popular options in enterprise storage systems storage area networks network attached storage and iSCSI storage systems. Review Questions What is the difference between primary and secondary storage Why are disks not tapes used to store online database files Define the following terms disk disk pack track block cylinder sector interblock gap read write head. Discuss the process of disk initialization. Discuss the mechanism used to read data from or write data to the disk. What are the components of a disk block address Why is accessing a disk block expensive Discuss the time components involved in accessing a disk block. How does double buffering improve block access time What are the reasons for having variable length records What types of separator characters are needed for each Discuss the techniques for allocating file blocks on disk. What is the difference between a file organization and an access method What is the difference between static and dynamic files What are the typical record at a time operations for accessing a file Which of these depend on the current file record Discuss the techniques for record deletion. Chapter Disk Storage Basic File Structures and Hashing Discuss the advantages and disadvantages of using an unordered file an ordered file and a static hash file with buckets and chaining. Which operations can be performed efficiently on each of these organizations and which operations are expensive Discuss the techniques for allowing a hash file to expand and shrink dynamically. What are the advantages and disadvantages of each What is the difference between the directories of extendible and dynamic hashing What are mixed files used for What are other types of primary file organizations Describe the mismatch between processor and disk technologies. What are the main goals of the RAID technology How does it achieve them How does disk mirroring help improve reliability Give a quantitative example. What characterizes the levels in RAID organization What are the highlights of the popular RAID levels and What are storage area networks What flexibility and advantages do they offer Describe the main features of network attached storage as an enterprise storage solution. How have new iSCSI systems improved the applicability of storage area networks Exercises Consider a disk with the following characteristics block size B bytes interblock gap size G bytes number of blocks per track number of tracks per surface A disk pack consists of double sided disks. a. What is the total capacity of a track and what is its useful capacity b. How many cylinders are there c. What are the total capacity and the useful capacity of a cylinder d. What are the total capacity and the useful capacity of a disk pack e. Suppose that the disk drive rotates the disk pack at a speed of rpm what are the transfer rate in bytes msec and the block transfer time in msec What is the average rotational delay in msec What is the bulk transfer rate f. Suppose that the average seek time is msec. How much time does it take in msec to locate and transfer a single block given its block address Exercises g. Calculate the average time it would take to transfer random blocks and compare this with the time it would take to transfer consecutive blocks using double buffering to save seek time and rotational delay. A file has r STUDENT records of fixed length. Each record has the following fields Name bytes Ssn bytes Address bytes PHONE bytes Birthdate bytes Sex byte Majordeptcode bytes Minordeptcode bytes Classcode bytes integer and Degreeprogram bytes . An additional byte is used as a deletion marker. The file is stored on the disk whose parameters are given in Exercise a. Calculate the record size R in bytes. b. Calculate the blocking factor bfr and the number of file blocks b assuming an unspanned organization. c. Calculate the average time it takes to find a record by doing a linear search on the file if the file blocks are stored contiguously and double buffering is used the file blocks are not stored contiguously. d. Assume that the file is ordered by Ssn by doing a binary search calculate the time it takes to search for a record given its Ssn value. Suppose that only percent of the STUDENT records from Exercise have a value for Phone percent for Majordeptcode percent for Minordeptcode and percent for Degreeprogram and suppose that we use a variable length record file. Each record has a field type for each field in the record plus the deletion marker and a end ofrecord marker. Suppose that we use a spanned record organization where each block has a pointer to the next block . a. Calculate the average record length R in bytes. b. Calculate the number of blocks needed for the file. Suppose that a disk unit has the following parameters seek time s msec rotational delay rd msec block transfer time btt msec block size B bytes interblock gap size G bytes. An EMPLOYEE file has the following fields Ssn bytes Lastname bytes Firstname bytes Middleinit byte Birthdate bytes Address bytes Phone bytes Supervisorssn bytes Department bytes Jobcode bytes deletion marker byte. The EMPLOYEE file has r records fixed length format and unspanned blocking. Write appropriate formulas and calculate the following values for the above EMPLOYEE file a. The record size R the blocking factor bfr and the number of disk blocks b. b. Calculate the wasted space in each disk block because of the unspanned organization. c. Calculate the transfer rate tr and the bulk transfer rate btr for this disk unit . Chapter Disk Storage Basic File Structures and Hashing d. Calculate the average number of block accesses needed to search for an arbitrary record in the file using linear search. e. Calculate in msec the average time needed to search for an arbitrary record in the file using linear search if the file blocks are stored on consecutive disk blocks and double buffering is used. f. Calculate in msec the average time needed to search for an arbitrary record in the file using linear search if the file blocks are not stored on consecutive disk blocks. g. Assume that the records are ordered via some key field. Calculate the average number of block accesses and the average time needed to search for an arbitrary record in the file using binary search. A PARTS file with Part# as the hash key includes records with the following Part# values and The file uses eight buckets numbered to Each bucket is one disk block and holds two records. Load these records into the file in the given order using the hash function h K mod Calculate the average number of block accesses for a random retrieval on Part#. Load the records of Exercise into expandable hash files based on extendible hashing. Show the structure of the directory at each step and the global and local depths. Use the hash function h K mod Load the records of Exercise into an expandable hash file using linear hashing. Start with a single disk block using the hash function K mod and show how the file grows and how the hash functions change as the records are inserted. Assume that blocks are split whenever an overflow occurs and show the value of n at each stage. Compare the file commands listed in Section to those available on a file access method you are familiar with. Suppose that we have an unordered file of fixed length records that uses an unspanned record organization. Outline algorithms for insertion deletion and modification of a file record. State any assumptions you make. Suppose that we have an ordered file of fixed length records and an unordered overflow file to handle insertion. Both files use unspanned records. Outline algorithms for insertion deletion and modification of a file record and for reorganizing the file. State any assumptions you make. Can you think of techniques other than an unordered overflow file that can be used to make insertions in an ordered file more efficient Suppose that we have a hash file of fixed length records and suppose that overflow is handled by chaining. Outline algorithms for insertion deletion and modification of a file record. State any assumptions you make. Exercises Can you think of techniques other than chaining to handle bucket overflow in external hashing Write pseudocode for the insertion algorithms for linear hashing and for extendible hashing. Write program code to access individual fields of records under each of the following circumstances. For each case state the assumptions you make concerning pointers separator characters and so on. Determine the type of information needed in the file header in order for your code to be general in each case. a. Fixed length records with unspanned blocking b. Fixed length records with spanned blocking c. Variable length records with variable length fields and spanned blocking d. Variable length records with repeating groups and spanned blocking e. Variable length records with optional fields and spanned blocking f. Variable length records that allow all three cases in parts c d and e Suppose that a file initially contains r records of R bytes each in an unsorted file. The block size B bytes the average seek time s ms the average rotational latency rd ms and the block transfer time btt ms. Assume that record is deleted for every records added until the total number of active records is a. How many block transfers are needed to reorganize the file b. How long does it take to find a record right before reorganization c. How long does it take to find a record right after reorganization Suppose we have a sequential file of records where each record is bytes. Assume that B bytes s ms rd ms and btt ms. Suppose we want to make X independent random record reads from the file. We could make X random block reads or we could perform one exhaustive read of the entire file looking for those X records. The question is to decide when it would be more efficient to perform one exhaustive read of the entire file than to perform X individual random reads. That is what is the value for X when an exhaustive read of the file is more efficient than random X reads Develop this as a function of X. Suppose that a static hash file initially has buckets in the primary area and that records are inserted that create an overflow area of buckets. If we reorganize the hash file we can assume that most of the overflow is eliminated. If the cost of reorganizing the file is the cost of the bucket transfers and the only periodic file operation is the fetch operation then how many times would we have to perform a fetch to make the reorganization cost effective That is the reorganization cost and subsequent search cost are less than the search cost before reorganization. Support your answer. Assume s ms rd ms and btt ms. Chapter Disk Storage Basic File Structures and Hashing Suppose we want to create a linear hash file with a file load factor of and a blocking factor of records per bucket which is to contain records initially. a. How many buckets should we allocate in the primary area b. What should be the number of bits used for bucket addresses Selected Bibliography Wiederhold has a detailed discussion and analysis of secondary storage devices and file organizations as a part of database design. Optical disks are described in Berg and Roth and analyzed in Ford and Christodoulakis Flash memory is discussed by Dipert and Levy Ruemmler and Wilkes present a survey of the magnetic disk technology. Most textbooks on databases include discussions of the material presented here. Most data structures textbooks including Knuth discuss static hashing in more detail Knuth has a complete discussion of hash functions and collision resolution techniques as well as of their performance comparison. Knuth also offers a detailed discussion of techniques for sorting external files. Textbooks on file structures include Claybrook Smith and Barnes and Salzberg they discuss additional file organizations including tree structured files and have detailed algorithms for operations on files. Salzberg et al. describe a distributed external sorting algorithm. File organizations with a high degree of fault tolerance are described by Bitton and Gray and by Gray et al. Disk striping was proposed in Salem and Garcia Molina The first paper on redundant arrays of inexpensive disks is by Patterson et al. Chen and Patterson and the excellent survey of RAID by Chen et al. are additional references. Grochowski and Hoyt discuss future trends in disk drives. Various formulas for the RAID architecture appear in Chen et al. Morris is an early paper on hashing. Extendible hashing is described in Fagin et al. Linear hashing is described by Litwin Algorithms for insertion and deletion for linear hashing are discussed with illustrations in Salzberg Dynamic hashing which we briefly introduced was proposed by Larson There are many proposed variations for extendible and linear hashing for examples see Cesarini and Soda Du and Tong and Hachem and Berra Details of disk storage devices can be found at manufacturer sites . Indexing Structures for Files I n this chapter we assume that a file already exists with some primary organization such as the unordered ordered or hashed organizations that were described in Chapter We will describe additional auxiliary access structures called indexes which are used to speed up the retrieval of records in response to certain search conditions. The index structures are additional files on disk that provide secondary access paths which provide alternative ways to access the records without affecting the physical placement of records in the primary data file on disk. They enable efficient access to records based on the indexing fields that are used to construct the index. Basically any field of the file can be used to create an index and multiple indexes on different fields as well as indexes on multiple fields can be constructed on the same file. A variety of indexes are possible each of them uses a particular data structure to speed up the search. To find a record or records in the data file based on a search condition on an indexing field the index is searched which leads to pointers to one or more disk blocks in the data file where the required records are located. The most prevalent types of indexes are based on ordered files and tree data structures . Indexes can also be constructed based on hashing or other search data structures. We also discuss indexes that are vectors of bits called bitmap indexes. We describe different types of single level ordered indexes primary secondary and clustering in Section By viewing a single level index as an ordered file one can develop additional indexes for it giving rise to the concept of multilevel indexes. A popular indexing scheme called ISAM is based on this idea. We discuss multilevel tree structured indexes in Section In Section we describe B trees and B+ trees which are data structures that are commonly used in DBMSs to implement dynamically changing multilevel indexes. B+ trees have become a commonly accepted default structure for chapter Chapter Indexing Structures for Files generating indexes on demand in most relational DBMSs. Section is devoted to alternative ways to access data based on a combination of multiple keys. In Section we discuss hash indexes and introduce the concept of logical indexes which give an additional level of indirection from physical indexes allowing for the physical index to be flexible and extensible in its organization. In Section we discuss multikey indexing and bitmap indexes used for searching on one or more keys. Section summarizes the chapter. Types of Single Level Ordered Indexes The idea behind an ordered index is similar to that behind the index used in a textbook which lists important terms at the end of the book in alphabetical order along with a list of page numbers where the term appears in the book. We can search the book index for a certain term in the textbook to find a list of addresses page numbers in this case and use these addresses to locate the specified pages first and then search for the term on each specified page. The alternative if no other guidance is given would be to sift slowly through the whole textbook word by word to find the term we are interested in this corresponds to doing a linear search which scans the whole file. Of course most books do have additional information such as chapter and section titles which help us find a term without having to search through the whole book. However the index is the only exact indication of the pages where each term occurs in the book. For a file with a given record structure consisting of several fields an index access structure is usually defined on a single field of a file called an indexing field . There is one index entry in the index file for each block in the data file. Each index entry has the value of the primary key field for the first record in a block and a pointer to that block as its two field values. We will refer to the two field values of index entry i as K P . To create a primary index on the ordered file shown in Figure we use the Name field as primary key because that is the ordering key field of the file . Each entry in the index has a Name value and a pointer. The first three index entries are as follows address of block address of block address of block Figure illustrates this primary index. The total number of entries in the index is the same as the number of disk blocks in the ordered data file. The first record in each block of the data file is called the anchor record of the block or simply the block anchor. Indexes can also be characterized as dense or sparse. A dense index has an index entry for every search key value in the data file. A sparse index on the other hand has index entries for only some of the search values. A sparse index has fewer entries than the number of records in the file. Thus a primary index is a nondense index since it includes an entry for each disk block of the data file and the keys of its anchor record rather than for every search value . The index file for a primary index occupies a much smaller space than does the data file for two reasons. First there are fewer index entries than there are records in the data file. Second each index entry is typically smaller in size than a data record because it has only two fields consequently more index entries than data records can fit in one block. Therefore a binary search on the index file requires fewer block accesses than a binary search on the data file. Referring to Table note that the binary search for an ordered data file required block accesses. But if the primary index file contains only bi blocks then to locate a record with a search key can use a scheme similar to the one described here with the last record in each block as the block anchor. This slightly improves the efficiency of the search algorithm. Chapter Indexing Structures for Files Index f le P entr es Block anchor primary key value Block pointer Name Aaron Ed Abbot Diane Acosta Marc Adams John Adams Robin Akers Jan Alexander Ed Alfred Bob Allen Sam Allen Troy Anders Keith Anderson Rob Anderson Zach Angel Joe Archer Sue Arnold Mack Arnold Steven Atkins Timothy Wong James Wood Donald Woods Manny Wright Pam Wyatt Charles Zimmer Byron Aaron Ed Adams John Alexander Ed Allen Troy Anderson Zach Arnold Mack Wong James Wright Pam Ssn Birthdate Job Salary Sex Figure Primary index on the ordering key field of the file shown in Figure Types of Single Level Ordered Indexes value requires a binary search of that index and access to the block containing that record a total of + accesses. A record whose primary key value is K lies in the block whose address is P where K ≤ K K⎦ records per block. The number of blocks needed for the file is b ⎡⎤ blocks. A binary search on the data file would need approximately block accesses. Now suppose that the ordering key field of the file is V bytes long a block pointer is P bytes long and we have constructed a primary index for the file. The size of each index entry is Ri + bytes so the blocking factor for the index is bfri ⎣⎦ entries per block. The total number of index entries ri is equal to the number of blocks in the data file which is The number of index blocks is hence bi ⎡⎤ blocks. To perform a binary search on the index file would need ⎤ block accesses. To search for a record using the index we need one additional block access to the data file for a total of + block accesses an improvement over binary search on the data file which required disk block accesses. A major problem with a primary index as with any ordered file is insertion and deletion of records. With a primary index the problem is compounded because if we attempt to insert a record in its correct position in the data file we must not only move records to make space for the new record but also change some index entries since moving records will change the anchor records of some blocks. Using an unordered overflow file as discussed in Section can reduce this problem. Another possibility is to use a linked list of overflow records for each block in the data file. This is similar to the method of dealing with overflow records described with hashing in Section Records within each block and its overflow linked list can be sorted to improve retrieval time. Record deletion is handled using deletion markers. Clustering Indexes If file records are physically ordered on a nonkey field which does not have a distinct value for each record that field is called the clustering field and the data file that the above formula would not be correct if the data file were ordered on a nonkey field in that case the same index value in the block anchor could be repeated in the last records of the previous block. Chapter Indexing Structures for Files is called a clustered file. We can create a different type of index called a clustering index to speed up retrieval of all the records that have the same value for the clustering field. This differs from a primary index which requires that the ordering field of the data file have a distinct value for each record. A clustering index is also an ordered file with two fields the first field is of the same type as the clustering field of the data file and the second field is a disk block pointer. There is one entry in the clustering index for each distinct value of the clustering field and it contains the value and a pointer to the first block in the data file that has a record with that value for its clustering field. Figure shows an example. Notice that record insertion and deletion still cause problems because the data records are physically ordered. To alleviate the problem of insertion it is common to reserve a whole block for each value of the clustering field all records with that value are placed in the block . This makes insertion and deletion relatively straightforward. Figure shows this scheme. A clustering index is another example of a nondense index because it has an entry for every distinct value of the indexing field which is a nonkey by definition and hence has duplicate values rather than a unique value for every record in the file. There is some similarity between Figures and and Figures and An index is somewhat similar to dynamic hashing can be created for the same file each represents an additional means of accessing that file based on some specific field. First we consider a secondary index access structure on a key field that has a distinct value for every record. Such a field is sometimes called a secondary key in the relational model this would correspond to any UNIQUE key attribute or to the primary key attribute of a table. In this case there is one index entry for each record in the data file which contains the value of the field for the record and a pointer either to the block in which the record is stored or to the record itself. Hence such an index is dense. Types of Single Level Ordered Indexes Data f le Deptnumber Name Ssn Birthdate Salary Job Index f le P entr es Clustering field value Block pointer Figure A clustering index on the Deptnumber ordering nonkey field of an EMPLOYEE file. Again we refer to the two field values of index entry i as K P . The entries are ordered by value of K so we can perform a binary search. Because the records of the data file are not physically ordered by values of the secondary key field we cannot use block anchors. That is why an index entry is created for each record in the data Chapter Indexing Structures for Files Data f le Block pointer NULL pointer Deptnumber Name Ssn Birthdate Salary Job Block pointer Block pointer Block pointer Block pointer Block pointer Block pointer Block pointer Block pointer NULL pointer NULL pointer NULL pointer NULL pointer NULL pointer NULL pointer Index f le P entr es Clustering field value Block pointer Figure Clustering index with a separate block cluster for each group of records that share the same value for the clustering field. Types of Single Level Ordered Indexes Data f le Indexing field Index f le P entr es Index field value Block pointer Figure A dense secondary index on a nonordering key field of a file. file rather than for each block as in the case of a primary index. Figure illustrates a secondary index in which the pointers P in the index entries are block pointers not record pointers. Once the appropriate disk block is transferred to a main memory buffer a search for the desired record within the block can be carried out. Chapter Indexing Structures for Files A secondary index usually needs more storage space and longer search time than does a primary index because of its larger number of entries. However the improvement in search time for an arbitrary record is much greater for a secondary index than for a primary index since we would have to do a linear search on the data file if the secondary index did not exist. For a primary index we could still use a binary search on the main file even if the index did not exist. Example illustrates the improvement in number of blocks accessed. Example Consider the file of Example with r fixed length records of size R bytes stored on a disk with block size B bytes. The file has b blocks as calculated in Example Suppose we want to search for a record with a specific value for the secondary key a nonordering key field of the file that is V bytes long. Without the secondary index to do a linear search on the file would require block accesses on the average. Suppose that we construct a secondary index on that nonordering key field of the file. As in Example a block pointer is P bytes long so each index entry is Ri + bytes and the blocking factor for the index is bfri ⎣⎦ entries per block. In a dense secondary index such as this the total number of index entries ri is equal to the number of records in the data file which is The number of blocks needed for the index is hence bi ⎡⎤ blocks. A binary search on this secondary index needs ⎤ block accesses. To search for a record using the index we need an additional block access to the data file for a total of + block accesses a vast improvement over the block accesses needed on the average for a linear search but slightly worse than the block accesses required for the primary index. This difference arose because the primary index was nondense and hence shorter with only blocks in length. We can also create a secondary index on a nonkey nonordering field of a file. In this case numerous records in the data file can have the same value for the indexing field. There are several options for implementing such an index Option is to include duplicate index entries with the same K value one for each record. This would be a dense index. Option is to have variable length records for the index entries with a repeating field for the pointer. We keep a list of pointers P in the index entry for K one pointer to each block that contains a record whose indexing field value equals K. In either option or option the binary search algorithm on the index must be modified appropriately to account for a variable number of index entries per index key value. Option which is more commonly used is to keep the index entries themselves at a fixed length and have a single entry for each index field value but to create an extra level of indirection to handle the multiple pointers. In this nondense scheme the pointer P in index entry K P points to a disk block which contains a set of record pointers each record pointer in that disk block points to one of the data file records with value K for the indexing field. If some value K occurs in too many records so that their record pointers cannot fit in a single disk block a cluster or linked list of blocks is Types of Single Level Ordered Indexes Data f le Deptnumber Name Ssn Birthdate Salary Job Blocks of record po nters Index f le P entr es Field value Block pointer Figure A secondary index on a nonkey field implemented using one level of indirection so that index entries are of fixed length and have unique field values. used. This technique is illustrated in Figure Retrieval via the index requires one or more additional block accesses because of the extra level but the algorithms for searching the index and for inserting of new records in the data file are straightforward. In addition retrievals on complex selection conditions may be handled by referring to the record pointers without having to retrieve many unnecessary records from the data file Indexing field is nonkey Clustering index Secondary index Table Properties of Index Types Type of Index Number of Index Entries Dense or Nondense Block Anchoring on the Data File Primary Number of blocks in data file Nondense Yes Clustering Number of distinct index field values Nondense Yes noa Secondary Number of records in data file Dense No Secondary Number of recordsb or number of distinct index field valuesc Dense or Nondense No aYes if every distinct value of the ordering field starts a new block no otherwise. bFor option cFor options and Multilevel Indexes Multilevel Indexes The indexing schemes we have described thus far involve an ordered index file. A binary search is applied to the index to locate pointers to a disk block or to a record in the file having a specific index field value. A binary search requires approximately block accesses for an index with bi blocks because each step of the algorithm reduces the part of the index file that we continue to search by a factor of This is why we take the log function to the base The idea behind a multilevel index is to reduce the part of the index that we continue to search by bfri the blocking factor for the index which is larger than Hence the search space is reduced much faster. The value bfri is called the fan out of the multilevel index and we will refer to it by the symbol fo. Whereas we divide the record search space into two halves at each step during a binary search we divide it n ways at each search step using the multilevel index. Searching a multilevel index requires approximately block accesses which is a substantially smaller number than for a binary search if the fan out is larger than In most cases the fan out is much larger than A multilevel index considers the index file which we will now refer to as the first level of a multilevel index as an ordered file with a distinct value for each K. Therefore by considering the first level index file as a sorted data file we can create a primary index for the first level this index to the first level is called the second level of the multilevel index. Because the second level is a primary index we can use block anchors so that the second level has one entry for each block of the first level. The blocking factor bfri for the second level and for all subsequent levels is the same as that for the first level index because all index entries are the same size each has one field value and one block address. If the first level has entries and the blocking factor which is also the fan out for the index is bfri fo then the first level needs blocks which is therefore the number of entries needed at the second level of the index. We can repeat this process for the second level. The third level which is a primary index for the second level has an entry for each second level block so the number of third level entries is Notice that we require a second level only if the first level needs more than one block of disk storage and similarly we require a third level only if the second level needs more than one block. We can repeat the preceding process until all the entries of some index level t fit in a single block. This block at the tth level is called the top index Each level reduces the number of entries at the previous level by a factor of fo the index fan out so we can use the formula ≤ to calculate t. Hence a multilevel index with first level entries will have approximately t levels where t When searching the numbering scheme for index levels used here is the reverse of the way levels are commonly defined for tree data structures. In tree data structures t is referred to as level t – is level and so on. Chapter Indexing Structures for Files index a single disk block is retrieved at each level. Hence t disk blocks are accessed for an index search where t is the number of index levels. The multilevel scheme described here can be used on any type of index whether it is primary clustering or secondary as long as the first level index has distinct values for K and fixed length entries. Figure shows a multilevel index built over a primary index. Example illustrates the improvement in number of blocks accessed when a multilevel index is used to search for a record. Example Suppose that the dense secondary index of Example is converted into a multilevel index. We calculated the index blocking factor bfri index entries per block which is also the fan out fo for the multilevel index the number of firstlevel blocks blocks was also calculated. The number of second level blocks will be blocks and the number of third level blocks will be block. Hence the third level is the top level of the index and t To access a record by searching the multilevel index we must access one block at each level plus one block from the data file so we need t + + block accesses. Compare this to Example where block accesses were needed when a single level index and binary search were used. Notice that we could also have a multilevel primary index which would be nondense. Exercise illustrates this case where we must access the data block from the file before we can determine whether the record being searched for is in the file. For a dense index this can be determined by accessing the first index level since there is an index entry for every record in the file. A common file organization used in business data processing is an ordered file with a multilevel primary index on its ordering key field. Such an organization is called an indexed sequential file and was used in a large number of early IBM systems. IBM’s ISAM organization incorporates a two level index that is closely related to the organization of the disk in terms of cylinders and tracks Pj and we search for a record whose primary key value is K. We assume that any overflow records are ignored. If the record is in the file there must be some entry at level with f K + and the record will be in the block of the data file whose address is Exercise discusses modifying the search algorithm for other types of indexes. Multilevel Indexes Data f le Primary key field Second level Two level ndex First level Figure A two level primary index resembling ISAM organization. Chapter Indexing Structures for Files Algorithm Searching a Nondense Multilevel Primary Index with t Levels p ← address of top level block of index for j ← t step – to do begin read the index block whose address is p search block p for entry i such that Kj ≤ K Kj is the last entry in the block it is sufficient to satisfy Kj ≤ K p ← Pj end read the data file block whose address is p search block p for record with key K As we have seen a multilevel index reduces the number of blocks accessed when searching for a record given its indexing field value. We are still faced with the problems of dealing with index insertions and deletions because all index levels are physically ordered files. To retain the benefits of using multilevel indexing while reducing index insertion and deletion problems designers adopted a multilevel index called a dynamic multilevel index that leaves some space in each of its blocks for inserting new entries and uses appropriate insertion deletion algorithms for creating and deleting new index blocks when the data file grows and shrinks. It is often implemented by using data structures called B trees and B+ trees which we describe in the next section. Dynamic Multilevel Indexes Using B Trees and B+ Trees B trees and B+ trees are special cases of the well known search data structure known as a tree. We briefly introduce the terminology used in discussing tree data structures. A tree is formed of nodes. Each node in the tree except for a special node called the root has one parent node and zero or more child nodes. The root node has no parent. A node that does not have any child nodes is called a leaf node a nonleaf node is called an internal node. The level of a node is always one more than the level of its parent with the level of the root node being zero. A subtree of a node consists of that node and all its descendant nodes its child nodes the child nodes of its child nodes and so on. A precise recursive definition of a subtree is that it consists of a node n and the subtrees of all the child nodes of n. Figure illustrates a tree data structure. In this figure the root node is A and its child nodes are B C and D. Nodes E J C G H and K are leaf nodes. Since the leaf nodes are at different levels of the tree this tree is called unbalanced. standard definition of the level of a tree node which we use throughout Section is different from the one we gave for multilevel indexes in Section Dynamic Multilevel Indexes Using B Trees and B+ Trees A B C Subtree for node B Root node and each Ki is a search value from some Chapter Indexing Structures for Files P P K K X X K X K X X K X Figure . . . . . . P K K P A node in a search tree with pointers to subtrees below it. Tree node pointer Null tree pointer Figure A search tree of order p ordered set of values. All search values are assumed to be Figure illustrates a node in a search tree. Two constraints must hold at all times on the search tree Within each node For all values X in the subtree pointed at by Pi we have X Ki for i q X Ki for i and X for i q . Each key value in the tree is associated with a pointer to the record in the data file having that value. Alternatively the pointer could be to the disk block containing that record. The search tree itself can be stored on disk by assigning each tree node to a disk block. When a new record is inserted in the file we must update the search tree by inserting an entry in the tree containing the search field value of the new record and a pointer to the new record. restriction can be relaxed. If the index is on a nonkey field duplicate search values may exist and the node structure and the navigation rules for the tree may be modified. Dynamic Multilevel Indexes Using B Trees and B+ Trees Algorithms are necessary for inserting and deleting search values into and from the search tree while maintaining the preceding two constraints. In general these algorithms do not guarantee that a search tree is balanced meaning that all of its leaf nodes are at the same The tree in Figure is not balanced because it has leaf nodes at levels and The goals for balancing a search tree are as follows To guarantee that nodes are evenly distributed so that the depth of the tree is minimized for the given set of keys and that the tree does not get skewed with some nodes being at very deep levels To make the search speed uniform so that the average time to find any random key is roughly the same While minimizing the number of levels in the tree is one goal another implicit goal is to make sure that the index tree does not need too much restructuring as records are inserted into and deleted from the main file. Thus we want the nodes to be as full as possible and do not want any nodes to be empty if there are too many deletions. Record deletion may leave some nodes in the tree nearly empty thus wasting storage space and increasing the number of levels. The B tree addresses both of these problems by specifying additional constraints on the search tree. B Trees. The B tree has additional constraints that ensure that the tree is always balanced and that the space wasted by deletion if any never becomes excessive. The algorithms for insertion and deletion though become more complex in order to maintain these constraints. Nonetheless most insertions and deletions are simple processes they become complicated only under special circumstances namely whenever we attempt an insertion into a node that is already full or a deletion from a node that makes it less than half full. More formally a B tree of order p when used as an access structure on a key field to search for records in a data file can be defined as follows Each internal node in the B tree . Within each node For all search key field values X in the subtree pointed at by Pi P X Data pointer Data pointer Data pointer Tree node pointer Data pointer Null tree pointer Data pointer P K Pr Pr K X X K K X K K X . . . . . . Pr P K Pr P Tree pointer Figure B tree structures. A node in a B tree with q – search values. A B tree of order p values were inserted in the order Each node except the root and leaf nodes has at least tree pointers. The root node has at least two tree pointers unless it is the only node in the tree. A node with q tree pointers q ≤ p has q – search key field values . All leaf nodes are at the same level. Leaf nodes have the same structure as internal nodes except that all of their tree pointers Pi are NULL. Figure illustrates a B tree of order p Notice that all search values K in the B tree are unique because we assumed that the tree is used as an access structure on a key field. If we use a B tree on a nonkey field we must change the definition of the file pointers Pri to point to a block or a cluster of blocks that contain the pointers to the file records. This extra level of indirection is similar to option discussed in Section for secondary indexes. A B tree starts with a single root node at level . Once the root node is full with p – search key values and we attempt to insert another entry in the tree the root node splits into two nodes at level Only the middle value is kept in the root node and the rest of the values are split evenly Dynamic Multilevel Indexes Using B Trees and B+ Trees between the other two nodes. When a nonroot node is full and a new entry is inserted into it that node is split into two nodes at the same level and the middle entry is moved to the parent node along with two pointers to the new split nodes. If the parent node is full it is also split. Splitting can propagate all the way to the root node creating a new level if the root is split. We do not discuss algorithms for Btrees in detail in this but we outline search and insertion procedures for B+ trees in the next section. If deletion of a value causes a node to be less than half full it is combined with its neighboring nodes and this can also propagate all the way to the root. Hence deletion can reduce the number of tree levels. It has been shown by analysis and simulation that after numerous random insertions and deletions on a B tree the nodes are approximately percent full when the number of values in the tree stabilizes. This is also true of B+ trees. If this happens node splitting and combining will occur only rarely so insertion and deletion become quite efficient. If the number of values grows the tree will expand without a problem although splitting of nodes may occur so some insertions will take more time. Each B tree node can have at most p tree pointers p – data pointers and p – search key field values if the search field is a key field. For a nonkey search field the pointer points to a block containing pointers to the data file records creating an extra level of indirection. The leaf nodes of the B+ tree are usually linked to provide ordered access on the search field to the records. These leaf nodes are similar to the first level of an index. Internal nodes of the B+ tree correspond to the other levels of a multilevel index. Some search field values from the leaf nodes are repeated in the internal nodes of the B+ tree to guide the search. The structure of the internal nodes of a B+ tree of order p but the principles remain the same. Dynamic Multilevel Indexes Using B Trees and B+ Trees Pointer to next leaf node in tree Data pointer Data pointer Data pointer Data pointer K Pr K Pr Pr Pr K K Pnext . . . . . . P K K X X X X K X . . . . . . P K K P Tree pointer Tree pointer Tree pointer X K Figure The nodes of a B+ tree. Internal node of a B+ tree with q – search values. Leaf node of a B+ tree with q – search values and q – data pointers. Within each leaf node ≤ q ≤ p. Each Pri is a data pointer that points to the record whose search field value is Ki or to a file block containing the record . Each leaf node has at least values. All leaf nodes are at the same level. The pointers in internal nodes are tree pointers to blocks that are tree nodes whereas the pointers in leaf nodes are data pointers to the data file records or blocks except for the Pnext pointer which is a tree pointer to the next leaf node. By starting at the leftmost leaf node it is possible to traverse leaf nodes as a linked list using the Pnext pointers. This provides ordered access to the data records on the indexing field. A Pprevious pointer can also be included. For a B+ tree on a nonkey field an extra level of indirection is needed similar to the one shown in Figure so the Pr pointers are block pointers to blocks that contain a set of record pointers to the actual records in the data file as discussed in option of Section Because entries in the internal nodes of a B+ tree include search values and tree pointers without any data pointers more entries can be packed into an internal node of a B+ tree than for a similar B tree. Thus for the same block size the order p will be larger for the B+ tree than for the B tree as we illustrate in Example This can lead to fewer B+ tree levels improving search time. Because the structures for internal and for leaf nodes of a B+ tree are different the order p can be different. We Chapter Indexing Structures for Files will use p to denote the order for internal nodes and pleaf to denote the order for leaf nodes which we define as being the maximum number of data pointers in a leaf node. Example To calculate the order p of a B+ tree suppose that the search key field is V bytes long the block size is B bytes a record pointer is Pr bytes and a block pointer is P bytes. An internal node of the B+ tree can have up to p tree pointers and p – search field values these must fit into a single block. Hence we have + ≤ B ≤ We can choose p to be the largest value satisfying the above inequality which gives p This is larger than the value of for the B tree resulting in a larger fan out and more entries in each internal node of a B+ tree than in the corresponding B tree. The leaf nodes of the B+ tree will have the same number of values and pointers except that the pointers are data pointers and a next pointer. Hence the order pleaf for the leaf nodes can be calculated as follows + P ≤ B ≤ It follows that each leaf node can hold up to pleaf key value data pointer combinations assuming that the data pointers are record pointers. As with the B tree we may need additional information to implement the insertion and deletion algorithms in each node. This information can include the type of node the number of current entries q in the node and pointers to the parent and sibling nodes. Hence before we do the above calculations for p and pleaf we should reduce the block size by the amount of space needed for all such information. The next example illustrates how we can calculate the number of entries in a B+ tree. Example Suppose that we construct a B+ tree on the field in Example To calculate the approximate number of entries in the B+ tree we assume that each node is percent full. On the average each internal node will have or approximately pointers and hence values. Each leaf node on the average will hold pleaf or approximately data record pointers. A B+ tree will have the following average number of entries at each level Root node key entries pointers Level nodes key entries pointers Level nodes key entries pointers Leaf level nodes data record pointers Dynamic Multilevel Indexes Using B Trees and B+ Trees For the block size pointer size and search field size given above a three level B+ tree holds up to record pointers with the average percent occupancy of nodes. Compare this to the entries for the corresponding B tree in Example This is the main reason that B+ trees are preferred to B trees as indexes to database files. Search Insertion and Deletion with B+ Trees. Algorithm outlines the procedure using the B+ tree as the access structure to search for a record. Algorithm illustrates the procedure for inserting a record in a file with a B+ tree access structure. These algorithms assume the existence of a key search field and they must be modified appropriately for the case of a B+ tree on a nonkey field. We illustrate insertion and deletion with an example. Algorithm Searching for a Record with Search Key Field Value K Using a B+ tree n ← block containing root node of B+ tree read block n while do begin q ← number of tree pointers in node n if K ≤ then n ← else if K then n ← else begin search node n for an entry i such that K ≤ n ← end read block n end search block n for entry with K Ki if found then read data file block with address Pri and retrieve record else the record with search field value K is not in the data file Algorithm Inserting a Record with Search Key Field Value K in a B+ tree of Order p n ← block containing root node of B+ tree read block n set stack S to empty while do begin push address of n on stack S q ← number of tree pointers in node n if K Chapter Indexing Structures for Files then n ← else if K then n ← else begin search node n for an entry i such that K n ← end read block n end search block n for entry with K Ki if found then record already in file cannot insert else begin create entry where Pr points to the new record if leaf node n is not full then insert entry in correct position in leaf node n else begin copy n to temp insert entry in temp in correct position new ← a new empty leaf node for the tree ← j ← ⎡ ← new new ← remaining entries in temp K ← Kj and insert in parent internal node however if parent is full split may propagate finished ← false repeat if stack S is empty then begin root ← a new empty internal node for the tree root ← n K new finished ← true end else begin n ← pop stack S if internal node n is not full then begin insert in correct position in internal node n finished ← true end else begin Dynamic Multilevel Indexes Using B Trees and B+ Trees copy n to temp insert in temp in correct position new ← a new empty internal node for the tree j ← ⎣ new ← entries from tree pointer in temp K ← Kj and insert in parent internal node end end until finished end end Figure illustrates insertion of records in a B+ tree of order p and pleaf First we observe that the root is the only node in the tree so it is also a leaf node. As soon as more than one level is created the tree is divided into internal nodes and leaf nodes. Notice that every key value must exist at the leaf level because all data pointers are at the leaf level. However only some values exist in internal nodes to guide the search. Notice also that every value appearing in an internal node also appears as the rightmost value in the leaf level of the subtree pointed at by the tree pointer to the left of the value. When a leaf node is full and a new entry is inserted there the node overflows and must be split. The first j ⎡ Tree node po nter Data po nter Insert Null tree po nter Insert Insert overflow Insert overflow Insert overflow Insert on sequence Figure An example of insertion in a B+ tree with p and pleaf Dynamic Multilevel Indexes Using B Trees and B+ Trees Delet on sequence Delete Delete underflow Delete underflow Figure An example of deletion from a B+ tree. and redistribute the entries among the node and its sibling so that both are at least half full otherwise the node is merged with its siblings and the number of leaf nodes is reduced. A common method is to try to redistribute entries with the left sibling if this is not possible an attempt to redistribute with the right sibling is Chapter Indexing Structures for Files made. If this is also not possible the three nodes are merged into two leaf nodes. In such a case underflow may propagate to internal nodes because one fewer tree pointer and search value are needed. This can propagate and reduce the tree levels. Notice that implementing the insertion and deletion algorithms may require parent and sibling pointers for each node or the use of a stack as in Algorithm Each node should also include the number of entries in it and its type . Another alternative is to implement insertion and deletion as recursive Variations of B Trees and B+ Trees. To conclude this section we briefly mention some variations of B trees and B+ trees. In some cases constraint on the Btree which requires each node to be at least half full can be changed to require each node to be at least two thirds full. In this case the B tree has been called a B tree. In general some systems allow the user to choose a fill factor between and where the latter means that the B tree nodes are to be completely full. It is also possible to specify two fill factors for a B+ tree one for the leaf level and one for the internal nodes of the tree. When the index is first constructed each node is filled up to approximately the fill factors specified. Some investigators have suggested relaxing the requirement that a node be half full and instead allow a node to become completely empty before merging to simplify the deletion algorithm. Simulation studies show that this does not waste too much additional space under randomly distributed insertions and deletions. Indexes on Multiple Keys In our discussion so far we have assumed that the primary or secondary keys on which files were accessed were single attributes . In many retrieval and update requests multiple attributes are involved. If a certain combination of attributes is used frequently it is advantageous to set up an access structure to provide efficient access by a key value that is a combination of those attributes. For example consider an EMPLOYEE file containing attributes Dno Age Street City Zipcode Salary and Skillcode with the key of Ssn . Consider the query List the employees in department number whose age is Note that both Dno and Age are nonkey attributes which means that a search value for either of these will point to multiple records. The following alternative search strategies may be considered Assuming Dno has an index but Age does not access the records having Dno using the index and then select from among them those records that satisfy Age more details on insertion and deletion algorithms for B+ trees consult Ramakrishnan and Gehrke Indexes on Multiple Keys Alternately if Age is indexed but Dno is not access the records having Age using the index and then select from among them those records that satisfy Dno If indexes have been created on both Dno and Age both indexes may be used each gives a set of records or a set of pointers . An intersection of these sets of records or pointers yields those records or pointers that satisfy both conditions. All of these alternatives eventually give the correct result. However if the set of records that meet each condition will be searched whose addresses are and Chapter Indexing Structures for Files L near Scale for Age EMPLOYEE f le Bucket pool Bucket pool Dno L near scale for Dno Figure Example of a grid array on Dno and Age attributes. so on. An advantage of partitioned hashing is that it can be easily extended to any number of attributes. The bucket addresses can be designed so that high order bits in the addresses correspond to more frequently accessed attributes. Additionally no separate access structure needs to be maintained for the individual attributes. The main drawback of partitioned hashing is that it cannot handle range queries on any of the component attributes. Grid Files Another alternative is to organize the EMPLOYEE file as a grid file. If we want to access a file on two keys say Dno and Age as in our example we can construct a grid array with one linear scale for each of the search attributes. Figure shows a grid array for the EMPLOYEE file with one linear scale for Dno and another for the Age attribute. The scales are made in a way as to achieve a uniform distribution of that attribute. Thus in our example we show that the linear scale for Dno has Dno combined as one value on the scale while Dno corresponds to the value on that scale. Similarly Age is divided into its scale of to by grouping ages so as to distribute the employees uniformly by age. The grid array shown for this file has a total of cells. Each cell points to some bucket address where the records corresponding to that cell are stored. Figure also shows the assignment of cells to buckets . Thus our request for Dno and Age maps into the cell corresponding to the grid array. The records for this combination will be found in the corresponding bucket. This method is particularly useful for range queries that would map into a set of cells corresponding to a group of values along the linear scales. If a range query corresponds to a match on the some of the grid cells it can be processed by accessing exactly the buckets for those grid cells. For example a query for Dno ≤ Other Types of Indexes and Age refers to the data in the top bucket shown in Figure The grid file concept can be applied to any number of search keys. For example for n search keys the grid array would have n dimensions. The grid array thus allows a partitioning of the file along the dimensions of the search key attributes and provides an access by combinations of values along those dimensions. Grid files perform well in terms of reduction in time for multiple key access. However they represent a space overhead in terms of the grid array structure. Moreover with dynamic files a frequent reorganization of the file adds to the maintenance Other Types of Indexes Hash Indexes It is also possible to create access structures similar to indexes that are based on hashing. The hash index is a secondary structure to access the file by using hashing on a search key other than the one used for the primary data file organization. The index entries are of the type K Pr or K P where Pr is a pointer to the record containing the key or P is a pointer to the block containing the record for that key. The index file with these index entries can be organized as a dynamically expandable hash file using one of the techniques described in Section searching for an entry uses the hash search algorithm on K. Once an entry is found the pointer Pr is used to locate the corresponding record in the data file. Figure illustrates a hash index on the Empid field for a file that has been stored as a sequential file ordered by Name. The Empid is hashed to a bucket number by using a hashing function the sum of the digits of Empid modulo For example to find Empid the hash function results in bucket number that bucket is accessed first. It contains the index entry Pr the pointer Pr leads us to the actual record in the file. In a practical application there may be thousands of buckets the bucket number which may be several bits long would be subjected to the directory schemes discussed about dynamic hashing in Section Other search structures can also be used as indexes. Bitmap Indexes The bitmap index is another popular data structure that facilitates querying on multiple keys. Bitmap indexing is used for relations that contain a large number of rows. It creates an index for one or more columns and each value or value range in those columns is indexed. Typically a bitmap index is created for those columns that contain a fairly small number of unique values. To build a bitmap index on a set of records in a relation the records must be numbered from to n with an id that can be mapped to a physical address made of a block number and a record offset within the block. algorithms for grid files may be found in Nievergelt et al. Chapter Indexing Structures for Files Bucket Empid . . . . . . . . . . Marcus M . . . . . . . . . . . . H a n s o n M . . . . . . . . . . . . Dunhill M . . . . . . . . . . . . Clarke F . . . . . . . . . . . . Ferragamo F . . . . . . . . . . . . Zara F . . . . . . . . . . . . Bass M . . . . . . . . . . . . England M . . . . . . . . . . . . Abercombe F . . . . . . . . . . . . Gucci F . . . . . . . . . . . . . . . . . Lastname Sex . . . . . Bucket . . . . . Bucket . . . . . Bucket . . . . . Bucket . . . . . Figure Hash based indexing. A bitmap index is built on one particular value of a particular field and is just an array of bits. Consider a bitmap index for the column C and a value V for that column. For a relation with n rows it contains n bits. The i th bit is set to if the row i has the value V for column C otherwise it is set to a If C contains the valueset vm with m distinct values then m bitmap indexes would be created for that column. Figure shows the relation EMPLOYEE with columns Empid Lname Sex Zipcode and Salarygrade and a bitmap index for the Sex and Zipcode columns. As an example if the bitmap for Sex F the bits for Rowids and are set to and the rest of the bits are set to the bitmap indexes could have the following query applications For the query the corresponding bitmap for value returns the Rowids containing the rows that qualify. Other Types of Indexes EMPLOYEE Rowid Empid Lname Sex Zipcode Salarygrade Bass M Clarke F England M Ferragamo F Gucci F Hanson M Marcus M Zara F B tmap ndex for Sex M F B tmap ndex for Z pcode Zipcode Zipcode Zipcode Figure Bitmap indexes for Sex and Zipcode For the query and the two corresponding bitmaps are retrieved and intersected to yield the set of Rowids that qualify. In general k bitvectors can be intersected to deal with k equality conditions. Complex AND OR conditions can also be supported using bitmap indexing. To retrieve a count of rows that qualify for the condition the entries in the corresponding bitvector are counted. Queries with negation such as ¬ can be handled by applying the Boolean complement operation on the corresponding bitmap. Consider the example in Figure To find employees with Sex F and Zipcode we intersect the bitmaps and yielding Rowids and Employees who do not live in Zipcode are obtained by complementing the bitvector and yields Rowids through In general if we assume uniform distribution of values for a given column and if one column has distinct values and another has distinct values the join condition on these two can be considered to have a selectivity of Hence only about percent of the records would actually have to be retrieved. If a column has only a few values like the Sex column in Figure retrieval of the Sex M condition on average would retrieve percent of the rows in such cases it is better to do a complete scan rather than use bitmap indexing. In general bitmap indexes are efficient in terms of the storage space that they need. If we consider a file of million rows with record size of bytes per row each bitmap index would take up only one bit per row and hence would use million bits or Kbytes. Suppose this relation is for million residents of a state and they are spread over ZIP Codes the bitmaps over Zipcodes contribute bits worth of space per row hence the bitmaps occupy only percent as much space as the data file. They allow an exact retrieval of all residents who live in a given ZIP Code by yielding their Rowids. Chapter Indexing Structures for Files When records are deleted renumbering rows and shifting bits in bitmaps becomes expensive. Another bitmap called the existence bitmap can be used to avoid this expense. This bitmap has a bit for the rows that have been deleted but are still present and a bit for rows that actually exist. Whenever a row is inserted in the relation an entry must be made in all the bitmaps of all the columns that have a bitmap index rows typically are appended to the relation or may replace deleted rows. This process represents an indexing overhead. Large bitvectors are handled by treating them as a series of or vectors and corresponding AND OR and NOT operators are used from the instruction set to deal with or input vectors in a single instruction. This makes bitvector operations computationally very efficient. Bitmaps for B+ Tree Leaf Nodes. Bitmaps can be used on the leaf nodes of B+ tree indexes as well as to point to the set of records that contain each specific value of the indexed field in the leaf node. When the B+ tree is built on a nonkey search field the leaf record must contain a list of record pointers alongside each value of the indexed attribute. For values that occur very frequently that is in a large percentage of the relation a bitmap index may be stored instead of the pointers. As an example for a relation with n rows suppose a value occurs in percent of the file records. A bitvector would have n bits having the bit for those Rowids that contain that search value which is or bytes in size. If the record pointer takes up bytes bits then the record pointers would take up or bytes. Since is more than times larger than it is better to store the bitmap index rather than the record pointers. Hence for search values that occur more frequently than a certain ratio Ahmed contributed most of this section. Other Types of Indexes This statement will create an index based on the function UPPER which returns the last name in uppercase letters for example UPPER will return ‘SMITH’. Function based indexes ensure that Oracle Database system will use the index rather than perform a full table scan even when a function is used in the search predicate of a query. For example the following query will use the index SELECT Firstname Lname FROM Employee WHERE UPPER "SMITH". Without the function based index an Oracle Database might perform a full table scan since a B+ tree index is searched only by using the column value directly the use of any function on a column prevents such an index from being used. Example In this example the EMPLOYEE table is supposed to contain two fields salary and commissionpct and an index is being created on the sum of salary and commission based on the commissionpct. CREATE INDEX incomeix ON Employee The following query uses the incomeix index even though the fields salary and commissionpct are occurring in the reverse order in the query when compared to the index definition. SELECT Firstname Lname FROM Employee WHERE + Salary Example This is a more advanced example of using function based indexing to define conditional uniqueness. The following statement creates a unique functionbased index on the ORDERS table that prevents a customer from taking advantage of a promotion id more than once. It creates a composite index on the Customerid and Promotionid fields together and it allows only one entry in the index for a given Customerid with the Promotionid of by declaring it as a unique index. CREATE UNIQUE INDEX promoix ON Orders Note that by using the CASE statement the objective is to remove from the index any rows where Promotionid is not equal to Oracle Database does not store in the B+ tree index any rows where all the keys are NULL. Therefore in this example we map both Customerid and Promotionid to NULL unless Promotionid is equal to The result is that the index constraint is violated only if Promotionid is equal to for two rows with the same Customerid value. Chapter Indexing Structures for Files Some General Issues Concerning Indexing Logical versus Physical Indexes In the earlier discussion we have assumed that the index entries K Pr always include a physical pointer Pr that specifies the physical record address on disk as a block number and offset. This is sometimes called a physical index and it has the disadvantage that the pointer must be changed if the record is moved to another disk location. For example suppose that a primary file organization is based on linear hashing or extendible hashing then each time a bucket is split some records are allocated to new buckets and hence have new physical addresses. If there was a secondary index on the file the pointers to those records would have to be found and updated which is a difficult task. To remedy this situation we can use a structure called a logical index whose index entries are of the form K Kp . Each entry has one value K for the secondary indexing field matched with the value Kp of the field used for the primary file organization. By searching the secondary index on the value of K a program can locate the corresponding value of Kp and use this to access the record through the primary file organization. Logical indexes thus introduce an additional level of indirection between the access structure and the data. They are used when physical record addresses are expected to change frequently. The cost of this indirection is the extra search based on the primary file organization. Discussion In many systems an index is not an integral part of the data file but can be created and discarded dynamically. That is why it is often called an access structure. Whenever we expect to access a file frequently based on some search condition involving a particular field we can request the DBMS to create an index on that field. Usually a secondary index is created to avoid physical ordering of the records in the data file on disk. The main advantage of secondary indexes is that theoretically at least they can be created in conjunction with virtually any primary record organization. Hence a secondary index could be used to complement other primary access methods such as ordering or hashing or it could even be used with mixed files. To create a B+ tree secondary index on some field of a file we must go through all records in the file to create the entries at the leaf level of the tree. These entries are then sorted and filled according to the specified fill factor simultaneously the other index levels are created. It is more expensive and much harder to create primary indexes and clustering indexes dynamically because the records of the data file must be physically sorted on disk in order of the indexing field. However some systems allow users to create these indexes dynamically on their files by sorting the file during index creation. It is common to use an index to enforce a key constraint on an attribute. While searching the index to insert a new record it is straightforward to check at the same Some General Issues Concerning Indexing time whether another record in the file and hence in the index tree has the same key attribute value as the new record. If so the insertion can be rejected. If an index is created on a nonkey field duplicates occur handling of these duplicates is an issue the DBMS product vendors have to deal with and affects data storage as well as index creation and management. Data records for the duplicate key may be contained in the same block or may span multiple blocks where many duplicates are possible. Some systems add a row id to the record so that records with duplicate keys have their own unique identifiers. In such cases the B+ tree index may regard a key Rowid combination as the de facto key for the index turning the index into a unique index with no duplicates. The deletion of a key K from such an index would involve deleting all occurrences of that key K hence the deletion algorithm has to account for this. In actual DBMS products deletion from B+ tree indexes is also handled in various ways to improve performance and response times. Deleted records may be marked as deleted and the corresponding index entries may also not be removed until a garbage collection process reclaims the space in the data file the index is rebuilt online after garbage collection. A file that has a secondary index on every one of its fields is often called a fully inverted file. Because all indexes are secondary new records are inserted at the end of the file therefore the data file itself is an unordered file. The indexes are usually implemented as B+ trees so they are updated dynamically to reflect insertion or deletion of records. Some commercial DBMSs such as Software AG’s Adabas use this method extensively. We referred to the popular IBM file organization called ISAM in Section Another IBM method the virtual storage access method is somewhat similar to the B+–tree access structure and is still being used in many commercial systems. Column Based Storage of Relations There has been a recent trend to consider a column based storage of relations as an alternative to the traditional way of storing relations row by row. Commercial relational DBMSs have offered B+ tree indexing on primary as well as secondary keys as an efficient mechanism to support access to data by various search criteria and the ability to write a row or a set of rows to disk at a time to produce write optimized systems. For data warehouses and run length encoding techniques have been devised. C Store and Vertica are examples of such systems. Further discussion on column store DBMSs can be found in the references mentioned in this chapter’s Selected Bibliography. Summary In this chapter we presented file organizations that involve additional access structures called indexes to improve the efficiency of retrieval of records from a data file. These access structures may be used in conjunction with the primary file organizations discussed in Chapter which are used to organize the file records themselves on disk. Three types of ordered single level indexes were introduced primary clustering and secondary. Each index is specified on a field of the file. Primary and clustering indexes are constructed on the physical ordering field of a file whereas secondary indexes are specified on nonordering fields as additional access structures to improve performance of queries and transactions. The field for a primary index must also be a key of the file whereas it is a nonkey field for a clustering index. A single level index is an ordered file and is searched using a binary search. We showed how multilevel indexes can be constructed to improve the efficiency of searching an index. Next we showed how multilevel indexes can be implemented as B trees and B+ trees which are dynamic structures that allow an index to expand and shrink dynamically. The nodes of these index structures are kept between half full and completely full by the insertion and deletion algorithms. Nodes eventually stabilize at an average occupancy of percent full allowing space for insertions without requiring reorganization of the index for the majority of insertions. B+ trees can generally hold more entries in their internal nodes than can B trees so they may have fewer levels or hold more entries than does a corresponding B tree. We gave an overview of multiple key access methods and showed how an index can be constructed based on hash data structures. We discussed the hash index in some detail it is a secondary structure to access the file by using hashing on a search key other than that used for the primary organization. Bitmap indexing is another important type of indexing used for querying by multiple keys and is particularly applicable on fields with a small number of unique values. Bitmaps can also be used at the leaf nodes of B+ tree indexes as well. We also discussed function based indexing which is being provided by relational vendors to allow special indexes on a function of one or more attributes. Review Questions We introduced the concept of a logical index and compared it with the physical indexes we described before. They allow an additional level of indirection in indexing in order to permit greater freedom for movement of actual record locations on disk. We also reviewed some general issues related to indexing and commented on column based storage of relations which has particular advantages for read only databases. Finally we discussed how combinations of the above organizations can be used. For example secondary indexes are often used with mixed files as well as with unordered and ordered files. Review Questions Define the following terms indexing field primary key field clustering field secondary key field block anchor dense index and nondense index. What are the differences among primary secondary and clustering indexes How do these differences affect the ways in which these indexes are implemented Which of the indexes are dense and which are not Why can we have at most one primary or clustering index on a file but several secondary indexes How does multilevel indexing improve the efficiency of searching an index file What is the order p of a B tree Describe the structure of B tree nodes. What is the order p of a B+ tree Describe the structure of both internal and leaf nodes of a B+ tree. How does a B tree differ from a B+ tree Why is a B+ tree usually preferred as an access structure to a data file Explain what alternative choices exist for accessing a file based on multiple search keys. What is partitioned hashing How does it work What are its limitations What is a grid file What are its advantages and disadvantages Show an example of constructing a grid array on two attributes on some file. What is a fully inverted file What is an indexed sequential file How can hashing be used to construct an index What is bitmap indexing Create a relation with two columns and sixteen tuples and show an example of a bitmap index on one or both. What is the concept of function based indexing What additional purpose does it serve What is the difference between a logical index and a physical index What is column based storage of a relational database Chapter Indexing Structures for Files Exercises Consider a disk with block size B bytes. A block pointer is P bytes long and a record pointer is PR bytes long. A file has r EMPLOYEE records of fixed length. Each record has the following fields Name bytes Ssn bytes Departmentcode bytes Address bytes Phone bytes Birthdate bytes Sex byte Jobcode bytes and Salary bytes real number . An additional byte is used as a deletion marker. a. Calculate the record size R in bytes. b. Calculate the blocking factor bfr and the number of file blocks b assuming an unspanned organization. c. Suppose that the file is ordered by the key field Ssn and we want to construct a primary index on Ssn. Calculate the index blocking factor bfri the number of first level index entries and the number of first level index blocks the number of levels needed if we make it into a multilevel index the total number of blocks required by the multilevel index and the number of block accesses needed to search for and retrieve a record from the file given its Ssn value using the primary index. d. Suppose that the file is not ordered by the key field Ssn and we want to construct a secondary index on Ssn. Repeat the previous exercise for the secondary index and compare with the primary index. e. Suppose that the file is not ordered by the nonkey field Departmentcode and we want to construct a secondary index on Departmentcode using option of Section with an extra level of indirection that stores record pointers. Assume there are distinct values of Departmentcode and that the EMPLOYEE records are evenly distributed among these values. Calculate the index blocking factor bfri the number of blocks needed by the level of indirection that stores record pointers the number of first level index entries and the number of first level index blocks the number of levels needed if we make it into a multilevel index the total number of blocks required by the multilevel index and the blocks used in the extra level of indirection and the approximate number of block accesses needed to search for and retrieve all records in the file that have a specific Departmentcode value using the index. f. Suppose that the file is ordered by the nonkey field Departmentcode and we want to construct a clustering index on Departmentcode that uses block anchors . Assume there are distinct values of Departmentcode and that the EMPLOYEE records are evenly distributed among these values. Calculate the index blocking factor bfri the number of first level index entries and the number of first level index blocks the number of levels needed if we make it into a multilevel index the total number of blocks Exercises required by the multilevel index and the number of block accesses needed to search for and retrieve all records in the file that have a specific Departmentcode value using the clustering index . g. Suppose that the file is not ordered by the key field Ssn and we want to construct a B+ tree access structure on Ssn. Calculate the orders p and pleaf of the B+ tree the number of leaf level blocks needed if blocks are approximately percent full the number of levels needed if internal nodes are also percent full the total number of blocks required by the B+ tree and the number of block accesses needed to search for and retrieve a record from the file given its Ssn value using the B+ tree. h. Repeat part g but for a B tree rather than for a B+ tree. Compare your results for the B tree and for the B+ tree. A PARTS file with Part# as the key field includes records with the following Part# values Suppose that the search field values are inserted in the given order in a B+ tree of order p and pleaf show how the tree will expand and what the final tree will look like. Repeat Exercise but use a B tree of order p instead of a B+ tree. Suppose that the following search field values are deleted in the given order from the B+ tree of Exercise show how the tree will shrink and show the final tree. The deleted values are Repeat Exercise but for the B tree of Exercise Algorithm outlines the procedure for searching a nondense multilevel primary index to retrieve a file record. Adapt the algorithm for each of the following cases a. A multilevel secondary index on a nonkey nonordering field of a file. Assume that option of Section is used where an extra level of indirection stores pointers to the individual records with the corresponding index field value. b. A multilevel secondary index on a nonordering key field of a file. c. A multilevel clustering index on a nonkey ordering field of a file. Suppose that several secondary indexes exist on nonkey fields of a file implemented using option of Section for example we could have secondary indexes on the fields Departmentcode Jobcode and Salary of the EMPLOYEE file of Exercise Describe an efficient way to search for and retrieve records satisfying a complex selection condition on these fields such as illustrates how this could be done for our example in Figure rather than splitting the leftmost leaf node when is inserted we do a left redistribution by moving to the leaf node to its left . Figure shows how the tree would look when redistribution is considered. It is also possible to consider right redistribution. Try to modify the B+ tree insertion algorithm to take redistribution into account. Outline an algorithm for deletion from a B+ tree. Repeat Exercise for a B tree. Selected Bibliography Bayer and McCreight introduced B trees and associated algorithms. Comer provides an excellent survey of B trees and their history and variations of Btrees. Knuth provides detailed analysis of many search techniques including B trees and some of their variations. Nievergelt discusses the use of binary search trees for file organization. Textbooks on file structures including Claybrook Smith and Barnes and Salzberg the algorithms and data structures textbook by Wirth as well as the database textbook by Ramakrihnan and Gehrke discuss indexing in detail and may be consulted for search insertion and deletion algorithms for B trees and B+ trees. Larson analyzes index sequential files and Held and Stonebraker compare static multilevel indexes with B tree dynamic indexes. Lehman and Yao and Srinivasan and Carey did further analysis of concurrent access to B trees. The books by Wiederhold Smith and Barnes and Salzberg among others discuss many of the search techniques described in this chapter. Grid files are introduced in Nievergelt et al. Partial match retrieval which uses partitioned hashing is discussed in Burkhard New techniques and applications of indexes and B+ trees are discussed in Lanka and Mays Zobel et al. and Faloutsos and Jagadish Mohan and Narang discuss index creation. The performance of various B–tree and B+ tree algorithms is assessed in Baeza Yates and Larson and Johnson and Shasha Buffer management for indexes is discussed in Chan et al. Column based storage of databases was proposed by Stonebraker et al. in the C Store database system by Boncz et al. is another implementation of the idea. Abadi et al. discuss the advantages of column stores over row stored databases for read only database applications. Selected Bibliography Insert overflow Insert overflow Insert overflow Figure B+ tree insertion with left redistribution. This page intentionally left blank Query Processing and Optimization and Database Tuning This page intentionally left blank Algorithms for Query Processing and Optimization I n this chapter we discuss the techniques used internally by a DBMS to process optimize and execute high level queries. A query expressed in a high level query language such as SQL must first be scanned parsed and The scanner identifies the query tokens such as SQL keywords attribute names and relation names that appear in the text of the query whereas the parser checks the query syntax to determine whether it is formulated according to the syntax rules of the query language. The query must also be validated by checking that all attribute and relation names are valid and semantically meaningful names in the schema of the particular database being queried. An internal representation of the query is then created usually as a tree data structure called a query tree. It is also possible to represent the query using a graph data structure called a query graph. The DBMS must then devise an execution strategy or query plan for retrieving the results of the query from the database files. A query typically has many possible execution strategies and the process of choosing a suitable one for processing a query is known as query optimization. Figure shows the different steps of processing a high level query. The query optimizer module has the task of producing a good execution plan and the code generator generates the code to execute that plan. The runtime database processor has the task of running the query code whether in compiled or interpreted mode to produce the query result. If a runtime error results an error message is generated by the runtime database processor. chapter will not discuss the parsing and syntax checking phase of query processing here this material is discussed in compiler textbooks. Chapter Algorithms for Query Processing and Optimization The term optimization is actually a misnomer because in some cases the chosen execution plan is not the optimal strategy it is just a reasonably efficient strategy for executing the query. Finding the optimal strategy is usually too time consuming except for the simplest of queries. In addition trying to find the optimal query execution strategy may require detailed information on how the files are implemented and even on the contents of the files information that may not be fully available in the DBMS catalog. Hence planning of a good execution strategy may be a more accurate description than query optimization. For lower level navigational database languages in legacy systems such as the network DML or the hierarchical or OQL is more declarative in nature because it specifies what the intended results of the query are rather than identifying the details of how the result should be obtained. Query optimization is thus necessary for queries that are specified in a high level query language. We will concentrate on describing query optimization in the context of an RDBMS because many of the techniques we describe have also been adapted for other types Query in a high level language Scann ng pars ng and val dat ng Immediate form of query Query opt m zer Execution plan Query code generator Code to execute the query Runt me database processor Code can be Executed d rectly Stored and executed later whenever needed Result of query Figure Typical steps when processing a high level query. Translating SQL Queries into Relational Algebra are some query optimization problems and techniques that are pertinent only to ODBMSs. However we do not discuss them here because we give only an introduction to query optimization. of database management systems such as A relational DBMS must systematically evaluate alternative query execution strategies and choose a reasonably efficient or near optimal strategy. Each DBMS typically has a number of general database access algorithms that implement relational algebra operations such as SELECT or JOIN FROM EMPLOYEE WHERE This query retrieves the names of employees who earn a salary that is greater than the highest salary in department The query includes a nested subquery and hence would be decomposed into two blocks. The inner block is FROM EMPLOYEE WHERE This retrieves the highest salary in department The outer query block is SELECT Lname Fname FROM EMPLOYEE WHERE Salary c where c represents the result returned from the inner block. The inner block could be translated into the following extended relational algebra expression ℑMAX and the outer block into the expression πLname Fname The query optimizer would then choose an execution plan for each query block. Notice that in the above example the inner block needs to be evaluated only once to produce the maximum salary of employees in department which is then used as the constant c by the outer block. We called this a nested query in Section It is much harder to optimize the more complex correlated nested queries and in duplicate elimination algorithms for the PROJECT operation . We will discuss one of these algorithms in this section. Note that sorting of a particular file may be avoided if an appropriate index such as a primary or clustering index of the file that can fit in the available buffer space are read into main memory sorted using an internal sorting algorithm and written back to disk as temporary sorted subfiles . The size of each run and the number of initial runs are dictated by the number of file blocks and the available buffer space . For example if the number of available main memory buffers nB disk blocks and the size of the file b disk blocks then nR ⎡⎤ or initial runs each of size blocks . Hence after the sorting phase sorted runs are stored as temporary subfiles on disk. In the merging phase the sorted runs are merged during one or more merge passes. Each merge pass can have one or more merge steps. The degree of merging is the number of sorted subfiles that can be merged in each merge step. During each merge step one buffer block is needed to hold one disk block from each of the sorted subfiles being merged and one additional buffer is needed for containing one disk block of the merge result which will produce a larger sorted file that is the result of merging several smaller sorted subfiles. Hence dM is the smaller of ⎤. In our example where nB dM so the initial sorted runs would be merged at a time in each step into larger sorted subfiles at the end of the first merge pass. These sorted files are then merged at a time into sorted files which are then merged into sorted files and then finally into fully sorted file which means that four passes are needed. sorting algorithms are suitable for sorting data structures such as tables and lists that can fit entirely in main memory. These algorithms are described in detail in data structures and algorithms books and include techniques such as quick sort heap sort bubble sort and many others. We do not discuss these here. Chapter Algorithms for Query Processing and Optimization set i ← j ← b size of the file in blocks k ← nB size of buffer in blocks m ← ⎡⎤ Sorting Phase while do read next k blocks of the file into the buffer or if there are less than k blocks remaining then read in the remaining blocks sort the records in the buffer and write as a temporary subfile i ← i + Merging Phase merge subfiles until only remains set i ← p ← p is the number of passes for the merging phase j ← m while do n ← q ← do read next subfiles or remaining subfiles one block at a time merge and write as new subfile one block at a time n ← n + j ← q i ← i + Figure Outline of the sort merge algorithm for external sorting. The performance of the sort merge algorithm can be measured in the number of disk block reads and writes before the sorting of the whole file is completed. The following formula approximates this cost b + b The first term b represents the number of block accesses for the sorting phase since each file block is accessed twice once for reading into a main memory buffer and once for writing the sorted records back to disk into one of the sorted subfiles. The second term represents the number of block accesses for the merging phase. During each merge pass a number of disk blocks approximately equal to the original file blocks b is read and written. Since the number of merge passes is we get the total merge cost of b . Algorithms for SELECT and JOIN Operations The minimum number of main memory buffers needed is nB which gives a dM of and an nR of The minimum dM of gives the worst case performance of the algorithm which is b + . The following sections discuss the various algorithms for the operations of the relational algebra σDnumber σDno σDno AND Salary AND Sex ‘F’ AND Pno Search Methods for Simple Selection. A number of search algorithms are possible for selecting records from a file. These are also known as file scans because they scan the records of a file to search for and retrieve records that satisfy a selection If the search algorithm involves the use of an index the index search is called an index scan. The following search methods through are examples of some of the search algorithms that can be used to implement a select operation search . Retrieve every record in the file and test whether its attribute values satisfy the selection condition. Since the records are grouped into disk blocks each disk block is read into a main memory buffer and then a search through the records within the disk block is conducted in main memory. selection operation is sometimes called a filter since it filters out the records in the file that do not satisfy the selection condition. Chapter Algorithms for Query Processing and Optimization search. If the selection condition involves an equality comparison on a key attribute on which the file is ordered binary search which is more efficient than linear search can be used. An example is if Ssn is the ordering attribute for the EMPLOYEE a primary index. If the selection condition involves an equality comparison on a key attribute with a primary index for example Ssn in the primary index to retrieve the record. Note that this condition retrieves a single record . a hash key. If the selection condition involves an equality comparison on a key attribute with a hash key for example Ssn in the hash key to retrieve the record. Note that this condition retrieves a single record . a primary index to retrieve multiple records. If the comparison condition is or on a key field with a primary index for example Dnumber in the index to find the record satisfying the corresponding equality condition file. For the condition Dnumber retrieve all the preceding records. a clustering index to retrieve multiple records. If the selection condition involves an equality comparison on a nonkey attribute with a clustering index for example Dno in the index to retrieve all the records satisfying the condition. a secondary index on an equality comparison. This search method can be used to retrieve a single record if the indexing field is a key or to retrieve multiple records if the indexing field is not a key. This can also be used for comparisons involving or . In Section we discuss how to develop formulas that estimate the access cost of these search methods in terms of the number of block accesses and access time. Method applies to any file but all the other methods depend on having the appropriate access path on the attribute used in the selection condition. Method requires the file to be sorted on the search attribute. The methods that use an index and are generally referred to as index searches and they require the appropriate index to exist on the search attribute. Methods and can be used to retrieve records in a certain range for example Salary Queries involving such conditions are called range queries. Search Methods for Complex Selection. If a condition of a SELECT operation is a conjunctive condition that is if it is made up of several simple conditions binary search is not used in database searches because ordered files are not used unless they also have a corresponding primary index. Algorithms for SELECT and JOIN Operations connected with the AND logical connective such as above the DBMS can use the following additional methods to implement the operation selection using an individual index. If an attribute involved in any single simple condition in the conjunctive select condition has an access path that permits the use of one of the methods to use that condition to retrieve the records and then check whether each retrieved record satisfies the remaining simple conditions in the conjunctive select condition. selection using a composite index. If two or more attributes are involved in equality conditions in the conjunctive select condition and a composite index exists on the combined fields for example if an index has been created on the composite key of the WORKSON file for can use the index directly. selection by intersection of record If secondary indexes are available on more than one of the fields involved in simple conditions in the conjunctive select condition and if the indexes include record pointers then each index can be used to retrieve the set of record pointers that satisfy the individual condition. The intersection of these sets of record pointers gives the record pointers that satisfy the conjunctive select condition which are then used to retrieve those records directly. If only some of the conditions have secondary indexes each retrieved record is further tested to determine whether it satisfies the remaining In general method assumes that each of the indexes is on a nonkey field of the file because if one of the conditions is an equality condition on a key field only one record will satisfy the whole condition. Whenever a single condition specifies the selection such as or the DBMS can only check whether or not an access path exists on the attribute involved in that condition. If an access path exists the method corresponding to that access path is used otherwise the brute force linear search approach of method can be used. Query optimization for a SELECT operation is needed mostly for conjunctive select conditions whenever more than one of the attributes involved in the conditions have an access path. The optimizer should choose the access path that retrieves the fewest records in the most efficient way by estimating the different costs is defined as the ratio of the number of records that satisfy the condition to the total number of records in the file and thus is a number between zero and one. Zero selectivity means none of the records in the file satisfies the selection condition and a selectivity of one means that all the records in the file satisfy the condition. In general the selectivity will not be either of these two extremes but will be a fraction that estimates the percentage of file records that will be retrieved. Although exact selectivities of all conditions may not be available estimates of selectivities are often kept in the DBMS catalog and are used by the optimizer. For example for an equality condition on a key attribute of relation r s where |r| is the number of tuples in relation r. For an equality condition on a nonkey attribute with i distinct values s can be estimated by | i |r| or assuming that the records are evenly or uniformly distributed among the distinct Under this assumption |r| i records will satisfy an equality condition on this attribute. In general the number of records satisfying a selection condition with selectivity sl is estimated to be |r| sl. The smaller this estimate is the higher the desirability of using that condition first to retrieve records. In certain cases the actual distribution of records among the various distinct values of the attribute is kept by the DBMS in the form of a histogram in order to get more accurate estimates of the number of records that satisfy a particular condition. Disjunctive Selection Conditions. Compared to a conjunctive selection condition a disjunctive condition is much harder to process and optimize. For example consider OR Salary OR Sex ‘F’ With such a condition little optimization can be done because the records satisfying the disjunctive condition are the union of the records satisfying the individual conditions. Hence if any one of the conditions does not have an access path we are compelled to use the brute force linear search approach. Only if an access path exists on every simple condition in the disjunction can we optimize the selection by retrieving the records satisfying each condition or their record ids and then applying the union operation to eliminate duplicates. A DBMS will have available many of the methods discussed above and typically many additional methods. The query optimizer must choose the appropriate one for executing each SELECT operation in a query. This optimization uses formulas that estimate the costs for each available access method as we will discuss in Section The optimizer chooses the access method with the lowest estimated cost. more sophisticated optimizers histograms representing the distribution of the records among the different attribute values can be kept in the catalog. Algorithms for SELECT and JOIN Operations Implementing the JOIN Operation The JOIN operation is one of the most time consuming operations in query processing. Many of the join operations encountered in queries are of the EQUIJOIN and NATURAL JOIN varieties so we consider just these two here since we are only giving an overview of query processing and optimization. For the remainder of this chapter the term join refers to an EQUIJOIN . There are many possible ways to implement a two way join which is a join on two files. Joins involving more than two files are called multiway joins. The number of possible ways to execute multiway joins grows very rapidly. In this section we discuss techniques for implementing only two way joins. To illustrate our discussion we refer to the relational schema in Figure once more specifically to the EMPLOYEE DEPARTMENT and PROJECT relations. The algorithms we discuss next are for a join operation of the form R A B S where A and B are the join attributes which should be domain compatible attributes of R and S respectively. The methods we discuss can be extended to more general forms of join. We illustrate four of the most common techniques for performing such a join using the following sample operations EMPLOYEE Dno Dnumber DEPARTMENT DEPARTMENT Mgrssn Ssn EMPLOYEE Methods for Implementing Joins. join . This is the default algorithm as it does not require any special access paths on either file in the join. For each record t in R retrieve every record s from S and test whether the two records satisfy the join condition t[A] join . If an index exists for one of the two join attributes say attribute B of file S retrieve each record t in R and then use the access structure to retrieve directly all matching records s from S that satisfy s[B] t[A]. join. If the records of R and S are physically sorted by value of the join attributes A and B respectively we can implement the join in the most efficient way possible. Both files are scanned concurrently in order of the join attributes matching the records that have the same values for A and B. If the files are not sorted they may be sorted first by using external sorting to refer to the ith record in file R. A variation of the sort merge join can be used when secondary indexes exist on both join attributes. The indexes provide the ability to access the records in order of the join attributes but the records themselves are physically scattered all over the file blocks so this method may be quite inefficient as every record access may involve accessing a different disk block. join. The records of files R and S are partitioned into smaller files. The partitioning of each file is done using the same hashing function h on the join attribute A of R and B of S . First a single pass through the file with fewer records hashes its records to the various partitions of R this is called the partitioning phase since the records of R are partitioned into the hash buckets. In the simplest case we assume that the smaller file can fit entirely in main memory after it is partitioned so that the partitioned subfiles of R are all kept in main memory. The collection of records with the same value of h are placed in the same partition which is a hash bucket in a hash table in main memory. In the second phase called the probing phase a single pass through the other file then hashes each of its records using the same hash function h to probe the appropriate bucket and that record is combined with all matching records from R in that bucket. This simplified description of partition hash join assumes that the smaller of the two files fits entirely into memory buckets after the first phase. We will discuss the general case of partition hash join that does not require this assumption below. In practice techniques to are implemented by accessing whole disk blocks of a file rather than individual records. Depending on the available number of buffers in memory the number of blocks read in from the file can be adjusted. How Buffer Space and Choice of Outer Loop File Affect Performance of Nested Loop Join. The buffer space available has an important effect on some of the join algorithms. First let us consider the nested loop approach Looking again at the operation above assume that the number of buffers available in main memory for implementing the join is nB blocks . Recall that we assume that each memory buffer is the same size as one disk block. For illustration assume that the DEPARTMENT file consists of rD records stored in bD disk blocks and that the EMPLOYEE file consists of rE records stored in bE disk blocks. It is advantageous to read as many blocks as possible at a time into memory from the file whose records are used for the outer loop . The algorithm can then read one block at a time for the inner loop file and use its records to probe the outer loop blocks that are currently in main memory for matching records. This reduces the total number of block accesses. An extra buffer in main memory is needed to contain the resulting records after they are joined and the contents of this result buffer can be appended to the result file the disk file that will contain the join result whenever it is filled. This result buffer block then is reused to hold additional join result records. sort the tuples in R on attribute A sort the tuples in S on attribute B set i ← j ← while and do if R[A] S[B] then set j ← j + elseif R[A] S[B] then set i ← i + else [A] S[B] so we output a matched tuple output the combined tuple R S to T if any set I ← j + while and [A] S[B] do output the combined tuple R S to T set l ← l + if any set k ← i + while and [A] S[B] do output the combined tuple R S to T set k ← k + set i ← k j ← l create a tuple t[ attribute list ] in T for each tuple t in R if attribute list includes a key of R then T ← T else sort the tuples in T set i ← j ← while i f n do output the tuple T[i] to T while T[i] T[j] and j ≤ n do j ← j + i ← j j ← i + Algorithms for SELECT and JOIN Operations Figure Implementing JOIN PROJECT UNION INTERSECTION and SET DIFFERENCE by using sort merge where R has n tuples and S has m tuples. Implementing the operation T ← R A BS. Implementing the operation T ← π attribute list . sort the tuples in R and S using the same unique sort attributes set i ← j ← while and do if R S then output S to T set j ← j + elseif R S then output R to T set i ← i + else set j ← j + S so we skip one of the duplicate tuples if then add tuples R to R to T if then add tuples S to S to T sort the tuples in R and S using the same unique sort attributes set i ← j ← while and do if R S then set j ← j + elseif R S then set i ← i + else output R to T S so we output the tuple set i ← i + j ← j + sort the tuples in R and S using the same unique sort attributes set i ← j ← while and do if R S then set j ← j + elseif R S then output R to T has no matching S so output R set i ← i + else set i ← i + j ← j + if then add tuples R to R to T Chapter Algorithms for Query Processing and Optimization Figure Implementing JOIN PROJECT UNION INTERSECTION and SET DIFFERENCE by using sort merge where R has n tuples and S has m tuples. Implementing the operation T ← R ∪ S. Implementing the operation T ← R ∩ S. Implementing the operation T ← R – S. Algorithms for SELECT and JOIN Operations In the nested loop join it makes a difference which file is chosen for the outer loop and which for the inner loop. If EMPLOYEE is used for the outer loop each block of EMPLOYEE is read once and the entire DEPARTMENT file is read once for each time we read in for outer loop file bE Number of times for inner loop file bD ⎡bE + + should be added to the preceding formulas in order to estimate the total cost of the join operation. The same holds for the formulas developed later for other join algorithms. As this example shows it is advantageous to use the file with fewer blocks as the outer loop file in the nested loop join. How the Join Selection Factor Affects Join Performance. Another factor that affects the performance of a join particularly the single loop method is the fraction of records in one file that will be joined with records in the other file. We call this the join selection of a file with respect to an equijoin condition with another file. This factor depends on the particular equijoin condition between the two files. To illustrate this consider the operation which joins each DEPARTMENT record with the EMPLOYEE record for the manager of that department. Here each DEPARTMENT record will be joined with a single EMPLOYEE record but many EMPLOYEE records will not be joined with any record from DEPARTMENT. Suppose that secondary indexes exist on both the attributes Ssn of EMPLOYEE and Mgrssn of DEPARTMENT with the number of index levels xSsn and xMgrssn we reserve two buffers for the result file double buffering can be used to speed the algorithm whereas the join selection factor of EMPLOYEE with respect to the same join condition is or . For method either the smaller file or the file that has a match for every record should be used in the join loop. It is also possible to create an index specifically for performing the join operation if one does not already exist. The sort merge join is quite efficient if both files are already sorted by their join attribute. Only a single pass is made through each file. Hence the number of blocks accessed is equal to the sum of the numbers of blocks in both files. For this method both and would need bE + bD + block accesses. However both files are required to be ordered by the join attributes if one or both are not a sorted copy of each file must be created specifically for performing the join operation. If we roughly estimate the cost of sorting an external file by on its join attribute the implementation is straightforward. If however the partitions of both files must be stored on disk the method becomes more complex and a number of variations to improve the efficiency have been proposed. We discuss two techniques the general case of partition hash join and a variation called hybrid hash join algorithm which has been shown to be quite efficient. In the general case of partition hash join each file is first partitioned into M partitions using the same partitioning hash function on the join attributes. Then each can use the more accurate formulas from Section if we know the number of available buffers for sorting. Algorithms for SELECT and JOIN Operations pair of corresponding partitions is joined. For example suppose we are joining relations R and S on the join attributes and R A B S In the partitioning phase R is partitioned into the M partitions RM and S into the M partitions SM. The property of each pair of corresponding partitions Ri Si with respect to the join operation is that records in Ri only need to be joined with records in Si and vice versa. This property is ensured by using the same hash function to partition both files on their join attributes attribute A for R and attribute B for S. The minimum number of in memory buffers needed for the partitioning phase is M + Each of the files R and S are partitioned separately. During partitioning of a file M in memory buffers are allocated to store the records that hash to each partition and one additional buffer is needed to hold one block at a time of the input file being partitioned. Whenever the in memory buffer for a partition gets filled its contents are appended to a disk subfile that stores the partition. The partitioning phase has two iterations. After the first iteration the first file R is partitioned into the subfiles RM where all the records that hashed to the same buffer are in the same partition. After the second iteration the second file S is similarly partitioned. In the second phase called the joining or probing phase M iterations are needed. During iteration i two corresponding partitions Ri and Si are joined. The minimum number of buffers needed for iteration i is the number of blocks in the smaller of the two partitions say Ri plus two additional buffers. If we use a nested loop join during iteration i the records from the smaller of the two partitions Ri are copied into memory buffers then all blocks from the other partition Si are read one at a time and each record is used to probe partition Ri for matching record. Any matching records are joined and written into the result file. To improve the efficiency of in memory probing it is common to use an in memory hash table for storing the records in partition Ri by using a different hash function from the partitioning hash We can approximate the cost of this partition hash join as + bRES for our example since each record is read once and written back to disk once during the partitioning phase. During the joining phase each record is read a second time to perform the join. The main difficulty of this algorithm is to ensure that the partitioning hash function is uniform that is the partition sizes are nearly equal in size. If the partitioning function is skewed then some partitions may be too large to fit in the available memory space for the second joining phase. Notice that if the available in memory buffer space nB are written to the result buffer and eventually to the result file on disk. The cost in terms of block accesses is hence plus bRES the cost of writing the result file. Hybrid Hash Join. The hybrid hash join algorithm is a variation of partition hash join where the joining phase for one of the partitions is included in the partitioning phase. To illustrate this let us assume that the size of a memory buffer is one disk block that nB such buffers are available and that the partitioning hash function used is h K mod M so that M partitions are being created where M nB. For illustration assume we are performing the join operation In the first pass of the partitioning phase when the hybrid hash join algorithm is partitioning the smaller of the two files . If an EMPLOYEE record hashes to a partition other than the first it is partitioned normally and stored to disk. Hence at the end of the second pass of the partitioning phase all records that hash to the first partition have been joined. At this point there are M − pairs of partitions on disk. Therefore during the second joining or probing phase M − iterations are needed instead of M. The goal is to join as many records during the partitioning phase so as to save the cost of storing those records on disk and then rereading them a second time during the joining phase. Algorithms for PROJECT and Set Operations A PROJECT operation π attribute list is straightforward to implement if attribute list includes a key of relation R because in this case the result of the operation will Algorithms for PROJECT and Set Operations have the same number of tuples as R but with only the values for the attributes in attribute list in each tuple. If attribute list does not include a key of R duplicate tuples must be eliminated. This can be done by sorting the result of the operation and then eliminating duplicate tuples which appear consecutively after sorting. A sketch of the algorithm is given in Figure Hashing can also be used to eliminate duplicates as each record is hashed and inserted into a bucket of the hash file in memory it is checked against those records already in the bucket if it is a duplicate it is not inserted in the bucket. It is useful to recall here that in SQL queries the default is not to eliminate duplicates from the query result duplicates are eliminated from the query result only if the keyword DISTINCT is included. Set operations UNION INTERSECTION SET DIFFERENCE and CARTESIAN PRODUCT are sometimes expensive to implement. In particular the CARTESIAN PRODUCT operation R × S is quite expensive because its result includes a record for each combination of records from R and S. Also each record in the result includes all attributes of R and S. If R has n records and j attributes and S has m records and k attributes the result relation for R × S will have n m records and each record will have j + k attributes. Hence it is important to avoid the CARTESIAN PRODUCT operation and to substitute other operations such as join during query optimization relations which have the same number of attributes and the same attribute domains. The customary way to implement these operations is to use variations of the sort merge technique the two relations are sorted on the same attributes and after sorting a single scan through each relation is sufficient to produce the result. For example we can implement the UNION operation R ∪ S by scanning and merging both sorted files concurrently and whenever the same tuple exists in both relations only one is kept in the merged result. For the INTERSECTION operation R ∩ S we keep in the merged result only those tuples that appear in both sorted relations. Figure to sketches the implementation of these operations by sorting and merging. Some of the details are not included in these algorithms. Hashing can also be used to implement UNION INTERSECTION and SET DIFFERENCE. One table is first scanned and then partitioned into an in memory hash table with buckets and the records in the other table are then scanned one at a time and used to probe the appropriate partition. For example to implement R ∪ S first hash the records of R then hash the records of S but do not insert duplicate records in the buckets. To implement R ∩ S first partition the records of R to the hash file. Then while hashing each record of S probe to check if an identical record from R is found in the bucket and if so add the record to the result file. To implement R – S first hash the records of R to the hash file buckets. While hashing each record of S if an identical record is found in the bucket remove that record from the bucket. DIFFERENCE is called EXCEPT in SQL. Chapter Algorithms for Query Processing and Optimization In SQL there are two variations of these set operations. The operations UNION INTERSECTION and EXCEPT apply to traditional sets where no duplicate records exist in the result. The operations UNION ALL INTERSECTION ALL and EXCEPT ALL apply to multisets and duplicates are fully considered. Variations of the above algorithms can be used for the multiset operations in SQL. We leave these as an exercise for the reader. Implementing Aggregate Operations and OUTER JOINs Implementing Aggregate Operations The aggregate operators when applied to an entire table can be computed by a table scan or by using an appropriate index if available. For example consider the following SQL query SELECT MAX FROM EMPLOYEE If an B+ tree index on Salary exists for the EMPLOYEE relation then the optimizer can decide on using the Salary index to search for the largest Salary value in the index by following the rightmost pointer in each index node from the root to the rightmost leaf. That node would include the largest Salary value as its last entry. In most cases this would be more efficient than a full table scan of EMPLOYEE since no actual records need to be retrieved. The MIN function can be handled in a similar manner except that the leftmost pointer in the index is followed from the root to leftmost leaf. That node would include the smallest Salary value as its first entry. The index could also be used for the AVERAGE and SUM aggregate functions but only if it is a dense index that is if there is an index entry for every record in the main file. In this case the associated computation would be applied to the values in the index. For a nondense index the actual number of records associated with each index value must be used for a correct computation. This can be done if the number of records associated with each value in the index is stored in each index entry. For the COUNT aggregate function the number of values can be also computed from the index in a similar manner. If a COUNT function is applied to a whole relation the number of records currently in each relation are typically stored in the catalog and so the result can be retrieved directly from the catalog. When a GROUP BY clause is used in a query the aggregate operator must be applied separately to each group of tuples as partitioned by the grouping attribute. Hence the table must first be partitioned into subsets of tuples where each partition has the same value for the grouping attributes. In this case the computation is more complex. Consider the following query SELECT Dno AVG FROM EMPLOYEE GROUP BY Dno Implementing Aggregate Operations and OUTER JOINs The usual technique for such queries is to first use either sorting or hashing on the grouping attributes to partition the file into the appropriate groups. Then the algorithm computes the aggregate function for the tuples in each group which have the same grouping attribute value. In the sample query the set of EMPLOYEE tuples for each department number would be grouped together in a partition and the average salary computed for each group. Notice that if a clustering index then the records are already partitioned into the appropriate subsets. In this case it is only necessary to apply the computation to each group. Implementing OUTER JOINs In Section the outer join operation was discussed with its three variations left outer join right outer join and full outer join. We also discussed in Chapter how these operations can be specified in SQL. The following is an example of a left outer join operation in SQL SELECT Lname Fname Dname FROM The result of this query is a table of employee names and their associated departments. It is similar to a regular join result with the exception that if an EMPLOYEE tuple does not have an associated department the employee’s name will still appear in the resulting table but the department name would be NULL for such tuples in the query result. Outer join can be computed by modifying one of the join algorithms such as nested loop join or single loop join. For example to compute a left outer join we use the left relation as the outer loop or single loop because every tuple in the left relation must appear in the result. If there are matching tuples in the other relation the joined tuples are produced and saved in the result. However if no matching tuple is found the tuple is still included in the result but is padded with NULL value. The sort merge and hash join algorithms can also be extended to compute outer joins. Theoretically outer join can also be computed by executing a combination of relational algebra operators. For example the left outer join operation shown above is equivalent to the following sequence of relational operations Compute the JOIN of the EMPLOYEE and DEPARTMENT tables. ← πLname Fname Dname Find the EMPLOYEE tuples that do not appear in the JOIN result. ← πLname Fname – πLname Fname Pad each tuple in with a NULL Dname field. ← × NULL Chapter Algorithms for Query Processing and Optimization Apply the UNION operation to to produce the LEFT OUTER JOIN result. RESULT ← ∪ The cost of the outer join as computed above would be the sum of the costs of the associated steps . However note that step can be done as the temporary relation is being constructed in step that is we can simply pad each resulting tuple with a NULL. In addition in step we know that the two operands of the union are disjoint so there is no need for duplicate elimination. Combining Operations Using Pipelining A query specified in SQL will typically be translated into a relational algebra expression that is a sequence of relational operations. If we execute a single operation at a time we must generate temporary files on disk to hold the results of these temporary operations creating excessive overhead. Generating and storing large temporary files on disk is time consuming and can be unnecessary in many cases since these files will immediately be used as input to the next operation. To reduce the number of temporary files it is common to generate query execution code that corresponds to algorithms for combinations of operations in a query. For example rather than being implemented separately a JOIN can be combined with two SELECT operations on the input files and a final PROJECT operation on the resulting file all this is implemented by one algorithm with two input files and a single output file. Rather than creating four temporary files we apply the algorithm directly and get just one result file. In Section we discuss how heuristic relational algebra optimization can group operations together for execution. This is called pipelining or stream based processing. It is common to create the query execution code dynamically to implement multiple operations. The generated code for producing the query combines several algorithms that correspond to individual operations. As the result tuples from one operation are produced they are provided as input for subsequent operations. For example if a join operation follows two select operations on base relations the tuples resulting from each select are provided as input for the join algorithm in a stream or pipeline as they are produced. Using Heuristics in Query Optimization In this section we discuss optimization techniques that apply heuristic rules to modify the internal representation of a query which is usually in the form of a query tree or a query graph data structure to improve its expected performance. The scanner and parser of an SQL query first generate a data structure that corresponds to an initial query representation which is then optimized according to heuristic rules. This leads to an optimized query representation which corresponds to the query execution strategy. Following that a query execution plan is generated Using Heuristics in Query Optimization to execute groups of operations based on the access paths available on the files involved in the query. One of the main heuristic rules is to apply SELECT and PROJECT operations before applying the JOIN or other binary operations because the size of the file resulting from a binary operation such as JOIN is usually a multiplicative function of the sizes of the input files. The SELECT and PROJECT operations reduce the size of a file and hence should be applied before a join or other binary operation. In Section we reiterate the query tree and query graph notations that we introduced earlier in the context of relational algebra and calculus in Sections and respectively. These can be used as the basis for the data structures that are used for internal representation of queries. A query tree is used to represent a relational algebra or extended relational algebra expression whereas a query graph is used to represent a relational calculus expression. Then in Section we show how heuristic optimization rules are applied to convert an initial query tree into an equivalent query tree which represents a different relational algebra expression that is more efficient to execute but gives the same result as the original tree. We also discuss the equivalence of various relational algebra expressions. Finally Section discusses the generation of query execution plans. Notation for Query Trees and Query Graphs A query tree is a tree data structure that corresponds to a relational algebra expression. It represents the input relations of the query as leaf nodes of the tree and represents the relational algebra operations as internal nodes. An execution of the query tree consists of executing an internal node operation whenever its operands are available and then replacing that internal node by the relation that results from executing the operation. The order of execution of operations starts at the leaf nodes which represents the input database relations for the query and ends at the root node which represents the final operation of the query. The execution terminates when the root node operation is executed and produces the result relation for the query. Figure shows a query tree Dnum Dnumber Mgrssn Ssn This corresponds to the following SQL query SELECT FROM PROJECT AS P DEPARTMENT AS D EMPLOYEE AS E WHERE AND AND ‘Stafford’ Chapter Algorithms for Query Processing and Optimization E P D PPnumber PDnum E Lname E Address E Bdate PDnum D Dnumber AND D Mgrssn E Ssn AND PPlocat on Stafford σ P D E [PPnumber PDnum] [E Lname E Address E Bdate] PDnum D Dnumber PPlocat on Stafford D Mgrssn E Ssn Stafford X X P Pnumber P Dnum E Lname E Address E Bdate π D Mgrssn E Ssn P Dnum D Dnumber σP Plocat on Stafford E D P EMPLOYEE DEPARTMENT PROJECT Figure Two query trees for the query Query tree corresponding to the relational algebra expression for Initial query tree for SQL query Query graph for In Figure the leaf nodes P D and E represent the three relations PROJECT DEPARTMENT and EMPLOYEE respectively and the internal tree nodes represent the relational algebra operations of the expression. When this query tree is executed the node marked in Figure must begin execution before node because some resulting tuples of operation must be available before we can begin executing operation Similarly node must begin executing and producing results before node can start execution and so on. As we can see the query tree represents a specific order of operations for executing a query. A more neutral data structure for representation of a query is the query graph notation. Figure operations. For example if the PROJECT DEPARTMENT and EMPLOYEE relations had record sizes of and bytes and contained and tuples respectively the result of the CARTESIAN PRODUCT would contain million tuples of record size bytes each. However the initial query tree in Figure is in a simple standard form that can be easily created from the SQL query. It will never be executed. The heuristic query optimizer will transform this initial query tree into an equivalent final query tree that is efficient to execute. The optimizer must include rules for equivalence among relational algebra expressions that can be applied to transform the initial tree into the final optimized query tree. First we discuss informally how a query tree is transformed by using heuristics and then we discuss general transformation rules and show how they can be used in an algebraic heuristic optimizer. Example of Transforming a Query. Consider the following query Q on the database in Figure Find the last names of employees born after who work on a project named ‘Aquarius’. This query can be specified in SQL as follows a query graph corresponds to a relational calculus expression as shown in Section same query may also be stated in various ways in a high level query language such as SQL Lname Pname Aquar us AND Pnumber Pno AND Essn Ssn AND Bdate PROJECT EMPLOYEE WORKSON Lname Pnumber Pno Bdate Essn Ssn Pname Aquar us π π σ σ σ σ σ EMPLOYEE PROJECT WORKSON X X X X Figure Steps in converting a query tree during heuristic optimization. Initial query tree for SQL query Q. Moving SELECT operations down the query tree. Applying the more restrictive SELECT operation first. Replacing CARTESIAN PRODUCT and SELECT with JOIN operations. Moving PROJECT operations down the query tree. Using Heuristics in Query Optimization π Lname σBdate σPname Aquar us π Pnumber π Essn Pno π Essn πSsn Lname EMPLOYEE WORKSON PROJECT π Lname σ Bdate σPname Aquar us WORKSON EMPLOYEE PROJECT Essn Ssn Pnumber Pno Pnumber Pno Essn Ssn σ Essn Ssn π Lname σ Pnumber Pno σ Bdate σPname Aquar us EMPLOYEE WORKSON PROJECT X X execute. This particular query needs only one record from the PROJECT relation for the ‘Aquarius’ project and only the EMPLOYEE records for those whose date of birth is after Figure shows an improved query tree that first applies the SELECT operations to reduce the number of tuples that appear in the CARTESIAN PRODUCT. A further improvement is achieved by switching the positions of the EMPLOYEE and PROJECT relations in the tree as shown in Figure This uses the information that Pnumber is a key attribute of the PROJECT relation and hence the SELECT operation on the PROJECT relation will retrieve a single record only. We can further improve the query tree by replacing any CARTESIAN PRODUCT operation that is followed by a join condition with a JOIN operation as shown in Figure Another improvement is to keep only the attributes needed by subsequent operations in the intermediate relations by including PROJECT operations as early as possible in the query tree as shown in Figure This reduces the attributes of the intermediate relations whereas the SELECT operations reduce the number of tuples . As the preceding example demonstrates a query tree can be transformed step by step into an equivalent query tree that is more efficient to execute. However we must make sure that the transformation steps always lead to an equivalent query tree. To do this the query optimizer must know which transformation rules preserve this equivalence. We discuss some of these transformation rules next. General Transformation Rules for Relational Algebra Operations. There are many rules for transforming relational algebra operations into equivalent ones. For query optimization purposes we are interested in the meaning of the operations and the resulting relations. Hence if two relations have the same set of attributes in a different order but the two relations represent the same information we consider the relations to be equivalent. In Section we gave an alternative definition of relation that makes the order of attributes unimportant we will use this definition here. We will state some transformation rules that are useful in query optimization without proving them Cascade of σ A conjunctive selection condition can be broken up into a cascade of individual σ operations σc AND c AND AND c n σc Commutativity of σ. The σ operation is commutative σc σc Cascade of π. In a cascade of π operations all but the last one can be ignored Commuting σ with π. If the selection condition c involves only those attributes An in the projection list the two operations can be commuted An σc An Chapter Algorithms for Query Processing and Optimization Using Heuristics in Query Optimization Commutativity of . The join operation is commutative as is the × operation R c S ≡ S c R R × S ≡ S × R Notice that although the order of attributes may not be the same in the relations resulting from the two joins the meaning is the same because the order of attributes is not important in the alternative definition of relation. Commuting σ with . If all the attributes in the selection condition c involve only the attributes of one of the relations being joined say R the two operations can be commuted as follows σc ≡ S Alternatively if the selection condition c can be written as AND where condition involves only the attributes of R and condition involves only the attributes of S the operations commute as follows σc The same rules apply if the is replaced by a × operation. Commuting π with . Suppose that the projection list is L An Bm where An are attributes of R and Bm are attributes of S. If the join condition c involves only attributes in L the two operations can be commuted as follows πL An c Bm If the join condition c contains additional attributes not in L these must be added to the projection list and a final π operation is needed. For example if attributes An+k of R and Bm+p of S are involved in the join condition c but are not in the projection list L the operations commute as follows πL πL c Bm Bm+p For × there is no condition c so the first transformation rule always applies by replacing c with ×. Commutativity of set operations. The set operations ∪ and ∩ are commutative but − is not. Associativity of × ∪ and ∩. These four operations are individually associative that is if θ stands for any one of these four operations we have θ T ≡ R θ Commuting σ with set operations. The σ operation commutes with ∪ ∩ and −. If θ stands for any one of these three operations we have σc ≡ θ Chapter Algorithms for Query Processing and Optimization The π operation commutes with ∪. πL ≡ ∪ Converting a sequence into . If the condition c of a σ that follows a × corresponds to a join condition convert the sequence into a as follows ≡ There are other possible transformations. For example a selection or join condition c can be converted into an equivalent condition by using the following standard rules from Boolean algebra NOT AND ≡ . The algorithm will lead to transformations similar to those discussed in our example in Figure The steps of the algorithm are as follows Using Rule break up any SELECT operations with conjunctive conditions into a cascade of SELECT operations. This permits a greater degree of freedom in moving SELECT operations down different branches of the tree. Using Rules and concerning the commutativity of SELECT with other operations move each SELECT operation as far down the query tree as is permitted by the attributes involved in the select condition. If the condition involves attributes from only one table which means that it represents a selection condition the operation is moved all the way to the leaf node that represents this table. If the condition involves attributes from two tables which means that it represents a join condition the condition is moved to a location down the tree after the two tables are combined. Using Rules and concerning commutativity and associativity of binary operations rearrange the leaf nodes of the tree using the following criteria. First position the leaf node relations with the most restrictive SELECT operations so they are executed first in the query tree representation. The definition of most restrictive SELECT can mean either the ones that produce a relation with the fewest tuples or with the smallest absolute Another possibility is to define the most restrictive SELECT as the one with the smallest selectivity this is more practical because estimates of selectivities are often available in the DBMS catalog. Second make sure that the ordering of leaf nodes does not cause CARTESIAN PRODUCT operations for example if definition can be used since these rules are heuristic. Using Heuristics in Query Optimization the two relations with the most restrictive SELECT do not have a direct join condition between them it may be desirable to change the order of leaf nodes to avoid Cartesian Using Rule combine a CARTESIAN PRODUCT operation with a subsequent SELECT operation in the tree into a JOIN operation if the condition represents a join condition. Using Rules and concerning the cascading of PROJECT and the commuting of PROJECT with other operations break down and move lists of projection attributes down the tree as far as possible by creating new PROJECT operations as needed. Only those attributes needed in the query result and in subsequent operations in the query tree should be kept after each PROJECT operation. Identify subtrees that represent groups of operations that can be executed by a single algorithm. In our example Figure shows the tree in Figure after applying steps and of the algorithm Figure shows the tree after step Figure after step and Figure after step In step we may group together the operations in the subtree whose root is the operation πEssn i nto a single algorithm. We may also group the remaining operations into another subtree where the tuples resulting from the first algorithm replace the subtree whose root is the operation πEssn because the first grouping means that this subtree is executed first. Summary of Heuristics for Algebraic Optimization. The main heuristic is to apply first the operations that reduce the size of intermediate results. This includes performing as early as possible SELECT operations to reduce the number of tuples and PROJECT operations to reduce the number of attributes by moving SELECT and PROJECT operations as far down the tree as possible. Additionally the SELECT and JOIN operations that are most restrictive that is result in relations with the fewest tuples or with the smallest absolute size should be executed before other similar operations. The latter rule is accomplished through reordering the leaf nodes of the tree among themselves while avoiding Cartesian products and adjusting the rest of the tree appropriately. Converting Query Trees into Query Execution Plans An execution plan for a relational algebra expression represented as a query tree includes information about the access methods available for each relation as well as the algorithms to be used in computing the relational operators represented in the tree. As a simple example consider query from Chapter whose corresponding relational algebra expression is πFname Lname Address Dnumber Dno EMPLOYEE that a CARTESIAN PRODUCT is acceptable in some cases for example if each relation has only a single tuple because each had a previous select condition on a key field. Chapter Algorithms for Query Processing and Optimization π Fname Lname Address σ Dname Research DEPARTMENT EMPLOYEE Dnumber Dno Figure A query tree for query The query tree is shown in Figure To convert this into an execution plan the optimizer might choose an index search for the SELECT operation on DEPARTMENT a single loop join algorithm that loops over the records in the result of the SELECT operation on DEPARTMENT for the join operation and a scan of the JOIN result for input to the PROJECT operator. Additionally the approach taken for executing the query may specify a materialized or a pipelined evaluation although in general a pipelined evaluation is preferred whenever feasible. With materialized evaluation the result of an operation is stored as a temporary relation . For instance the JOIN operation can be computed and the entire result stored as a temporary relation which is then read as input by the algorithm that computes the PROJECT operation which would produce the query result table. On the other hand with pipelined evaluation as the resulting tuples of an operation are produced they are forwarded directly to the next operation in the query sequence. For example as the selected tuples from DEPARTMENT are produced by the SELECT operation they are placed in a buffer the JOIN operation algorithm would then consume the tuples from the buffer and those tuples that result from the JOIN operation are pipelined to the projection operation algorithm. The advantage of pipelining is the cost savings in not having to write the intermediate results to disk and not having to read them back for the next operation. Using Selectivity and Cost Estimates in Query Optimization A query optimizer does not depend solely on heuristic rules it also estimates and compares the costs of executing a query using different execution strategies and algorithms and it then chooses the strategy with the lowest cost estimate. For this approach to work accurate cost estimates are required so that different strategies can be compared fairly and realistically. In addition the optimizer must limit the number of execution strategies to be considered otherwise too much time will be spent making cost estimates for the many possible execution strategies. Hence this approach is more suitable for compiled queries where the optimization is done at compile time and the resulting execution strategy code is stored and executed directly at runtime. For interpreted queries where the entire process shown in Using Selectivity and Cost Estimates in Query Optimization Figure occurs at runtime a full scale optimization may slow down the response time. A more elaborate optimization is indicated for compiled queries whereas a partial less time consuming optimization works best for interpreted queries. This approach is generally referred to as cost based query optimization. It uses traditional optimization techniques that search the solution space to a problem for a solution that minimizes an objective function. The cost functions used in query optimization are estimates and not exact cost functions so the optimization may select a query execution strategy that is not the optimal one. In Section we discuss the components of query execution cost. In Section we discuss the type of information needed in cost functions. This information is kept in the DBMS catalog. In Section we give examples of cost functions for the SELECT operation and in Section we discuss cost functions for two way JOIN operations. Section discusses multiway joins and Section gives an example. Cost Components for Query Execution The cost of executing a query includes the following components Access cost to secondary storage. This is the cost of transferring data blocks between secondary disk storage and main memory buffers. This is also known as disk I O cost. The cost of searching for records in a disk file depends on the type of access structures on that file such as ordering hashing and primary or secondary indexes. In addition factors such as whether the file blocks are allocated contiguously on the same disk cylinder or scattered on the disk affect the access cost. Disk storage cost. This is the cost of storing on disk any intermediate files that are generated by an execution strategy for the query. Computation cost. This is the cost of performing in memory operations on the records within the data buffers during query execution. Such operations include searching for and sorting records merging records for a join or a sort operation and performing computations on field values. This is also known as CPU cost. Memory usage cost. This is the cost pertaining to the number of main memory buffers needed during query execution. Communication cost. This is the cost of shipping the query and its results from the database site to the site or terminal where the query originated. In distributed databases cost function because of the difficulty of assigning suitable weights to the cost components. That is why some cost functions consider a single factor only disk access. In the next section we discuss some of the information that is needed for formulating cost functions. Catalog Information Used in Cost Functions To estimate the costs of various execution strategies we must keep track of any information that is needed for the cost functions. This information may be stored in the DBMS catalog where it is accessed by the query optimizer. First we must know the size of each file. For a file whose records are all of the same type the number of records the record size and the number of file blocks are needed. The blocking factor for the file may also be needed. We must also keep track of the primary file organization for each file. The primary file organization records may be unordered ordered by an attribute with or without a primary or clustering index or hashed on a key attribute. Information is also kept on all primary secondary or clustering indexes and their indexing attributes. The number of levels of each multilevel index is needed for cost functions that estimate the number of block accesses that occur during query execution. In some cost functions the number of first level index blocks is needed. Another important parameter is the number of distinct values of an attribute and the attribute selectivity which is the fraction of records satisfying an equality condition on the attribute. This allows estimation of the selection cardinality of an attribute which is the average number of records that will satisfy an equality selection condition on that attribute. For a key attribute d r sl and s For a nonkey attribute by making an assumption that the d distinct values are uniformly distributed among the records we estimate sl and so s Information such as the number of index levels is easy to maintain because it does not change very often. However other information may change frequently for example the number of records r in a file changes every time a record is inserted or deleted. The query optimizer will need reasonably close but not necessarily completely up to the minute values of these parameters for use in estimating the cost of various execution strategies. For a nonkey attribute with d distinct values it is often the case that the records are not uniformly distributed among these values. For example suppose that a company has departments numbered through and employees who are accurate optimizers store histograms of the distribution of records over the data values for an attribute. Using Selectivity and Cost Estimates in Query Optimization uted among the departments as follows In such cases the optimizer can store a histogram that reflects the distribution of employee records over different departments in a table with the two attributes which would contain the following values for our example The selectivity values stored in the histogram can also be estimates if the employee table changes frequently. In the next two sections we examine how some of these parameters are used in cost functions for a cost based query optimizer. Examples of Cost Functions for SELECT We now give cost functions for the selection algorithms to discussed in Section in terms of number of block transfers between memory and disk. Algorithm involves an intersection of record pointers after they have been retrieved by some other means such as algorithm and so the cost function will be based on the cost for These cost functions are estimates that ignore computation time storage cost and other factors. The cost for method Si is referred to as CSi block accesses. search approach. We search all the file blocks to retrieve all records satisfying the selection condition hence b. For an equality condition on a key attribute only half the file blocks are searched on the average before finding the record so a rough estimate for if the record is found if no record is found that satisfies the condition b. search. This search accesses approximately + ⎡⎤ − file blocks. This reduces to if the equality condition is on a unique attribute because s in this case. a primary index to retrieve a single record. For a primary index retrieve one disk block at each index level plus one disk block from the data file. Hence the cost is one more disk block than the number of index levels x + a hash key to retrieve a single record. For hashing only one disk block needs to be accessed in most cases. The cost function is approximately for static hashing or linear hashing and it is disk block accesses for extendible hashing ⎤ file blocks will be in the cluster of file blocks that hold all the selected records giving x + ⎡⎤. a secondary index. For a secondary index on a key attribute the cost is x + disk block accesses. For a secondary index on a nonkey attribute s records will satisfy an equality condition where s is the selection cardinality of the indexing attribute. However because the index is nonclustering each of the records may reside on a different disk block so the cost estimate is x + + s. The additional is to account for the disk block that contains the record pointers after the index is searched half the first level index blocks are accessed plus half the file records via the index. The cost estimate for this case approximately is x + + The factor can be refined if better selectivity estimates are available through a histogram. The latter method can be very costly. selection. We can use either or one of the methods to discussed above. In the latter case we use one condition to retrieve the records and then check in the main memory buffers whether each retrieved record satisfies the remaining conditions in the conjunction. If multiple indexes exist the search of each index can produce a set of record pointers in the main memory buffers. The intersection of the sets of record pointers cost estimate efficiently without having to consider all possible execution strategies. We do not discuss optimization algorithms here rather we use a simple example to illustrate how cost estimates may be used. Suppose that the EMPLOYEE file in Figure has rE records stored in bE disk blocks with blocking factor bfrE records block and the following access paths A clustering index on Salary with levels xSalary and average selection cardinality sSalary Using Selectivity and Cost Estimates in Query Optimization A secondary index on Sex with xSex There are dSex values for the Sex attribute so the average selection cardinality is sSex We illustrate the use of cost functions with the following examples AND AND Sex ‘F’ The cost of the brute force option will be estimated as bE or . For we can use either method or method the cost estimate for is xSsn + + and it is chosen over method whose average cost is For we can use either method first gives a cost estimate xSex + sSex + The optimizer would then choose method on the secondary index on Dno because it has the lowest cost estimate. The condition is checked for each selected record after it is retrieved into memory. Only the records that satisfy these additional conditions are included in the result of the operation. Examples of Cost Functions for JOIN To develop reasonably accurate cost functions for JOIN operations we need to have an estimate for the size of the file that results after the JOIN operation. This is usually kept as a ratio of the size of the resulting join file to the size of the CARTESIAN PRODUCT file if both are applied to the same input files and it is called the join selectivity . If we denote the number of tuples of a relation R by |R| we have js || || || If there is no join condition c then js and the join is the same as the CARTESIAN PRODUCT. If no tuples from the relations satisfy the join condition then js In Chapter Algorithms for Query Processing and Optimization general f js f For a join where the condition c is an equality comparison we get the following two special cases If A is a key of R then || ≤ |S| so js ≤ This is because each record in file S will be joined with at most one record in file R since A is a key of R. A special case of this condition is when attribute B is a foreign key of S that references the primary key A of R. In addition if the foreign key B has the NOT NULL constraint then js and the result file of the join will contain |S| records. If B is a key of S then || ≤ |R| so js ≤ Having an estimate of the join selectivity for commonly occurring join conditions enables the query optimizer to estimate the size of the resulting file after the join operation given the sizes of the two input files by using the formula || js |R| |S|. We can now give some sample approximate cost functions for estimating the cost of some of the join algorithms given in Section The join operations are of the form R A B S where A and B are domain compatible attributes of R and S respectively. Assume that R has bR blocks and that S has bS blocks join. Suppose that we use R for the outer loop then we get the following cost function to estimate the number of block accesses for this method assuming three memory buffers. We assume that the blocking factor for the resulting file is bfrRS and that the join selectivity is known bR + + bfrRS The last part of the formula is the cost of writing the resulting file to disk. This cost formula can be modified to take into account different numbers of memory buffers as presented in Section If nB main memory buffers are available to perform the join the cost formula becomes bR + + bfrRS join . If an index exists for the join attribute B of S with index levels xB we can retrieve each record s in R and then use the index to retrieve all the matching records t from S that satisfy t[B] s[A]. The cost depends on the type of index. For a secondary index where s B is the selection cardinality for the join attribute B of S we get bR + + bfrRS For a clustering index where s B is the selection cardinality of B we get bR + + bfrRS For a primary index we get cardinality was defined as the average number of records that satisfy an equality condition on an attribute which is the average number of records that have the same value for the attribute and hence will be joined to a single record in the other file. Using Selectivity and Cost Estimates in Query Optimization bR + bfrRS If a hash key exists for one of the two join attributes say B of S we get bR + + bfrRS where h ≥ is the average number of block accesses to retrieve a record given its hash key value. Usually h is estimated to be for static and linear hashing and for extendible hashing. join. If the files are already sorted on the join attributes the cost function for this method is bR + bS + bfrRS If we must sort the files the cost of sorting must be added. We can use the formulas from Section to estimate the sorting cost. Example of Using the Cost Functions. Suppose that we have the EMPLOYEE file described in the example in the previous section and assume that the DEPARTMENT file in Figure consists of rD records stored in bD disk blocks. Consider the following two join operations EMPLOYEE Dno Dnumber DEPARTMENT DEPARTMENT Mgrssn Ssn EMPLOYEE Suppose that we have a primary index on Dnumber of DEPARTMENT with xDnumber level and a secondary index on Mgrssn of DEPARTMENT with selection cardinality sMgrssn and levels xMgrssn Assume that the join selectivity for is because Dnumber is a key of DEPARTMENT. Also assume that the blocking factor for the resulting join file is bfrED records per block. We can estimate the worst case costs for the JOIN operation using the applicable methods and as follows Using method with EMPLOYEE as outer loop bE + + bfrED + + Using method with DEPARTMENT as outer loop bD + + bfrED + + Using method with EMPLOYEE as outer loop bE + bfrED + + Using method with DEPARTMENT as outer loop bD + + bfrED + + + Case has the lowest cost estimate and will be chosen. Notice that in case above if memory buffers were available for executing the join instead of just of them could be used to hold the entire DEPARTMENT relation query trees. relation in memory one could be used as buffer for the result and one would be used to hold one block at a time of the EMPLOYEE file and the cost for case could be drastically reduced to just bE + bD + bfrED or as discussed in Section If some other number of main memory buffers was available say nB then the cost for case would be calculated as follows which would also give better performance than case bD + + bfrRS + query tree to that of left deep trees. A left deep tree is a binary tree in which the right child of each nonleaf node is always a base relation. The optimizer would choose the particular left deep tree with the lowest estimated cost. Two examples of left deep trees are shown in Figure With left deep trees the right child is considered to be the inner relation when executing a nested loop join or the probing relation when executing a single loop join. One advantage of left deep trees is that they are amenable to pipelining as discussed in Section For instance consider the first left deep tree in Figure and assume that the join algorithm is the single loop method in this case a disk page of tuples of the outer relation is used to probe the inner relation for Using Selectivity and Cost Estimates in Query Optimization matching tuples. As resulting tuples are produced from the join of and they can be used to probe to locate their matching records for joining. Likewise as resulting tuples are produced from this join they could be used to probe Another advantage of left deep trees is that having a base relation as one of the inputs of each join allows the optimizer to utilize any access paths on that relation that may be useful in executing the join. If materialization is used instead of pipelining are used by subsequent operators and hence affect the execution cost of those operators. Example to Illustrate Cost Based Query Optimization We will consider query and its query tree shown in Figure to illustrate cost based query optimization SELECT Pnumber Dnum Lname Address Bdate FROM PROJECT DEPARTMENT EMPLOYEE WHERE Dnum Dnumber AND Mgrssn Ssn AND Plocation ‘Stafford’ Suppose we have the information about the relations shown in Figure The LOWVALUE and HIGHVALUE statistics have been normalized for clarity. The tree in Figure is assumed to represent the result of the algebraic heuristic optimization process and the start of cost based optimization . The first cost based optimization to consider is join ordering. As previously mentioned we assume the optimizer considers only left deep trees so the potential join orders without CARTESIAN PRODUCT are PROJECT DEPARTMENT EMPLOYEE DEPARTMENT PROJECT EMPLOYEE DEPARTMENT EMPLOYEE PROJECT EMPLOYEE DEPARTMENT PROJECT Assume that the selection operation has already been applied to the PROJECT relation. If we assume a materialized approach then a new temporary relation is created after each join operation. To examine the cost of join order the first join is between PROJECT and DEPARTMENT. Both the join method and the access methods for the input relations must be determined. Since DEPARTMENT has no index according to Figure the only available access method is a table scan . The PROJECT relation will have the selection operation performed before the join so two options exist table scan or utilizing its PROJPLOC index so the optimizer must compare their estimated costs. Chapter Algorithms for Query Processing and Optimization Tablename PROJECT PROJECT PROJECT DEPARTMENT DEPARTMENT EMPLOYEE EMPLOYEE EMPLOYEE Dnum Dnumber Plocation Pnumber Dno Salary Mgrssn Ssn Columnname Numdistinct Lowvalue Highvalue Indexname Blevel is the number of levels without the leaf level. PROJPLOC EMPSSN EMPSAL NONUNIQUE NONUNIQUE UNIQUE Uniqueness Blevel Leafblocks Distinctkeys Tablename PROJECT DEPARTMENT EMPLOYEE Numrows Blocks Figure Sample statistical information for relations in Column information. Table information. Index information. The statistical information on the PROJPLOC index . The index is nonunique so the optimizer assumes a uniform data distribution and estimates the number of record pointers for each Plocation value to be This is computed from the tables in Figure by multiplying Selectivity Numrows where Selectivity is estimated by So the cost of using the index and accessing the records is estimated to be block accesses for the index and for the data blocks . The cost of a table scan is estimated to be block accesses so the index access is more efficient as expected. In the materialized approach a temporary file of size block is created to hold the result of the selection operation. The file size is calculated by determining the blocking factor using the formula Numrows Blocks which gives or rows per block. Hence the records selected from the PROJECT relation will fit Overview of Query Optimization in Oracle into a single block. Now we can compute the estimated cost of the first join. We will consider only the nested loop join method where the outer relation is the temporary file and the inner relation is DEPARTMENT. Since the entire file fits in the available buffer space we need to read each of the DEPARTMENT table’s five blocks only once so the join cost is six block accesses plus the cost of writing the temporary result file The optimizer would have to determine the size of Since the join attribute Dnumber is the key for DEPARTMENT any Dnum value from will join with at most one record from DEPARTMENT so the number of rows in will be equal to the number of rows in which is The optimizer would determine the record size for and the number of blocks needed to store these rows. For brevity assume that the blocking factor for is five rows per block so a total of two blocks are needed to store Finally the cost of the last join needs to be estimated. We can use a single loop join on since in this case the index EMPSSN where ROWID specifies the record’s physical address that includes the data file data block and row offset within the block to a full table scan where all rows in the table are searched by doing multiblock reads. However the rule based approach is being phased out in favor of the cost based approach where the optimizer examines alternative access paths and operator algorithms and chooses the execution plan with the lowest estimated cost. The estimated query cost is proportional to the expected elapsed time needed to execute the query with the given execution plan. discussion in this section is primarily based on version of Oracle. More optimization techniques have been added to subsequent versions. Chapter Algorithms for Query Processing and Optimization The Oracle optimizer calculates this cost based on the estimated usage of resources such as I O CPU time and memory needed. The goal of cost based optimization in Oracle is to minimize the elapsed time to process the entire query. An interesting addition to the Oracle query optimizer is the capability for an application developer to specify hints to the The idea is that an application developer might know more information about the data than the optimizer. For example consider the EMPLOYEE table shown in Figure The Sex column of that table has only two distinct values. If there are employees then the optimizer would estimate that half are male and half are female assuming a uniform data distribution. If a secondary index exists it would more than likely not be used. However if the application developer knows that there are only male employees a hint could be specified in an SQL query whose WHERE clause condition is Sex ‘M’ so that the associated index would be used in processing the query. Various hints can be specified such as The optimization approach for an SQL statement The access path for a table accessed by the statement The join order for a join statement A particular join operation in a join statement The cost based optimization of Oracle and later versions is a good example of the sophisticated approach taken to optimize SQL queries in commercial RDBMSs. Semantic Query Optimization A different approach to query optimization called semantic query optimization has been suggested. This technique which may be used in combination with the techniques discussed previously uses constraints specified on the database schema such as unique attributes and other more complex constraints in order to modify one query into another query that is more efficient to execute. We will not discuss this approach in detail but we will illustrate it with a simple example. Consider the SQL query SELECT FROM EMPLOYEE AS E EMPLOYEE AS M WHERE AND This query retrieves the names of employees who earn more than their supervisors. Suppose that we had a constraint on the database schema that stated that no employee can earn more than his or her direct supervisor. If the semantic query optimizer checks for the existence of this constraint it does not need to execute the query at all because it knows that the result of the query will be empty. This may save considerable time if the constraint checking can be done efficiently. However searching through many constraints to find those that are applicable to a given hints have also been called query annotations. Review Questions query and that may semantically optimize it can also be quite time consuming. With the inclusion of active rules and additional metadata in database systems and the initial and final query trees of part . A file of blocks is to be sorted with an available buffer space of blocks. How many passes will be needed in the merge phase of the external sort merge algorithm Develop cost functions for the PROJECT UNION INTERSECTION SET DIFFERENCE and CARTESIAN PRODUCT algorithms discussed in Section Develop cost functions for an algorithm that consists of two SELECTs a JOIN and a final PROJECT in terms of the cost functions for the individual operations. Can a nondense index be used in the implementation of an aggregate operator Why or why not Calculate the cost functions for different options of executing the JOIN operation discussed in Section Develop formulas for the hybrid hash join algorithm for calculating the size of the buffer for the first bucket. Develop more accurate cost estimation formulas for the algorithm. Selected Bibliography Estimate the cost of operations and using the formulas developed in Exercise Extend the sort merge join algorithm to implement the LEFT OUTER JOIN operation. Compare the cost of two different query plans for the following query σSalary Dno DnumberDEPARTMENT Use the database statistics in Figure Selected Bibliography A detailed algorithm for relational algebra optimization is given by Smith and Chang The . thesis of Kooi provides a foundation for query processing techniques. A survey paper by Jarke and Koch gives a taxonomy of query optimization and includes a bibliography of work in this area. A survey by Graefe discusses query execution in database systems and includes an extensive bibliography. Whang discusses query optimization in OBE which is a system based on the language QBE. Cost based optimization was introduced in the SYSTEM R experimental DBMS and is discussed in Astrahan et al. Selinger et al. is a classic paper that discussed cost based optimization of multiway joins in SYSTEM R. Join algorithms are discussed in Gotlieb Blasgen and Eswaran and Whang et al. Hashing algorithms for implementing joins are described and analyzed in DeWitt et al. Bratbergsengen Shapiro Kitsuregawa et al. and Blakeley and Martin among others. Approaches to finding a good join order are presented in Ioannidis and Kang and in Swami and Gupta A discussion of the implications of left deep and bushy join trees is presented in Ioannidis and Kang Kim discusses transformations of nested SQL queries into canonical representations. Optimization of aggregate functions is discussed in Klug and Muralikrishna Salzberg et al. describe a fast external sorting algorithm. Estimating the size of temporary relations is crucial for query optimization. Sampling based estimation schemes are presented in Haas et al. and in Haas and Swami Lipton et al. also discuss selectivity estimation. Having the database system store and use more detailed statistics in the form of histograms is the topic of Muralikrishna and DeWitt and Poosala et al. Kim et al. discuss advanced topics in query optimization. Semantic query optimization is discussed in King and Malley and Zdonick Work on semantic query optimization is reported in Chakravarthy et al. Shenoy and Ozsoyoglu and Siegel et al. This page intentionally left blank Physical Database Design and Tuning I n the last chapter we discussed various techniques by which queries can be processed efficiently by the DBMS. These techniques are mostly internal to the DBMS and invisible to the programmer. In this chapter we discuss additional issues that affect the performance of an application running on a DBMS. In particular we discuss some of the options available to database administrators and programmers for storing databases and some of the heuristics rules and techniques that they can use to tune the database for performance improvement. First in Section we discuss the issues that arise in physical database design dealing with storage and access of data. Then in Section we discuss how to improve database performance through tuning indexing of data database design and the queries themselves. Physical Database Design in Relational Databases In this section we begin by discussing the physical design factors that affect the performance of applications and transactions and then we comment on the specific guidelines for RDBMSs. Factors That Influence Physical Database Design Physical design is an activity where the goal is not only to create the appropriate structuring of data in storage but also to do so in a way that guarantees good performance. For a given conceptual schema there are many physical design alternatives in a given DBMS. It is not possible to make meaningful physical design chapter Chapter Physical Database Design and Tuning decisions and performance analyses until the database designer knows the mix of queries transactions and applications that are expected to run on the database. This is called the job mix for the particular set of database system applications. The database administrators designers must analyze these applications their expected frequencies of invocation any timing constraints on their execution speed the expected frequency of update operations and any unique constraints on attributes. We discuss each of these factors next. A. Analyzing the Database Queries and Transactions. Before undertaking the physical database design we must have a good idea of the intended use of the database by defining in a high level form the queries and transactions that are expected to run on the database. For each retrieval query the following information about the query would be needed The files that will be accessed by the The attributes on which any selection conditions for the query are specified. Whether the selection condition is an equality inequality or a range condition. The attributes on which any join conditions or conditions to link multiple tables or objects for the query are specified. The attributes whose values will be retrieved by the query. The attributes listed in items and above are candidates for the definition of access structures such as indexes hash keys or sorting of the file. For each update operation or update transaction the following information would be needed The files that will be updated. The type of operation on each file . The attributes on which selection conditions for a delete or update are specified. The attributes whose values will be changed by an update operation. Again the attributes listed in item are candidates for access structures on the files because they would be used to locate the records that will be updated or deleted. On the other hand the attributes listed in item are candidates for avoiding an access structure since modifying them will require updating the access structures. B. Analyzing the Expected Frequency of Invocation of Queries and Transactions. Besides identifying the characteristics of expected retrieval queries and update transactions we must consider their expected rates of invocation. This frequency information along with the attribute information collected on each query and transaction is used to compile a cumulative list of the expected frequency of use for all queries and transactions. This is expressed as the expected frequency of using each attribute in each file as a selection attribute or a join attribute simplicity we use the term files here but this can also mean tables or relations. Physical Database Design in Relational Databases over all the queries and transactions. Generally for large volumes of processing the informal rule can be used approximately percent of the processing is accounted for by only percent of the queries and transactions. Therefore in practical situations it is rarely necessary to collect exhaustive statistics and invocation rates on all the queries and transactions it is sufficient to determine the percent or so most important ones. C. Analyzing the Time Constraints of Queries and Transactions. Some queries and transactions may have stringent performance constraints. For example a transaction may have the constraint that it should terminate within seconds on percent of the occasions when it is invoked and that it should never take more than seconds. Such timing constraints place further priorities on the attributes that are candidates for access paths. The selection attributes used by queries and transactions with time constraints become higher priority candidates for primary access structures for the files because the primary access structures are generally the most efficient for locating records in a file. D. Analyzing the Expected Frequencies of Update Operations. A minimum number of access paths should be specified for a file that is frequently updated because updating the access paths themselves slows down the update operations. For example if a file that has frequent record insertions has indexes on different attributes each of these indexes must be updated whenever a new record is inserted. The overhead for updating indexes can slow down the insert operations. E. Analyzing the Uniqueness Constraints on Attributes. Access paths should be specified on all candidate key attributes or sets of attributes that are either the primary key of a file or unique attributes. The existence of an index makes it sufficient to only search the index when checking this uniqueness constraint since all values of the attribute will exist in the leaf nodes of the index. For example when inserting a new record if a key attribute value of the new record already exists in the index the insertion of the new record should be rejected since it would violate the uniqueness constraint on the attribute. Once the preceding information is compiled it is possible to address the physical database design decisions which consist mainly of deciding on the storage structures and access paths for the database files. Physical Database Design Decisions Most relational systems represent each base relation as a physical database file. The access path options include specifying the type of primary file organization for each relation and the attributes of which indexes that should be defined. At most one of the indexes on each file may be a primary or a clustering index. Any number of additional secondary indexes can be reader should review the various types of indexes described in Section For a clearer understanding of this discussion it is also helpful to be familiar with the algorithms for query processing discussed in Chapter Chapter Physical Database Design and Tuning Design Decisions about Indexing. The attributes whose values are required in equality or range conditions are those that are keys or that participate in join conditions requiring access paths such as indexes. The performance of queries largely depends upon what indexes or hashing schemes exist to expedite the processing of selections and joins. On the other hand during insert delete or update operations the existence of indexes adds to the overhead. This overhead must be justified in terms of the gain in efficiency by expediting queries and transactions. The physical design decisions for indexing fall into the following categories Whether to index an attribute. The general rules for creating an index on an attribute are that the attribute must either be a key or there must be some query that uses that attribute either in a selection condition or in a join condition. One reason for creating multiple indexes is that some operations can be processed by just scanning the indexes without having to access the actual data file in a garment inventory database a multiattribute index is warranted. The ordering of attributes within a multiattribute index must correspond to the queries. For instance the above index assumes that queries would be based on an ordering of colors within a Garmentstyle# rather than vice versa. Whether to set up a clustered index. At most one index per table can be a primary or clustering index because this implies that the file be physically ordered on that attribute. In most RDBMSs this is specified by the keyword CLUSTER. the corresponding index should not be clustered since the main benefit of clustering is achieved when retrieving the records themselves. A clustering index may be set up as a multiattribute index if range retrieval by that composite key is useful in report creation . Whether to use a hash index over a tree index. In general RDBMSs use B+ trees for indexing. However ISAM and hash indexes are also provided in some systems but they do not support range queries. Whether to use dynamic hashing for the file. For files that are very volatile that is those that grow and shrink continuously one of the dynamic hashing schemes discussed in Section would be suitable. Currently they are not offered by many commercial RDBMSs. How to Create an Index. Many RDBMSs have a similar type of command for creating an index although it is not part of the SQL standard. The general form of this command is CREATE [ UNIQUE ] INDEX index name ON table name [ CLUSTER ] The keywords UNIQUE and CLUSTER are optional. The keyword CLUSTER is used when the index to be created should also sort the data file records on the indexing attribute. Thus specifying CLUSTER on a key attribute would create some variation of a primary index whereas specifying CLUSTER on a nonkey attribute would create some variation of a clustering index. The value for order can be either ASC or DESC and specifies whether the data file should be ordered in ascending or descending values of the indexing attribute. The default is ASC. For example the following would create a clustering index on the nonkey attribute Dno of the EMPLOYEE file CREATE INDEX DnoIndex ON EMPLOYEE CLUSTER Denormalization as a Design Decision for Speeding Up Queries. The ultimate goal during normalization which corresponds exactly to the headers in a report called The Employee Assignment Roster. This relation is only in because of the following functional dependencies Projid → Projname Projmgrid Projmgrid → Projmgrname Empid → Empname Empjobtitle This relation may be preferred over the design in PROJ EMPPROJ This is because to produce the The Employee Assignment Roster report the latter multirelation design requires two NATURAL JOIN operations plus a final JOIN between PROJ and EMP to retrieve the Projmgrname from the Projmgrid. Thus the following JOINs would be needed of the last EMP table which is not shown PROJ EMP It is also possible to create a view for the ASSIGN table. This does not mean that the join operations will be avoided but that the user need not specify the joins. If the view table is materialized the joins would be avoided but if the virtual view table is not stored as a materialized file the join computations would still be necessary. Other forms of denormalization consist of storing extra tables to maintain original functional dependencies that are lost during BCNF decomposition. For example Figure shows the TEACH relation with the functional dependencies Student Course → Instructor Instructor → Course . A lossless decomposition of TEACH into Instructor and Course does not allow queries of the form what course did student Smith take from instructor Navathe to be answered without joining and Therefore storing and TEACH may be a possible solution which reduces the design from BCNF to Here TEACH is a materialized join of the other two tables representing an extreme redundancy. Any updates to and would have to be applied to TEACH. An alternate strategy is to create and as updatable base tables and to create TEACH as a view on and that can only be queried. An Overview of Database Tuning in Relational Systems An Overview of Database Tuning in Relational Systems After a database is deployed and is in operation actual use of the applications transactions queries and views reveals factors and problem areas that may not have been accounted for during the initial physical design. The inputs to physical design listed in Section can be revised by gathering actual statistics about usage patterns. Resource utilization as well as internal DBMS processing such as query optimization can be monitored to reveal bottlenecks such as contention for the same data or devices. Volumes of activity and sizes of data can be better estimated. Therefore it is necessary to monitor and revise the physical database design constantly an activity referred to as database tuning. The goals of tuning are as follows To make applications run faster. To improve the response time of queries and transactions. To improve the overall throughput of transactions. The dividing line between physical design and tuning is very thin. The same design decisions that we discussed in Section are revisited during database tuning which is a continual adjustment of the physical design. We give a brief overview of the tuning process The inputs to the tuning process include statistics related to the same factors mentioned in Section In particular DBMSs can internally collect the following statistics Sizes of individual tables. Number of distinct values in a column. The number of times a particular query or transaction is submitted and executed in an interval of time. The times required for different phases of query and transaction processing . These and other statistics create a profile of the contents and use of the database. Other information obtained from monitoring the database system activities and processes includes the following Storage statistics. Data about allocation of storage into tablespaces indexspaces and buffer pools. I O and device performance statistics. Total read write activity on disk extents and disk hot spots. Query transaction processing statistics. Execution times of queries and transactions and optimization times during query optimization. readers should consult Shasha and Bonnet for a detailed discussion of tuning. Chapter Physical Database Design and Tuning Locking logging related statistics. Rates of issuing different types of locks transaction throughput rates and log records Index statistics. Number of levels in an index number of noncontiguous leaf pages and so on. Some of the above statistics relate to transactions concurrency control and recovery which are discussed in Chapters through Tuning a database involves dealing with the following types of problems How to avoid excessive lock contention thereby increasing concurrency among transactions. How to minimize the overhead of logging and unnecessary dumping of data. How to optimize the buffer size and scheduling of processes. How to allocate resources such as disks RAM and processes for most efficient utilization. Most of the previously mentioned problems can be solved by the DBA by setting appropriate physical DBMS parameters changing configurations of devices changing operating system parameters and other similar activities. The solutions tend to be closely tied to specific systems. The DBAs are typically trained to handle these tuning problems for the specific DBMS. We briefly discuss the tuning of various physical database design decisions below. Tuning Indexes The initial choice of indexes may have to be revised for the following reasons Certain queries may take too long to run for lack of an index. Certain indexes may not get utilized at all. Certain indexes may undergo too much updating because the index is on an attribute that undergoes frequent changes. Most DBMSs have a command or trace facility which can be used by the DBA to ask the system to show how a query was executed what operations were performed in what order and what secondary access structures were used. By analyzing these execution plans it is possible to diagnose the causes of the above problems. Some indexes may be dropped and some new indexes may be created based on the tuning analysis. The goal of tuning is to dynamically evaluate the requirements which sometimes fluctuate seasonally or during different times of the month or week and to reorganize the indexes and file organizations to yield the best overall performance. Dropping and building new indexes is an overhead that can be justified in terms of performance improvements. Updating of a table is generally suspended while an reader should preview Chapters for an explanation of these terms. An Overview of Database Tuning in Relational Systems index is dropped or created this loss of service must be accounted for. Besides dropping or creating indexes and changing from a nonclustered to a clustered index and vice versa rebuilding the index may improve performance. Most RDBMSs use B+ trees for an index. If there are many deletions on the index key index pages may contain wasted space which can be claimed during a rebuild operation. Similarly too many insertions may cause overflows in a clustered index that affect performance. Rebuilding a clustered index amounts to reorganizing the entire table ordered on that key. The available options for indexing and the way they are defined created and reorganized varies from system to system. As an illustration consider the sparse and dense indexes in Chapter A sparse index such as a primary index in the data file a dense index such as a unique secondary index will have an index pointer for each record. Sybase provides clustering indexes as sparse indexes in the form of B+ trees whereas INGRES provides sparse clustering indexes as ISAM files and dense clustering indexes as B+ trees. In some versions of Oracle and the option of setting up a clustering index is limited to a dense index and the DBA has to work with this limitation. Tuning the Database Design In Section we discussed the need for a possible denormalization which is a departure from keeping all tables as BCNF relations. If a given physical database design does not meet the expected objectives the DBA may revert to the logical database design make adjustments such as denormalizations to the logical schema and remap it to a new set of physical tables and indexes. As discussed the entire database design has to be driven by the processing requirements as much as by data requirements. If the processing requirements are dynamically changing the design needs to respond by making changes to the conceptual schema if necessary and to reflect those changes into the logical schema and physical design. These changes may be of the following nature Existing tables may be joined because certain attributes from two or more tables are frequently needed together This reduces the normalization level from BCNF to or For the given set of tables there may be alternative design choices all of which achieve or BCNF. We illustrated alternative equivalent designs in Chapter One normalized design may be replaced by another. A relation of the form R with K as a set of key attributes that is in BCNF can be stored in multiple tables that are also in BCNF for example A B C D replicating the key K in each table. Such a process is known as vertical partitioning. Each table groups that and address different types of problem dependencies that are independent of each other hence the normalization order between them is arbitrary. Chapter Physical Database Design and Tuning sets of attributes that are accessed together. For example the table EMPLOYEE may be split into two tables Name Phone and Grade Salary . If the original table has a large number of rows from one table may be repeated in another even though this creates redundancy and a potential anomaly. For example Partname may be replicated in tables wherever the Part# appears but there may be one master table called PARTMASTER where the Partname is guaranteed to be up to date. Just as vertical partitioning splits a table vertically into multiple tables horizontal partitioning takes horizontal slices of a table and stores them as distinct tables. For example product sales data may be separated into ten tables based on ten product lines. Each table has the same set of columns but contains a distinct set of products . If a query or transaction applies to all product data it may have to run against all the tables and the results may have to be combined. These types of adjustments designed to meet the high volume of queries or transactions with or without sacrificing the normal forms are commonplace in practice. Tuning Queries We already discussed how query performance is dependent upon the appropriate selection of indexes and how indexes may have to be tuned after analyzing queries that give poor performance by using the commands in the RDBMS that show the execution plan of the query. There are mainly two indications that suggest that query tuning may be needed A query issues too many disk accesses . The query plan shows that relevant indexes are not being used. Some typical instances of situations prompting query tuning include the following Many query optimizers do not use indexes in the presence of arithmetic expressions NULL comparisons and substring comparisons . Indexes are often not used for nested queries using IN for example the following query SELECT Ssn FROM EMPLOYEE WHERE Dno IN An Overview of Database Tuning in Relational Systems may not use the index on Dno in EMPLOYEE whereas using Dno Dnumber in the WHERE clause with a single block query may cause the index to be used. Some DISTINCTs may be redundant and can be avoided without changing the result. A DISTINCT often causes a sort operation and must be avoided as much as possible. Unnecessary use of temporary result tables can be avoided by collapsing multiple queries into a single query unless the temporary relation is needed for some intermediate processing. In some situations involving the use of correlated queries temporaries are useful. Consider the following query which retrieves the highest paid employee in each department SELECT Ssn FROM EMPLOYEE E WHERE Salary SELECT MAX FROM EMPLOYEE AS M WHERE This has the potential danger of searching all of the inner EMPLOYEE table M for each tuple from the outer EMPLOYEE table E. To make the execution more efficient the process can be broken into two queries where the first query just computes the maximum salary in each department as follows SELECT MAX AS Highsalary Dno INTO TEMP FROM EMPLOYEE GROUP BY Dno SELECT FROM EMPLOYEE TEMP WHERE AND If multiple options for a join condition are possible choose one that uses a clustering index and avoid those that contain string comparisons. For example assuming that the Name attribute is a candidate key in EMPLOYEE and STUDENT it is better to use as a join condition rather than if Ssn has a clustering index in one or both tables. One idiosyncrasy with some query optimizers is that the order of tables in the FROM clause may affect the join processing. If that is the case one may have to switch this order so that the smaller of the two relations is scanned and the larger relation is used with an appropriate index. Some query optimizers perform worse on nested queries compared to their equivalent unnested counterparts. There are four types of nested queries Uncorrelated subqueries with aggregates in an inner query. Uncorrelated subqueries without aggregates. Correlated subqueries with aggregates in an inner query. Chapter Physical Database Design and Tuning Correlated subqueries without aggregates. Of the four types above the first one typically presents no problem since most query optimizers evaluate the inner query once. However for a query of the second type such as the example in item most query optimizers may not use an index on Dno in EMPLOYEE. However the same optimizers may do so if the query is written as an unnested query. Transformation of correlated subqueries may involve setting temporary tables. Detailed examples are outside our scope Finally many applications are based on views that define the data of interest to those applications. Sometimes these views become overkill because a query may be posed directly against a base table rather than going through a view that is defined by a JOIN. Additional Query Tuning Guidelines Additional techniques for improving queries apply in certain situations as follows A query with multiple selection conditions that are connected via OR may not be prompting the query optimizer to use any index. Such a query may be split up and expressed as a union of queries each with a condition on an attribute that causes an index to be used. For example SELECT Fname Lname Salary FROM EMPLOYEE WHERE Age OR Salary may be executed using sequential scan giving poor performance. Splitting it up as SELECT Fname Lname Salary Age FROM EMPLOYEE WHERE Age UNION SELECT Fname Lname Salary Age FROM EMPLOYEE WHERE Salary may utilize indexes on Age as well as on Salary. To help expedite a query the following transformations may be tried NOT condition may be transformed into a positive expression. Embedded SELECT blocks using IN ALL ANY and SOME may be replaced by joins. If an equality join is set up between two tables the range predicate on the joining attribute set up in one table may be repeated for the other table. further details see Shasha and Bonnet modified the schema and used Age in EMPLOYEE instead of Bdate. Review Questions WHERE conditions may be rewritten to utilize the indexes on multiple columns. For example SELECT Region# Prodtype Month Sales FROM SALESSTATISTICS WHERE Region# AND and work much more efficiently. In this section we have covered many of the common instances where the inefficiency of a query may be fixed by some simple corrective action such as using a temporary table avoiding certain types of query constructs or avoiding the use of views. The goal is to have the RDBMS use existing single attribute or composite attribute indexes as much as possible. This avoids full scans of data blocks or entire scanning of index leaf nodes. Redundant processes like sorting must be avoided at any cost. The problems and the remedies will depend upon the workings of a query optimizer within an RDBMS. Detailed literature exists in database tuning guidelines for database administration by the RDBMS vendors. Major relational DBMS vendors like Oracle IBM and Microsoft encourage their large customers to share ideas of tuning at the annual expos and other forums so that the entire industry benefits by using performance enhancement techniques. These techniques are typically available in trade literature and on various Web sites. Summary In this chapter we discussed the factors that affect physical database design decisions and provided guidelines for choosing among physical design alternatives. We discussed changes to logical design such as denormalization as well as modifications of indexes and changes to queries to illustrate different techniques for database performance tuning. These are only a representative sample of a large number of measures and techniques adopted in the design of large commercial applications of relational DBMSs. Review Questions What are the important factors that influence physical database design Discuss the decisions made during physical database design. Discuss the guidelines for physical database design in RDBMSs. Chapter Physical Database Design and Tuning Discuss the types of modifications that may be applied to the logical database design of a relational database. Under what situations would denormalization of a database schema be used Give examples of denormalization. Discuss the tuning of indexes for relational databases. Discuss the considerations for reevaluating and modifying SQL queries. Illustrate the types of changes to SQL queries that may be worth considering for improving the performance during database tuning. Selected Bibliography Wiederhold covers issues related to physical design. O’Neil and O’Neil has a detailed discussion of physical design and transaction issues in reference to commercial RDBMSs. Navathe and Kerschberg discuss all phases of database design and point out the role of data dictionaries. Rozen and Shasha and Carlis and March present different models for the problem of physical database design. Shasha and Bonnet has an elaborate discussion of guidelines for database tuning. Niemiec is one among several books available for Oracle database administration and tuning Schneider is focused on designing and tuning MySQL databases. Transaction Processing Concurrency Control and Recovery This page intentionally left blank Introduction to Transaction Processing Concepts and Theory The concept of transaction provides a mechanism for describing logical units of database processing. Transaction processing systems are systems with large databases and hundreds of concurrent users executing database transactions. Examples of such systems include airline reservations banking credit card processing online retail purchasing stock markets supermarket checkouts and many other applications. These systems require high availability and fast response time for hundreds of concurrent users. In this chapter we present the concepts that are needed in transaction processing systems. We define the concept of a transaction which is used to represent a logical unit of database processing that must be completed in its entirety to ensure correctness. A transaction is typically implemented by a computer program which includes database commands such as retrievals insertions deletions and updates. We introduced some of the basic techniques for database programming in Chapters and In this chapter we focus on the basic concepts and theory that are needed to ensure the correct executions of transactions. We discuss the concurrency control problem which occurs when multiple transactions submitted by various users interfere with one another in a way that produces incorrect results. We also discuss the problems that can occur when transactions fail and how the database system can recover from various types of failures. This chapter is organized as follows. Section informally discusses why concurrency control and recovery are necessary in a database system. Section defines the term transaction and discusses additional concepts related to transaction processing in database systems. Section presents the important properties of atomicity consistency preservation isolation and durability or permanency called the chapter Chapter Introduction to Transaction Processing Concepts and Theory ACID properties that are considered desirable in transaction processing systems. Section introduces the concept of schedules of executing transactions and characterizes the recoverability of schedules. Section discusses the notion of serializability of concurrent transaction execution which can be used to define correct execution sequences of concurrent transactions. In Section we present some of the commands that support the transaction concept in SQL. Section summarizes the chapter. The two following chapters continue with more details on the actual methods and techniques used to support transaction processing. Chapter gives an overview of the basic concurrency control protocols and Chapter introduces recovery techniques. Introduction to Transaction Processing In this section we discuss the concepts of concurrent execution of transactions and recovery from transaction failures. Section compares single user and multiuser database systems and demonstrates how concurrent execution of transactions can take place in multiuser systems. Section defines the concept of transaction and presents a simple model of transaction execution based on read and write database operations. This model is used as the basis for defining and formalizing concurrency control and recovery concepts. Section uses informal examples to show why concurrency control techniques are needed in multiuser systems. Finally Section discusses why techniques are needed to handle recovery from system and transaction failures by discussing the different ways in which transactions can fail while executing. Single User versus Multiuser Systems One criterion for classifying a database system is according to the number of users who can use the system concurrently. A DBMS is single user if at most one user at a time can use the system and it is multiuserif many users can use the system and hence access the database concurrently. Single user DBMSs are mostly restricted to personal computer systems most other DBMSs are multiuser. For example an airline reservations system is used by hundreds of travel agents and reservation clerks concurrently. Database systems used in banks insurance agencies stock exchanges supermarkets and many other applications are multiuser systems. In these systems hundreds or thousands of users are typically operating on the database by submitting transactions concurrently to the system. Multiple users can access databases and use computer systems simultaneously because of the concept of multiprogramming which allows the operating system of the computer to execute multiple programs or processes at the same time. A single central processing unit can only execute at most one process at a time. However multiprogramming operating systems execute some commands from one process then suspend that process and execute some commands from the next Introduction to Transaction Processing A A B B C D CPU CPU Time Figure Interleaved processing versus parallel processing of concurrent transactions. process and so on. A process is resumed at the point where it was suspended whenever it gets its turn to use the CPU again. Hence concurrent execution of processes is actually interleaved as illustrated in Figure which shows two processes A and B executing concurrently in an interleaved fashion. Interleaving keeps the CPU busy when a process requires an input or output operation such as reading a block from disk. The CPU is switched to execute another process rather than remaining idle during I O time. Interleaving also prevents a long process from delaying other processes. If the computer system has multiple hardware processors parallel processing of multiple processes is possible as illustrated by processes C and D in Figure Most of the theory concerning concurrency control in databases is developed in terms of interleaved concurrency so for the remainder of this chapter we assume this model. In a multiuser DBMS the stored data items are the primary resources that may be accessed concurrently by interactive users or application programs which are constantly retrieving information from and modifying the database. Transactions Database Items Read and Write Operations and DBMS Buffers A transaction is an executing program that forms a logical unit of database processing. A transaction includes one or more database access operations these can include insertion deletion modification or retrieval operations. The database operations that form a transaction can either be embedded within an application program or they can be specified interactively via a high level query language such as SQL. One way of specifying the transaction boundaries is by specifying explicit begin transaction and end transaction statements in an application program in this case all database access operations between the two are considered as forming one transaction. A single application program may contain more than one transaction if it contains several transaction boundaries. If the database operations in a transaction do not update the database but only retrieve data the transaction is called a read only transaction otherwise it is known as a read write transaction. Chapter Introduction to Transaction Processing Concepts and Theory The database model that is used to present transaction processing concepts is quite simple when compared to the data models that we discussed earlier in the book such as the relational model or the object model. A database is basically represented as a collection of named data items. The size of a data item is called its granularity. A data item can be a database record but it can also be a larger unit such as a whole disk block or even a smaller unit such as an individual field value of some record in the database. The transaction processing concepts we discuss are independent of the data item granularity and apply to data items in general. Each data item has a unique name but this name is not typically used by the programmer rather it is just a means to uniquely identify each data item. For example if the data item granularity is one disk block then the disk block address can be used as the data item name. Using this simplified database model the basic database access operations that a transaction can include are as follows readitem. Reads a database item named X into a program variable. To simplify our notation we assume that the program variable is also named X. writeitem. Writes the value of program variable X into the database item named X. As we discussed in Chapter the basic unit of data transfer from disk to main memory is one block. Executing a readitem command includes the following steps Find the address of the disk block that contains item X. Copy that disk block into a buffer in main memory . Copy item X from the buffer to the program variable named X. Executing a writeitem command includes the following steps Find the address of the disk block that contains item X. Copy that disk block into a buffer in main memory . Copy item X from the program variable named X into its correct location in the buffer. Store the updated block from the buffer back to disk . It is step that actually updates the database on disk. In some cases the buffer is not immediately stored to disk in case additional changes are to be made to the buffer. Usually the decision about when to store a modified disk block whose contents are in a main memory buffer is handled by the recovery manager of the DBMS in cooperation with the underlying operating system. The DBMS will maintain in the database cache a number of data buffers in main memory. Each buffer typically holds the contents of one database disk block which contains some of the database items being processed. When these buffers are all occupied and additional database disk blocks must be copied into memory some buffer replacement policy is used to Introduction to Transaction Processing readitem X X – N writeitem readitem Y Y + N writeitem readitem X X + M writeitem T Figure Two sample transactions. Transaction Transaction choose which of the current buffers is to be replaced. If the chosen buffer has been modified it must be written back to disk before it is A transaction includes readitem and writeitem operations to access and update the database. Figure shows examples of two very simple transactions. The read set of a transaction is the set of all items that the transaction reads and the write set is the set of all items that the transaction writes. For example the read set of in Figure is X Y and its write set is also X Y . Concurrency control and recovery mechanisms are mainly concerned with the database commands in a transaction. Transactions submitted by the various users may execute concurrently and may access and update the same database items. If this concurrent execution is uncontrolled it may lead to problems such as an inconsistent database. In the next section we informally introduce some of the problems that may occur. Why Concurrency Control Is Needed Several problems can occur when concurrent transactions execute in an uncontrolled manner. We illustrate some of these problems by referring to a much simplified airline reservations database in which a record is stored for each airline flight. Each record includes the number of reserved seats on that flight as a named data item among other information. Figure shows a transaction that transfers N reservations from one flight whose number of reserved seats is stored in the database item named X to another flight whose number of reserved seats is stored in the database item named Y. Figure shows a simpler transaction that just reserves M seats on the first flight referenced in transaction To simplify our example we do not show additional portions of the transactions such as checking whether a flight has enough seats available before reserving additional seats. will not discuss buffer replacement policies here because they are typically discussed in operating systems textbooks. similar more commonly used example assumes a bank database with one transaction doing a transfer of funds from account X to account Y and the other transaction doing a deposit to account X. Chapter Introduction to Transaction Processing Concepts and Theory When a database access program is written it has the flight number flight date and the number of seats to be booked as parameters hence the same program can be used to execute many different transactions each with a different flight number date and number of seats to be booked. For concurrency control purposes a transaction is a particular execution of a program on a specific date flight and number of seats. In Figure and the transactions and are specific executions of the programs that refer to the specific flights whose numbers of seats are stored in data items X and Y in the database. Next we discuss the types of problems we may encounter with these two simple transactions if they run concurrently. The Lost Update Problem. This problem occurs when two transactions that access the same database items have their operations interleaved in a way that makes the value of some database items incorrect. Suppose that transactions and are submitted at approximately the same time and suppose that their operations are interleaved as shown in Figure then the final value of item X is incorrect because reads the value of X before changes it in the database and hence the updated value resulting from is lost. For example if X at the start N transfers seat reservations from the flight corresponding to X to the flight corresponding to Y and M reserves seats on X the final result should be X However in the interleaving of operations shown in Figure it is X because the update in that removed the five seats from X was lost. The Temporary Update Problem. This problem occurs when one transaction updates a database item and then the transaction fails for some reason by another transaction before it is changed back to its original value. Figure shows an example where updates item X and then fails before completion so the system must change X back to its original value. Before it can do so however transaction reads the temporary value of X which will not be recorded permanently in the database because of the failure of The value of item X that is read by is called dirty data because it has been created by a transaction that has not completed and committed yet hence this problem is also known as the dirty read problem. The Incorrect Summary Problem. If one transaction is calculating an aggregate summary function on a number of database items while other transactions are updating some of these items the aggregate function may calculate some values before they are updated and others after they are updated. For example suppose that a transaction is calculating the total number of reservations on all the flights meanwhile transaction is executing. If the interleaving of operations shown in Figure occurs the result of will be off by an amount N because reads the value of X after N seats have been subtracted from it but reads the value of Y before those N seats have been added to it. Introduction to Transaction Processing readitem X X – N writeitem readitem readitem X X + M writeitem Time Item X has an incorrect value because its update by T is lost . Y Y + N writeitem readitem X X – N writeitem readitem X X + M writeitem Time Transaction T fails and must change the value of X back to its old value meanwhile T has read the temporary incorrect value of X readitem T T readitem X X – N writeitem readitem Y Y + N writeitem readitem sum sum + X readitem sum sum + Y T reads X after N is subtracted and reads Y before N is added a wrong summary is the result . T T sum readitem sum sum + A T T Figure Some problems that occur when concurrent execution is uncontrolled. The lost update problem. The temporary update problem. The incorrect summary problem. Chapter Introduction to Transaction Processing Concepts and Theory The Unrepeatable Read Problem. Another problem that may occur is called unrepeatable read where a transaction T reads the same item twice and the item is changed by another transaction T between the two reads. Hence T receives different values for its two reads of the same item. This may occur for example if during an airline reservation transaction a customer inquires about seat availability on several flights. When the customer decides on a particular flight the transaction then reads the number of seats on that flight a second time before completing the reservation and it may end up reading a different value for the item. Why Recovery Is Needed Whenever a transaction is submitted to a DBMS for execution the system is responsible for making sure that either all the operations in the transaction are completed successfully and their effect is recorded permanently in the database or that the transaction does not have any effect on the database or any other transactions. In the first case the transaction is said to be committed whereas in the second case the transaction is aborted. The DBMS must not permit some operations of a transaction T to be applied to the database while other operations of T are not because the whole transaction is a logical unit of database processing. If a transaction fails after executing some of its operations but before executing all of them the operations already executed must be undone and have no lasting effect. Types of Failures. Failures are generally classified as transaction system and media failures. There are several possible reasons for a transaction to fail in the middle of execution A computer failure . A hardware software or network error occurs in the computer system during transaction execution. Hardware crashes are usually media failures for example main memory failure. A transaction or system error. Some operation in the transaction may cause it to fail such as integer overflow or division by zero. Transaction failure may also occur because of erroneous parameter values or because of a logical programming Additionally the user may interrupt the transaction during its execution. Local errors or exception conditions detected by the transaction. During transaction execution certain conditions may occur that necessitate cancellation of the transaction. For example data for the transaction may not be found. An exception such as insufficient account balance in a banking database may cause a transaction such as a fund withdrawal to be canceled. This exception could be programmed in the transaction itself and in such a case would not be considered as a transaction failure. general a transaction should be thoroughly tested to ensure that it does not have any bugs . conditions if programmed correctly do not constitute transaction failures. Transaction and System Concepts Concurrency control enforcement. The concurrency control method or whether the transaction has to be aborted because it violates serializability executed by the transaction can be safely committed to the database and will not be undone. ROLLBACK . This signals that the transaction has ended unsuccessfully so that any changes or effects that the transaction may have applied to the database must be undone. Figure shows a state transition diagram that illustrates how a transaction moves through its execution states. A transaction goes into an active state immediately after it starts execution where it can execute its READ and WRITE operations. When the transaction ends it moves to the partially committed state. At this point some recovery protocols need to ensure that a system failure will not result in an inability to record the changes of the transaction permanently main memory buffers hold the last part of the log file so that log entries are first added to the main memory buffer. When the log buffer is filled or when certain other conditions occur the log buffer is appended to the end of the log file on disk. In addition the log file from disk is periodically backed up to archival storage to guard against catastrophic failures. The following are the types of entries called log records that are written to the log file and the corresponding action for each log record. In these entries T refers to a unique transaction id that is generated automatically by the system for each transaction and that is used to identify each transaction [starttransaction T]. Indicates that transaction T has started execution. [writeitem T X oldvalue newvalue]. Indicates that transaction T has changed the value of database item X from oldvalue to newvalue. [readitem T X]. Indicates that transaction T has read the value of database item X. [commit T]. Indicates that transaction T has completed successfully and affirms that its effect can be committed to the database. [abort T]. Indicates that transaction T has been aborted. Protocols for recovery that avoid cascading rollbacks then such entries can be included. Additionally some recovery protocols require simpler WRITE entries only include one of newvalue and oldvalue instead of including both in the database at a given point in time. A consistent state of the database satisfies the constraints specified in the schema as well as any other constraints on the database that should hold. A database program should be written in a way that guarantees that if the database is in a consistent state before executing the transaction it will be in a consistent state after the complete execution of the transaction assuming that no interference with other transactions occurs. The isolation property is enforced by the concurrency control subsystem of the If every transaction does not make its updates visible to other transactions until it is committed one form of isolation is enforced that solves the temporary update problem and eliminates cascading rollbacks isolation if it does not overwrite the dirty reads of higher level transactions. Level isolation has no lost updates and level isolation has no lost updates and no dirty reads. Finally level isolation has in addition to level properties repeatable And last the durability property is the responsibility of the recovery subsystem of the DBMS. We will introduce how recovery protocols enforce durability and atomicity in the next section and then discuss this in more detail in Chapter Characterizing Schedules Based on Recoverability When transactions are executing concurrently in an interleaved fashion then the order of execution of operations from all the various transactions is known as a schedule . In this section first we define the concept of schedules and will discuss concurrency control protocols in Chapter SQL syntax for isolation level discussed later in Section is closely related to these levels. Chapter Introduction to Transaction Processing Concepts and Theory then we characterize the types of schedules that facilitate recovery when failures occur. In Section we characterize schedules in terms of the interference of participating transactions leading to the concepts of serializability and serializable schedules. Schedules of Transactions A schedule S of n transactions Tn is an ordering of the operations of the transactions. Operations from different transactions can be interleaved in the schedule S. However for each transaction Ti that participates in the schedule S the operations of Ti in S must appear in the same order in which they occur in Ti . The order of operations in S is considered to be a total ordering meaning that for any two operations in the schedule one must occur before the other. It is possible theoretically to deal with schedules whose operations form partial orders but we will assume for now total ordering of the operations in a schedule. For the purpose of recovery and concurrency control we are mainly interested in the readitem and writeitem operations of the transactions as well as the commit and abort operations. A shorthand notation for describing a schedule uses the symbols b r w e c and a for the operations begintransaction readitem writeitem endtransaction commit and abort respectively and appends as a subscript the transaction id to each operation in the schedule. In this notation the database item X that is read or written follows the r and w operations in parentheses. In some schedules we will only show the read and write operations whereas in other schedules we will show all the operations. For example the schedule in Figure which we shall call Sa can be written as follows in this notation Sa Similarly the schedule for Figure which we call Sb can be written as follows if we assume that transaction aborted after its readitem operation Sb Two operations in a schedule are said to conflict if they satisfy all three of the following conditions they belong to different transactions they access the same item X and at least one of the operations is a writeitem. For example in schedule Sa the operations and conflict as do the operations and and the operations and However the operations and do not conflict since they are both read operations the operations and do not conflict because they operate on distinct data items X and Y and the operations and do not conflict because they belong to the same transaction. Intuitively two operations are conflicting if changing their order can result in a different outcome. For example if we change the order of the two operations to then the value of X that is read by transaction changes because in the second order the value of X is changed by before it is read by Characterizing Schedules Based on Recoverability whereas in the first order the value is read before it is changed. This is called a read write conflict. The other type is called a write write conflict and is illustrated by the case where we change the order of two operations such as to For a write write conflict the last value of X will differ because in one case it is written by and in the other case by Notice that two read operations are not conflicting because changing their order makes no difference in outcome. The rest of this section covers some theoretical definitions concerning schedules. A schedule S of n transactions Tn is said to be a complete schedule if the following conditions hold The operations in S are exactly those operations in Tn including a commit or abort operation as the last operation for each transaction in the schedule. For any pair of operations from the same transaction Ti their relative order of appearance in S is the same as their order of appearance in Ti . For any two conflicting operations one of the two must occur before the other in the The preceding condition allows for two nonconflicting operations to occur in the schedule without defining which occurs first thus leading to the definition of a schedule as a partial order of the operations in the n However a total order must be specified in the schedule for any pair of conflicting operations of a schedule S which includes only the operations in S that belong to committed transactions that is transactions Ti whose commit operation ci is in S. Characterizing Schedules Based on Recoverability For some schedules it is easy to recover from transaction and system failures whereas for other schedules the recovery process can be quite involved. In some cases it is even not possible to recover correctly after a failure. Hence it is important to characterize the types of schedules for which recovery is possible as well as those for which recovery is relatively simple. These characterizations do not actually provide the recovery algorithm they only attempt to theoretically characterize the different types of schedules. it is not necessary to determine an order between pairs of nonconflicting operations. practice most schedules have a total order of operations. If parallel processing is employed it is theoretically possible to have schedules with partially ordered nonconflicting operations. Chapter Introduction to Transaction Processing Concepts and Theory First we would like to ensure that once a transaction T is committed it should never be necessary to roll back T. This ensures that the durability property of transactions is not violated . Some recoverable schedules may require a complex recovery process as we shall see but if sufficient information is kept a recovery algorithm can be devised for any recoverable schedule. The schedules Sa and Sb from the preceding section are both recoverable since they satisfy the above definition. Consider the schedule Sa given below which is the same as schedule Sa except that two commit operations have been added to Sa Sa Sais recoverable even though it suffers from the lost update problem this problem is handled by serializability theory schedules Sc and Sd that follow Sc Sd Se Sc is not recoverable because reads item X from but commits before commits. The problem occurs if aborts after the operation in Sc then the value of X that read is no longer valid and must be aborted after it is committed leading to a schedule that is not recoverable. For the schedule to be recoverable the operation in Sc must be postponed until after commits as shown in Sd. If aborts instead of committing then should also abort as shown in Se because the value of X it read is no longer valid. In Se aborting is acceptable since it has not committed yet which is not the case for the nonrecoverable schedule Sc . In a recoverable schedule no committed transaction ever needs to be rolled back and so the definition of committed transaction as durable is not violated. However it is possible for a phenomenon known as cascading rollback to occur in some recoverable schedules where an uncommitted transaction has to be rolled back because it read an item from a transaction that failed. This is illustrated in schedule Se where transaction has to be rolled back because it read item X from and then aborted. Because cascading rollback can be quite time consuming since numerous transactions can be rolled back thus delaying but ensuring no cascading rollback if aborts. Finally there is a third more restrictive type of schedule called a strict schedule in which transactions can neither read nor write an item X until the last transaction that wrote X has committed . Strict schedules simplify the recovery process. In a strict schedule the process of undoing a writeitem operation of an aborted transaction is simply to restore the before image of data item X. This simple procedure always works correctly for strict schedules but it may not work for recoverable or cascadeless schedules. For example consider schedule Sf Sf Suppose that the value of X was originally which is the before image stored in the system log along with the operation. If aborts as in Sf the recovery procedure that restores the before image of an aborted write operation will restore the value of X to even though it has already been changed to by transaction thus leading to potentially incorrect results. Although schedule Sf is cascadeless it is not a strict schedule since it permits to write item X even though the transaction that last wrote X had not yet committed . A strict schedule does not have this problem. It is important to note that any strict schedule is also cascadeless and any cascadeless schedule is also recoverable. Suppose we have i transactions Ti and their number of operations are ni respectively. If we make a set of all possible schedules of these transactions we can divide the schedules into two disjoint subsets recoverable and nonrecoverable. The cascadeless schedules will be a subset of the recoverable schedules and the strict schedules will be a subset of the cascadeless schedules. Thus all strict schedules are cascadeless and all cascadeless schedules are recoverable. Characterizing Schedules Based on Serializability In the previous section we characterized schedules based on their recoverability properties. Now we characterize the types of schedules that are always considered to be correct when concurrent transactions are executing. Such schedules are known as serializable schedules. Suppose that two users for example two airline reservations agents submit to the DBMS transactions and in Figure at approximately the same time. If no interleaving of operations is permitted there are only two possible outcomes Execute all the operations of transaction followed by all the operations of transaction . Schedule A Schedule B readitem X X – N writeitem readitem readitem X X + M writeitem Time Y Y + N writeitem readitem X X + M writeitem Time readitem X X – N writeitem readitem Y Y + N writeitem T T Schedule C Schedule D readitem X X – N writeitem readitem readitem X X + M writeitem Time Y Y + N writeitem readitem X X + M writeitem readitem X X – N writeitem readitem Y Y + N writeitem T T T T T T Time Chapter Introduction to Transaction Processing Concepts and Theory Figure Examples of serial and nonserial schedules involving transactions and Serial schedule A followed by Serial schedule B followed by Two nonserial schedules C and D with interleaving of operations. Execute all the operations of transaction followed by all the operations of transaction . These two schedules called serial schedules are shown in Figure and respectively. If interleaving of operations is allowed there will be many possible orders in which the system can execute the individual operations of the transactions. Two possible schedules are shown in Figure The concept of serializability of schedules is used to identify which schedules are correct when transaction executions have interleaving of their operations in the schedules. This section defines serializability and discusses how it may be used in practice. Characterizing Schedules Based on Serializability Serial Nonserial and Conflict Serializable Schedules Schedules A and B in Figure and are called serial because the operations of each transaction are executed consecutively without any interleaved operations from the other transaction. In a serial schedule entire transactions are performed in serial order and then in Figure and and then in Figure Schedules C and D in Figure are called nonserial because each sequence interleaves operations from the two transactions. Formally a schedule S is serial if for every transaction T participating in the schedule all the operations of T are executed consecutively in the schedule otherwise the schedule is called nonserial. Therefore in a serial schedule only one transaction at a time is active the commit of the active transaction initiates execution of the next transaction. No interleaving occurs in a serial schedule. One reasonable assumption we can make if we consider the transactions to be independent is that every serial schedule is considered correct. We can assume this because every transaction is assumed to be correct if executed on its own X X + writeitem readitem X X writeitem Figure Two schedules that are result equivalent for the initial value of X but are not result equivalent in general. The definition of serializable schedule is as follows A schedule S of n transactions is serializable if it is equivalent to some serial schedule of the same n transactions. We will define the concept of equivalence of schedules shortly. Notice that there are n! possible serial schedules of n transactions and many more possible nonserial schedules. We can form two disjoint groups of the nonserial schedules those that are equivalent to one of the serial schedules and hence are serializable and those that are not equivalent to any serial schedule and hence are not serializable. Saying that a nonserial schedule S is serializable is equivalent to saying that it is correct because it is equivalent to a serial schedule which is considered correct. The remaining question is When are two schedules considered equivalent There are several ways to define schedule equivalence. The simplest but least satisfactory definition involves comparing the effects of the schedules on the database. Two schedules are called result equivalent if they produce the same final state of the database. However two different schedules may accidentally produce the same final state. For example in Figure schedules and will produce the same final database state if they execute on a database with an initial value of X however for other initial values of X the schedules are not result equivalent. Additionally these schedules execute different transactions so they definitely should not be considered equivalent. Hence result equivalence alone cannot be used to define equivalence of schedules. The safest and most general approach to defining schedule equivalence is not to make any assumptions about the types of operations included in the transactions. For two schedules to be equivalent the operations applied to each data item affected by the schedules should be applied to that item in both schedules in the same order. Two definitions of equivalence of schedules are generally used conflict equivalence and view equivalence. We discuss conflict equivalence next which is the more commonly used definition. The definition of conflict equivalence of schedules is as follows Two schedules are said to be conflict equivalent if the order of any two conflicting operations is the same in both schedules. Recall from Section that two operations in a schedule are said to conflict if they belong to different transactions access the same database item and either both are writeitem operations or one is a writeitem and the other a readitem. If two conflicting operations are applied in different orders in two schedules the effect can be different on the database or on the transactions in the schedule and hence the schedules are not conflict equivalent. For example as we discussed in Section if a read and write operation occur in the order in schedule and in the reverse order in schedule the value read by can be different in the two schedules. Similarly if two write operations Characterizing Schedules Based on Serializability occur in the order in and in the reverse order in the next r operation in the two schedules will read potentially different values or if these are the last operations writing item X in the schedules the final value of item X in the database will be different. Using the notion of conflict equivalence we define a schedule S to be conflict if it is equivalent to some serial schedule S. In such a case we can reorder the nonconflicting operations in S until we form the equivalent serial schedule S. According to this definition schedule D in Figure is equivalent to the serial schedule A in Figure In both schedules the readitem of reads the value of X written by while the other readitem operations read the database values from the initial database state. Additionally is the last transaction to write Y and is the last transaction to write X in both schedules. Because A is a serial schedule and schedule D is equivalent to A D is a serializable schedule. Notice that the operations and of schedule D do not conflict with the operations and since they access different data items. Therefore we can move before leading to the equivalent serial schedule Schedule C in Figure is not equivalent to either of the two possible serial schedules A and B and hence is not serializable. Trying to reorder the operations of schedule C to find an equivalent serial schedule fails because and conflict which means that we cannot move down to get the equivalent serial schedule Similarly because and conflict we cannot move down to get the equivalent serial schedule Another more complex definition of equivalence called view equivalence which leads to the concept of view serializability is discussed in Section Testing for Conflict Serializability of a Schedule There is a simple algorithm for determining whether a particular schedule is conflict serializable or not. Most concurrency control methods do not actually test for serializability. Rather protocols or rules are developed that guarantee that any schedule that follows these rules will be serializable. We discuss the algorithm for testing conflict serializability of schedules here to gain a better understanding of these concurrency control protocols which are discussed in Chapter Algorithm can be used to test a schedule for conflict serializability. The algorithm looks at only the readitem and writeitem operations in a schedule to construct a precedence graph which is a directed graph G that consists of a set of nodes N Tn and a set of directed edges E em . There is one node in the graph for each transaction Ti in the schedule. Each edge ei in the graph is of the form ≤ j ≤ n ≤ k f n where Tj is the starting node of ei and Tk is the ending node of ei . Such an edge from node Tj to will use serializable to mean conflict serializable. Another definition of serializable used in practice . Chapter Introduction to Transaction Processing Concepts and Theory node Tk is created by the algorithm if one of the operations in Tj appears in the schedule before some conflicting operation in Tk. Algorithm Testing Conflict Serializability of a Schedule S For each transaction Ti participating in schedule S create a node labeled Ti in the precedence graph. For each case in S where Tj executes a readitem after Ti executes a writeitem create an edge in the precedence graph. For each case in S where Tj executes a writeitem after Ti executes a readitem create an edge in the precedence graph. For each case in S where Tj executes a writeitem after Ti executes a writeitem create an edge in the precedence graph. The schedule S is serializable if and only if the precedence graph has no cycles. The precedence graph is constructed as described in Algorithm If there is a cycle in the precedence graph schedule S is not serializable if there is no cycle S is serializable. A cycle in a directed graph is a sequence of edges C with the property that the starting node of each edge except the first edge is the same as the ending node of the previous edge and the starting node of the first edge is the same as the ending node of the last edge . In the precedence graph an edge from Ti to Tj means that transaction Ti must come before transaction Tj in any serial schedule that is equivalent to S because two conflicting operations appear in the schedule in that order. If there is no cycle in the precedence graph we can create an equivalent serial schedule Sthat is equivalent to S by ordering the transactions that participate in S as follows Whenever an edge exists in the precedence graph from Ti to Tj Ti must appear before Tj in the equivalent serial schedule S. Notice that the edges in a precedence graph can optionally be labeled by the name of the data item that led to creating the edge. Figure shows such labels on the edges. In general several serial schedules can be equivalent to S if the precedence graph for S has no cycle. However if the precedence graph has a cycle it is easy to show that we cannot create any equivalent serial schedule so S is not serializable. The precedence graphs created for schedules A to D respectively in Figure appear in Figure to . The graph for schedule C has a cycle so it is not serializable. The graph for schedule D has no cycle so it is serializable and the equivalent serial schedule is followed by The graphs for schedules A and B have no cycles as expected because the schedules are serial and hence serializable. Another example in which three transactions participate is shown in Figure Figure shows the readitem and writeitem operations in each transaction. Two schedules E and F for these transactions are shown in Figure and process of ordering the nodes of an acrylic graph is known as topological sorting. Characterizing Schedules Based on Serializability T T T X X X X T T T T T X Figure Constructing the precedence graphs for schedules A to D from Figure to test for conflict serializability. Precedence graph for serial schedule A. Precedence graph for serial schedule B. Precedence graph for schedule C . Precedence graph for schedule D . respectively and the precedence graphs for schedules E and F are shown in parts and . Schedule E is not serializable because the corresponding precedence graph has cycles. Schedule F is serializable and the serial schedule equivalent to F is shown in Figure Although only one equivalent serial schedule exists for F in general there may be more than one equivalent serial schedule for a serializable schedule. Figure shows a precedence graph representing a schedule that has two equivalent serial schedules. To find an equivalent serial schedule start with a node that does not have any incoming edges and then make sure that the node order for every edge is not violated. How Serializability Is Used for Concurrency Control As we discussed earlier saying that a schedule S is serializable that is S is equivalent to a serial schedule is tantamount to saying that S is correct. Being serializable is distinct from being serial however. A serial schedule represents inefficient processing because no interleaving of operations from different transactions is permitted. This can lead to low CPU utilization while a transaction waits for disk I O or for another transaction to terminate thus slowing down processing considerably. A serializable schedule gives the benefits of concurrent execution without giving up any correctness. In practice it is quite difficult to test for the serializability of a schedule. The interleaving of operations from concurrent transactions which are usually executed as processes by the operating system is typically determined by the operating system scheduler which allocates resources to Chapter Introduction to Transaction Processing Concepts and Theory Transaction T readitem writeitem readitem writeitem readitem writeitem readitem writeitem Transaction T readitem readitem writeitem writeitem readitem readitem writeitem writeitem Transaction T readitem readitem writeitem readitem writeitem readitem readitem writeitem readitem writeitem Schedule E Time readitem writeitem readitem writeitem readitem readitem writeitem writeitem readitem readitem writeitem readitem writeitem Schedule F Time Transaction T Transaction T Transaction T Transaction T Transaction T Transaction T Figure Another example of serializability testing. The read and write operations of three transactions and Schedule E. Schedule F. all processes. Factors such as system load time of transaction submission and priorities of processes contribute to the ordering of operations in a schedule. Hence it is difficult to determine how the operations of a schedule will be interleaved beforehand to ensure serializability. Characterizing Schedules Based on Serializability X Y Y Y Z T Equivalent serial schedules None Reason Cycle X T T Y T T Cycle X T T YZ Y T T X Y Y Y Z Equivalent serial schedules Equivalent serial schedules T T T T T T T T T T T T T T T T T If transactions are executed at will and then the resulting schedule is tested for serializability we must cancel the effect of the schedule if it turns out not to be serializable. This is a serious problem that makes this approach impractical. Hence the approach taken in most practical systems is to determine methods or protocols that ensure serializability without having to test the schedules themselves. The approach taken in most commercial DBMSs is to design protocols that if followed by every individual transaction or if enforced by a DBMS concurrency control subsystem will ensure serializability of all schedules in which the transactions participate. Another problem appears here When transactions are submitted continuously to the system it is difficult to determine when a schedule begins and when it ends. Serializability theory can be adapted to deal with this problem by considering only the committed projection of a schedule S. Recall from Section that the committed projection C of a schedule S includes only the operations in S that belong to committed transactions. We can theoretically define a schedule S to be serializable if its committed projection C is equivalent to some serial schedule since only committed transactions are guaranteed by the DBMS. Figure Another example of serializability testing. Precedence graph for schedule E. Precedence graph for schedule F. Precedence graph with two equivalent serial schedules. Chapter Introduction to Transaction Processing Concepts and Theory In Chapter we discuss a number of different concurrency control protocols that guarantee serializability. The most common technique called two phase locking is based on locking data items to prevent concurrent transactions from interfering with one another and enforcing an additional condition that guarantees serializability. This is used in the majority of commercial DBMSs. Other protocols have been these include timestamp ordering where each transaction is assigned a unique timestamp and the protocol ensures that any conflicting operations are executed in the order of the transaction timestamps multiversion protocols which are based on maintaining multiple versions of data items and optimistic protocols which check for possible serializability violations after the transactions terminate but before they are permitted to commit. View Equivalence and View Serializability In Section we defined the concepts of conflict equivalence of schedules and conflict serializability. Another less restrictive definition of equivalence of schedules is called view equivalence. This leads to another definition of serializability called view serializability. Two schedules S and S are said to be view equivalent if the following three conditions hold The same set of transactions participates in S and S and S and Sinclude the same operations of those transactions. For any operation ri of Ti in S if the value of X read by the operation has been written by an operation wj of Tj the same condition must hold for the value of X read by operation ri of Ti in S. If the operation wk of Tk is the last operation to write item Y in S then wk of Tk must also be the last operation to write item Y in S. The idea behind view equivalence is that as long as each read operation of a transaction reads the result of the same write operation in both schedules the write operations of each transaction must produce the same results. The read operations are hence said to see the same view in both schedules. Condition ensures that the final write operation on each data item is the same in both schedules so the database state should be the same at the end of both schedules. A schedule S is said to be view serializable if it is view equivalent to a serial schedule. The definitions of conflict serializability and view serializability are similar if a condition known as the constrained write assumption holds on all transactions in the schedule. This condition states that any write operation wi in Ti is preceded by a ri in Ti and that the value written by wi in Ti depends only on the value of X read by ri . This assumes that computation of the new value of X is a function f based on the old value of X read from the database. A blind write is a write operation in a transaction T on an item X that is not dependent on the value of X so it is not preceded by a read of X in the transaction T. other protocols have not been incorporated much into commercial systems most relational DBMSs use some variation of the two phase locking protocol. Characterizing Schedules Based on Serializability The definition of view serializability is less restrictive than that of conflict serializability under the unconstrained write assumption where the value written by an operation wi in Ti can be independent of its old value from the database. This is possible when blind writes are allowed and it is illustrated by the following schedule Sg of three transactions and Sg In Sg the operations and are blind writes since and do not read the value of X. The schedule Sg is view serializable since it is view equivalent to the serial schedule However Sg is not conflict serializable since it is not conflict equivalent to any serial schedule. It has been shown that any conflictserializable schedule is also view serializable but not vice versa as illustrated by the preceding example. There is an algorithm to test whether a schedule S is view serializable or not. However the problem of testing for view serializability has been shown to be NP hard meaning that finding an efficient polynomial time algorithm for this problem is highly unlikely. Other Types of Equivalence of Schedules Serializability of schedules is sometimes considered to be too restrictive as a condition for ensuring the correctness of concurrent executions. Some applications can produce schedules that are correct by satisfying conditions less stringent than either conflict serializability or view serializability. An example is the type of transactions known as debit credit transactions for example those that apply deposits and withdrawals to a data item whose value is the current balance of a bank account. The semantics of debit credit operations is that they update the value of a data item X by either subtracting from or adding to the value of the data item. Because addition and subtraction operations are commutative that is they can be applied in any order it is possible to produce correct schedules that are not serializable. For example consider the following transactions each of which may be used to transfer an amount of money between two bank accounts X X − Y Y + Y Y − X X + Consider the following nonserializable schedule Sh for the two transactions Sh With the additional knowledge or semantics that the operations between each ri and wi are commutative we know that the order of executing the sequences consisting of is not important as long as each sequence by a particular transaction Ti on a particular item I is not interrupted by conflicting operations. Hence the schedule Sh is considered to be correct even though it is not serializable. Researchers have been working on extending concurrency control theory to deal with cases where serializability is considered to be too restrictive as a condition for correctness of schedules. Also in certain domains of applications such as computer aided design of complex systems like aircraft Chapter Introduction to Transaction Processing Concepts and Theory design transactions last over a long time period. In such applications more relaxed schemes of concurrency control have been proposed to maintain consistency of the database. Transaction Support in SQL In this section we give a brief introduction to transaction support in SQL. There are many more details and the newer standards have more commands for transaction processing. The basic definition of an SQL transaction is similar to our already defined concept of a transaction. That is it is a logical unit of work and is guaranteed to be atomic. A single SQL statement is always considered to be atomic either it completes execution without an error or it fails and leaves the database unchanged. With SQL there is no explicit BeginTransaction statement. Transaction initiation is done implicitly when particular SQL statements are encountered. However every transaction must have an explicit end statement which is either a COMMIT or a ROLLBACK. Every transaction has certain characteristics attributed to it. These characteristics are specified by a SET TRANSACTION statement in SQL. The characteristics are the access mode the diagnostic area size and the isolation level. The access mode can be specified as READ ONLY or READ WRITE. The default is READ WRITE unless the isolation level of READ UNCOMMITTED is specified in which case READ ONLY is assumed. A mode of READ WRITE allows select update insert delete and create commands to be executed. A mode of READ ONLY as the name implies is simply for data retrieval. The diagnostic area size option DIAGNOSTIC SIZE n specifies an integer value n which indicates the number of conditions that can be held simultaneously in the diagnostic area. These conditions supply feedback information to the user or program on the n most recently executed SQL statement. The isolation level option is specified using the statement ISOLATION LEVEL isolation where the value for isolation can be READ UNCOMMITTED READ COMMITTED REPEATABLE READ or SERIALIZABLE. The default isolation level is SERIALIZABLE although some systems use READ COMMITTED as their default. The use of the term SERIALIZABLE here is based on not allowing violations that cause dirty read unrepeatable read and and it is thus not identical to the way serializability was defined earlier in Section If a transaction executes at a lower isolation level than SERIALIZABLE then one or more of the following three violations may occur Dirty read. A transaction may read the update of a transaction which has not yet committed. If fails and is aborted then would have read a value that does not exist and is incorrect. are similar to the isolation levels discussed briefly at the end of Section dirty read and unrepeatable read problems were discussed in Section Phantoms are discussed in Section Transaction Support in SQL Table Possible Violations Based on Isolation Levels as Defined in SQL Type of Violation Isolation Level Dirty Read Nonrepeatable Read Phantom READ UNCOMMITTED Yes Yes Yes READ COMMITTED No Yes Yes REPEATABLE READ No No Yes SERIALIZABLE No No No Nonrepeatable read. A transaction may read a given value from a table. If another transaction later updates that value and reads that value again will see a different value. Phantoms. A transaction may read a set of rows from a table perhaps based on some condition specified in the SQL WHERE clause. Now suppose that a transaction inserts a new row that also satisfies the WHERE clause condition used in into the table used by If is repeated then will see a phantom a row that previously did not exist. Table summarizes the possible violations for the different isolation levels. An entry of Yes indicates that a violation is possible and an entry of No indicates that it is not possible. READ UNCOMMITTED is the most forgiving and SERIALIZABLE is the most restrictive in that it avoids all three of the problems mentioned above. A sample SQL transaction might look like the following EXEC SQL WHENEVER SQLERROR GOTO UNDO EXEC SQL SET TRANSACTION READ WRITE DIAGNOSTIC SIZE ISOLATION LEVEL SERIALIZABLE EXEC SQL INSERT INTO EMPLOYEE VALUES would be restored to its previous value and that the newly inserted row would be removed. As we have seen SQL provides a number of transaction oriented features. The DBA or database programmers can take advantage of these options to try improving Chapter Introduction to Transaction Processing Concepts and Theory transaction performance by relaxing serializability if that is acceptable for their applications. Summary In this chapter we discussed DBMS concepts for transaction processing. We introduced the concept of a database transaction and the operations relevant to transaction processing. We compared single user systems to multiuser systems and then presented examples of how uncontrolled execution of concurrent transactions in a multiuser system can lead to incorrect results and database values. We also discussed the various types of failures that may occur during transaction execution. Next we introduced the typical states that a transaction passes through during execution and discussed several concepts that are used in recovery and concurrency control methods. The system log keeps track of database accesses and the system uses this information to recover from failures. A transaction either succeeds and reaches its commit point or it fails and has to be rolled back. A committed transaction has its changes permanently recorded in the database. We presented an overview of the desirable properties of transactions atomicity consistency preservation isolation and durability which are often referred to as the ACID properties. Then we defined a schedule as an execution sequence of the operations of several transactions with possible interleaving. We characterized schedules in terms of their recoverability. Recoverable schedules ensure that once a transaction commits it never needs to be undone. Cascadeless schedules add an additional condition to ensure that no aborted transaction requires the cascading abort of other transactions. Strict schedules provide an even stronger condition that allows a simple recovery scheme consisting of restoring the old values of items that have been changed by an aborted transaction. We defined equivalence of schedules and saw that a serializable schedule is equivalent to some serial schedule. We defined the concepts of conflict equivalence and view equivalence which led to definitions for conflict serializability and view serializability. A serializable schedule is considered correct. We presented an algorithm for testing the serializability of a schedule. We discussed why testing for serializability is impractical in a real system although it can be used to define and verify concurrency control protocols and we briefly mentioned less restrictive definitions of schedule equivalence. Finally we gave a brief overview of how transaction concepts are used in practice within SQL. Review Questions What is meant by the concurrent execution of database transactions in a multiuser system Discuss why concurrency control is needed and give informal examples. Exercises Discuss the different types of failures. What is meant by catastrophic failure Discuss the actions taken by the readitem and writeitem operations on a database. Draw a state diagram and discuss the typical states that a transaction goes through during execution. What is the system log used for What are the typical kinds of records in a system log What are transaction commit points and why are they important Discuss the atomicity durability isolation and consistency preservation properties of a database transaction. What is a schedule Define the concepts of recoverable cascadeless and strict schedules and compare them in terms of their recoverability. Discuss the different measures of transaction equivalence. What is the difference between conflict equivalence and view equivalence What is a serial schedule What is a serializable schedule Why is a serial schedule considered correct Why is a serializable schedule considered correct What is the difference between the constrained write and the unconstrained write assumptions Which is more realistic Discuss how serializability is used to enforce concurrency control in a database system. Why is serializability sometimes considered too restrictive as a measure of correctness for schedules Describe the four levels of isolation in SQL. Define the violations caused by each of the following dirty read nonrepeatable read and phantoms. Exercises Change transaction in Figure to read readitem X X + M if X then exit else writeitem Discuss the final result of the different schedules in Figure and where M and N with respect to the following questions Does adding the above condition change the final outcome Does the outcome obey the implied consistency rule and which are not. How many serial schedules exist for the three transactions in Figure What are they What is the total number of possible schedules Write a program to create all possible schedules for the three transactions in Figure and to determine which of those schedules are conflict serializable and which are not. For each conflict serializable schedule your program should print the schedule and list all equivalent serial schedules. Why is an explicit transaction end statement needed in SQL but not an explicit begin statement Describe situations where each of the different isolation levels would be useful for transaction processing. Which of the following schedules is serializable For each serializable schedule determine the equivalent serial schedules. a. b. c. d. Consider the three transactions and and the schedules and given below. Draw the serializability graphs for and and state whether each schedule is serializable or not. If a schedule is serializable write down the equivalent serial schedule. Consider schedules and below. Determine whether each schedule is strict cascadeless recoverable or nonrecoverable. Selected Bibliography The concept of serializability and related ideas to maintain consistency in a database were introduced in Gray et al. The concept of the database transaction was first discussed in Gray Gray won the coveted ACM Turing Award in for his work on database transactions and implementation of transactions in relational DBMSs. Bernstein Hadzilacos and Goodman focus on concurrency control and recovery techniques in both centralized and distributed database systems it is an excellent reference. Papadimitriou offers a more theoretical perspective. A large reference book of more than a thousand pages by Gray and Reuter offers a more practical perspective of transaction processing concepts and techniques. Elmagarmid offers collections of research papers on transaction processing for advanced applications. Transaction support in SQL is described in Date and Darwen View serializability is defined in Yannakakis Recoverability of schedules and reliability in databases is discussed in Hadzilacos Selected Bibliography This page intentionally left blank Concurrency Control Techniques I n this chapter we discuss a number of concurrency control techniques that are used to ensure the noninterference or isolation property of concurrently executing transactions. Most of these techniques ensure serializability of schedules which we defined in Section concurrency control protocols that guarantee serializability. One important set of protocols known as two phase locking protocols employ the technique of locking data items to prevent multiple transactions from accessing the items concurrently a number of locking protocols are described in Sections and Locking protocols are used in most commercial DBMSs. Another set of concurrency control protocols use timestamps. A timestamp is a unique identifier for each transaction generated by the system. Timestamps values are generated in the same order as the transaction start times. Concurrency control protocols that use timestamp ordering to ensure serializability are introduced in Section In Section we discuss multiversion concurrency control protocols that use multiple versions of a data item. One multiversion protocol extends timestamp order to multiversion timestamp ordering value or as large as a disk block or even a whole file or the entire database. We discuss granularity of items and a multiple granularity concurrency control protocol which is an extension of two phase locking in Section In Section we describe concurrency control issues that arise when chapter Chapter Concurrency Control Techniques indexes are used to process transactions and in Section we discuss some additional concurrency control concepts. Section summarizes the chapter. It is sufficient to read Sections and and possibly if your main interest is an introduction to the concurrency control techniques that are based on locking which are used most often in practice. The other techniques are mainly of theoretical interest. Two Phase Locking Techniques for Concurrency Control Some of the main techniques used to control concurrent execution of transactions are based on the concept of locking data items. A lock is a variable associated with a data item that describes the status of the item with respect to possible operations that can be applied to it. Generally there is one lock for each data item in the database. Locks are used as a means of synchronizing the access by concurrent transactions to the database items. In Section we discuss the nature and types of locks. Then in Section we present protocols that use locking to guarantee serializability of transaction schedules. Finally in Section we describe two problems associated with the use of locks deadlock and starvation and show how these problems are handled in concurrency control protocols. Types of Locks and System Lock Tables Several types of locks are used in concurrency control. To introduce locking concepts gradually first we discuss binary locks which are simple but are also too restrictive for database concurrency control purposes and so are not used in practice. Then we discuss shared exclusive locks also known as read write locks which provide more general locking capabilities and are used in practical database locking schemes. In Section we describe an additional type of lock called a certify lock and show how it can be used to improve performance of locking protocols. Binary Locks. A binary lock can have two states or values locked and unlocked . A distinct lock is associated with each database item X. If the value of the lock on X is item X cannot be accessed by a database operation that requests the item. If the value of the lock on X is the item can be accessed when requested and the lock value is changed to We refer to the current value of the lock associated with item X as lock. Two operations lockitem and unlockitem are used with binary locking. A transaction requests access to an item X by first issuing a lockitem operation. If LOCK the transaction is forced to wait. If LOCK it is set to and the transaction is allowed to access item X. When the transaction is through using the item it issues an unlockitem operation which sets LOCK back to so that X may be accessed by other transactions. Hence a binary lock enforces mutual exclusion on the data item. A description of the lockitem and unlockitem operations is shown in Figure Two Phase Locking Techniques for Concurrency Control lockitem B if LOCK then LOCK else begin wait and the lock manager wakes up the transaction go to B end unlockitem LOCK ← if any transactions are waiting then wakeup one of the waiting transactions Figure Lock and unlock operations for binary locks. Notice that the lockitem and unlockitem operations must be implemented as indivisible units that is no interleaving should be allowed once a lock or unlock operation is started until the operation terminates or the transaction waits. In Figure the wait command within the lockitem operation is usually implemented by putting the transaction in a waiting queue for item X until X is unlocked and the transaction can be granted access to it. Other transactions that also want to access X are placed in the same queue. Hence the wait command is considered to be outside the lockitem operation. It is quite simple to implement a binary lock all that is needed is a binary valued variable LOCK associated with each data item X in the database. In its simplest form each lock can be a record with three fields Dataitemname LOCK Lockingtransaction plus a queue for transactions that are waiting to access the item. The system needs to maintain only these records for the items that are currently locked in a lock table which could be organized as a hash file on the item name. Items not in the lock table are considered to be unlocked. The DBMS has a lock manager subsystem to keep track of and control access to locks. If the simple binary locking scheme described here is used every transaction must obey the following rules A transaction T must issue the operation lockitem before any readitem or writeitem operations are performed in T. A transaction T must issue the operation unlockitem after all readitem and writeitem operations are completed in T. A transaction T will not issue a lockitem operation if it already holds the lock on item X. A transaction T will not issue an unlockitem operation unless it already holds the lock on item X. rule may be removed if we modify the lockitem operation in Figure so that if the item is currently locked by the requesting transaction the lock is granted. Chapter Concurrency Control Techniques These rules can be enforced by the lock manager module of the DBMS. Between the lockitem and unlockitem operations in transaction T T is said to hold the lock on item X. At most one transaction can hold the lock on a particular item. Thus no two transactions can access the same item concurrently. Shared Exclusive Locks. The preceding binary locking scheme is too restrictive for database items because at most one transaction can hold a lock on a given item. We should allow several transactions to access the same item X if they all access X for reading purposes only. This is because read operations on the same item by different transactions are not conflicting writelock and unlock. A lock associated with an item X LOCK now has three possible states read locked write locked or unlocked. A read locked item is also called share locked because other transactions are allowed to read the item whereas a write locked item is called exclusive locked because a single transaction exclusively holds the lock on the item. One method for implementing the preceding operations on a read write lock is to keep track of the number of transactions that hold a shared lock on an item in the lock table. Each record in the lock table will have four fields Dataitemname LOCK Noofreads Lockingtransaction . Again to save space the system needs to maintain lock records only for locked items in the lock table. The value of LOCK is either read locked or write locked suitably coded . If LOCK write locked the value of lockingtransaction is a single transaction that holds the exclusive lock on X. If LOCK read locked the value of locking transaction is a list of one or more transactions that hold the shared lock on X. The three operations readlock writelock and unlock are described in Figure As before each of the three locking operations should be considered indivisible no interleaving should be allowed once one of the operations is started until either the operation terminates by granting the lock or the transaction is placed in a waiting queue for the item. When we use the shared exclusive locking scheme the system must enforce the following rules A transaction T must issue the operation readlock or writelock before any readitem operation is performed in T. A transaction T must issue the operation writelock before any writeitem operation is performed in T. algorithms do not allow upgrading or downgrading of locks as described later in this section. The reader can extend the algorithms to allow these additional operations. readlock B if LOCK “unlocked” thenbegin LOCK ← “read locked” noofreads ← end else if LOCK “read locked” then noofreads ← noofreads + else begin wait “unlocked” and the lock manager wakes up the transaction go to B end writelock B if LOCK “unlocked” then LOCK ← “write locked” else begin wait “unlocked” and the lock manager wakes up the transaction go to B end unlock if LOCK “write locked” thenbegin LOCK ← “unlocked” wakeup one of the waiting transactions if any end else it LOCK “read locked” thenbegin noofreads ← noofreads if noofreads thenbegin LOCK “unlocked” wakeup one of the waiting transactions if any end end Two Phase Locking Techniques for Concurrency Control Figure Locking and unlocking operations for twomode locks. A transaction T must issue the operation unlock after all readitem and writeitem operations are completed in A transaction T will not issue a readlock operation if it already holds a read lock or a write lock on item X. This rule may be relaxed as we discuss shortly. rule may be relaxed to allow a transaction to unlock an item then lock it again later. Chapter Concurrency Control Techniques A transaction T will not issue a writelock operation if it already holds a read lock or write lock on item X. This rule may also be relaxed as we discuss shortly. A transaction T will not issue an unlock operation unless it already holds a read lock or a write lock on item X. Conversion of Locks. Sometimes it is desirable to relax conditions and in the preceding list in order to allow lock conversion that is a transaction that already holds a lock on item X is allowed under certain conditions to convert the lock from one locked state to another. For example it is possible for a transaction T to issue a readlock and then later to upgrade the lock by issuing a writelock operation. If T is the only transaction holding a read lock on X at the time it issues the writelock operation the lock can be upgraded otherwise the transaction must wait. It is also possible for a transaction T to issue a writelock and then later to downgrade the lock by issuing a readlock operation. When upgrading and downgrading of locks is used the lock table must include transaction identifiers in the record structure for each lock field to store the information on which transactions hold locks on the item. The descriptions of the readlock and writelock operations in Figure must be changed appropriately to allow for lock upgrading and downgrading. We leave this as an exercise for the reader. Using binary locks or read write locks in transactions as described earlier does not guarantee serializability of schedules on its own. Figure shows an example where the preceding locking rules are followed but a nonserializable schedule may result. This is because in Figure the items Y in and X in were unlocked too early. This allows a schedule such as the one shown in Figure to occur which is not a serializable schedule and hence gives incorrect results. To guarantee serializability we must follow an additional protocol concerning the positioning of locking and unlocking operations in every transaction. The best known protocol two phase locking is described in the next section. Guaranteeing Serializability by Two Phase Locking A transaction is said to follow the two phase locking protocol if all locking operations precede the first unlock operation in the Such a transaction can be divided into two phases an expanding or growing phase during which new locks on items can be acquired but none can be released and a shrinking phase during which existing locks can be released but no new locks can be acquired. If lock conversion is allowed then upgrading of locks must be done during the expanding phase and downgrading of locks must be done in the is unrelated to the two phase commit protocol for recovery in distributed databases T Initial values Result serial schedule T followed by T Result of serial schedule T followed by T readlock readitem unlock writelock readitem X X + Y writeitem unlock writelock readitem X X + Y writeitem unlock readlock readitem unlock writelock readitem Y Y X + Y writeitem unlock readlock readitem unlock writelock readitem Y X + Y writeitem unlock Time readlock readitem unlock Result of schedule S T T T Figure Transactions that do not obey two phase locking. Two transactions and Results of possible serial schedules of and A nonserializable schedule S that uses locks. shrinking phase. Hence a readlock operation that downgrades an already held write lock on X can appear only in the shrinking phase. Transactions and in Figure do not follow the two phase locking protocol because the writelock operation follows the unlock operation in and similarly the writelock operation follows the unlock operation in If we enforce two phase locking the transactions can be rewritten as and as shown in Figure Now the schedule shown in Figure is not permitted for and under the rules of locking described in Section because will issue its writelock before it unlocks item Y consequently when its readlock it is forced to wait until the lock by issuing an unlock in the schedule. Chapter Concurrency Control Techniques readlock readitem writelock unlock readitem X X + Y writeitem unlock readlock readitem writelock unlock readitem Y X + Y writeitem unlock Figure Transactions and which are the same as and in Figure but follow the two phase locking protocol. Note that they can produce a deadlock. It can be proved that if every transaction in a schedule follows the two phase locking protocol the schedule is guaranteed to be serializable obviating the need to test for serializability of schedules. The locking protocol by enforcing two phase locking rules also enforces serializability. Two phase locking may limit the amount of concurrency that can occur in a schedule because a transaction T may not be able to release an item X after it is through using it if T must lock an additional item Y later or conversely T must lock the additional item Y before it needs it so that it can release X. Hence X must remain locked by T until all items that the transaction needs to read or write have been locked only then can X be released by T. Meanwhile another transaction seeking to access X may be forced to wait even though T is done with X conversely if Y is locked earlier than it is needed another transaction seeking to access Y is forced to wait even though T is not using Y yet. This is the price for guaranteeing serializability of all schedules without having to check the schedules themselves. Although the two phase locking protocol guarantees serializability it does not permit all possible serializable schedules . Basic Conservative Strict and Rigorous Two Phase Locking. There are a number of variations of two phase locking The technique just described is known as basic A variation known as conservative locks until after it commits or aborts. Hence no other transaction can read or write an item that is written by T unless T has committed leading to a strict schedule for recoverability. Strict is not deadlock free. A more restrictive variation of strict is rigorous which also guarantees strict schedules. In this variation a transaction T does not release any of its locks until after it commits or aborts and so it is easier to implement than strict Notice the difference between conservative and rigorous the former must lock all its items before it starts so once the transaction starts it is in its shrinking phase the latter does not unlock any of its items until after it terminates so the transaction is in its expanding phase until it ends. In many cases the concurrency control subsystem itself is responsible for generating the readlock and writelock requests. For example suppose the system is to enforce the strict protocol. Then whenever transaction T issues a readitem the system calls the readlock operation on behalf of T. If the state of LOCK is writelocked by some other transaction T the system places T in the waiting queue for item X otherwise it grants the readlock request and permits the readitem operation of T to execute. On the other hand if transaction T issues a writeitem the system calls the writelock operation on behalf of T. If the state of LOCK is writelocked or readlocked by some other transaction T the system places T in the waiting queue for item X if the state of LOCK is readlocked and T itself is the only transaction holding the read lock on X the system upgrades the lock to writelocked and permits the writeitem operation by T. Finally if the state of LOCK is unlocked the system grants the writelock request and permits the writeitem operation to execute. After each action the system must update its lock table appropriately. The use of locks can cause two additional problems deadlock and starvation. We discuss these problems and their solutions in the next section. Dealing with Deadlock and Starvation Deadlock occurs when each transaction T in a set of two or more transactions is waiting for some item that is locked by some other transaction Tin the set. Hence each transaction in the set is in a waiting queue waiting for one of the other transactions in the set to release the lock on an item. But because the other transaction is also waiting it will never release the lock. A simple example is shown in Figure where the two transactions and are deadlocked in a partial schedule is in the waiting queue for X which is locked by while is in the waiting queue for Y which is locked by Meanwhile neither nor nor any other transaction can access items X and Y. Deadlock Prevention Protocols. One way to prevent deadlock is to use a deadlock prevention protocol. One deadlock prevention protocol which is used protocols are not generally used in practice either because of unrealistic assumptions or because of their possible overhead. Deadlock detection and timeouts are more practical. Chapter Concurrency Control Techniques T readlock readitem Time writelock readlock readitem writelock T T T X Y Figure Illustrating the deadlock problem. A partial schedule of and is in a state of deadlock. A wait for graph for the partial schedule in . in conservative two phase locking requires that every transaction lock all the items it needs in advance if any of the items cannot be obtained none of the items are locked. Rather the transaction waits and then tries again to lock all the items it needs. Obviously this solution further limits concurrency. A second protocol which also limits concurrency involves ordering all the items in the database and making sure that a transaction that needs several items will lock them according to that order. This requires that the programmer is aware of the chosen order of the items which is also not practical in the database context. A number of other deadlock prevention schemes have been proposed that make a decision about what to do with a transaction involved in a possible deadlock situation Should it be blocked and made to wait or should it be aborted or should the transaction preempt and abort another transaction Some of these techniques use the concept of transaction timestamp TS which is a unique identifier assigned to each transaction. The timestamps are typically based on the order in which transactions are started hence if transaction starts before transaction then Notice that the older transaction has the smaller timestamp value. Two schemes that prevent deadlock are called wait die and woundwait. Suppose that transaction Ti tries to lock an item X but is not able to because X is locked by some other transaction Tj with a conflicting lock. The rules followed by these schemes are Wait die. If TS TS then Ti is allowed to wait otherwise abort Ti and restart it later with the same timestamp. Wound wait. If TS TS then abort Tj and restart it later with the same timestamp otherwise Ti is allowed to wait. In wait die an older transaction is allowed to wait for a younger transaction whereas a younger transaction requesting an item held by an older transaction is aborted and restarted. The wound wait approach does the opposite A younger transaction is allowed to wait for an older one whereas an older transaction requesting an item Two Phase Locking Techniques for Concurrency Control held by a younger transaction preempts the younger transaction by aborting it. Both schemes end up aborting the younger of the two transactions that may be involved in a deadlock assuming that this will waste less processing. It can be shown that these two techniques are deadlock free since in wait die transactions only wait for younger transactions so no cycle is created. Similarly in wound wait transactions only wait for older transactions so no cycle is created. However both techniques may cause some transactions to be aborted and restarted needlessly even though those transactions may never actually cause a deadlock. Another group of protocols that prevent deadlock do not require timestamps. These include the no waiting and cautious waiting algorithms. In the no waiting algorithm if a transaction is unable to obtain a lock it is immediately aborted and then restarted after a certain time delay without checking whether a deadlock will actually occur or not. In this case no transaction ever waits so no deadlock will occur. However this scheme can cause transactions to abort and restart needlessly. The cautious waiting algorithm was proposed to try to reduce the number of needless aborts restarts. Suppose that transaction Ti tries to lock an item X but is not able to do so because X is locked by some other transaction Tj with a conflicting lock. The cautious waiting rules are as follows Cautious waiting. If Tj is not blocked then Ti is blocked and allowed to wait otherwise abort Ti . It can be shown that cautious waiting is deadlock free because no transaction will ever wait for another blocked transaction. By considering the time b at which each blocked transaction T was blocked if the two transactions Ti and Tj above both become blocked and Ti is waiting for Tj then b b since Ti can only wait for Tj at a time when Tj is not blocked itself. Hence the blocking times form a total ordering on all blocked transactions so no cycle that causes deadlock can occur. Deadlock Detection. A second more practical approach to dealing with deadlock is deadlock detection where the system checks if a state of deadlock actually exists. This solution is attractive if we know there will be little interference among the transactions that is if different transactions will rarely access the same items at the same time. This can happen if the transactions are short and each transaction locks only a few items or if the transaction load is light. On the other hand if transactions are long and each transaction uses many items or if the transaction load is quite heavy it may be advantageous to use a deadlock prevention scheme. A simple way to detect a state of deadlock is for the system to construct and maintain a wait for graph. One node is created in the wait for graph for each transaction that is currently executing. Whenever a transaction Ti is waiting to lock an item X that is currently locked by a transaction Tj a directed edge is created in the wait for graph. When Tj releases the lock on the items that Ti was waiting for the directed edge is dropped from the wait for graph. We have a state of deadlock if and only if the wait for graph has a cycle. One problem with this approach is the matter of determining when the system should check for a deadlock. One possi Chapter Concurrency Control Techniques bility is to check for a cycle every time an edge is added to the wait for graph but this may cause excessive overhead. Criteria such as the number of currently executing transactions or the period of time several transactions have been waiting to lock items may be used instead to check for a cycle. Figure shows the wait for graph for the schedule shown in Figure If the system is in a state of deadlock some of the transactions causing the deadlock must be aborted. Choosing which transactions to abort is known as victim selection. The algorithm for victim selection should generally avoid selecting transactions that have been running for a long time and that have performed many updates and it should try instead to select transactions that have not made many changes . Timeouts. Another simple scheme to deal with deadlock is the use of timeouts. This method is practical because of its low overhead and simplicity. In this method if a transaction waits for a period longer than a system defined timeout period the system assumes that the transaction may be deadlocked and aborts it regardless of whether a deadlock actually exists or not. Starvation. Another problem that may occur when we use locking is starvation which occurs when a transaction cannot proceed for an indefinite period of time while other transactions in the system continue normally. This may occur if the waiting scheme for locked items is unfair giving priority to some transactions over others. One solution for starvation is to have a fair waiting scheme such as using a first come first served queue transactions are enabled to lock an item in the order in which they originally requested the lock. Another scheme allows some transactions to have priority over others but increases the priority of a transaction the longer it waits until it eventually gets the highest priority and proceeds. Starvation can also occur because of victim selection if the algorithm selects the same transaction as victim repeatedly thus causing it to abort and never finish execution. The algorithm can use higher priorities for transactions that have been aborted multiple times to avoid this problem. The wait die and wound wait schemes discussed previously avoid starvation because they restart a transaction that has been aborted with its same original timestamp so the possibility that the same transaction is aborted repeatedly is slim. Concurrency Control Based on Timestamp Ordering The use of locks combined with the protocol guarantees serializability of schedules. The serializable schedules produced by have their equivalent serial schedules based on the order in which executing transactions lock the items they acquire. If a transaction needs an item that is already locked it may be forced to wait until the item is released. Some transactions may be aborted and restarted because of the deadlock problem. A different approach that guarantees serializability involves using transaction timestamps to order transaction execution for an equiva Concurrency Control Based on Timestamp Ordering lent serial schedule. In Section we discuss timestamps and in Section we discuss how serializability is enforced by ordering transactions based on their timestamps. Timestamps Recall that a timestamp is a unique identifier created by the DBMS to identify a transaction. Typically timestamp values are assigned in the order in which the transactions are submitted to the system so a timestamp can be thought of as the transaction start time. We will refer to the timestamp of transaction T as TS. Concurrency control techniques based on timestamp ordering do not use locks hence deadlocks cannot occur. Timestamps can be generated in several ways. One possibility is to use a counter that is incremented each time its value is assigned to a transaction. The transaction timestamps are numbered in this scheme. A computer counter has a finite maximum value so the system must periodically reset the counter to zero when no transactions are executing for some short period of time. Another way to implement timestamps is to use the current date time value of the system clock and ensure that no two timestamp values are generated during the same tick of the clock. The Timestamp Ordering Algorithm The idea for this scheme is to order the transactions based on their timestamps. A schedule in which the transactions participate is then serializable and the only equivalent serial schedule permitted has the transactions in order of their timestamp values. This is called timestamp ordering . Notice how this differs from where a schedule is serializable by being equivalent to some serial schedule allowed by the locking protocols. In timestamp ordering however the schedule is equivalent to the particular serial order corresponding to the order of the transaction timestamps. The algorithm must ensure that for each item accessed by conflicting operations in the schedule the order in which the item is accessed does not violate the timestamp order. To do this the algorithm associates with each database item X two timestamp values readTS. The read timestamp of item X is the largest timestamp among all the timestamps of transactions that have successfully read item X that is readTS TS where T is the youngest transaction that has read X successfully. writeTS. The write timestamp of item X is the largest of all the timestamps of transactions that have successfully written item X that is writeTS TS where T is the youngest transaction that has written X successfully. Basic Timestamp Ordering . Whenever some transaction T tries to issue a readitem or a writeitem operation the basic TO algorithm compares the timestamp of T with readTS and writeTS to ensure that the timestamp Chapter Concurrency Control Techniques order of transaction execution is not violated. If this order is violated then transaction T is aborted and resubmitted to the system as a new transaction with a new timestamp. If T is aborted and rolled back any transaction that may have used a value written by T must also be rolled back. Similarly any transaction that may have used a value written by must also be rolled back and so on. This effect is known as cascading rollback and is one of the problems associated with basic TO since the schedules produced are not guaranteed to be recoverable. An additional protocol must be enforced to ensure that the schedules are recoverable cascadeless or strict. We first describe the basic TO algorithm here. The concurrency control algorithm must check whether conflicting operations violate the timestamp ordering in the following two cases Whenever a transaction T issues a writeitem operation the following is checked a. If readTS TS or if writeTS TS then abort and roll back T and reject the operation. This should be done because some younger transaction with a timestamp greater than TS and hence after T in the timestamp ordering has already read or written the value of item X before T had a chance to write X thus violating the timestamp ordering. b. If the condition in part does not occur then execute the writeitem operation of T and set writeTS to TS. Whenever a transaction T issues a readitem operation the following is checked a. If writeTS TS then abort and roll back T and reject the operation. This should be done because some younger transaction with timestamp greater than TS and hence after T in the timestamp ordering has already written the value of item X before T had a chance to read X. b. If writeTS ≤ TS then execute the readitem operation of T and set readTS to the larger of TS and the current readTS. Whenever the basic TO algorithm detects two conflicting operations that occur in the incorrect order it rejects the later of the two operations by aborting the transaction that issued it. The schedules produced by basic TO are hence guaranteed to be conflict serializable like the protocol. However some schedules are possible under each protocol that are not allowed under the other. Thus neither protocol allows all possible serializable schedules. As mentioned earlier deadlock does not occur with timestamp ordering. However cyclic restart may occur if a transaction is continually aborted and restarted. Strict Timestamp Ordering . A variation of basic TO called strict TO ensures that the schedules are both strict and serializable. In this variation a transaction T that issues a readitem or writeitem such that TS writeTS has its read or write operation delayed until the transaction Tthat wrote the value of X writeTS has committed or aborted. To implement this algorithm it is necessary to simulate the Multiversion Concurrency Control Techniques locking of an item X that has been written by transaction T until Tis either committed or aborted. This algorithm does not cause deadlock since T waits for T only if TS TS. Thomas’s Write Rule. A modification of the basic TO algorithm known as Thomas’s write rule does not enforce conflict serializability but it rejects fewer write operations by modifying the checks for the writeitem operation as follows If readTS TS then abort and roll back T and reject the operation. If writeTS TS then do not execute the write operation but continue processing. This is because some transaction with timestamp greater than TS and hence after T in the timestamp ordering has already written the value of X. Thus we must ignore the writeitem operation of T because it is already outdated and obsolete. Notice that any conflict arising from this situation would be detected by case If neither the condition in part nor the condition in part occurs then execute the writeitem operation of T and set writeTS to TS. Multiversion Concurrency Control Techniques Other protocols for concurrency control keep the old values of a data item when the item is updated. These are known as multiversion concurrency control because several versions of an item are maintained. When a transaction requires access to an item an appropriate version is chosen to maintain the serializability of the currently executing schedule if possible. The idea is that some read operations that would be rejected in other techniques can still be accepted by reading an older version of the item to maintain serializability. When a transaction writes an item it writes a new version and the old version of the item are retained. Some multiversion concurrency control algorithms use the concept of view serializability rather than conflict serializability. An obvious drawback of multiversion techniques is that more storage is needed to maintain multiple versions of the database items. However older versions may have to be maintained anyway for example for recovery purposes. In addition some database applications require older versions to be kept to maintain a history of the evolution of data item values. The extreme case is a temporal database . The read timestamp of Xi is the largest of all the timestamps of transactions that have successfully read version Xi . writeTS. The write timestamp of Xi is the timestamp of the transaction that wrote the value of version Xi . Whenever a transaction T is allowed to execute a writeitem operation a new version of item X is created with both the and the set to TS. Correspondingly when a transaction T is allowed to read the value of version Xi the value of readTS is set to the larger of the current readTS and TS. To ensure serializability the following rules are used If transaction T issues a writeitem operation and version i of X has the highest writeTS of all versions of X that is also less than or equal to TS and readTS TS then abort and roll back transaction T otherwise create a new version Xj of X with readTS writeTS TS. If transaction T issues a readitem operation find the version i of X that has the highest writeTS of all versions of X that is also less than or equal to TS then return the value of Xi to transaction T and set the value of readTS to the larger of TS and the current readTS. As we can see in case a readitem is always successful since it finds the appropriate version Xi to read based on the writeTS of the various existing versions of X. In case however transaction T may be aborted and rolled back. This happens if T attempts to write a version of X that should have been read by another transaction T whose timestamp is readTS however T has already read version Xi which was written by the transaction with timestamp equal to writeTS. If this conflict occurs T is rolled back otherwise a new version of X written by transaction T is created. Notice that if T is rolled back cascading rollback may occur. Hence to ensure recoverability a transaction T should not be allowed to commit until after all the transactions that have written some version that T has read have committed. Multiversion Two Phase Locking Using Certify Locks In this multiple mode locking scheme there are three locking modes for an item read write and certify instead of just the two modes discussed previously. Hence the state of LOCK for an item X can be one of read locked writelocked certify locked or unlocked. In the standard locking scheme with only read and write locks Read Write Read Write Certify YesNoNo No No No Yes Yes No Certify Read Write Read Write No No YesNo Figure Lock compatibility tables. A compatibility table for read write locking scheme. A compatibility table for read write certify locking scheme. on item X and if transaction Trequests the type of lock specified in the row header on the same item X then Tcan obtain the lock because the locking modes are compatible. On the other hand an entry of No in the table indicates that the locks are not compatible so Tmust wait until T releases the lock. In the standard locking scheme once a transaction obtains a write lock on an item no other transactions can access that item. The idea behind multiversion is to allow other transactions T to read an item X while a single transaction T holds a write lock on X. This is accomplished by allowing two versions for each item X one version must always have been written by some committed transaction. The second version Xis created when a transaction T acquires a write lock on the item. Other transactions can continue to read the committed version of X while T holds the write lock. Transaction T can write the value of X as needed without affecting the value of the committed version X. However once T is ready to commit it must obtain a certify lock on all items that it currently holds write locks on before it can commit. The certify lock is not compatible with read locks so the transaction may have to delay its commit until all its write locked items are released by any reading transactions in order to obtain the certify locks. Once the certify locks which are exclusive locks are acquired the committed version X of the data item is set to the value of version X version X is discarded and the certify locks are then released. The lock compatibility table for this scheme is shown in Figure In this multiversion scheme reads can proceed concurrently with a single write operation an arrangement not permitted under the standard schemes. The cost is that a transaction may have to delay its commit until it obtains exclusive certify locks on all the items it has updated. It can be shown that this scheme avoids cascading aborts since transactions are only allowed to read the version X that was written by a committed transaction. However deadlocks may occur if upgrading of a read lock to a write lock is allowed and these must be handled by variations of the techniques discussed in Section Chapter Concurrency Control Techniques Validation Concurrency Control Techniques In all the concurrency control techniques we have discussed so far a certain degree of checking is done before a database operation can be executed. For example in locking a check is done to determine whether the item being accessed is locked. In timestamp ordering the transaction timestamp is checked against the read and write timestamps of the item. Such checking represents overhead during transaction execution with the effect of slowing down the transactions. In optimistic concurrency control techniques also known as validation or certification techniques no checking is done while the transaction is executing. Several theoretical concurrency control methods are based on the validation technique. We will describe only one scheme here. In this scheme updates in the transaction are not applied directly to the database items until the transaction reaches its end. During transaction execution all updates are applied to local copies of the data items that are kept for the At the end of transaction execution a validation phase checks whether any of the transaction’s updates violate serializability. Certain information needed by the validation phase must be kept by the system. If serializability is not violated the transaction is committed and the database is updated from the local copies otherwise the transaction is aborted and then restarted later. There are three phases for this concurrency control protocol Read phase. A transaction can read values of committed data items from the database. However updates are applied only to local copies of the data items kept in the transaction workspace. Validation phase. Checking is performed to ensure that serializability will not be violated if the transaction updates are applied to the database. Write phase. If the validation phase is successful the transaction updates are applied to the database otherwise the updates are discarded and the transaction is restarted. The idea behind optimistic concurrency control is to do all the checks at once hence transaction execution proceeds with a minimum of overhead until the validation phase is reached. If there is little interference among transactions most will be validated successfully. However if there is much interference many transactions that execute to completion will have their results discarded and must be restarted later. Under these circumstances optimistic techniques do not work well. The techniques are called optimistic because they assume that little interference will occur and hence that there is no need to do checking during transaction execution. The optimistic protocol we describe uses transaction timestamps and also requires that the writesets and readsets of the transactions be kept by the system. Additionally start and end times for some of the three phases need to be kept for that this can be considered as keeping multiple versions of items! Granularity of Data Items and Multiple Granularity Locking each transaction. Recall that the writeset of a transaction is the set of items it writes and the readset is the set of items it reads. In the validation phase for transaction Ti the protocol checks that Ti does not interfere with any committed transactions or with any other transactions currently in their validation phase. The validation phase for Ti checks that for each such transaction Tj that is either committed or is in its validation phase one of the following conditions holds Transaction Tj completes its write phase before Ti starts its read phase. Ti starts its write phase after Tj completes its write phase and the readset of Ti has no items in common with the writeset of Tj . Both the readset and writeset of Ti have no items in common with the writeset of Tj and Tj completes its read phase before Ti completes its read phase. When validating transaction Ti the first condition is checked first for each transaction Tj since is the simplest condition to check. Only if condition is false is condition checked and only if is false is condition most complex to evaluate checked. If any one of these three conditions holds there is no interference and Ti is validated successfully. If none of these three conditions holds the validation of transaction Ti fails and it is aborted and restarted later because interference may have occurred. Granularity of Data Items and Multiple Granularity Locking All concurrency control techniques assume that the database is formed of a number of named data items. A database item could be chosen to be one of the following A database record A field value of a database record A disk block A whole file The whole database The granularity can affect the performance of concurrency control and recovery. In Section we discuss some of the tradeoffs with regard to choosing the granularity level used for locking and in Section we discuss a multiple granularity locking scheme where the granularity level may be changed dynamically. Granularity Level Considerations for Locking The size of data items is often called the data item granularity. Fine granularity refers to small item sizes whereas coarse granularity refers to large item sizes. Several tradeoffs must be considered in choosing the data item size. We will discuss data item size in the context of locking although similar arguments can be made for other concurrency control techniques. Chapter Concurrency Control Techniques db nj m m . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p p m Figure A granularity hierarchy for illustrating multiple granularity level locking. First notice that the larger the data item size is the lower the degree of concurrency permitted. For example if the data item size is a disk block a transaction T that needs to lock a record B must lock the whole disk block X that contains B because a lock is associated with the whole data item . Now if another transaction S wants to lock a different record C that happens to reside in the same block X in a conflicting lock mode it is forced to wait. If the data item size was a single record transaction S would be able to proceed because it would be locking a different data item . On the other hand the smaller the data item size is the more the number of items in the database. Because every item is associated with a lock the system will have a larger number of active locks to be handled by the lock manager. More lock and unlock operations will be performed causing a higher overhead. In addition more storage space will be required for the lock table. For timestamps storage is required for the readTS and writeTS for each data item and there will be similar overhead for handling a large number of items. Given the above tradeoffs an obvious question can be asked What is the best item size The answer is that it depends on the types of transactions involved. If a typical transaction accesses a small number of records it is advantageous to have the data item granularity be one record. On the other hand if a transaction typically accesses many records in the same file it may be better to have block or file granularity so that the transaction will consider all those records as one data items. Multiple Granularity Level Locking Since the best granularity size depends on the given transaction it seems appropriate that a database system should support multiple levels of granularity where the granularity level can be different for various mixes of transactions. Figure shows a simple granularity hierarchy with a database containing two files each file containing several disk pages and each page containing several records. This can be used to illustrate a multiple granularity level protocol where a lock can be requested at any level. However additional types of locks will be needed to support such a protocol efficiently. Granularity of Data Items and Multiple Granularity Locking Consider the following scenario with only shared and exclusive lock types that refers to the example in Figure Suppose transaction wants to update all the records in file f and requests and is granted an exclusive lock for f Then all of f pages through the records contained on those pages are locked in exclusive mode. This is beneficial for because setting a single file level lock is more efficient than setting n page level locks or having to lock each individual record. Now suppose another transaction only wants to read record from page of file f then would request a shared record level lock on However the database system must verify the compatibility of the requested lock with already held locks. One way to verify this is to traverse the tree from the leaf to to f to db. If at any time a conflicting lock is held on any of those items then the lock request for is denied and is blocked and must wait. This traversal would be fairly efficient. However what if transaction request came before transaction request In this case the shared record lock is granted to for but when file level lock is requested it is quite difficult for the lock manager to check all nodes that are descendants of node f for a lock conflict. This would be very inefficient and would defeat the purpose of having multiple granularity level locks. To make multiple granularity level locking practical additional types of locks called intention locks are needed. The idea behind intention locks is for a transaction to indicate along the path from the root to the desired node what type of lock it will require from one of the node’s descendants. There are three types of intention locks Intention shared indicates that one or more shared locks will be requested on some descendant node. Intention exclusive indicates that one or more exclusive locks will be requested on some descendant node. Shared intention exclusive indicates that the current node is locked in shared mode but that one or more exclusive locks will be requested on some descendant node. The compatibility table of the three intention locks and the shared and exclusive locks is shown in Figure Besides the introduction of the three types of intention locks an appropriate locking protocol must be used. The multiple granularity locking protocol consists of the following rules The lock compatibility . Chapter Concurrency Control Techniques I S I X S SIX X I S Yes Yes Yes Yes No I X Yes No Yes No No S No Yes Yes No No SIX No No Yes No No X No No No No No Figure Lock compatibility matrix for multiple granularity locking. A transaction T can unlock a node N only if none of the children of node N are currently locked by T. Rule simply states that conflicting locks cannot be granted. Rules and state the conditions when a transaction may lock a given node in any of the lock modes. Rules and of the MGL protocol enforce rules to produce serializable schedules. To illustrate the MGL protocol with the database hierarchy in Figure consider the following three transactions wants to update record and record wants to update all records on page wants to read record and the entire f file. Figure shows a possible serializable schedule for these three transactions. Only the lock and unlock operations are shown. The notation locktype is used to display the locking operations in the schedule. The multiple granularity level protocol is especially suited when processing a mix of transactions that include short transactions that access only a few items and long transactions that access entire files. In this environment less transaction blocking and less locking overhead is incurred by such a protocol when compared to a single level granularity locking approach. Using Locks for Concurrency Control in Indexes Two phase locking can also be applied to indexes the root would be locked in exclusive mode so all other conflicting lock requests for the index must wait until the transaction enters its shrinking phase. This blocks all other transactions from accessing the index so in practice other approaches to locking an index must be used. Using Locks for Concurrency Control in Indexes IX is being executed a path in the tree is traversed from the root to a leaf. Once a lowerlevel node in the path has been accessed the higher level nodes in that path will not be used again. So once a read lock on a child node is obtained the lock on the parent can be released. When an insertion is being applied to a leaf node then a specific leaf node must be locked in exclusive mode. However if that node is not full the insertion will not cause changes to higher level index nodes which implies that they need not be locked exclusively. A conservative approach for insertions would be to lock the root node in exclusive mode and then to access the appropriate child node of the root. If the child node is Chapter Concurrency Control Techniques not full then the lock on the root node can be released. This approach can be applied all the way down the tree to the leaf which is typically three or four levels from the root. Although exclusive locks are held they are soon released. An alternative more optimistic approach would be to request and hold shared locks on the nodes leading to the leaf node with an exclusive lock on the leaf. If the insertion causes the leaf to split insertion will propagate to one or more higher level nodes. Then the locks on the higher level nodes can be upgraded to exclusive mode. Another approach to index locking is to use a variant of the B+ tree called the Blink tree. In a B link tree sibling nodes on the same level are linked at every level. This allows shared locks to be used when requesting a page and requires that the lock be released before accessing the child node. For an insert operation the shared lock on a node would be upgraded to exclusive mode. If a split occurs the parent node must be relocked in exclusive mode. One complication is for search operations executed concurrently with the update. Suppose that a concurrent update operation follows the same path as the search and inserts a new entry into the leaf node. Additionally suppose that the insert causes that leaf node to split. When the insert is done the search process resumes following the pointer to the desired leaf only to find that the key it is looking for is not present because the split has moved that key into a new leaf node which would be the right sibling of the original leaf node. However the search process can still succeed if it follows the pointer in the original leaf node to its right sibling where the desired key has been moved. Handling the deletion case where two or more nodes from the index tree merge is also part of the B link tree concurrency protocol. In this case locks on the nodes to be merged are held as well as a lock on the parent of the two nodes to be merged. Other Concurrency Control Issues In this section we discuss some other issues relevant to concurrency control. In Section we discuss problems associated with insertion and deletion of records and the so called phantom problem which may occur when records are inserted. This problem was described as a potential problem requiring a concurrency control measure in Section In Section we discuss problems that may occur when a transaction outputs some data to a monitor before it commits and then the transaction is later aborted. Insertion Deletion and Phantom Records When a new data item is inserted in the database it obviously cannot be accessed until after the item is created and the insert operation is completed. In a locking environment a lock for the item can be created and set to exclusive mode the lock can be released at the same time as other write locks would be released based on the concurrency control protocol being used. For a timestamp based protocol the read and write timestamps of the new item are set to the timestamp of the creating transaction. Other Concurrency Control Issues Next consider a deletion operation that is applied on an existing data item. For locking protocols again an exclusive lock must be obtained before the transaction can delete the item. For timestamp ordering the protocol must ensure that no later transaction has read or written the item before allowing the item to be deleted. A situation known as the phantom problem can occur when a new record that is being inserted by some transaction T satisfies a condition that a set of records accessed by another transaction T must satisfy. For example suppose that transaction T is inserting a new EMPLOYEE record whose Dno while transaction T is accessing all EMPLOYEE records whose Dno in common between the two transactions since T may have locked all the records with Dno before T inserted the new record. This is because the record that causes the conflict is a phantom record that has suddenly appeared in the database on being inserted. If other operations in the two transactions conflict the conflict due to the phantom record may not be recognized by the concurrency control protocol. One solution to the phantom record problem is to use index locking as discussed in Section Recall from Chapter that an index includes entries that have an attribute value plus a set of pointers to all records in the file with that value. For example an index on Dno of EMPLOYEE would include an entry for each distinct Dno value plus a set of pointers to all EMPLOYEE records with that value. If the index entry is locked before the record itself can be accessed then the conflict on the phantom record can be detected because transaction T would request a read lock on the index entry for Dno and T would request a write lock on the same entry before they could place the locks on the actual records. Since the index locks conflict the phantom conflict would be detected. A more general technique called predicate locking would lock access to all records that satisfy an arbitrary predicate in a similar manner however predicate locks have proved to be difficult to implement efficiently. Interactive Transactions Another problem occurs when interactive transactions read input and write output to an interactive device such as a monitor screen before they are committed. The problem is that a user can input a value of a data item to a transaction T that is based on some value written to the screen by transaction T which may not have committed. This dependency between T and T cannot be modeled by the system concurrency control method since it is only based on the user interacting with the two transactions. An approach to dealing with this problem is to postpone output of transactions to the screen until they have committed. Chapter Concurrency Control Techniques Latches Locks held for a short duration are typically called latches. Latches do not follow the usual concurrency control protocol such as two phase locking. For example a latch can be used to guarantee the physical integrity of a page when that page is being written from the buffer to disk. A latch would be acquired for the page the page written to disk and then the latch released. Summary In this chapter we discussed DBMS techniques for concurrency control. We started by discussing lock based protocols which are by far the most commonly used in practice. We described the two phase locking protocol and a number of its variations basic strict conservative and rigorous The strict and rigorous variations are more common because of their better recoverability properties. We introduced the concepts of shared and exclusive locks and showed how locking can guarantee serializability when used in conjunction with the two phase locking rule. We also presented various techniques for dealing with the deadlock problem which can occur with locking. In practice it is common to use timeouts and deadlock detection . We presented other concurrency control protocols that are not used often in practice but are important for the theoretical alternatives they show for solving this problem. These include the timestamp ordering protocol which ensures serializability based on the order of transaction timestamps. Timestamps are unique system generated transaction identifiers. We discussed Thomas’s write rule which improves performance but does not guarantee conflict serializability. The strict timestamp ordering protocol was also presented. We discussed two multiversion protocols which assume that older versions of data items can be kept in the database. One technique called multiversion two phase locking assumes that two versions can exist for an item and attempts to increase concurrency by making write and read locks compatible . We also presented a multiversion protocol based on timestamp ordering and an example of an optimistic protocol which is also known as a certification or validation protocol. Then we turned our attention to the important practical issue of data item granularity. We described a multigranularity locking protocol that allows the change of granularity based on the current transaction mix with the goal of improving the performance of concurrency control. An important practical issue was then presented which is to develop locking protocols for indexes so that indexes do not become a hindrance to concurrent access. Finally we introduced the phantom problem and problems with interactive transactions and briefly described the concept of latches and how it differs from locks. Review Questions Review Questions What is the two phase locking protocol How does it guarantee serializability What are some variations of the two phase locking protocol Why is strict or rigorous two phase locking often preferred Discuss the problems of deadlock and starvation and the different approaches to dealing with these problems. Compare binary locks to exclusive shared locks. Why is the latter type of locks preferable Describe the wait die and wound wait protocols for deadlock prevention. Describe the cautious waiting no waiting and timeout protocols for deadlock prevention. What is a timestamp How does the system generate timestamps Discuss the timestamp ordering protocol for concurrency control. How does strict timestamp ordering differ from basic timestamp ordering Discuss two multiversion techniques for concurrency control. What is a certify lock What are the advantages and disadvantages of using certify locks How do optimistic concurrency control techniques differ from other concurrency control techniques Why are they also called validation or certification techniques Discuss the typical phases of an optimistic concurrency control method. How does the granularity of data items affect the performance of concurrency control What factors affect selection of granularity size for data items What type of lock is needed for insert and delete operations What is multiple granularity locking Under what circumstances is it used What are intention locks When are latches used What is a phantom record Discuss the problem that a phantom record can cause for concurrency control. How does index locking resolve the phantom problem What is a predicate lock Chapter Concurrency Control Techniques Exercises Prove that the basic two phase locking protocol guarantees conflict serializability of schedules. Modify the data structures for multiple mode locks and the algorithms for readlock writelock and unlock so that upgrading and downgrading of locks are possible. that hold the lock if Prove that strict two phase locking guarantees strict schedules. Prove that the wait die and wound wait protocols avoid deadlock and starvation. Prove that cautious waiting avoids deadlock. Apply the timestamp ordering algorithm to the schedules in Figure and and determine whether the algorithm will allow the execution of the schedules. Repeat Exercise but use the multiversion timestamp ordering method. Why is two phase locking not used as a concurrency control method for indexes such as B+ trees The compatibility matrix in Figure shows that IS and IX locks are compatible. Explain why this is valid. The MGL protocol states that a transaction T can unlock a node N only if none of the children of node N are still locked by transaction T. Show that without this condition the MGL protocol would be incorrect. Selected Bibliography The two phase locking protocol and the concept of predicate locks were first proposed by Eswaran et al. Bernstein et al. Gray and Reuter and Papadimitriou focus on concurrency control and recovery. Kumar focuses on performance of concurrency control methods. Locking is discussed in Gray et al. Lien and Weinberger Kedem and Silbershatz and Korth Deadlocks and wait for graphs were formalized by Holt and the wait wound and wound die schemes are presented in Rosenkrantz et al. Cautious waiting is discussed in Hsu and Zhang Helal et al. compares various locking approaches. Timestamp based concurrency control techniques are discussed in Bernstein and Goodman and Reed Optimistic concurrency control is discussed in Kung and Robinson and Bassiouni Papadimitriou and Kanellakis and Bernstein and Selected Bibliography Goodman discuss multiversion techniques. Multiversion timestamp ordering was proposed in Reed and multiversion two phase locking is discussed in Lai and Wilkinson A method for multiple locking granularities was proposed in Gray et al. and the effects of locking granularities are analyzed in Ries and Stonebraker Bhargava and Reidl presents an approach for dynamically choosing among various concurrency control and recovery methods. Concurrency control methods for indexes are presented in Lehman and Yao and in Shasha and Goodman A performance study of various B+ tree concurrency control algorithms is presented in Srinivasan and Carey Other work on concurrency control includes semantic based concurrency control the effect of an incomplete or failed transaction. In Section we present recovery techniques based on deferred update also known as the NOUNDO REDO technique where the data on disk is not updated until after a transaction commits. In Section we discuss recovery techniques based on immediate update where data can be updated on disk during transaction execution these include the UNDO REDO and UNDO NO REDO algorithms. We discuss the technique known as shadowing or shadow paging which can be categorized as a NO UNDO NO REDO algorithm in Section An example of a practical DBMS recovery scheme called ARIES is presented in Section Recovery in multidatabases is briefly discussed in Section Finally techniques for recovery from catastrophic failure are discussed in Section Section summarizes the chapter. Our emphasis is on conceptually describing several different approaches to recovery. For descriptions of recovery features in specific systems the reader should consult the bibliographic notes at the end of the chapter and the online and printed user manuals for those systems. Recovery techniques are often intertwined with the chapter Chapter Database Recovery Techniques concurrency control mechanisms. Certain recovery techniques are best used with specific concurrency control methods. We will discuss recovery concepts independently of concurrency control mechanisms but we will discuss the circumstances under which a particular recovery mechanism is best used with a certain concurrency control protocol. Recovery Concepts Recovery Outline and Categorization of Recovery Algorithms Recovery from transaction failures usually means that the database is restored to the most recent consistent state just before the time of failure. To do this the system must keep information about the changes that were applied to data items by the various transactions. This information is typically kept in the system log as we discussed in Section A typical strategy for recovery may be summarized informally as follows If there is extensive damage to a wide portion of the database due to catastrophic failure such as a disk crash the recovery method restores a past copy of the database that was backed up to archival storage and reconstructs a more current state by reapplying or redoing the operations of committed transactions from the backed up log up to the time of failure. When the database on disk is not physically damaged and a noncatastrophic failure of types through in Section has occurred the recovery strategy is to identify any changes that may cause an inconsistency in the database. For example a transaction that has updated some database items on disk but has not been committed needs to have its changes reversed by undoing its write operations. It may also be necessary to redo some operations in order to restore a consistent state of the database for example if a transaction has committed but some of its write operations have not yet been written to disk. For noncatastrophic failure the recovery protocol does not need a complete archival copy of the database. Rather the entries kept in the online system log on disk are analyzed to determine the appropriate actions for recovery. Conceptually we can distinguish two main techniques for recovery from noncatastrophic transaction failures deferred update and immediate update. The deferred update techniques do not physically update the database on disk until after a transaction reaches its commit point then the updates are recorded in the database. Before reaching commit all transaction updates are recorded in the local transaction workspace or in the main memory buffers that the DBMS maintains . Before commit the updates are recorded persistently in the log and then after commit the updates are written to the database on disk. If a transaction fails before reaching its commit point it will not have changed the Recovery Concepts database in any way so UNDO is not needed. It may be necessary to REDO the effect of the operations of a committed transaction from the log because their effect may not yet have been recorded in the database on disk. Hence deferred update is also known as the NO UNDO REDO algorithm. We discuss this technique in Section In the immediate update techniques the database may be updated by some operations of a transaction before the transaction reaches its commit point. However these operations must also be recorded in the log on disk by force writing before they are applied to the database on disk making recovery still possible. If a transaction fails after recording some changes in the database on disk but before reaching its commit point the effect of its operations on the database must be undone that is the transaction must be rolled back. In the general case of immediate update both undo and redo may be required during recovery. This technique known as the UNDO REDO algorithm requires both operations during recovery and is used most often in practice. A variation of the algorithm where all updates are required to be recorded in the database on disk before a transaction commits requires undo only so it is known as the UNDO NO REDO algorithm. We discuss these techniques in Section The UNDO and REDO operations are required to be idempotent that is executing an operation multiple times is equivalent to executing it just once. In fact the whole recovery process should be idempotent because if the system were to fail during the recovery process the next recovery attempt might UNDO and REDO certain writeitem operations that had already been executed during the first recovery process. The result of recovery from a system crash during recovery should be the same as the result of recovering when there is no crash during recovery! Caching of Disk Blocks The recovery process is often closely intertwined with operating system functions in particular the buffering of database disk pages in the DBMS main memory cache. Typically multiple disk pages that include the data items to be updated are cached into main memory buffers and then updated in memory before being written back to disk. The caching of disk pages is traditionally an operating system function but because of its importance to the efficiency of recovery procedures it is handled by the DBMS by calling low level operating systems routines. In general it is convenient to consider recovery in terms of the database disk pages . Typically a collection of in memory buffers called the DBMS cache is kept under the control of the DBMS for the purpose of holding these buffers. A directory for the cache is used to keep track of which database items are in the This can be a table of Diskpageaddress Bufferlocation entries. When the DBMS requests action on some item first it checks the cache directory to determine whether the disk page containing the item is in the DBMS cache. If it is is somewhat similar to the concept of page tables used by the operating system. Chapter Database Recovery Techniques not the item must be located on disk and the appropriate disk pages are copied into the cache. It may be necessary to replace some of the cache buffers to make space available for the new item. Some page replacement strategy similar to these used in operating systems such as least recently used or first in firstout or a new strategy that is DBMS specific can be used to select the buffers for replacement such as DBMIN or Least Likely to Use . The entries in the DBMS cache directory hold additional information relevant to buffer management. Associated with each buffer in the cache is a dirty bit which can be included in the directory entry to indicate whether or not the buffer has been modified. When a page is first read from the database disk into a cache buffer a new entry is inserted in the cache directory with the new disk page address and the dirty bit is set to . As soon as the buffer is modified the dirty bit for the corresponding directory entry is set to . Additional information such as the transaction id of the transaction that modified the buffer can also be kept in the directory. When the buffer contents are replaced from the cache the contents must first be written back to the corresponding disk page only if its dirty bit is Another bit called the pin unpin bit is also needed a page in the cache is pinned if it cannot be written back to disk as yet. For example the recovery protocol may restrict certain buffer pages from being written back to the disk until the transactions that changed this buffer have committed. Two main strategies can be employed when flushing a modified buffer back to disk. The first strategy known as in place updating writes the buffer to the same original disk location thus overwriting the old value of any changed data items on Hence a single copy of each database disk block is maintained. The second strategy known as shadowing writes an updated buffer at a different disk location so multiple versions of data items can be maintained but this approach is not typically used in practice. In general the old value of the data item before updating is called the before image and the new value after updating is called the after image . If shadowing is used both the BFIM and the AFIM can be kept on disk hence it is not strictly necessary to maintain a log for recovering. We briefly discuss recovery based on shadowing in Section Write Ahead Logging Steal No Steal and Force No Force When in place updating is used it is necessary to use a log for recovery of the item written by the operation since this is needed to redo the effect of the operation from the log . The UNDO type log entries include the old value of the item since this is needed to undo the effect of the operation from the log . In an UNDO REDO algorithm both types of log entries are combined. Additionally when cascading rollback is possible readitem entries in the log are considered to be UNDO type entries disk file and the DBMS cache may contain several log blocks in main memory buffers . When an update to a data block stored in the DBMS cache is made an associated log record is written to the last log buffer in the DBMS cache. With the write ahead logging approach the log buffers that contain the associated log records for a particular data block update must first be written to disk before the data block itself can be written back to disk from its main memory buffer. Standard DBMS recovery terminology includes the terms steal no steal and force no force which specify the rules that govern when a page from the database can be written to disk from the cache If a cache buffer page updated by a transaction cannot be written to disk before the transaction commits the recovery method is called a no steal approach. The pin unpin bit will be used to indicate if a page cannot be written back to disk. On the other hand if the recovery protocol allows writing an updated buffer before the transaction commits it is called steal. Steal is used when the DBMS cache manager needs a buffer frame for another transaction and the buffer manager replaces an existing page that had been updated but whose transaction has not committed. The no steal rule means that UNDO will never be needed during recovery since a committed transaction will not have any of its updates on disk before it commits. If all pages updated by a transaction are immediately written to disk before the transaction commits it is called a force approach. Otherwise it is called no force. The force rule means that REDO will never be needed during recovery since any committed transaction will have all its updates on disk before it is committed. The deferred update recovery scheme discussed in Section follows a no steal approach. However typical database systems employ a steal no force strategy. The advantage of steal is that it avoids the need for a very large buffer space to store all updated pages in memory. The advantage of no force is that an updated Chapter Database Recovery Techniques page of a committed transaction may still be in the buffer when another transaction needs to update it thus eliminating the I O cost to write that page multiple times to disk and possibly to have to read it again from disk. This may provide a substantial saving in the number of disk I O operations when a specific page is updated heavily by multiple transactions. To permit recovery when in place updating is used the appropriate entries required for recovery must be permanently recorded in the log on disk before changes are applied to the database. For example consider the following write ahead logging protocol for a recovery algorithm that requires both UNDO and REDO The before image of an item cannot be overwritten by its after image in the database on disk until all UNDO type log records for the updating transaction up to this point have been force written to disk. The commit operation of a transaction cannot be completed until all the REDO type and UNDO type log records for that transaction have been forcewritten to disk. To facilitate the recovery process the DBMS recovery subsystem may need to maintain a number of lists related to the transactions being processed in the system. These include a list for active transactions that have started but not committed as yet and it may also include lists of all committed and aborted transactions since the last checkpoint . Maintaining these lists makes the recovery process more efficient. Checkpoints in the System Log and Fuzzy Checkpointing Another type of entry in the log is called a checkpoint. A [checkpoint list of active transactions] record is written into the log periodically at that point when the system writes out to the database on disk all DBMS buffers that have been modified. As a consequence of this all transactions that have their [commit T ] entries in the log before a [checkpoint] entry do not need to have their WRITE operations redone in case of a system crash since all their updates will be recorded in the database on disk during checkpointing. As part of checkpointing the list of transaction ids for active transactions at the time of the checkpoint is included in the checkpoint record so that these transactions can be easily identified during recovery. The recovery manager of a DBMS must decide at what intervals to take a checkpoint. The interval may be measured in time say every m minutes or in the number t of committed transactions since the last checkpoint where the values of m or t are system parameters. Taking a checkpoint consists of the following actions Suspend execution of transactions temporarily. Force write all main memory buffers that have been modified to disk. term checkpoint has been used to describe more restrictive situations in some systems such as It has also been used in the literature to describe entirely different concepts. Recovery Concepts Write a [checkpoint] record to the log and force write the log to disk. Resume executing transactions. As a consequence of step a checkpoint record in the log may also include additional information such as a list of active transaction ids and the locations of the first and most recent records in the log for each active transaction. This can facilitate undoing transaction operations in the event that a transaction must be rolled back. The time needed to force write all modified memory buffers may delay transaction processing because of step To reduce this delay it is common to use a technique called fuzzy checkpointing. In this technique the system can resume transaction processing after a [begincheckpoint] record is written to the log without having to wait for step to finish. When step is completed an [endcheckpoint record is written in the log with the relevant information collected during checkpointing. However until step is completed the previous checkpoint record should remain valid. To accomplish this the system maintains a file on disk that contains a pointer to the valid checkpoint which continues to point to the previous checkpoint record in the log. Once step is concluded that pointer is changed to point to the new checkpoint in the log. Transaction Rollback and Cascading Rollback If a transaction fails for whatever reason after updating the database but before the transaction commits it may be necessary to roll back the transaction. If any data item values have been changed by the transaction and written to the database they must be restored to their previous values . The undo type log entries are used to restore the old values of data items that must be rolled back. If a transaction T is rolled back any transaction S that has in the interim read the value of some data item X written by T must also be rolled back. Similarly once S is rolled back any transaction R that has read the value of some data item Y written by S must also be rolled back and so on. This phenomenon is called cascading rollback and can occur when the recovery protocol ensures recoverable schedules but does not ensure strict or cascadeless schedules [starttransaction T [readitem T C [writeitem T [starttransaction T [readitem T B [writeitem T [writeitem T [writeitem T [starttransaction T [readitem T A [readitem T D [readitem T D [readitem T A T is rolled back because it did not reach its commit point. T is rolled back because it reads the value of item B written by T readitem READ. The read and write operations of three transactions. System log at point of crash. Operations before the crash. NO UNDO REDO Recovery Based on Deferred Update We must now check for cascading rollback. From Figure we see that transaction reads the value of item B that was written by transaction this can also be determined by examining the log. Because is rolled back must now be rolled back too. The WRITE operations of marked by in the log are the ones that are undone. Note that only writeitem operations need to be undone during transaction rollback readitem operations are recorded in the log only to determine whether cascading rollback of additional transactions is necessary. In practice cascading rollback of transactions is never required because practical recovery methods guarantee cascadeless or strict schedules. Hence there is also no need to record any readitem operations in the log because these are needed only for determining cascading rollback. Transaction Actions That Do Not Affect the Database In general a transaction will have actions that do not affect the database such as generating and printing messages or reports from information retrieved from the database. If a transaction fails before completion we may not want the user to get these reports since the transaction has failed to complete. If such erroneous reports are produced part of the recovery process would have to inform the user that these reports are wrong since the user may take an action based on these reports that affects the database. Hence such reports should be generated only after the transaction reaches its commit point. A common method of dealing with such actions is to issue the commands that generate the reports but keep them as batch jobs which are executed only after the transaction reaches its commit point. If the transaction fails the batch jobs are canceled. NO UNDO REDO Recovery Based on Deferred Update The idea behind deferred update is to defer or postpone any actual updates to the database on disk until the transaction completes its execution successfully and reaches its commit During transaction execution the updates are recorded only in the log and in the cache buffers. After the transaction reaches its commit point and the log is forcewritten to disk the updates are recorded in the database. If a transaction fails before reaching its commit point there is no need to undo any operations because the transaction has not affected the database on disk in any way. Therefore only REDOtype log entries are needed in the log which include the new value of the item written by a write operation. The UNDO type log entries are not needed since no undoing of operations will be required during recovery. Although this may simplify the recovery process it cannot be used in practice unless transactions are short deferred update can generally be characterized as a no steal approach. Chapter Database Recovery Techniques and each transaction changes few items. For other types of transactions there is the potential for running out of buffer space because transaction changes must be held in the cache buffers until the commit point. We can state a typical deferred update protocol as follows A transaction cannot change the database on disk until it reaches its commit point. A transaction does not reach its commit point until all its REDO type log entries are recorded in the log and the log buffer is force written to disk. Notice that step of this protocol is a restatement of the write ahead logging protocol. Because the database is never updated on disk until after the transaction commits there is never a need to UNDO any operations. REDO is needed in case the system fails after a transaction commits but before all its changes are recorded in the database on disk. In this case the transaction operations are redone from the log entries during recovery. For multiuser systems with concurrency control the concurrency control and recovery processes are interrelated. Consider a system in which concurrency control uses strict two phase locking so the locks on items remain in effect until the transaction reaches its commit point. After that the locks can be released. This ensures strict and serializable schedules. Assuming that [checkpoint] entries are included in the log a possible recovery algorithm for this case which we call RDUM is given next. Procedure RDUM . Use two lists of transactions maintained by the system the committed transactions T since the last checkpoint and the active transactions T. REDO all the WRITE operations of the committed transactions from the log in the order in which they were written into the log. The transactions that are active and did not commit are effectively canceled and must be resubmitted. The REDO procedure is defined as follows Procedure REDO . Redoing a writeitem operation WRITEOP consists of examining its log entry [writeitem T X newvalue] and setting the value of item X in the database to newvalue which is the after image . Figure illustrates a timeline for a possible schedule of executing transactions. When the checkpoint was taken at time t transaction had committed whereas transactions and had not. Before the system crash at time t and were committed but not and According to the RDUM method there is no need to redo the writeitem operations of transaction any transactions committed before the last checkpoint time t The writeitem operations of and must be redone however because both transactions reached their commit points after the last checkpoint. Recall that the log is force written before committing a transaction. Transactions and are ignored They are effectively canceled or rolled back because none of their writeitem operations were recorded in the database on disk under the deferred update protocol. Recovery Techniques Based on Immediate Update Checkpoint System crash Time T T T T T Figure An example of a recovery timeline to illustrate the effect of checkpointing. We can make the NO UNDO REDO recovery algorithm more efficient by noting that if a data item X has been updated as indicated in the log entries more than once by committed transactions since the last checkpoint it is only necessary to REDO the last update of X from the log during recovery because the other updates would be overwritten by this last REDO. In this case we start from the end of the log then whenever an item is redone it is added to a list of redone items. Before REDO is applied to an item the list is checked if the item appears on the list it is not redone again since its last value has already been recovered. If a transaction is aborted for any reason it is simply resubmitted since it has not changed the database on disk. A drawback of the method described here is that it limits the concurrent execution of transactions because all write locked items remain locked until the transaction reaches its commit point. Additionally it may require excessive buffer space to hold all updated items until the transactions commit. The method’s main benefit is that transaction operations never need to be undone for two reasons A transaction does not record any changes in the database on disk until after it reaches its commit point that is until it completes its execution successfully. Hence a transaction is never rolled back because of failure during transaction execution. A transaction will never read the value of an item that is written by an uncommitted transaction because items remain locked until a transaction reaches its commit point. Hence no cascading rollback will occur. Figure shows an example of recovery for a multiuser system that utilizes the recovery and concurrency control method just described. Recovery Techniques Based on Immediate Update In these techniques when a transaction issues an update command the database on disk can be updated immediately without any need to wait for the transaction to reach its commit point. Notice that it is not a requirement that every update be Chapter Database Recovery Techniques T readitem readitem The READ and WRITE operations of four transactions. System log at the point of crash. applied immediately to disk it is just possible that some updates are applied to disk before the transaction commits. Provisions must be made for undoing the effect of update operations that have been applied to the database by a failed transaction. This is accomplished by rolling back the transaction and undoing the effect of the transaction’s writeitem operations. Therefore the UNDO type log entries which include the old value of the item must be stored in the log. Because UNDO can be needed during recovery these methods follow a steal strategy for deciding when updated main memory buffers can be written back to disk outlines a recovery algorithm for concurrent transactions with immediate update . Assume that the log includes checkpoints and that the concurrency control protocol produces strict schedules as for example the strict two phase locking protocol does. Recall that a strict schedule does not allow a transaction to read or write an item unless the transaction that last wrote the item has committed . However deadlocks can occur in strict two phase locking thus requiring abort and UNDO of transactions. For a strict schedule UNDO of an operation requires changing the item back to its old value . Procedure RIUM . Use two lists of transactions maintained by the system the committed transactions since the last checkpoint and the active transactions. Undo all the writeitem operations of the active transactions using the UNDO procedure. The operations should be undone in the reverse of the order in which they were written into the log. Redo all the writeitem operations of the committed transactions from the log in the order in which they were written into the log using the REDO procedure defined earlier. The UNDO procedure is defined as follows Procedure UNDO . Undoing a writeitem operation writeop consists of examining its log entry [writeitem T X oldvalue newvalue] and setting the value of item X in the database to oldvalue which is the before image . Undoing a number of writeitem operations from one or more transactions from the log must proceed in the reverse order from the order in which the operations were written in the log. As we discussed for the NO UNDO REDO procedure step is more efficiently done by starting from the end of the log and redoing only the last update of each item X. Whenever an item is redone it is added to a list of redone items and is not redone again. A similar procedure can be devised to improve the efficiency of step so that an item can be undone at most once during recovery. In this case the earliest UNDO is applied first by scanning the log in the forward direction Shadow directory Page Page Page Page Page Page Page Page Figure An example of shadow paging. beginning of the log . Whenever an item is undone it is added to a list of undone items and is not undone again. Shadow Paging This recovery scheme does not require the use of a log in a single user environment. In a multiuser environment a log may be needed for the concurrency control method. Shadow paging considers the database to be made up of a number of fixedsize disk pages say n for recovery purposes. A directory with n is constructed where the ith entry points to the ith database page on disk. The directory is kept in main memory if it is not too large and all references reads or writes to database pages on disk go through it. When a transaction begins executing the current directory whose entries point to the most recent or current database pages on disk is copied into a shadow directory. The shadow directory is then saved on disk while the current directory is used by the transaction. During transaction execution the shadow directory is never modified. When a writeitem operation is performed a new copy of the modified database page is created but the old copy of that page is not overwritten. Instead the new page is written elsewhere on some previously unused disk block. The current directory entry is modified to point to the new disk block whereas the shadow directory is not modified and continues to point to the old unmodified disk block. Figure illustrates the concepts of shadow and current directories. For pages updated by the transaction two versions are kept. The old version is referenced by the shadow directory and the new version by the current directory. directory is similar to the page table maintained by the operating system for each process. The ARIES Recovery Algorithm To recover from a failure during transaction execution it is sufficient to free the modified database pages and to discard the current directory. The state of the database before transaction execution is available through the shadow directory and that state is recovered by reinstating the shadow directory. The database thus is returned to its state prior to the transaction that was executing when the crash occurred and any modified pages are discarded. Committing a transaction corresponds to discarding the previous shadow directory. Since recovery involves neither undoing nor redoing data items this technique can be categorized as a NOUNDO NO REDO technique for recovery. In a multiuser environment with concurrent transactions logs and checkpoints must be incorporated into the shadow paging technique. One disadvantage of shadow paging is that the updated database pages change location on disk. This makes it difficult to keep related database pages close together on disk without complex storage management strategies. Furthermore if the directory is large the overhead of writing shadow directories to disk as transactions commit is significant. A further complication is how to handle garbage collection when a transaction commits. The old pages referenced by the shadow directory that have been updated must be released and added to a list of free pages for future use. These pages are no longer needed after the transaction commits. Another issue is that the operation to migrate between current and shadow directories must be implemented as an atomic operation. The ARIES Recovery Algorithm We now describe the ARIES algorithm as an example of a recovery algorithm used in database systems. It is used in many relational database related products of IBM. ARIES uses a steal no force approach for writing and it is based on three concepts write ahead logging repeating history during redo and logging changes during undo. We discussed write ahead logging in Section The second concept repeating history means that ARIES will retrace all actions of the database system prior to the crash to reconstruct the database state when the crash occurred. Transactions that were uncommitted at the time of the crash are undone. The third concept logging during undo will prevent ARIES from repeating the completed undo operations if a failure occurs during recovery which causes a restart of the recovery process. The ARIES recovery procedure consists of three main steps analysis REDO and UNDO. The analysis step identifies the dirty pages in the and the set of transactions active at the time of the crash. The appropriate point in the log where the REDO operation should start is also determined. The REDO phase actually reapplies updates from the log to the database. Generally the REDO operation is applied only to committed transactions. However this is not the case in ARIES. Certain information in the ARIES log will provide the start point for REDO from actual buffers may be lost during a crash since they are in main memory. Additional tables stored in the log during checkpointing allows ARIES to identify this information . Chapter Database Recovery Techniques which REDO operations are applied until the end of the log is reached. Additionally information stored by ARIES and in the data pages will allow ARIES to determine whether the operation to be redone has actually been applied to the database and therefore does not need to be reapplied. Thus only the necessary REDO operations are applied during recovery. Finally during the UNDO phase the log is scanned backward and the operations of transactions that were active at the time of the crash are undone in reverse order. The information needed for ARIES to accomplish its recovery procedure includes the log the Transaction Table and the Dirty Page Table. Additionally checkpointing is used. These tables are maintained by the transaction manager and written to the log during checkpointing. In ARIES every log record has an associated log sequence number that is monotonically increasing and indicates the address of the log record on disk. Each LSN corresponds to a specific change of some transaction. Also each data page will store the LSN of the latest log record corresponding to a change for that page. A log record is written for any of the following actions updating a page committing a transaction aborting a transaction undoing an update and ending a transaction . The need for including the first three actions in the log has been discussed but the last two need some explanation. When an update is undone a compensation log record is written in the log. When a transaction ends whether by committing or aborting an end log record is written. Common fields in all log records include the previous LSN for that transaction the transaction ID and the type of log record. The previous LSN is important because it links the log records for each transaction. For an update action additional fields in the log record include the page ID for the page that contains the item the length of the updated item its offset from the beginning of the page the before image of the item and its after image. Besides the log two tables are needed for efficient recovery the Transaction Table and the Dirty Page Table which are maintained by the transaction manager. When a crash occurs these tables are rebuilt in the analysis phase of recovery. The Transaction Table contains an entry for each active transaction with information such as the transaction ID transaction status and the LSN of the most recent log record for the transaction. The Dirty Page Table contains an entry for each dirty page in the buffer which includes the page ID and the LSN corresponding to the earliest update to that page. Checkpointing in ARIES consists of the following writing a begincheckpoint record to the log writing an endcheckpoint record to the log and writing the LSN of the begincheckpoint record to a special file. This special file is accessed during recovery to locate the last checkpoint information. With the endcheckpoint record the contents of both the Transaction Table and Dirty Page Table are appended to the end of the log. To reduce the cost fuzzy checkpointing is used so that the DBMS can continue to execute transactions during checkpointing . During analysis the log records being analyzed may cause modifications to these two tables. For instance if an end log record was encountered for a transaction T in the Transaction Table then the entry for T is deleted from that table. If some other type of log record is encountered for a transaction T then an entry for Tis inserted into the Transaction Table if not already present and the last LSN field is modified. If the log record corresponds to a change for page P then an entry would be made for page P and the associated LSN field would be modified. When the analysis phase is complete the necessary information for REDO and UNDO has been compiled in the tables. The REDO phase follows next. To reduce the amount of unnecessary work ARIES starts redoing at a point in the log where it knows that previous changes to dirty pages have already been applied to the database on disk. It can determine this by finding the smallest LSN M of all the dirty pages in the Dirty Page Table which indicates the log position where ARIES needs to start the REDO phase. Any changes corresponding to an LSN M for redoable transactions must have already been propagated to disk or already been overwritten in the buffer otherwise those dirty pages with that LSN would be in the buffer . So REDO starts at the log record with LSN M and scans forward to the end of the log. For each change recorded in the log the REDO algorithm would verify whether or not the change has to be reapplied. For example if a change recorded in the log pertains to page P that is not in the Dirty Page Table then this change is already on disk and does not need to be reapplied. Or if a change recorded in the log pertains to page P and the Dirty Page Table contains an entry for P with LSN greater than N then the change is already present. If neither of these two conditions hold page P is read from disk and the LSN stored on that page LSN is compared with N. If N LSN then the change has been applied and the page does not need to be rewritten to disk. Once the REDO phase is finished the database is in the exact state that it was in when the crash occurred. The set of active transactions called the undoset has been identified in the Transaction Table during the analysis phase. Now the UNDO phase proceeds by scanning backward from the end of the log and undoing the appropriate actions. A compensating log record is written for each action that is undone. The UNDO reads backward in the log until every action of the set of transactions in the undoset has been undone. When this is completed the recovery process is finished and normal processing can begin again. Consider the recovery example shown in Figure There are three transactions and updates page C updates pages B and C and updates page A. Chapter Database Recovery Techniques TRANSACTION TABLE Lastlsn Status Lsn Lastlsn Tranid Type Pageid Otherinformation Transactionid TRANSACTION TABLE DIRTY PAGE TABLE Transactionid T Lastlsn commit Status Pageid C Lsn T T in progress commit A B T T DIRTY PAGE TABLE Pageid C Lsn B commit in progress end checkpoint begin checkpoint T T T T T T update commit update update commit update B C A C . . . . . . . . . . . . . . . . . . Figure An example of recovery in ARIES. The log at point of crash. The Transaction and Dirty Page Tables at time of checkpoint. The Transaction and Dirty Page Tables after the analysis phase. Figure shows the partial contents of the log and Figure shows the contents of the Transaction Table and Dirty Page Table. Now suppose that a crash occurs at this point. Since a checkpoint has occurred the address of the associated begincheckpoint record is retrieved which is location The analysis phase starts from location until it reaches the end. The endcheckpoint record would contain the Transaction Table and Dirty Page Table in Figure and the analysis phase will further reconstruct these tables. When the analysis phase encounters log record a new entry for transaction is made in the Transaction Table and a new entry for page A is made in the Dirty Page Table. After log record is analyzed the status of transaction is changed to committed in the Transaction Table. Figure shows the two tables after the analysis phase. Recovery in Multidatabase Systems For the REDO phase the smallest LSN in the Dirty Page Table is Hence the REDO will start at log record and proceed with the REDO of updates. The LSNs corresponding to the updates for pages C B A and C respectively are not less than the LSNs of those pages . So those data pages will be read again and the updates reapplied from the log . At this point the REDO phase is finished and the UNDO phase starts. From the Transaction Table is followed and undone. Recovery in Multidatabase Systems So far we have implicitly assumed that a transaction accesses a single database. In some cases a single transaction called a multidatabase transaction may require access to multiple databases. These databases may even be stored on different types of DBMSs for example some DBMSs may be relational whereas others are objectoriented hierarchical or network DBMSs. In such a case each DBMS involved in the multidatabase transaction may have its own recovery technique and transaction manager separate from those of the other DBMSs. This situation is somewhat similar to the case of a distributed database management system . The coordinator usually follows a protocol called the two phase commit protocol whose two phases can be stated as follows Phase When all participating databases signal the coordinator that the part of the multidatabase transaction involving each has concluded the coordinator sends a message prepare for commit to each participant to get ready for committing the transaction. Each participating database receiving that message will force write all log records and needed information for local recovery to disk and then send a ready to commit or OK signal to the coordinator. If the force writing to disk fails or the local transaction cannot commit for some reason the participating database sends a cannot commit or not OK signal to the coordinator. If the coordinator does not receive a reply from the database within a certain time out interval it assumes a not OK response. Phase If all participating databases reply OK and the coordinator’s vote is also OK the transaction is successful and the coordinator sends a commit signal for the transaction to the participating databases. Because all the local Chapter Database Recovery Techniques effects of the transaction and information needed for local recovery have been recorded in the logs of the participating databases recovery from failure is now possible. Each participating database completes transaction commit by writing a [commit] entry for the transaction in the log and permanently updating the database if needed. On the other hand if one or more of the participating databases or the coordinator have a not OK response the transaction has failed and the coordinator sends a message to roll back or UNDO the local effect of the transaction to each participating database. This is done by undoing the transaction operations using the log. The net effect of the two phase commit protocol is that either all participating databases commit the effect of the transaction or none of them do. In case any of the participants or the coordinator fails it is always possible to recover to a state where either the transaction is committed or it is rolled back. A failure during or before Phase usually requires the transaction to be rolled back whereas a failure during Phase means that a successful transaction can recover and commit. Database Backup and Recovery from Catastrophic Failures So far all the techniques we have discussed apply to noncatastrophic failures. A key assumption has been that the system log is maintained on the disk and is not lost as a result of the failure. Similarly the shadow directory must be stored on disk to allow recovery when shadow paging is used. The recovery techniques we have discussed use the entries in the system log or the shadow directory to recover from failure by bringing the database back to a consistent state. The recovery manager of a DBMS must also be equipped to handle more catastrophic failures such as disk crashes. The main technique used to handle such crashes is a database backup in which the whole database and the log are periodically copied onto a cheap storage medium such as magnetic tapes or other large capacity offline storage devices. In case of a catastrophic system failure the latest backup copy can be reloaded from the tape to the disk and the system can be restarted. Data from critical applications such as banking insurance stock market and other databases is periodically backed up in its entirety and moved to physically separate safe locations. Subterranean storage vaults have been used to protect such data from flood storm earthquake or fire damage. Events like the terrorist attack in New York and after image of a data item What is the difference between in place updating and shadowing with respect to their handling of BFIM and AFIM What are UNDO type and REDO type log entries Describe the write ahead logging protocol. Identify three typical lists of transactions that are maintained by the recovery subsystem. What is meant by transaction rollback What is meant by cascading rollback Why do practical recovery methods use protocols that do not permit cascading rollback Which recovery techniques do not require any rollback Discuss the UNDO and REDO operations and the recovery techniques that use each. Discuss the deferred update technique of recovery. What are the advantages and disadvantages of this technique Why is it called the NO UNDO REDO method How can recovery handle transaction operations that do not affect the database such as the printing of reports by a transaction Discuss the immediate update recovery technique in both single user and multiuser environments. What are the advantages and disadvantages of immediate update What is the difference between the UNDO REDO and the UNDO NO REDO algorithms for recovery with immediate update Develop the outline for an UNDO NO REDO algorithm. Describe the shadow paging recovery technique. Under what circumstances does it not require a log Describe the three phases of the ARIES recovery method. What are log sequence numbers in ARIES How are they used What information do the Dirty Page Table and Transaction Table contain Describe how fuzzy checkpointing is used in ARIES. Exercises [checkpoint] [starttransaction T [starttransaction [starttransaction [readitem T A [readitem T D [readitem D [readitem D [readitem B [writeitem T D [writeitem B [readitem A [writeitem D [writeitem C [writeitem D [writeitem A [commit T [commit [starttransaction System crash Figure A sample schedule and its corresponding log. What do the terms steal no steal and force no force mean with regard to buffer management for transaction processing Describe the two phase commit protocol for multidatabase transactions. Discuss how disaster recovery from catastrophic failures is handled. Exercises Suppose that the system crashes before the [readitem A] entry is written to the log in Figure Will that make any difference in the recovery process Suppose that the system crashes before the [writeitem D entry is written to the log in Figure Will that make any difference in the recovery process Figure shows the log corresponding to a particular schedule at the point of a system crash for four transactions and Suppose that we use the immediate update protocol with checkpointing. Describe the recovery process from the system crash. Specify which transactions are rolled back which operations in the log are redone and which are undone and whether any cascading rollback takes place. Chapter Database Recovery Techniques Suppose that we use the deferred update protocol for the example in Figure Show how the log would be different in the case of deferred update by removing the unnecessary log entries then describe the recovery process using your modified log. Assume that only REDO operations are applied and specify which operations in the log are redone and which are ignored. How does checkpointing in ARIES differ from checkpointing as described in Section How are log sequence numbers used by ARIES to reduce the amount of REDO work needed for recovery Illustrate with an example using the information shown in Figure You can make your own assumptions as to when a page is written to disk. What implications would a no steal force buffer management policy have on checkpointing and recovery Choose the correct answer for each of the following multiple choice questions Incremental logging with deferred updates implies that the recovery system must necessarily a. store the old value of the updated item in the log. b. store the new value of the updated item in the log. c. store both the old and new value of the updated item in the log. d. store only the Begin Transaction and Commit Transaction records in the log. The write ahead logging protocol simply means that a. writing of a data item should be done ahead of any logging operation. b. the log record for an operation should be written before the actual data is written. c. all log records should be written before a new transaction begins execution. d. the log never needs to be written to disk. In case of transaction failure under a deferred update incremental logging scheme which of the following will be needed a. an undo operation b. a redo operation c. an undo and redo operation d. none of the above For incremental logging with immediate updates a log record for a transaction would contain a. a transaction name a data item name and the old and new value of the item. Exercises b. a transaction name a data item name and the old value of the item. c. a transaction name a data item name and the new value of the item. d. a transaction name and a data item name. For correct behavior during recovery undo and redo operations must be a. commutative. b. associative. c. idempotent. d. distributive. When a failure occurs the log is consulted and each operation is either undone or redone. This is a problem because a. searching the entire log is time consuming. b. many redos are unnecessary. c. both and . d. none of the above. When using a log based recovery scheme it might improve performance as well as providing a recovery mechanism by a. writing the log records to disk when each transaction commits. b. writing the appropriate log records to disk during the transaction’s execution. c. waiting to write the log records until multiple transactions commit and writing them as a batch. d. never writing the log records to disk. There is a possibility of a cascading rollback when a. a transaction writes items that have been written only by a committed transaction. b. a transaction writes an item that is previously written by an uncommitted transaction. c. a transaction reads an item that is previously written by an uncommitted transaction. d. both and . To cope with media failures it is necessary a. for the DBMS to only execute transactions in a single user environment. b. to keep a redundant copy of the database. c. to never abort a transaction. d. all of the above. If the shadowing approach is used for flushing a data item back to disk then a. the item is written to disk only after the transaction commits. b. the item is written to a different location on disk. c. the item is written to disk before the transaction commits. d. the item is written to the same disk location from which it was read. Selected Bibliography The books by Bernstein et al. and Papadimitriou are devoted to the theory and principles of concurrency control and recovery. The book by Gray and Reuter is an encyclopedic work on concurrency control recovery and other transaction processing issues. Verhofstad presents a tutorial and survey of recovery techniques in database systems. Categorizing algorithms based on their UNDO REDO characteristics is discussed in Haerder and Reuter and in Bernstein et al. Gray discusses recovery along with other system aspects of implementing operating systems for databases. The shadow paging technique is discussed in Lorie Verhofstad and Reuter Gray et al. discuss the recovery mechanism in SYSTEM R. Lockemann and Knutsen Davies and Bjork are early papers that discuss recovery. Chandy et al. discuss transaction rollback. Lilien and Bhargava discuss the concept of integrity block and its use to improve the efficiency of recovery. Recovery using write ahead logging is analyzed in Jhingran and Khedkar and is used in the ARIES system key infrastructure schemes. It also discusses digital certificates. Section introduces privacy preserving techniques and Section presents the current challenges to database security. In Section we discuss Oracle label based security. Finally Section summarizes the chapter. Readers who are interested only in basic database security mechanisms will find it sufficient to cover the material in Sections and chapter Chapter Database Security Introduction to Database Security Types of Security Database security is a broad area that addresses many issues including the following Various legal and ethical issues regarding the right to access certain information for example some information may be deemed to be private and cannot be accessed legally by unauthorized organizations or persons. In the United States there are numerous laws governing privacy of information. Policy issues at the governmental institutional or corporate level as to what kinds of information should not be made publicly available for example credit ratings and personal medical records. System related issues such as the system levels at which various security functions should be enforced for example whether a security function should be handled at the physical hardware level the operating system level or the DBMS level. The need in some organizations to identify multiple security levels and to categorize the data and users based on these classifications for example top secret secret confidential and unclassified. The security policy of the organization with respect to permitting access to various classifications of data must be enforced. Threats to Databases. Threats to databases can result in the loss or degradation of some or all of the following commonly accepted security goals integrity availability and confidentiality. Loss of integrity. Database integrity refers to the requirement that information be protected from improper modification. Modification of data includes creation insertion updating changing the status of data and deletion. Integrity is lost if unauthorized changes are made to the data by either intentional or accidental acts. If the loss of system or data integrity is not corrected continued use of the contaminated system or corrupted data could result in inaccuracy fraud or erroneous decisions. Loss of availability. Database availability refers to making objects available to a human user or a program to which they have a legitimate right. Loss of confidentiality. Database confidentiality refers to the protection of data from unauthorized disclosure. The impact of unauthorized disclosure of confidential information can range from violation of the Data Privacy Act to the jeopardization of national security. Unauthorized unanticipated or unintentional disclosure could result in loss of public confidence embarrassment or legal action against the organization. substantial contribution of Fariborz Farahmand and Bharath Rengarajan to this and subsequent sections in this chapter is much appreciated. Introduction to Database Security Issues To protect databases against these types of threats it is common to implement four kinds of control measures access control inference control flow control and encryption. We discuss each of these in this chapter. In a multiuser database system the DBMS must provide techniques to enable certain users or user groups to access selected portions of a database without gaining access to the rest of the database. This is particularly important when a large integrated database is to be used by many different users within the same organization. For example sensitive information such as employee salaries or performance reviews should be kept confidential from most of the database system’s users. A DBMS typically includes a database security and authorization subsystem that is responsible for ensuring the security of portions of a database against unauthorized access. It is now customary to refer to two types of database security mechanisms Discretionary security mechanisms. These are used to grant privileges to users including the capability to access specific data files records or fields in a specified mode . Mandatory security mechanisms. These are used to enforce multilevel security by classifying the data and users into various security classes and then implementing the appropriate security policy of the organization. For example a typical security policy is to permit users at a certain classification level to see only the data items classified at the user’s own classification level. An extension of this is role based security which enforces policies and privileges based on the concept of organizational roles. We discuss discretionary security in Section and mandatory and role based security in Section Control Measures Four main control measures are used to provide security of data in databases Access control Inference control Flow control Data encryption A security problem common to computer systems is that of preventing unauthorized persons from accessing the system itself either to obtain information or to make malicious changes in a portion of the database. The security mechanism of a DBMS must include provisions for restricting access to the database system as a whole. This function called access control is handled by creating user accounts and passwords to control the login process by the DBMS. We discuss access control techniques in Section Statistical databases are used to provide statistical information or summaries of values based on various criteria. For example a database for population statistics Chapter Database Security may provide statistics based on age groups income levels household size education levels and other criteria. Statistical database users such as government statisticians or market research firms are allowed to access the database to retrieve statistical information about a population but not to access the detailed confidential information about specific individuals. Security for statistical databases must ensure that information about individuals cannot be accessed. It is sometimes possible to deduce or infer certain facts concerning individuals from queries that involve only summary statistics on groups consequently this must not be permitted either. This problem called statistical database security is discussed briefly in Section The corresponding control measures are called inference control measures. Another security issue is that of flow control which prevents information from flowing in such a way that it reaches unauthorized users. It is discussed in Section Channels that are pathways for information to flow implicitly in ways that violate the security policy of an organization are called covert channels. We briefly discuss some issues related to covert channels in Section A final control measure is data encryption which is used to protect sensitive data that is transmitted via some type of communications network. Encryption can be used to provide additional protection for sensitive portions of a database as well. The data is encoded using some coding algorithm. An unauthorized user who accesses encoded data will have difficulty deciphering it but authorized users are given decoding or decrypting algorithms to decipher the data. Encrypting techniques that are very difficult to decode without a key have been developed for military applications. Section briefly discusses encryption techniques including popular techniques such as public key encryption which is heavily used to support Web based transactions against databases and digital signatures which are used in personal communications. A comprehensive discussion of security in computer systems and databases is outside the scope of this textbook. We give only a brief overview of database security techniques here. The interested reader can refer to several of the references discussed in the Selected Bibliography at the end of this chapter for a more comprehensive discussion. Database Security and the DBA As we discussed in Chapter the database administrator is the central authority for managing a database system. The DBA’s responsibilities include granting privileges to users who need to use the system and classifying users and data in accordance with the policy of the organization. The DBA has a DBA account in the DBMS sometimes called a system or superuser account which provides powerful capabilities that are not made available to regular database accounts and DBA privileged commands include commands for granting and revoking privileges account is similar to the root or superuser accounts that are given to computer system administrators which allow access to restricted operating system commands. Introduction to Database Security Issues to individual accounts users or user groups and for performing the following types of actions Account creation. This action creates a new account and password for a user or a group of users to enable access to the DBMS. Privilege granting. This action permits the DBA to grant certain privileges to certain accounts. Privilege revocation. This action permits the DBA to revoke certain privileges that were previously given to certain accounts. Security level assignment. This action consists of assigning user accounts to the appropriate security clearance level. The DBA is responsible for the overall security of the database system. Action in the preceding list is used to control access to the DBMS as a whole whereas actions and are used to control discretionary database authorization and action is used to control mandatory authorization. Access Control User Accounts and Database Audits Whenever a person or a group of persons needs to access a database system the individual or group must first apply for a user account. The DBA will then create a new account number and password for the user if there is a legitimate need to access the database. The user must log in to the DBMS by entering the account number and password whenever database access is needed. The DBMS checks that the account number and password are valid if they are the user is permitted to use the DBMS and to access the database. Application programs can also be considered users and are required to log in to the database or not for individual users or for categories of users. Several factors need to be considered before deciding whether it is safe to reveal the data. The three most important factors are data availability access acceptability and authenticity assurance. Data availability. If a user is updating a field then this field becomes inaccessible and other users should not be able to view this data. This blocking is Introduction to Database Security Issues only temporary and only to ensure that no user sees any inaccurate data. This is typically handled by the concurrency control mechanism in industry government and academia raises challenging questions and problems regarding the protection and use of personal information. Questions of who has what rights to information about individuals for which purposes become more important as we move toward a world in which it is technically possible to know just about anything about anyone. Deciding how to design privacy considerations in technology for the future includes philosophical legal and practical dimensions. There is a considerable overlap between issues related to access to resources and issues related to appropriate use of information . We now define the difference between security versus privacy. Security in information technology refers to many aspects of protecting a system from unauthorized use including authentication of users information encryption access control firewall policies and intrusion detection. For our purposes here we Chapter Database Security will limit our treatment of security to the concepts associated with how well a system can protect access to information it contains. The concept of privacy goes beyond security. Privacy examines how well the use of personal information that the system acquires about a user conforms to the explicit or implicit assumptions regarding that use. From an end user perspective privacy can be considered from two different perspectives preventing storage of personal information versus ensuring appropriate use of personal information. For the purposes of this chapter a simple but useful definition of privacy is the ability of individuals to control the terms under which their personal information is acquired and used. In summary security involves technology to ensure that information is appropriately protected. Security is a required building block for privacy to exist. Privacy involves mechanisms to support compliance with some basic principles and other explicitly stated policies. One basic principle is that people should be informed about information collection told in advance what will be done with their information and given a reasonable opportunity to approve of such use of the information. A related concept trust relates to both security and privacy and is seen as increasing when it is perceived that both security and privacy are provided for. Discretionary Access Control Based on Granting and Revoking Privileges The typical method of enforcing discretionary access control in a database system is based on the granting and revoking of privileges. Let us consider privileges in the context of a relational DBMS. In particular we will discuss a system of privileges somewhat similar to the one originally developed for the SQL language . For simplicity we will use the words user or account interchangeably in place of authorization identifier. The DBMS must provide selective access to each relation in the database based on specific accounts. Operations may also be controlled thus having an account does not necessarily entitle the account holder to all the functionality provided by the DBMS. Informally there are two levels for assigning privileges to use the database system The account level. At this level the DBA specifies the particular privileges that each account holds independently of the relations in the database. The relation level. At this level the DBA can control the privilege to access each individual relation or view in the database. privileges were incorporated into and are applicable to later versions of SQL. Discretionary Access Control Based on Granting and Revoking Privileges The privileges at the account level apply to the capabilities provided to the account itself and can include the CREATE SCHEMA or CREATE TABLE privilege to create a schema or base relation the CREATE VIEW privilege the ALTER privilege to apply schema changes such as adding or removing attributes from relations the DROP privilege to delete relations or views the MODIFY privilege to insert delete or update tuples and the SELECT privilege to retrieve information from the database by using a SELECT query. Notice that these account privileges apply to the account in general. If a certain account does not have the CREATE TABLE privilege no relations can be created from that account. Account level privileges are not defined as part of they are left to the DBMS implementers to define. In earlier versions of SQL a CREATETAB privilege existed to give an account the privilege to create tables . The second level of privileges applies to the relation level whether they are base relations or virtual relations. These privileges are defined for In the following discussion the term relation may refer either to a base relation or to a view unless we explicitly specify one or the other. Privileges at the relation level specify for each user the individual relations on which each type of command can be applied. Some privileges also refer to individual columns of relations. commands provide privileges at the relation and attribute level only. Although this is quite general it makes it difficult to create accounts with limited privileges. The granting and revoking of privileges generally follow an authorization model for discretionary privileges known as the access matrix model where the rows of a matrix M represent subjects and the columns represent objects . Each position M in the matrix represents the types of privileges that subject i holds on object j. To control the granting and revoking of relation privileges each relation R in a database is assigned an owner account which is typically the account that was used when the relation was created in the first place. The owner of a relation is given all privileges on that relation. In the DBA can assign an owner to a whole schema by creating the schema and associating the appropriate authorization identifier with that schema using the CREATE SCHEMA command privilege on R. Gives the account retrieval privilege. In SQL this gives the account the privilege to use the SELECT statement to retrieve tuples from R. Modification privileges on R. This gives the account the capability to modify the tuples of R. In SQL this includes three privileges UPDATE DELETE and INSERT. These correspond to the three SQL commands a relation R when specifying integrity constraints. This privilege can also be restricted to specific attributes of R. Notice that to create a view the account must have the SELECT privilege on all relations involved in the view definition in order to specify the query that corresponds to the view. Specifying Privileges through the Use of Views The mechanism of views is an important discretionary authorization mechanism in its own right. For example if the owner A of a relation R wants another account B to be able to retrieve only some fields of R then A can create a view V of R that includes only those attributes and then grant SELECT on V to B. The same applies to limiting B to retrieving only certain tuples of R a view V can be created by defining the view by means of a query that selects only those tuples from R that A wants to allow B to access. We will illustrate this discussion with the example given in Section Revoking of Privileges In some cases it is desirable to grant a privilege to a user temporarily. For example the owner of a relation may want to grant the SELECT privilege to a user for a specific task and then revoke that privilege once the task is completed. Hence a mechanism for revoking privileges is needed. In SQL a REVOKE command is included for the purpose of canceling privileges. We will see how the REVOKE command is used in the example in Section Propagation of Privileges Using the GRANT OPTION Whenever the owner A of a relation R grants a privilege on R to another account B the privilege can be given to B with or without the GRANT OPTION. If the GRANT OPTION is given this means that B can also grant that privilege on R to other accounts. Suppose that B is given the GRANT OPTION by A and that B then grants the privilege on R to a third account C also with the GRANT OPTION. In this way privileges on R can propagate to other accounts without the knowledge of the owner of R. If the owner account A now revokes the privilege granted to B all the privileges that B propagated based on that privilege should automatically be revoked by the system. It is possible for a user to receive a certain privilege from two or more sources. For example may receive a certain UPDATE R privilege from both and In such a case if revokes this privilege from will still continue to have the privilege by virtue of having been granted it from If later revokes the privilege from totally loses the privilege. Hence a DBMS that allows propagation of privileges must keep track of how all the privileges were granted so that revoking of privileges can be done correctly and completely. Discretionary Access Control Based on Granting and Revoking Privileges An Example to Illustrate Granting and Revoking of Privileges Suppose that the DBA creates four and wants only to be able to create base relations. To do this the DBA must issue the following GRANT command in SQL GRANT CREATETAB TO The CREATETAB privilege gives account the capability to create new database tables and is hence an account privilege. This privilege was part of earlier versions of SQL but is now left to each individual system implementation to define. In the same effect can be accomplished by having the DBA issue a CREATE SCHEMA command as follows CREATE SCHEMA EXAMPLE AUTHORIZATION User account can now create tables under the schema called EXAMPLE. To continue our example suppose that creates the two base relations EMPLOYEE and DEPARTMENT shown in Figure is then the owner of these two relations and hence has all the relation privileges on each of them. Next suppose that account wants to grant to account the privilege to insert and delete tuples in both of these relations. However does not want to be able to propagate these privileges to additional accounts. can issue the following command GRANT INSERT DELETE ON EMPLOYEE DEPARTMENT TO Notice that the owner account of a relation automatically has the GRANT OPTION allowing it to grant privileges on the relation to other accounts. However account cannot grant INSERT and DELETE privileges on the EMPLOYEE and DEPARTMENT tables because was not given the GRANT OPTION in the preceding command. Next suppose that wants to allow account to retrieve information from either of the two tables and also to be able to propagate the SELECT privilege to other accounts. can issue the following command GRANT SELECT ON EMPLOYEE DEPARTMENT TO WITH GRANT OPTION DEPARTMENT Dnumber Mgrssn Dname Name Bdate Address Sex Salary Dno EMPLOYEE Ssn Figure Schemas for the two relations EMPLOYEE and DEPARTMENT. Chapter Database Security The clause WITH GRANT OPTION means that can now propagate the privilege to other accounts by using GRANT. For example can grant the SELECT privilege on the EMPLOYEE relation to by issuing the following command GRANT SELECT ON EMPLOYEE TO Notice that cannot propagate the SELECT privilege to other accounts because the GRANT OPTION was not given to Now suppose that decides to revoke the SELECT privilege on the EMPLOYEE relation from then can issue this command REVOKE SELECT ON EMPLOYEE FROM The DBMS must now revoke the SELECT privilege on EMPLOYEE from and it must also automatically revoke the SELECT privilege on EMPLOYEE from This is because granted that privilege to but does not have the privilege any more. Next suppose that wants to give back to a limited capability to SELECT from the EMPLOYEE relation and wants to allow to be able to propagate the privilege. The limitation is to retrieve only the Name Bdate and Address attributes and only for the tuples with Dno then can create the following view CREATE VIEW AS SELECT Name Bdate Address FROM EMPLOYEE WHERE Dno After the view is created can grant SELECT on the view to as follows GRANT SELECT ON TO WITH GRANT OPTION Finally suppose that wants to allow to update only the Salary attribute of EMPLOYEE can then issue the following command GRANT UPDATE ON EMPLOYEE TO The UPDATE and INSERT privileges can specify particular attributes that may be updated or inserted in a relation. Other privileges are not attribute specific because this specificity can easily be controlled by creating the appropriate views that include only the desired attributes and granting the corresponding privileges on the views. However because updating views is not always possible or this is because must reduce the vertical propagation by at least when passing the privilege to others. In addition the horizontal propagation must be less than or equal to the originally granted horizontal propagation. For example if account A grants a privilege to account B with the horizontal propagation set to an integer number j this means that B can grant the privilege to other accounts only with a horizontal propagation less than or equal to j. As this example shows horizontal and vertical propagation techniques are designed to limit the depth and breadth of propagation of privileges. Mandatory Access Control and Role Based Access Control for Multilevel Security The discretionary access control technique of granting and revoking privileges on relations has traditionally been the main security mechanism for relational database systems. This is an all or nothing method A user either has or does not have a certain privilege. In many applications an additional security policy is needed that classifies data and users based on security classes. This approach known as mandatory access control would typically be combined with the discretionary access control mechanisms described in Section It is important to note that most commercial DBMSs currently provide mechanisms only for discretionary access control. However the need for multilevel security exists in government military and intelligence applications as well as in many industrial and corporate applications. Some DBMS vendors for example Oracle have released special versions of their RDBMSs that incorporate mandatory access control for government use. Typical security classes are top secret secret confidential and unclassified where TS is the highest level and U the lowest. Other more complex security classification schemes exist in which the security classes are organized in a lattice. For simplicity we will use the system with four security classification levels where TS ≥ S ≥ C ≥ U to illustrate our discussion. The commonly used model for multilevel security known as the Bell LaPadula model classifies each subject and object into one of the security classifications TS S C or U. We will refer to the clearance of a subject S as class and to the classification of an object O as class. Two restrictions are enforced on data access based on the subject object classifications A subject S is not allowed read access to an object O unless class ≥ class. This is known as the simple security property. A subject S is not allowed to write an object O unless class ≤ class. This is known as the star property . The first restriction is intuitive and enforces the obvious rule that no subject can read an object whose security classification is higher than the subject’s security clearance. The second restriction is less intuitive. It prohibits a subject from writing an object at a lower security classification than the subject’s security clearance. Violation of this rule would allow information to flow from higher to lower classifications which violates a basic tenet of multilevel security. For example a user with TS clearance may make a copy of an object with classification TS and then write it back as a new object with classification U thus making it visible throughout the system. To incorporate multilevel security notions into the relational database model it is common to consider attribute values and tuples as data objects. Hence each attribute A is associated with a classification attribute C in the schema and each attribute value in a tuple is associated with a corresponding security classification. In addition in some models a tuple classification attribute TC is added to the relation attributes to provide a classification for each tuple as a whole. The model we describe here is known as the multilevel model because it allows classifications at multiple security levels. A multilevel relation schema R with n attributes would be represented as An Cn TC where each Ci represents the classification attribute associated with attribute Ai . The value of the tuple classification attribute TC in each tuple t which is the highest of all attribute classification values within t provides a general classification for the tuple itself. Each attribute classification Ci provides a finer security classification for each attribute value within the tuple. The value of TC in each tuple t is the highest of all attribute classification values Ci within t. The apparent key of a multilevel relation is the set of attributes that would have formed the primary key in a regular relation. A multilevel relation will appear to contain different data to subjects with different clearance levels. In some cases it is possible to store a single tuple in the relation at a higher classification level and produce the corresponding tuples at a lower level classification through a process known as filtering. In other cases it is necessary to store two or more tuples at different classification levels with the same value for the apparent key. Mandatory Access Control and Role Based Access Control for Multilevel Security This leads to the concept of polyinstantiation where several tuples can have the same apparent key value but have different attribute values for users at different clearance levels. We illustrate these concepts with the simple example of a multilevel relation shown in Figure where we display the classification attribute values next to each attribute’s value. Assume that the Name attribute is the apparent key and consider the query SELECT FROM EMPLOYEE. A user with security clearance S would see the same relation shown in Figure since all tuple classifications are less than or equal to S. However a user with security clearance C would not be allowed to see the values for Salary of ‘Brown’ and Jobperformance of ‘Smith’ since they have higher classification. The tuples would be filtered to appear as shown in Figure with Salary and Jobperformance appearing as null. For a user with security clearance U the filtering allows only the Name attribute of ‘Smith’ to appear with all the other Name Salary JobPerformance TC Sm th U C S Fa r S Sm th U C Excellent C C Brown C S C Good S EMPLOYEE Name Salary JobPerformance TC Sm th U C S Fa r S Brown C S C Good S EMPLOYEE Name Salary JobPerformance TC Sm th U C C NULL C Brown C C NULL C Good C EMPLOYEE Name Salary JobPerformance TC Sm th U U NULL U NULL U EMPLOYEE Figure A multilevel relation to illustrate multilevel security. The original EMPLOYEE tuples. Appearance of EMPLOYEE after filtering for classification C users. Appearance of EMPLOYEE after filtering for classification U users. Polyinstantiation of the Smith tuple. is similar to the notion of having multiple versions in the database that represent the same realworld object. Chapter Database Security attributes appearing as null from a higher classified tuple then it is sufficient to store the higher classified tuple in the multilevel relation. To illustrate polyinstantiation further suppose that a user with security clearance C tries to update the value of Jobperformance of ‘Smith’ in Figure to ‘Excellent’ this corresponds to the following SQL update being submitted by that user UPDATE EMPLOYEE SET Jobperformance ‘Excellent’ WHERE Name ‘Smith’ Since the view provided to users with security clearance C must be modified to handle this and similar situations but this aspect of the problem is outside the scope of our presentation. We refer the interested reader to the Selected Bibliography at the end of this chapter for further details. Comparing Discretionary Access Control and Mandatory Access Control Discretionary access control policies are characterized by a high degree of flexibility which makes them suitable for a large variety of application domains. The main drawback of DAC models is their vulnerability to malicious attacks such as Trojan horses embedded in application programs. The reason is that discretionary authorization models do not impose any control on how information is propagated and used once it has been accessed by users authorized to do so. By contrast mandatory policies ensure a high degree of protection in a way they prevent Mandatory Access Control and Role Based Access Control for Multilevel Security any illegal flow of information. Therefore they are suitable for military and high security types of applications which require a higher degree of protection. However mandatory policies have the drawback of being too rigid in that they require a strict classification of subjects and objects into security levels and therefore they are applicable to few environments. In many practical situations discretionary policies are preferred because they offer a better tradeoff between security and applicability. Role Based Access Control Role based access control emerged rapidly in the as a proven technology for managing and enforcing security in large scale enterprise wide systems. Its basic notion is that privileges and other permissions are associated with organizational roles rather than individual users. Individual users are then assigned to appropriate roles. Roles can be created using the CREATE ROLE and DESTROY ROLE commands. The GRANT and REVOKE commands discussed in Section can then be used to assign and revoke privileges from roles as well as for individual users when needed. For example a company may have roles such as sales account manager purchasing agent mailroom clerk department manager and so on. Multiple individuals can be assigned to each role. Security privileges that are common to a role are granted to the role name and any individual assigned to this role would automatically have those privileges granted. RBAC can be used with traditional discretionary and mandatory access controls it ensures that only authorized users in their specified roles are given access to certain data or resources. Users create sessions during which they may activate a subset of roles to which they belong. Each session can be assigned to several roles but it maps to one user or a single subject only. Many DBMSs have allowed the concept of roles where privileges can be assigned to roles. Separation of duties is another important requirement in various commercial DBMSs. It is needed to prevent one user from doing work that requires the involvement of two or more people thus preventing collusion. One method in which separation of duties can be successfully implemented is with mutual exclusion of roles. Two roles are said to be mutually exclusive if both the roles cannot be used simultaneously by the user. Mutual exclusion of roles can be categorized into two types namely authorization time exclusion and runtime exclusion . In authorization time exclusion two roles that have been specified as mutually exclusive cannot be part of a user’s authorization at the same time. In runtime exclusion both these roles can be authorized to one user but cannot be activated by the user at the same time. Another variation in mutual exclusion of roles is that of complete and partial exclusion. The role hierarchy in RBAC is a natural way to organize roles to reflect the organization’s lines of authority and responsibility. By convention junior roles at the bottom are connected to progressively senior roles as one moves up the hierarchy. The hierarchic diagrams are partial orders so they are reflexive transitive and Chapter Database Security antisymmetric. In other words if a user has one role the user automatically has roles lower in the hierarchy. Defining a role hierarchy involves choosing the type of hierarchy and the roles and then implementing the hierarchy by granting roles to other roles. Role hierarchy can be implemented in the following manner GRANT ROLE fulltime TO GRANT ROLE intern TO The above are examples of granting the roles fulltime and intern to two types of employees. Another issue related to security is identity management. Identity refers to a unique name of an individual person. Since the legal names of persons are not necessarily unique the identity of a person must include sufficient additional information to make the complete name unique. Authorizing this identity and managing the schema of these identities is called Identity Management. Identity Management addresses how organizations can effectively authenticate people and manage their access to confidential information. It has become more visible as a business requirement across all industries affecting organizations of all sizes. Identity Management administrators constantly need to satisfy application owners while keeping expenditures under control and increasing IT efficiency. Another important consideration in RBAC systems is the possible temporal constraints that may exist on roles such as the time and duration of role activations and timed triggering of a role by an activation of another role. Using an RBAC model is a highly desirable goal for addressing the key security requirements of Web based applications. Roles can be assigned to workflow tasks so that a user with any of the roles related to a task may be authorized to execute it and may play a certain role only for a certain duration. RBAC models have several desirable features such as flexibility policy neutrality better support for security management and administration and other aspects that make them attractive candidates for developing secure Web based applications. These features are lacking in DAC and MAC models. In addition RBAC models include the capabilities available in traditional DAC and MAC policies. Furthermore an RBAC model provides mechanisms for addressing the security issues related to the execution of tasks and workflows and for specifying userdefined and organization specific policies. Easier deployment over the Internet has been another reason for the success of RBAC models. Label Based Security and Row Level Access Control Many commercial DBMSs currently use the concept of row level access control where sophisticated access control rules can be implemented by considering the data row by row. In row level access control each data row is given a label which is used to store information about data sensitivity. Row level access control provides finer granularity of data security by allowing the permissions to be set for each row and not just for the table or column. Initially the user is given a default session label by the database administrator. Levels correspond to a hierarchy of data sensitivity Mandatory Access Control and Role Based Access Control for Multilevel Security levels to exposure or corruption with the goal of maintaining privacy or security. Labels are used to prevent unauthorized users from viewing or altering certain data. A user having a low authorization level usually represented by a low number is denied access to data having a higher level number. If no such label is given to a row a row label is automatically assigned to it depending upon the user’s session label. A policy defined by an administrator is called a Label Security policy. Whenever data affected by the policy is accessed or queried through an application the policy is automatically invoked. When a policy is implemented a new column is added to each row in the schema. The added column contains the label for each row that reflects the sensitivity of the row as per the policy. Similar to MAC where each user has a security clearance each user has an identity in label based security. This user’s identity is compared to the label assigned to each row to determine whether the user has access to view the contents of that row. However the user can write the label value himself within certain restrictions and guidelines for that specific row. This label can be set to a value that is between the user’s current session label and the user’s minimum level. The DBA has the privilege to set an initial default row label. The Label Security requirements are applied on top of the DAC requirements for each user. Hence the user must satisfy the DAC requirements and then the label security requirements to access a row. The DAC requirements make sure that the user is legally authorized to carry on that operation on the schema. In most applications only some of the tables need label based security. For the majority of the application tables the protection provided by DAC is sufficient. Security policies are generally created by managers and human resources personnel. The policies are high level technology neutral and relate to risks. Policies are a result of management instructions to specify organizational procedures guiding principles and courses of action that are considered to be expedient prudent or advantageous. Policies are typically accompanied by a definition of penalties and countermeasures if the policy is transgressed. These policies are then interpreted and converted to a set of label oriented policies by the Label Security administrator who defines the security labels for data and authorizations for users these labels and authorizations govern access to specified protected objects. Suppose a user has SELECT privileges on a table. When the user executes a SELECT statement on that table Label Security will automatically evaluate each row returned by the query to determine whether the user has rights to view the data. For example if the user has a sensitivity of then the user can view all rows having a security level of or lower. The level determines the sensitivity of the information contained in a row the more sensitive the row the higher its security label value. Such Label Security can be configured to perform security checks on UPDATE DELETE and INSERT statements as well. XML Access Control With the worldwide use of XML in commercial and scientific applications efforts are under way to develop security standards. Among these efforts are digital Chapter Database Security signatures and encryption standards for XML. The XML Signature Syntax and Processing specification describes an XML syntax for representing the associations between cryptographic signatures and XML documents or other electronic resources. The specification also includes procedures for computing and verifying XML signatures. An XML digital signature differs from other protocols for message signing such as PGP in its support for signing only specific portions of the XML tree and virtual private networks XML encryption also applies to parts of documents and to documents in persistent storage. Access Control Policies for E Commerce and the Web Electronic commerce environments are characterized by any transactions that are done electronically. They require elaborate access control policies that go beyond traditional DBMSs. In conventional database environments access control is usually performed using a set of authorizations stated by security officers or users according to some security policies. Such a simple paradigm is not well suited for a dynamic environment like e commerce. Furthermore in an e commerce environment the resources to be protected are not only traditional data but also knowledge and experience. Such peculiarities call for more flexibility in specifying access control policies. The access control mechanism must be flexible enough to support a wide spectrum of heterogeneous protection objects. A second related requirement is the support for content based access control. Content based access control allows one to express access control policies that take the protection object content into account. In order to support content based access control access control policies must allow inclusion of conditions based on the object content. A third requirement is related to the heterogeneity of subjects which requires access control policies based on user characteristics and qualifications rather than on specific and individual characteristics . A possible solution to better take into account user profiles in the formulation of access control policies is to support the notion of credentials. A credential is a set of properties concerning a user that are relevant for security purposes . For instance by using credentials one can simply formulate policies such as Only permanent staff with five or more years of service can access documents related to the internals of the system. It is believed that the XML is expected to play a key role in access control for e commerce because XML is becoming the common representation language for document interchange over the Web and is also becoming the language for e commerce. Thus on the one hand there is the need to make XML representations secure by providing access control mechanisms specifically tailored to the protection of XML documents. On the other hand access control information can be expressed using XML itself. The Directory Services Markup Language is a representation of directory service information in XML syntax. It provides a foundation for a standard for communicating with the directory services that will be responsible for providing and authenticating user credentials. The uniform presentation of both protection objects and access control policies can be applied to policies and credentials themselves. For instance some credential properties may be accessible to everyone whereas other properties may be visible only to a restricted class of users. Additionally the use of an XML based language for specifying credentials and access control policies facilitates secure credential submission and export of access control policies. SQL Injection SQL Injection is one of the most common threats to a database system. We will discuss it in detail later in this section. Some of the other attacks on databases that are quite frequent are Unauthorized privilege escalation. This attack is characterized by an individual attempting to elevate his or her privilege by attacking vulnerable points in the database systems. Privilege abuse. While the previous attack is done by an unauthorized user this attack is performed by a privileged user. For example an administrator who is allowed to change student information can use this privilege to update student grades without the instructor’s permission. Denial of service. A Denial of Service attack is an attempt to make resources unavailable to its intended users. It is a general attack category in which access to network applications or data is denied to intended users by overflowing the buffer or consuming resources. Weak Authentication. If the user authentication scheme is weak an attacker can impersonate the identity of a legitimate user by obtaining their login credentials. Thuraisingham et al. Chapter Database Security SQL Injection Methods As we discussed in Chapter Web programs and applications that access a database can send commands and data to the database as well as display data retrieved from the database through the Web browser. In an SQL Injection attack the attacker injects a string input through the application which changes or manipulates the SQL statement to the attacker’s advantage. An SQL Injection attack can harm the database in various ways such as unauthorized manipulation of the database or retrieval of sensitive data. It can also be used to execute system level commands that may cause the system to deny service to the application. This section describes types of injection attacks. SQL Manipulation. A manipulation attack which is the most common type of injection attack changes an SQL command in the application for example by adding conditions to the WHERE clause of a query or by expanding a query with additional query components using set operations such as UNION INTERSECT or MINUS. Other types of manipulation attacks are also possible. A typical manipulation attack occurs during database login. For example suppose that a simplistic authentication procedure issues the following query and checks to see if any rows were returned SELECT FROM users WHERE username ‘jake’ and PASSWORD ‘jakespasswd’. The attacker can try to change the SQL statement by changing it as follows SELECT FROM users WHERE username ‘jake’ and As a result the attacker who knows that ‘jake’ is a valid login of some user is able to log into the database system as ‘jake’ without knowing his password and is able to do everything that ‘jake’ may be authorized to do to the database system. Code Injection. This type of attack attempts to add additional SQL statements or commands to the existing SQL statement by exploiting a computer bug which is caused by processing invalid data. The attacker can inject or introduce code into a computer program to change the course of execution. Code injection is a popular technique for system hacking or cracking to gain information. Function Call Injection. In this kind of attack a database function or operating system function call is inserted into a vulnerable SQL statement to manipulate the data or make a privileged system call. For example it is possible to exploit a function that performs some aspect related to network communication. In addition functions that are contained in a customized database package or any custom database function can be executed as part of an SQL query. In particular dynamically created SQL queries FROM dual Here TRANSLATE is used to replace a string of characters with another string of characters. The TRANSLATE function above will replace the characters of the ‘fromstring’ with the characters in the ‘tostring’ one by one. This means that the f will be replaced with the t the r with the o the o with the  and so on. This type of SQL statement can be subjected to a function injection attack. Consider the following example SELECT TRANSLATE where || is the concatenate operator thus requesting a page from a Web server. UTLHTTP makes Hypertext Transfer Protocol callouts from SQL. The REQUEST object takes a URL in this example as a parameter contacts that site and returns the data obtained from that site. The attacker could manipulate the string he inputs as well as the URL to include other functions and do other illegal operations. We just used a dummy example to show conversion of to but the user’s intent would be to access the URL and get sensitive information. The attacker can then retrieve useful information from the database server located at the URL that is passed as a parameter and send it to the Web server . Risks Associated with SQL Injection SQL injection is harmful and the risks associated with it provide motivation for attackers. Some of the risks associated with SQL injection attacks are explained below. Database Fingerprinting. The attacker can determine the type of database being used in the backend so that he can use database specific attacks that correspond to weaknesses in a particular DBMS. Denial of Service. The attacker can flood the server with requests thus denying service to valid users or they can delete some data. Bypassing Authentication. This is one of the most common risks in which the attacker can gain access to the database as an authorized user and perform all the desired tasks. Chapter Database Security Identifying Injectable Parameters. In this type of attack the attacker gathers important information about the type and structure of the back end database of a Web application. This attack is made possible by the fact that the default error page returned by application servers is often overly descriptive. Executing Remote Commands. This provides attackers with a tool to execute arbitrary commands on the database. For example a remote user can execute stored database procedures and functions from a remote SQL interactive interface. Performing Privilege Escalation. This type of attack takes advantage of logical flaws within the database to upgrade the access level. Protection Techniques against SQL Injection Protection against SQL injection attacks can be achieved by applying certain programming rules to all Web accessible procedures and functions. This section describes some of these techniques. Bind Variables . The use of bind variables employeeid password Instead of embedding the user input into the statement the input should be bound to a parameter. In this example the input is assigned to a bind variable ‘employeeid’ and input to the bind variable ‘password’ instead of directly passing string parameters. Filtering Input . This technique can be used to remove escape characters from input strings by using the SQL Replace function. For example the delimiter single quote can be replaced by two single quotes . Some SQL Manipulation attacks can be prevented by using this technique since escape characters can be used to inject manipulation attacks. However because there can be a large number of escape characters this technique is not reliable. Function Security. Database functions both standard and custom should be restricted as they can be exploited in the SQL function injection attacks. Introduction to Statistical Database Security Introduction to Statistical Database Security Statistical databases are used mainly to produce statistics about various populations. The database may contain confidential data about individuals which should be protected from user access. However users are permitted to retrieve statistical information about the populations such as averages sums counts maximums minimums and standard deviations. The techniques that have been developed to protect the privacy of individual information are beyond the scope of this book. We will illustrate the problem with a very simple example which refers to the relation shown in Figure This is a PERSON relation with the attributes Name Ssn Income Address City State Zip Sex and Lastdegree. A population is a set of tuples of a relation that satisfy some selection condition. Hence each selection condition on the PERSON relation will specify a particular population of PERSON tuples. For example the condition Sex ‘M’ specifies the male population the condition AND specifies the female population that has an . or . degree as their highest degree and the condition City ‘Houston’ specifies the population that lives in Houston. Statistical queries involve applying statistical functions to a population of tuples. For example we may want to retrieve the number of individuals in a population or the average income in the population. However statistical users are not allowed to retrieve individual data such as the income of a specific person. Statistical database security techniques must prohibit the retrieval of individual data. This can be achieved by prohibiting queries that retrieve attribute values and by allowing only queries that involve statistical aggregate functions such as COUNT SUM MIN MAX AVERAGE and STANDARD DEVIATION. Such queries are sometimes called statistical queries. It is the responsibility of a database management system to ensure the confidentiality of information about individuals while still providing useful statistical summaries of data about those individuals to users. Provision of privacy protection of users in a statistical database is paramount its violation is illustrated in the following example. In some cases it is possible to infer the values of individual tuples from a sequence of statistical queries. This is particularly true when the conditions result in a Name Ssn Income Address C ty State Z p Sex Lastdegree PERSON Figure The PERSON relation schema for illustrating statistical database security. Chapter Database Security population consisting of a small number of tuples. As an illustration consider the following statistical queries SELECT COUNT FROM PERSON WHERE condition SELECT AVG FROM PERSON WHERE condition Now suppose that we are interested in finding the Salary of Jane Smith and we know that she has a . degree and that she lives in the city of Bellaire Texas. We issue the statistical query with the following condition If we get a result of for this query we can issue with the same condition and find the Salary of Jane Smith. Even if the result of on the preceding condition is not but is a small number say or can issue statistical queries using the functions MAX MIN and AVERAGE to identify the possible range of values for the Salary of Jane Smith. The possibility of inferring individual information from statistical queries is reduced if no statistical queries are permitted whenever the number of tuples in the population specified by the selection condition falls below some threshold. Another technique for prohibiting retrieval of individual information is to prohibit sequences of queries that refer repeatedly to the same population of tuples. It is also possible to introduce slight inaccuracies or noise into the results of statistical queries deliberately to make it difficult to deduce individual information from the results. Another technique is partitioning of the database. Partitioning implies that records are stored in groups of some minimum size queries can refer to any complete group or set of groups but never to subsets of records within a group. The interested reader is referred to the bibliography at the end of this chapter for a discussion of these techniques. Introduction to Flow Control Flow control regulates the distribution or flow of information among accessible objects. A flow between object X and object Y occurs when a program reads values from X and writes values into Y. Flow controls check that information contained in some objects does not flow explicitly or implicitly into less protected objects. Thus a user cannot get indirectly in Y what he or she cannot get directly in X. Active flow control began in the early Most flow controls employ some concept of security class the transfer of information from a sender to a receiver is allowed only if the receiver’s security class is at least as privileged as the sender’s. Examples of a flow control include preventing a service program from leaking a customer’s confidential data and blocking the transmission of secret military data to an unknown classified user. A flow policy specifies the channels along which information is allowed to move. The simplest flow policy specifies just two classes of information confidential Introduction to Flow Control and nonconfidential and allows all flows except those from class C to class N. This policy can solve the confinement problem that arises when a service program handles data such as customer information some of which may be confidential. For example an income tax computing service might be allowed to retain a customer’s address and the bill for services rendered but not a customer’s income or deductions. Access control mechanisms are responsible for checking users’ authorizations for resource access Only granted operations are executed. Flow controls can be enforced by an extended access control mechanism which involves assigning a security class to each running program. The program is allowed to read a particular memory segment only if its security class is as high as that of the segment. It is allowed to write in a segment only if its class is as low as that of the segment. This automatically ensures that no information transmitted by the person can move from a higher to a lower class. For example a military program with a secret clearance can only read from objects that are unclassified and confidential and can only write into objects that are secret or top secret. Two types of flow can be distinguished explicit flows occurring as a consequence of assignment instructions such as Y and implicit flows generated by conditional instructions such as if Xn then Y f Flow control mechanisms must verify that only authorized flows both explicit and implicit are executed. A set of rules must be satisfied to ensure secure information flows. Rules can be expressed using flow relations among classes and assigned to information stating the authorized flows within a system. These relations can define for a class the set of classes where information can flow or can state the specific relations to be verified between two classes to allow information to flow from one to the other. In general flow control mechanisms implement the controls by assigning a label to each object and by specifying the security class of the object. Labels are then used to verify the flow relations defined in the model. Covert Channels A covert channel allows a transfer of information that violates the security or the policy. Specifically a covert channel allows information to pass from a higher classification level to a lower classification level through improper means. Covert channels can be classified into two broad categories timing channels and storage. The distinguishing feature between the two is that in a timing channel the information is conveyed by the timing of events or processes whereas storage channels do not require any temporal synchronization in that information is conveyed by accessing system information or what is otherwise inaccessible to the user. In a simple example of a covert channel consider a distributed database system in which two nodes have user security levels of secret and unclassified . In order Chapter Database Security for a transaction to commit both nodes must agree to commit. They mutually can only do operations that are consistent with the property which states that in any transaction the S site cannot write or pass information to the U site. However if these two sites collude to set up a covert channel between them a transaction involving secret data may be committed unconditionally by the U site but the S site may do so in some predefined agreed upon way so that certain information may be passed from the S site to the U site violating the property. This may be achieved where the transaction runs repeatedly but the actions taken by the S site implicitly convey information to the U site. Measures such as locking which we discussed in Chapters and prevent concurrent writing of the information by users with different security levels into the same objects preventing the storage type covert channels. Operating systems and distributed databases provide control over the multiprogramming of operations that allows a sharing of resources without the possibility of encroachment of one program or process into another’s memory or other resources in the system thus preventing timing oriented covert channels. In general covert channels are not a major problem in well implemented robust database implementations. However certain schemes may be contrived by clever users that implicitly transfer information. Some security experts believe that one way to avoid covert channels is to disallow programmers to actually gain access to sensitive data that a program will process after the program has been put into operation. For example a programmer for a bank has no need to access the names or balances in depositors’ accounts. Programmers for brokerage firms do not need to know what buy and sell orders exist for clients. During program testing access to a form of real data or some sample test data may be justifiable but not after the program has been accepted for regular use. Encryption and Public Key Infrastructures The previous methods of access and flow control despite being strong control measures may not be able to protect databases from some threats. Suppose we communicate data but our data falls into the hands of a nonlegitimate user. In this situation by using encryption we can disguise the message so that even if the transmission is diverted the message will not be revealed. Encryption is the conversion of data into a form called a ciphertext which cannot be easily understood by unauthorized persons. It enhances security and privacy when access controls are bypassed because in cases of data loss or theft encrypted data cannot be easily understood by unauthorized persons. With this background we adhere to following standard Ciphertext Encrypted data. definitions are from NIST from http Encryption and Public Key Infrastructures Plaintext Intelligible data that has meaning and can be read or acted upon without the application of decryption. Encryption The process of transforming plaintext into ciphertext. Decryption The process of transforming ciphertext back into plaintext. Encryption consists of applying an encryption algorithm to data using some prespecified encryption key. The resulting data has to be decrypted using a decryption key to recover the original data. The Data Encryption and Advanced Encryption Standards The Data Encryption Standard is a system developed by the . government for use by the general public. It has been widely accepted as a cryptographic standard both in the United States and abroad. DES can provide end to end encryption on the channel between sender A and receiver B. The DES algorithm is a careful and complex combination of two of the fundamental building blocks of encryption substitution and permutation . The algorithm derives its strength from repeated application of these two techniques for a total of cycles. Plaintext is encrypted as blocks of bits. Although the key is bits long in effect the key can be any number. After questioning the adequacy of DES the NIST introduced the Advanced Encryption Standard . This algorithm has a block size of bits compared with DES’s size and can use keys of or bits compared with DES’s key. AES introduces more possible keys compared with DES and thus takes a much longer time to crack. Symmetric Key Algorithms A symmetric key is one key that is used for both encryption and decryption. By using a symmetric key fast encryption and decryption is possible for routine use with sensitive data in the database. A message encrypted with a secret key can be decrypted only with the same secret key. Algorithms used for symmetric key encryption are called secret key algorithms. Since secret key algorithms are mostly used for encrypting the content of a message they are also called contentencryption algorithms. The major liability associated with secret key algorithms is the need for sharing the secret key. A possible method is to derive the secret key from a user supplied password string by applying the same function to the string at both the sender and receiver this is known as a password based encryption algorithm. The strength of the symmetric key encryption depends on the size of the key used. For the same algorithm encrypting using a longer key is tougher to break than the one using a shorter key. Public Key Encryption In Diffie and Hellman proposed a new kind of cryptosystem which they called public key encryption. Public key algorithms are based on mathematical Chapter Database Security functions rather than operations on bit patterns. They address one drawback of symmetric key encryption namely that both sender and recipient must exchange the common key in a secure manner. In public key systems two keys are used for encryption decryption. The public key can be transmitted in a non secure way whereas the private key is not transmitted at all. These algorithms which use two related keys a public key and a private key to perform complementary operations are known as asymmetric key encryption algorithms. The use of two keys can have profound consequences in the areas of confidentiality key distribution and authentication. The two keys used for public key encryption are referred to as the public key and the private key. The private key is kept secret but it is referred to as a private key rather than a secret key to avoid confusion with conventional encryption. The two keys are mathematically related since one of the keys is used to perform encryption and the other to perform decryption. However it is very difficult to derive the private key from the public key. A public key encryption scheme or infrastructure has six ingredients Plaintext. This is the data or readable message that is fed into the algorithm as input. Encryption algorithm. This algorithm performs various transformations on the plaintext. and Public and private keys. These are a pair of keys that have been selected so that if one is used for encryption the other is used for decryption. The exact transformations performed by the encryption algorithm depend on the public or private key that is provided as input. For example if a message is encrypted using the public key it can only be decrypted using the private key. Ciphertext. This is the scrambled message produced as output. It depends on the plaintext and the key. For a given message two different keys will produce two different ciphertexts. Decryption algorithm. This algorithm accepts the ciphertext and the matching key and produces the original plaintext. As the name suggests the public key of the pair is made public for others to use whereas the private key is known only to its owner. A general purpose public key cryptographic algorithm relies on one key for encryption and a different but related key for decryption. The essential steps are as follows Each user generates a pair of keys to be used for the encryption and decryption of messages. Each user places one of the two keys in a public register or other accessible file. This is the public key. The companion key is kept private. If a sender wishes to send a private message to a receiver the sender encrypts the message using the receiver’s public key. Encryption and Public Key Infrastructures When the receiver receives the message he or she decrypts it using the receiver’s private key. No other recipient can decrypt the message because only the receiver knows his or her private key. The RSA Public Key Encryption Algorithm. One of the first public key schemes was introduced in by Ron Rivest Adi Shamir and Len Adleman at MIT and is named after them as the RSA scheme. The RSA scheme has since then reigned supreme as the most widely accepted and implemented approach to public key encryption. The RSA encryption algorithm incorporates results from number theory combined with the difficulty of determining the prime factors of a target. The RSA algorithm also operates with modular arithmetic mod n. Two keys d and e are used for decryption and encryption. An important property is that they can be interchanged. n is chosen as a large integer that is a product of two large distinct prime numbers a and b n a × b. The encryption key e is a randomly chosen number between and n that is relatively prime to d mod n P. The decryption key d can be computed from the condition that d × e mod d mod n P and recovers P without having to factor Pe . Digital Signatures A digital signature is an example of using encryption techniques to provide authentication services in electronic commerce applications. Like a handwritten signature a digital signature is a means of associating a mark unique to an individual with a body of text. The mark should be unforgettable meaning that others should be able to check that the signature comes from the originator. A digital signature consists of a string of symbols. If a person’s digital signature were always the same for each message then one could easily counterfeit it by simply copying the string of symbols. Thus signatures must be different for each use. This can be achieved by making each digital signature a function of the message that it is signing together with a timestamp. To be unique to each signer and counterfeitproof each digital signature must also depend on some secret number that is unique to the signer. Thus in general a counterfeitproof digital signature must depend on the message and a unique secret number of the signer. The verifier of the signature however should not need to know any secret number. Public key techniques are the best means of creating digital signatures with these properties. Digital Certificates A digital certificate is used to combine the value of a public key with the identity of the person or service that holds the corresponding private key into a digitally signed Chapter Database Security statement. Certificates are issued and signed by a certification authority . The entity receiving this certificate from a CA is the subject of that certificate. Instead of requiring each participant in an application to authenticate every user third party authentication relies on the use of digital certificates. The digital certificate itself contains various types of information. For example both the certification authority and the certificate owner information are included. The following list describes all the information included in the certificate The certificate owner information which is represented by a unique identifier known as the distinguished name of the owner. This includes the owner’s name as well as the owner’s organization and other information about the owner. The certificate also includes the public key of the owner. The date of issue of the certificate is also included. The validity period is specified by ‘Valid From’ and ‘Valid To’ dates which are included in each certificate. Issuer identifier information is included in the certificate. Finally the digital signature of the issuing CA for the certificate is included. All the information listed is encoded through a message digest function which creates the digital signature. The digital signature basically certifies that the association between the certificate owner and public key is valid. Privacy Issues and Preservation Preserving data privacy is a growing challenge for database security and privacy experts. In some perspectives to preserve data privacy we should even limit performing large scale data mining and analysis. The most commonly used techniques to address this concern are to avoid building mammoth central warehouses as a single repository of vital information. Another possible measure is to intentionally modify or perturb data. If all data were available at a single warehouse violating only a single repository’s security could expose all data. Avoiding central warehouses and using distributed data mining algorithms minimizes the exchange of data needed to develop globally valid models. By modifying perturbing and anonymizing data we can also mitigate privacy risks associated with data mining. This can be done by removing identity information from the released data and injecting noise into the data. However by using these techniques we should pay attention to the quality of the resulting data in the database which may undergo too many modifications. We must be able to estimate the errors that may be introduced by these modifications. Privacy is an important area of ongoing research in database management. It is complicated due to its multidisciplinary nature and the issues related to the subjectivity in the interpretation of privacy trust and so on. As an example consider medical and legal records and transactions which must maintain certain privacy Challenges of Database Security requirements while they are being defined and enforced. Providing access control and privacy for mobile devices is also receiving increased attention. DBMSs need robust techniques for efficient storage of security relevant information on small devices as well as trust negotiation techniques. Where to keep information related to user identities profiles credentials and permissions and how to use it for reliable user identification remains an important problem. Because large sized streams of data are generated in such environments efficient techniques for access control must be devised and integrated with processing techniques for continuous queries. Finally the privacy of user location data acquired from sensors and communication networks must be ensured. Challenges of Database Security Considering the vast growth in volume and speed of threats to databases and information assets research efforts need to be devoted to the following issues data quality intellectual property rights and database survivability. These are only some of the main challenges that researchers in database security are trying to address. Data Quality The database community needs techniques and organizational solutions to assess and attest the quality of data. These techniques may include simple mechanisms such as quality stamps that are posted on Web sites. We also need techniques that provide more effective integrity semantics verification and tools for the assessment of data quality based on techniques such as record linkage. Application level recovery techniques are also needed for automatically repairing incorrect data. The ETL tools widely used to load data in data warehouses Technology Virtual Private Databases is a feature of the Oracle Enterprise Edition that adds predicates to user statements to limit their access in a transparent manner to the user and the application. The VPD concept allows server enforced fine grained access control for a secure application. VPD provides access control based on policies. These VPD policies enforce objectlevel access control or row level security. It provides an application programming Oracle Label Based Security interface that allows security policies to be attached to database tables or views. Using PL SQL a host programming language used in Oracle applications developers and security administrators can implement security policies with the help of stored VPD policies allow developers to remove access security mechanisms from applications and centralize them within the Oracle Database. VPD is enabled by associating a security “policy” with a table view or synonym. An administrator uses the supplied PL SQL package DBMSRLS to bind a policy function with a database object. When an object having a security policy associated with it is accessed the function implementing this policy is consulted. The policy function returns a predicate which is then appended to the user’s SQL statement thus transparently and dynamically modifying the user’s data access. Oracle Label Security is a technique of enforcing row level security in the form of a security policy. Label Security Architecture Oracle Label Security is built on the VPD technology delivered in the Oracle Database Enterprise Edition. Figure illustrates how data is accessed under Oracle Label Security showing the sequence of DAC and label security checks. Figure shows the sequence of discretionary access control and label security checks. The left part of the figure shows an application user in an Oracle Database Release session sending out an SQL request. The Oracle DBMS checks the DAC privileges of the user making sure that he or she has SELECT privileges on the table. Then it checks whether the table has a Virtual Private Database policy associated with it to determine if the table is protected using Oracle Label Security. If it is the VPD SQL modification is added to the original SQL statement to find the set of accessible rows for the user to view. Then Oracle Label Security checks the labels on each row to determine the subset of rows to which the user has access . This modified query gets processed optimized and executed. How Data Labels and User Labels Work Together A user’s label indicates the information the user is permitted to access. It also determines the type of access that the user has on that information. A row’s label shows the sensitivity of the information that the row contains as well as the ownership of the information. When a table in the database has a label based access associated with it a row can be accessed only if the user’s label meet certain criteria defined in the policy definitions. Access is granted or denied based on the result of comparing the data label and the session label of the user. Compartments allow a finer classification of sensitivity of the labeled data. All data related to the same project can be labeled with the same compartment. Compartments are optional a label can contain zero or more compartments. procedures are discussed in Section Chapter Database Security Oracle User Request for Data in SQL Check DAC Access Control Check Virtual Private Database Policy Process and Execute Data Request Enforce LabelBased Security Oracle Data Server Table Level Privileges Table Data Rows in Table Label Security Policies Row Level Access Control VPD Based Control User Defined VPD Policies Figure Oracle Label Security architecture. Source Oracle Groups are used to identify organizations as owners of the data with corresponding group labels. Groups are hierarchical for example a group can be associated with a parent group. If a user has a maximum level of SENSITIVE then the user potentially has access to all data having levels SENSITIVE CONFIDENTIAL and UNCLASSIFIED. This user has no access to HIGHLYSENSITIVE data. Figure shows how data labels and user labels work together to provide access control in Oracle Label Security. As shown in Figure User can access the rows and because his maximum level is HS . He has access to the FIN compartment and his access to group WR hierarchically includes group WRSAL . He cannot access row because he does not have the CHEM compartment. It is important that a user has authorization for all compartments in a row’s data label to be able to access that row. Based on this example user can access both rows and and has a maximum level of S which is less than the HS in row So although user has access to the FIN compartment he can only access the group WRSAL and thus cannot acces row Summary In this chapter we discussed several techniques for enforcing database system security. We presented different threats to databases in terms of loss of integrity availability and confidentiality. We discussed the types of control measures to deal with these problems access control inference control flow control and encryption. In Summary User Labels HS FIN WR S FIN WRSAL Legend for Labels HS Highly sensitive S Sensitive C Confidential U Unclassified Maximum Access Level All compartments to which the user has access Minimum Access Level Required All compartments to which the user must have access User Label Data Label Rows in Table Data Labels Row Row Row Row S CHEM FIN WR HS FIN WRSAL U FIN C FIN WRSAL Figure Data labels and user labels in Oracle. Source Oracle the introduction we covered various issues related to security including data sensitivity and type of disclosures providing security vs. precision in the result when a user requests information and the relationship between information security and privacy. Security enforcement deals with controlling access to the database system as a whole and controlling authorization to access specific portions of a database. The former is usually done by assigning accounts with passwords to users. The latter can be accomplished by using a system of granting and revoking privileges to individual accounts for accessing specific parts of the database. This approach is generally referred to as discretionary access control . We presented some SQL commands for granting and revoking privileges and we illustrated their use with examples. Then we gave an overview of mandatory access control mechanisms that enforce multilevel security. These require the classifications of users and data values into security classes and enforce the rules that prohibit flow of information from higher to lower security levels. Some of the key concepts underlying the multilevel relational model including filtering and polyinstantiation were presented. Role based access control was introduced which assigns privileges based on roles that users play. We introduced the notion of role hierarchies mutual exclusion of roles and row and label based security. We briefly discussed the problem of controlling access to statistical databases to protect the privacy of individual information while concurrently providing statistical access to populations of records. We explained the main ideas behind the threat of SQL Injection the methods in which it can be induced and the various types of risks associated with it. Then we gave an Chapter Database Security idea of the various ways SQL injection can be prevented. The issues related to flow control and the problems associated with covert channels were discussed next as well as encryption and public private key based infrastructures. The idea of symmetric key algorithms and the use of the popular asymmetric key based public key infrastructure scheme was explained. We also covered the concepts of digital signatures and digital certificates. We highlighted the importance of privacy issues and hinted at some privacy preservation techniques. We discussed a variety of challenges to security including data quality intellectual property rights and data survivability. We ended the chapter by introducing the implementation of security policies by using a combination of label based security and virtual private databases in Oracle Review Questions Discuss what is meant by each of the following terms database authorization access control data encryption privileged account database audit audit trail. Which account is designated as the owner of a relation What privileges does the owner of a relation have How is the view mechanism used as an authorization mechanism Discuss the types of privileges at the account level and those at the relation level. What is meant by granting a privilege What is meant by revoking a privilege Discuss the system of propagation of privileges and the restraints imposed by horizontal and vertical propagation limits. List the types of privileges available in SQL. What is the difference between discretionary and mandatory access control What are the typical security classifications Discuss the simple security property and the property and explain the justification behind these rules for enforcing multilevel security. Describe the multilevel relational data model. Define the following terms apparent key polyinstantiation filtering. What are the relative merits of using DAC or MAC What is role based access control In what ways is it superior to DAC and MAC What are the two types of mutual exclusion in role based access control What is meant by row level access control What is label security How does an administrator enforce it Exercises What are the different types of SQL injection attacks What risks are associated with SQL injection attacks What preventive measures are possible against SQL injection attacks What is a statistical database Discuss the problem of statistical database security. How is privacy related to statistical database security What measures can be taken to ensure some degree of privacy in statistical databases What is flow control as a security measure What types of flow control exist What are covert channels Give an example of a covert channel. What is the goal of encryption What process is involved in encrypting data and then recovering it at the other end Give an example of an encryption algorithm and explain how it works. Repeat the previous question for the popular RSA algorithm. What is a symmetric key algorithm for key based security What is the public key infrastructure scheme How does it provide security What are digital signatures How do they work What type of information does a digital certificate include Exercises How can privacy of data be preserved in a database What are some of the current outstanding challenges for database security Consider the relational database schema in Figure Suppose that all the relations were created by user X who wants to grant the following privileges to user accounts A B C D and E a. Account A can retrieve or modify any relation except DEPENDENT and can grant any of these privileges to other users. b. Account B can retrieve all the attributes of EMPLOYEE and DEPARTMENT except for Salary Mgrssn and Mgrstartdate. c. Account C can retrieve or modify WORKSON but can only retrieve the Fname Minit Lname and Ssn attributes of EMPLOYEE and the Pname and Pnumber attributes of PROJECT. d. Account D can retrieve any attribute of EMPLOYEE or DEPENDENT and can modify DEPENDENT. e. Account E can retrieve any attribute of EMPLOYEE but only for EMPLOYEE tuples that have Dno f. Write SQL statements to grant these privileges. Use views where appropriate. Chapter Database Security Suppose that privilege of Exercise is to be given with GRANT OPTION but only so that account A can grant it to at most five accounts and each of these accounts can propagate the privilege to other accounts but without the GRANT OPTION privilege. What would the horizontal and vertical propagation limits be in this case Consider the relation shown in Figure How would it appear to a user with classification U Suppose that a classification U user tries to update the salary of ‘Smith’ to what would be the result of this action Selected Bibliography Authorization based on granting and revoking privileges was proposed for the SYSTEM R experimental DBMS and is presented in Griffiths and Wade Several books discuss security in databases and computer systems in general including the books by Leiss and Fernandez et al. and Fugini et al. Natan is a practical book on security and auditing implementation issues in all major RDBMSs. Many papers discuss different techniques for the design and protection of statistical databases. They include McLeish Chin and Ozsoyoglu Leiss Wong and Denning Ghosh discusses the use of statistical databases for quality control. There are also many papers discussing cryptography and data encryption including Diffie and Hellman Rivest et al. Akl Pfleeger and Pfleeger Omura et al. Stallings and Iyer at al. Halfond et al. helps understand the concepts of SQL injection attacks and the various threats imposed by them. The white paper Oracle explains how Oracle is less prone to SQL injection attack as compared to SQL Server. It also gives a brief explanation as to how these attacks can be prevented from occurring. Further proposed frameworks are discussed in Boyd and Keromytis Halfond and Orso and McClure and Krüger Multilevel security is discussed in Jajodia and Sandhu Denning et al. Smith and Winslett Stachour and Thuraisingham Lunt et al. and Bertino et al. Overviews of research issues in database security are given by Lunt and Fernandez Jajodia and Sandhu Bertino Castano et al. and Thuraisingham et al. The effects of multilevel security on concurrency control are discussed in Atluri et al. Security in next generation semantic and object oriented databases is discussed in Rabbiti et al. Jajodia and Kogan and Smith Oh presents a model for both discretionary and mandatory security. Security models for Web based applications and role based access control are discussed in Joshi et al. Security issues for managers in the context of e commerce applications and the need for risk assessment models for selection of appropriate security control measures are discussed in Selected Bibliography Farahmand et al. Row level access control is explained in detail in Oracle and Sybase The latter also provides details on role hierarchy and mutual exclusion. Oracle explains how Oracle uses the concept of identity management. Recent advances as well as future challenges for security and privacy of databases are discussed in Bertino and Sandhu . Govt. OECD and NRC are good references on the view of privacy by important government bodies. Karat et al. discusses a policy framework for security and privacy. XML and access control are discussed in Naedele More details can be found on privacy preserving techniques in Vaidya and Clifton intellectual property rights in Sion et al. and database survivability in Jajodia et al. Oracle’s VPD technology and label based security is discussed in more detail in Oracle This page intentionally left blank Distributed Databases I n this chapter we turn our attention to distributed databases distributed database management systems and how the client server architecture is used as a platform for database application development. Distributed databases bring the advantages of distributed computing to the database management domain. A distributed computing system consists of a number of processing elements not necessarily homogeneous that are interconnected by a computer network and that cooperate in performing certain assigned tasks. As a general goal distributed computing systems partition a big unmanageable problem into smaller pieces and solve it efficiently in a coordinated manner. The economic viability of this approach stems from two reasons more computing power is harnessed to solve a complex task and each autonomous processing element can be managed independently to develop its own applications. DDB technology resulted from a merger of two technologies database technology and network and data communication technology. Computer networks allow distributed processing of data. Traditional databases on the other hand focus on providing centralized controlled access to data. Distributed databases allow an integration of information and its processing by applications that may themselves be centralized or distributed. Several distributed database prototype systems were developed in the to address the issues of data distribution distributed query and transaction processing distributed database metadata management and other topics. However a fullscale comprehensive DDBMS that implements the functionality and techniques proposed in DDB research never emerged as a commercially viable product. Most major vendors redirected their efforts from developing a pure DDBMS product into developing systems based on client server concepts or toward developing technologies for accessing distributed heterogeneous data sources. chapter Chapter Distributed Databases Organizations continue to be interested in the decentralization of processing while achieving an integration of the information resources within their geographically distributed systems of databases applications and users. There is now a general endorsement of the client server approach to application development and the three tier approach to Web applications development as a collection of multiple logically interrelated databases distributed over a computer network and a distributed database management system as a software system that manages a distributed database while making the distribution transparent to the Distributed databases are different from Internet Web files. Web pages are basically a very large collection of files stored on different nodes in a network the Internet with interrelationships among the files represented via hyperlinks. The common functions of database management including uniform query processing and transaction processing do not apply to this scenario yet. The technology is however moving in a direction such that distributed World Wide Web databases will become a reality in the future. We have discussed some of the issues of substantial contribution of Narasimhan Srinivasan to this and several other sections in this chapter is appreciated. definition and discussions in this section are based largely on Ozsu and Valduriez Distributed Database Concepts accessing databases on the Web in Chapters and The proliferation of data at millions of Websites in various forms does not qualify as a DDB by the definition given earlier. Differences between DDB and Multiprocessor Systems We need to distinguish distributed databases from multiprocessor systems that use shared storage . For a database to be called distributed the following minimum conditions should be satisfied Connection of database nodes over a computer network. There are multiple computers called sites or nodes. These sites must be connected by an underlying communication network to transmit data and commands among sites as shown later in Figure Logical interrelation of the connected databases. It is essential that the information in the databases be logically related. Absence of homogeneity constraint among connected nodes. It is not necessary that all nodes be identical in terms of data hardware and software. The sites may all be located in physical proximity say within the same building or a group of adjacent buildings and connected via a local area network or they may be geographically distributed over large distances and connected via a long haul or wide area network. Local area networks typically use wireless hubs or cables whereas long haul networks use telephone lines or satellites. It is also possible to use a combination of networks. Networks may have different topologies that define the direct communication paths among sites. The type and topology of the network used may have a significant impact on the performance and hence on the strategies for distributed query processing and distributed database design. For high level architectural issues however it does not matter what type of network is used what matters is that each site be able to communicate directly or indirectly with every other site. For the remainder of this chapter we assume that some type of communication network exists among sites regardless of any particular topology. We will not address any networkspecific issues although it is important to understand that for an efficient operation of a distributed database system network design and performance issues are critical and are an integral part of the overall solution. The details of the underlying communication network are invisible to the end user. Transparency The concept of transparency extends the general idea of hiding implementation details from end users. A highly transparent system offers a lot of flexibility to the end user application developer since it requires little or no awareness of underlying details on their part. In the case of a traditional centralized database transparency simply pertains to logical and physical data independence for application developers. However in a DDB scenario the data and software are distributed over multiple Chapter Distributed Databases sites connected by a computer network so additional types of transparencies are introduced. Consider the company database in Figure that we have been discussing throughout the book. The EMPLOYEE PROJECT and WORKSON tables may be fragmented horizontally . This refers to freedom for the user from the operational details of the network and the placement of the data in the distributed system. It may be divided into location transparency and naming transparency. Location transparency refers to the fact that the command used to perform a task is independent of the location of the data and the location of the node where the command was issued. Naming transparency implies that once a name is associated with an object the named objects can be accessed unambiguously without additional specification as to where the data is located. Replication transparency. As we show in Figure copies of the same data objects may be stored at multiple sites for better availability performance and reliability. Replication transparency makes the user unaware of the existence of these copies. Fragmentation transparency. Two types of fragmentation are possible. Horizontal fragmentation distributes a relation into subrelations EMPLOYEES PROJECTS WORKSON All All All EMPLOYEES PROJECTS WORKSON San Franc sco and Los Angeles San Franc sco San Franc sco employees EMPLOYEES PROJECTS WORKSON Los Angeles Los Angeles and San Franc sco Los Angeles employees EMPLOYEES PROJECTS WORKSON New York All New York employees EMPLOYEES PROJECTS WORKSON Atlanta Atlanta Atlanta employees Ch cago New York Los Angeles Atlanta San Franc sco Commun cat ons Network Figure Data distribution and replication among distributed databases. Distributed Database Concepts that are subsets of the tuples in the original relation. Vertical fragmentation distributes a relation into subrelations where each subrelation is defined by a subset of the columns of the original relation. A global query by the user must be transformed into several fragment queries. Fragmentation transparency makes the user unaware of the existence of fragments. Other transparencies include design transparency and execution transparency referring to freedom from knowing how the distributed database is designed and where a transaction executes. Autonomy Autonomy determines the extent to which individual nodes or DBs in a connected DDB can operate independently. A high degree of autonomy is desirable for increased flexibility and customized maintenance of an individual node. Autonomy can be applied to design communication and execution. Design autonomy refers to independence of data model usage and transaction management techniques among nodes. Communication autonomy determines the extent to which each node can decide on sharing of information with other nodes. Execution autonomy refers to independence of users to act as they please. Reliability and Availability Reliability and availability are two of the most common potential advantages cited for distributed databases. Reliability is broadly defined as the probability that a system is running at a certain time point whereas availability is the probability that the system is continuously available during a time interval. We can directly relate reliability and availability of the database to the faults errors and failures associated with it. A failure can be described as a deviation of a system’s behavior from that which is specified in order to ensure correct execution of operations. Errors constitute that subset of system states that causes the failure. Fault is the cause of an error. To construct a system that is reliable we can adopt several approaches. One common approach stresses fault tolerance it recognizes that faults will occur and designs mechanisms that can detect and remove faults before they can result in a system failure. Another more stringent approach attempts to ensure that the final system does not contain any faults. This is done through an exhaustive design process followed by extensive quality control and testing. A reliable DDBMS tolerates failures of underlying components and processes user requests so long as database consistency is not violated. A DDBMS recovery manager has to deal with failures arising from transactions hardware and communication networks. Hardware failures can either be those that result in loss of main memory contents or loss of secondary storage contents. Communication failures occur due to errors associated with messages and line failures. Message errors can include their loss corruption or out of order arrival at destination. Chapter Distributed Databases Advantages of Distributed Databases Organizations resort to distributed database management for various reasons. Some important advantages are listed below. Improved ease and flexibility of application development. Developing and maintaining applications at geographically distributed sites of an organization is facilitated owing to transparency of data distribution and control. Increased reliability and availability. This is achieved by the isolation of faults to their site of origin without affecting the other databases connected to the network. When the data and DDBMS software are distributed over several sites one site may fail while other sites continue to operate. Only the data and software that exist at the failed site cannot be accessed. This improves both reliability and availability. Further improvement is achieved by judiciously replicating data and software at more than one site. In a centralized system failure at a single site makes the whole system unavailable to all users. In a distributed database some of the data may be unreachable but users may still be able to access other parts of the database. If the data in the failed site had been replicated at another site prior to the failure then the user will not be affected at all. Improved performance. A distributed DBMS fragments the database by keeping the data closer to where it is needed most. Data localization reduces the contention for CPU and I O services and simultaneously reduces access delays involved in wide area networks. When a large database is distributed over multiple sites smaller databases exist at each site. As a result local queries and transactions accessing data at a single site have better performance because of the smaller local databases. In addition each site has a smaller number of transactions executing than if all transactions are submitted to a single centralized database. Moreover interquery and intraquery parallelism can be achieved by executing multiple queries at different sites or by breaking up a query into a number of subqueries that execute in parallel. This contributes to improved performance. Easier expansion. In a distributed environment expansion of the system in terms of adding more data increasing database sizes or adding more processors is much easier. The transparencies we discussed in Section lead to a compromise between ease of use and the overhead cost of providing transparency. Total transparency provides the global user with a view of the entire DDBS as if it is a single centralized system. Transparency is provided as a complement to autonomy which gives the users tighter control over local databases. Transparency features may be implemented as a part of the user language which may translate the required services into appropriate operations. Additionally transparency impacts the features that must be provided by the operating system and the DBMS. Types of Distributed Database Systems Additional Functions of Distributed Databases Distribution leads to increased complexity in the system design and implementation. To achieve the potential advantages listed previously the DDBMS software must be able to provide the following functions in addition to those of a centralized DBMS Keeping track of data distribution. The ability to keep track of the data distribution fragmentation and replication by expanding the DDBMS catalog. Distributed query processing. The ability to access remote sites and transmit queries and data among the various sites via a communication network. Distributed transaction management. The ability to devise execution strategies for queries and transactions that access data from more than one site and to synchronize the access to distributed data and maintain the integrity of the overall database. Replicated data management. The ability to decide which copy of a replicated data item to access and to maintain the consistency of copies of a replicated data item. Distributed database recovery. The ability to recover from individual site crashes and from new types of failures such as the failure of communication links. Security. Distributed transactions must be executed with the proper management of the security of the data and the authorization access privileges of users. Distributed directory management. A directory contains information about data in the database. The directory may be global for the entire DDB or local for each site. The placement and distribution of the directory are design and policy issues. These functions themselves increase the complexity of a DDBMS over a centralized DBMS. Before we can realize the full potential advantages of distribution we must find satisfactory solutions to these design issues and problems. Including all this additional functionality is hard to accomplish and finding optimal solutions is a step beyond that. Types of Distributed Database Systems The term distributed database management system can describe various systems that differ from one another in many respects. The main thing that all such systems have in common is the fact that data and software are distributed over multiple sites connected by some form of communication network. In this section we discuss a number of types of DDBMSs and the criteria and factors that make some of these systems different. Chapter Distributed Databases The first factor we consider is the degree of homogeneity of the DDBMS software. If all servers use identical software and all users use identical software the DDBMS is called homogeneous otherwise it is called heterogeneous. Another factor related to the degree of homogeneity is the degree of local autonomy. If there is no provision for the local site to function as a standalone DBMS then the system has no local autonomy. On the other hand if direct access by local transactions to a server is permitted the system has some degree of local autonomy. Figure shows classification of DDBMS alternatives along orthogonal axes of distribution autonomy and heterogeneity. For a centralized database there is complete autonomy but a total lack of distribution and heterogeneity . We see that the degree of local autonomy provides further ground for classification into federated and multidatabase systems. At one extreme of the autonomy spectrum we have a DDBMS that looks like a centralized DBMS to the user with zero autonomy . A single conceptual schema exists and all access to the system is obtained through a site that is part of the DDBMS which means that no local autonomy exists. Along the autonomy axis we encounter two types of DDBMSs called federated database system and multidatabase system . In such systems each server is an independent and autonomous centralized DBMS that has its own local users local transactions and DBA and hence has B D str but on Heterogene ty Legend A Trad onal central zed database systems B Pure d str buted database systems C Federated database systems D Mult database or peer to peer database systems C D A Autonomy Figure Classification of distributed databases. Types of Distributed Database Systems a very high degree of local autonomy. The term federated database system is used when there is some global view or schema of the federation of databases that is shared by the applications . On the other hand a multidatabase system has full local autonomy in that it does not have a global schema but interactively constructs one as needed by the application or hierarchical DBMS in such a case it is necessary to have a canonical system language and to include language translators to translate subqueries from the canonical language to the language of each server. We briefly discuss the issues affecting the design of FDBSs next. Federated Database Management Systems Issues The type of heterogeneity present in FDBSs may arise from several sources. We discuss these sources first and then point out how the different types of autonomies contribute to a semantic heterogeneity that must be resolved in a heterogeneous FDBS. Differences in data models. Databases in an organization come from a variety of data models including the so called legacy models the relational data model the object data model and even files. The modeling capabilities of the models vary. Hence to deal with them uniformly via a single global schema or to process them in a single language is challenging. Even if two databases are both from the RDBMS environment the same information may be represented as an attribute name as a relation name or as a value in different databases. This calls for an intelligent query processing mechanism that can relate information based on metadata. Differences in constraints. Constraint facilities for specification and implementation vary from system to system. There are comparable features that must be reconciled in the construction of a global schema. For example the relationships from ER models are represented as referential integrity constraints in the relational model. Triggers may have to be used to implement certain constraints in the relational model. The global schema must also deal with potential conflicts among constraints. term multidatabase system is not easily applicable to most enterprise IT environments. The notion of constructing a global schema as and when the need arises is not very feasible in practice for enterprise databases. Chapter Distributed Databases Differences in query languages. Even with the same data model the languages and their versions vary. For example SQL has multiple versions like and and each system has its own set of data types comparison operators string manipulation features and so on. Semantic Heterogeneity. Semantic heterogeneity occurs when there are differences in the meaning interpretation and intended use of the same or related data. Semantic heterogeneity among component database systems creates the biggest hurdle in designing global schemas of heterogeneous databases. The design autonomy of component DBSs refers to their freedom of choosing the following design parameters which in turn affect the eventual complexity of the FDBS The universe of discourse from which the data is drawn. For example for two customer accounts databases in the federation may be from the United States and Japan and have entirely different sets of attributes about customer accounts required by the accounting practices. Currency rate fluctuations would also present a problem. Hence relations in these two databases that have identical names CUSTOMER or ACCOUNT may have some common and some entirely distinct information. Representation and naming. The representation and naming of data elements and the structure of the data model may be prespecified for each local database. The understanding meaning and subjective interpretation of data. This is a chief contributor to semantic heterogeneity. Transaction and policy constraints. These deal with serializability criteria compensating transactions and other transaction policies. Derivation of summaries. Aggregation summarization and other dataprocessing features and operations supported by the system. The above problems related to semantic heterogeneity are being faced by all major multinational and governmental organizations in all application areas. In today’s commercial environment most enterprises are resorting to heterogeneous FDBSs having heavily invested in the development of individual database systems using diverse data models on different platforms over the last to years. Enterprises are using various forms of software typically called the middleware or Webbased packages called application servers and even generic systems called Enterprise Resource Planning systems to manage the transport of queries and transactions from the global application to individual databases and the data from the heterogeneous database servers to the global application. Detailed discussion of these types of software systems is outside the scope of this book. Just as providing the ultimate transparency is the goal of any distributed database architecture local component databases strive to preserve autonomy. Communication autonomy of a component DBS refers to its ability to decide whether to communicate with another component DBS. Execution autonomy Distributed Database Architectures refers to the ability of a component DBS to execute local operations without interference from external operations by other component DBSs and its ability to decide the order in which to execute them. The association autonomy of a component DBS implies that it has the ability to decide whether and how much to share its functionality and resources with other component DBSs. The major challenge of designing FDBSs is to let component DBSs interoperate while still providing the above types of autonomies to them. Distributed Database Architectures In this section we first briefly point out the distinction between parallel and distributed database architectures. While both are prevalent in industry today there are various manifestations of the distributed architectures that are continuously evolving among large enterprises. The parallel architecture is more common in highperformance computing where there is a need for multiprocessor architectures to cope with the volume of data undergoing transaction processing and warehousing applications. We then introduce a generic architecture of a distributed database. This is followed by discussions on the architecture of three tier client server and federated database systems. Parallel versus Distributed Architectures There are two main types of multiprocessor system architectures that are commonplace Shared memory architecture. Multiple processors share secondary storage and also share primary memory. Shared disk architecture. Multiple processors share secondary storage but each has their own primary memory. These architectures enable processors to communicate without the overhead of exchanging messages over a Database management systems developed using the above types of architectures are termed parallel database management systems rather than DDBMSs since they utilize parallel processor technology. Another type of multiprocessor architecture is called shared nothing architecture. In this architecture every processor has its own primary and secondary memory no common memory exists and the processors communicate over a highspeed interconnection network . Although the shared nothing architecture resembles a distributed database computing environment major differences exist in the mode of operation. In shared nothing multiprocessor systems there is symmetry and homogeneity of nodes this is not true of the distributed database environment where heterogeneity of hardware and operating system at each node is very common. Shared nothing architecture is also considered as an environment for both primary and secondary memories are shared the architecture is also known as shared everything architecture. Chapter Distributed Databases parallel databases. Figure illustrates a parallel database whereas Figure illustrates a centralized database with distributed access and Figure shows a pure distributed database. We will not expand on parallel architectures and related data management issues here. Sw tch CPU Computer System Memory D B CPU Computer System Memory D B CPU Memory D B Computer System n Central S te S te S te S te S te D B D B Commun cat ons Network S te S te S te S te S te Commun cat ons Network Figure Some different database system architectures. Shared nothing architecture. A networked architecture with a centralized database at one of the sites. A truly distributed database architecture. Distributed Database Architectures User Stored Data Global Conceptual Schema External V ew User External V ew Local Conceptual Schema Local Conceptual Schema Local Internal Schema Local Internal Schema Stored Data Site Site Sites to n Figure Schema architecture of distributed databases. General Architecture of Pure Distributed Databases In this section we discuss both the logical and component architectural models of a DDB. In Figure which describes the generic schema architecture of a DDB the enterprise is presented with a consistent unified view showing the logical structure of underlying data across all nodes. This view is represented by the global conceptual schema which provides network transparency based on physical organization details at that particular site. The logical organization of data at each site is specified by the local conceptual schema . The GCS LCS and their underlying mappings provide the fragmentation and replication transparency discussed in Section Figure shows the component architecture of a DDB. It is an extension of its centralized counterpart and estimated sizes of intermediate results. The latter is particularly important in queries involving joins. Having computed the cost for each candidate the optimizer selects the candidate with the minimum cost for execution. Each local DBMS would have their local query optimizer transaction manager and execution engines as well as the local system catalog which houses the local schemas. The global transaction manager is responsible for coordinating the execution across multiple sites in conjunction with the local transaction manager at those sites. Federated Database Schema Architecture Typical five level schema architecture to support global applications in the FDBS environment is shown in Figure In this architecture the local schema is the External schema Federated schema Component schema Local schema Component DBS External schema External schema Federated schema Export schema Component schema Local schema Component DBS Export schema Export schema Distributed Database Architectures Figure The five level schema architecture in a federated database system . Source Adapted from Sheth and Larson “Federated Database Systems for Managing Distributed Heterogeneous and Autonomous ACM Computing Surveys of a component database and the component schema is derived by translating the local schema into a canonical data model or common data model for the FDBS. Schema translation from the local schema to the component schema is accompanied by generating mappings to transform commands on a component schema into commands on the corresponding local schema. The export schema represents the subset of a component schema that is available to the FDBS. The federated schema is the global schema or view which is the result of integrating all the shareable export schemas. The external schemas define the schema for a user group or an application as in the three level schema All the problems related to query processing transaction processing and directory and metadata management and recovery apply to FDBSs with additional considerations. It is not within our scope to discuss them in detail here. a detailed discussion of the autonomies and the five level architecture of FDBMSs see Sheth and Larson Chapter Distributed Databases Client User nterface or presentat on t er log c t er . This provides the user interface and interacts with the user. The programs at this layer present Web interfaces or forms to the client in order to interface with the application. Web browsers are often utilized and the languages and specifications used include HTML XHTML CSS Flash MathML Scalable Vector Graphics Java JavaScript Adobe Flex and others. This layer handles user input output and navigation by accepting user commands and displaying the needed information usually in the form of static or dynamic Web pages. The latter are employed when the interaction involves database access. When a Web interface is used this layer typically communicates with the application layer via the HTTP protocol. Application layer . This layer programs the application logic. For example queries can be formulated based on user input from the client or query results can be formatted and sent to the client for presentation. Additional application functionality can be handled at this layer such Distributed Database Architectures as security checks identity verification and other functions. The application layer can interact with one or more databases or data sources as needed by connecting to the database using ODBC JDBC SQL CLI or other database access techniques. Database server. This layer handles query and update requests from the application layer processes the requests and sends the results. Usually SQL is used to access the database if it is relational or object relational and stored database procedures may also be invoked. Query results may be formatted into XML is that of ensuring consistency of replicated copies of a data item by employing distributed concurrency control techniques. The application server must also ensure the atomicity of global transactions by performing global recovery when certain sites fail. Chapter Distributed Databases If the DDBMS has the capability to hide the details of data distribution from the application server then it enables the application server to execute global queries and transactions as though the database were centralized without having to specify the sites at which the data referenced in the query or transaction resides. This property is called distribution transparency. Some DDBMSs do not provide distribution transparency instead requiring that applications are aware of the details of data distribution. Data Fragmentation Replication and Allocation Techniques for Distributed Database Design In this section we discuss techniques that are used to break up the database into logical units called fragments which may be assigned for storage at the various sites. We also discuss the use of data replication which permits certain data to be stored in more than one site and the process of allocating fragments or replicas of fragments for storage at the various sites. These techniques are used during the process of distributed database design. The information concerning data fragmentation allocation and replication is stored in a global directory that is accessed by the DDBS applications as needed. Data Fragmentation In a DDB decisions must be made regarding which site should be used to store which portions of the database. For now we will assume that there is no replication that is each relation or portion of a relation is stored at one site only. We discuss replication and its effects later in this section. We also use the terminology of relational databases but similar concepts apply to other data models. We assume that we are starting with a relational database schema and must decide on how to distribute the relations over the various sites. To illustrate our discussion we use the relational database schema in Figure Before we decide on how to distribute the data we must determine the logical units of the database that are to be distributed. The simplest logical units are the relations themselves that is each whole relation is to be stored at a particular site. In our example we must decide on a site to store each of the relations EMPLOYEE DEPARTMENT PROJECT WORKSON and DEPENDENT in Figure In many cases however a relation can be divided into smaller logical units for distribution. For example consider the company database shown in Figure and assume there are three computer sites one for each department in the We may want to store the database information relating to each department at the computer site for that department. A technique called horizontal fragmentation can be used to partition each relation by department. course in an actual situation there will be many more tuples in the relation than those shown in Figure Data Fragmentation Replication and Allocation Techniques for Distributed Database Design Horizontal Fragmentation. A horizontal fragment of a relation is a subset of the tuples in that relation. The tuples that belong to the horizontal fragment are specified by a condition on one or more attributes of the relation. Often only a single attribute is involved. For example we may define three horizontal fragments on the EMPLOYEE relation in Figure with the following conditions to other secondary relations which are related to the primary via a foreign key. This way related data between the primary and the secondary relations gets fragmented in the same way. Vertical Fragmentation. Each site may not need all the attributes of a relation which would indicate the need for a different type of fragmentation. Vertical fragmentation divides a relation “vertically” by columns. A vertical fragment of a relation keeps only certain attributes of the relation. For example we may want to fragment the EMPLOYEE relation into two vertical fragments. The first fragment includes personal information Name Bdate Address and Sex and the second includes work related information Ssn Salary Superssn and Dno. This vertical fragmentation is not quite proper because if the two fragments are stored separately we cannot put the original employee tuples back together since there is no common attribute between the two fragments. It is necessary to include the primary key or some candidate key attribute in every vertical fragment so that the full relation can be reconstructed from the fragments. Hence we must add the Ssn attribute to the personal information fragment. Notice that each horizontal fragment on a relation R can be specified in the relational algebra by a σCi operation. A set of horizontal fragments whose conditions Cn include all the tuples in R that is every tuple in R satisfies OR OR OR Cn is called a complete horizontal fragmentation of R. In many cases a complete horizontal fragmentation is also disjoint that is no tuple in R satisfies for any i ≠ j. Our two earlier examples of horizontal fragmentation for the EMPLOYEE and PROJECT relations were both complete and disjoint. To reconstruct the relation R from a complete horizontal fragmentation we need to apply the UNION operation to the fragments. A vertical fragment on a relation R can be specified by a πLi operation in the relational algebra. A set of vertical fragments whose projection lists Ln include all the attributes in R but share only the primary key attribute of R is called a Chapter Distributed Databases complete vertical fragmentation of R. In this case the projection lists satisfy the following two conditions ∪ ∪ ∪ Ln ATTRS. Li ∩ Lj PK for any i ≠ j where ATTRS is the set of attributes of R and PK is the primary key of R. To reconstruct the relation R from a complete vertical fragmentation we apply the OUTER UNION operation to the vertical fragments . Notice that we could also apply a FULL OUTER JOIN operation and get the same result for a complete vertical fragmentation even when some horizontal fragmentation may also have been applied. The two vertical fragments of the EMPLOYEE relation with projection lists Ssn Name Bdate Address Sex and Ssn Salary Superssn Dno constitute a complete vertical fragmentation of EMPLOYEE. Two horizontal fragments that are neither complete nor disjoint are those defined on the EMPLOYEE relation in Figure by the conditions Fragmentation. We can intermix the two types of fragmentation yielding a mixed fragmentation. For example we may combine the horizontal and vertical fragmentations of the EMPLOYEE relation given earlier into a mixed fragmentation that includes six fragments. In this case the original relation can be reconstructed by applying UNION and OUTER UNION operations in the appropriate order. In general a fragment of a relation R can be specified by a SELECT PROJECT combination of operations πL . If C TRUE and L ≠ ATTRS we get a vertical fragment and if C ≠ TRUE and L ATTRS we get a horizontal fragment. Finally if C ≠ TRUE and L ≠ ATTRS we get a mixed fragment. Notice that a relation can itself be considered a fragment with C TRUE and L ATTRS. In the following discussion the term fragment is used to refer to a relation or to any of the preceding types of fragments. A fragmentation schema of a database is a definition of a set of fragments that includes all attributes and tuples in the database and satisfies the condition that the whole database can be reconstructed from the fragments by applying some sequence of OUTER UNION and UNION operations. It is also sometimes useful although not necessary to have all the fragments be disjoint except for the repetition of primary keys among vertical fragments. In the latter case all replication and distribution of fragments is clearly specified at a subsequent stage separately from fragmentation. An allocation schema describes the allocation of fragments to sites of the DDBS hence it is a mapping that specifies for each fragment the site at which it is Data Fragmentation Replication and Allocation Techniques for Distributed Database Design stored. If a fragment is stored at more than one site it is said to be replicated. We discuss data replication and allocation next. Data Replication and Allocation Replication is useful in improving the availability of data. The most extreme case is replication of the whole database at every site in the distributed system thus creating a fully replicated distributed database. This can improve availability remarkably because the system can continue to operate as long as at least one site is up. It also improves performance of retrieval for global queries because the results of such queries can be obtained locally from any one site hence a retrieval query can be processed at the local site where it is submitted if that site includes a server module. The disadvantage of full replication is that it can slow down update operations drastically since a single logical update must be performed on every copy of the database to keep the copies consistent. This is especially true if many copies of the database exist. Full replication makes the concurrency control and recovery techniques more expensive than they would be if there was no replication as we will see in Section The other extreme from full replication involves having no replication that is each fragment is stored at exactly one site. In this case all fragments must be disjoint except for the repetition of primary keys among vertical fragments. This is also called nonredundant allocation. Between these two extremes we have a wide spectrum of partial replication of the data that is some fragments of the database may be replicated whereas others may not. The number of copies of each fragment can range from one up to the total number of sites in the distributed system. A special case of partial replication is occurring heavily in applications where mobile workers such as sales forces financial planners and claims adjustors carry partially replicated databases with them on laptops and PDAs and synchronize them periodically with the server A description of the replication of fragments is sometimes called a replication schema. Each fragment or each copy of a fragment must be assigned to a particular site in the distributed system. This process is called data distribution . The choice of sites and the degree of replication depend on the performance and availability goals of the system and on the types and frequencies of transactions submitted at each site. For example if high availability is required transactions can be submitted at any site and most transactions are retrieval only a fully replicated database is a good choice. However if certain transactions that access particular parts of the database are mostly submitted at a particular site the corresponding set of fragments can be allocated at that site only. Data that is accessed at multiple sites can be replicated at those sites. If many updates are performed it may be useful to limit replication. Finding an optimal or even a good solution to distributed data allocation is a complex optimization problem. a proposed scalable approach to synchronize partially replicated databases see Mahajan et al. Chapter Distributed Databases Example of Fragmentation Allocation and Replication We now consider an example of fragmenting and distributing the company database in Figures and Suppose that the company has three computer sites one for each current department. Sites and are for departments and respectively. At each of these sites we expect frequent access to the EMPLOYEE and PROJECT information for the employees who work in that department and the projects controlled by that department. Further we assume that these sites mainly access the Name Ssn Salary and Superssn attributes of EMPLOYEE. Site is used by company headquarters and accesses all employee and project information regularly in addition to keeping track of DEPENDENT information for insurance purposes. According to these requirements the whole database in Figure can be stored at site To determine the fragments to be replicated at sites and first we can horizontally fragment DEPARTMENT by its key Dnumber. Then we apply derived fragmentation to the EMPLOYEE PROJECT and DEPTLOCATIONS relations based on their foreign keys for department number called Dno Dnum and Dnumber respectively in Figure We can vertically fragment the resulting EMPLOYEE fragments to include only the attributes Name Ssn Salary Superssn Dno . Figure shows the mixed fragments and which include the EMPLOYEE tuples satisfying the conditions Dno and Dno respectively. The horizontal fragments of PROJECT DEPARTMENT and DEPTLOCATIONS are similarly fragmented by department number. All these fragments stored at sites and replicated because they are also stored at headquarters site We must now fragment the WORKSON relation and decide which fragments of WORKSON to store at sites and We are confronted with the problem that no attribute of WORKSON directly indicates the department to which each tuple belongs. In fact each tuple in WORKSON relates an employee e to a project P. We could fragment WORKSON based on the department D in which e works or based on the department Dthat controls P. Fragmentation becomes easy if we have a constraint stating that D Dfor all WORKSON tuples that is if employees can work only on projects controlled by the department they work for. However there is no such constraint in our database in Figure For example the WORKSON tuple relates an employee who works for department with a project controlled by department In this case we could fragment WORKSON based on the department in which the employee works and then fragment further based on the department that controls the projects that employee is working on as shown in Figure In Figure the union of fragments and gives all WORKSON tuples for employees who work for department Similarly the union of fragments and gives all WORKSON tuples for employees who work for department On the other hand the union of fragments and gives all WORKSON tuples for projects controlled by department The condition for each of the fragments through is shown in Figure The relations that represent M N relationships such as WORKSON often have several possible logical fragmentations. In our distribution in Figure we choose to include all fragments that can be joined to Data Fragmentation Replication and Allocation Techniques for Distributed Database Design Fname John B Sm th Frankl n T Wong K Narayan A Engl sh Ramesh Joyce M Lname Ssn Salary Superssn Dno Data at site Data at site Fname Al c a J Zelaya Jenn fer S Wallace Ahmad V Jabbar M Lname Ssn Salary Superssn Dno Dname Research Dnumber Mgrssn Mgrstartdate Dnumber Bella re Sugarland Houston Locat on Dname Adm strat on Dnumber Mgrssn Mgrstartdate Essn Pno Hours Pname Product X Product Y Product Z Bella re Sugarland Houston Pnumber Plocat on Dnum Essn Pno Hours Pname Computer zat on Newbenef ts Stafford Stafford Pnumber Plocat on Dnum Dnumber Stafford Locat on Figure Allocation of fragments to sites. Relation fragments at site corresponding to department Relation fragments at site corresponding to department Chapter Distributed Databases Essn C C and C and Fragments of WORKSON for employees working in department Fragments of WORKSON for employees working in department Fragments of WORKSON for employees working in department . a detailed discussion of optimization algorithms see Ozsu and Valduriez Chapter Distributed Databases Local Query Optimization. This stage is common to all sites in the DDB. The techniques are similar to those used in centralized systems. The first three stages discussed above are performed at a central control site while the last stage is performed locally. Data Transfer Costs of Distributed Query Processing We discussed the issues involved in processing and optimizing a query in a centralized DBMS in Chapter In a distributed system several additional factors further complicate query processing. The first is the cost of transferring data over the network. This data includes intermediate files that are transferred to other sites for further processing as well as the final result files that may have to be transferred to the site where the query result is needed. Although these costs may not be very high if the sites are connected via a high performance local area network they become quite significant in other types of networks. Hence DDBMS query optimization algorithms consider the goal of reducing the amount of data transfer as an optimization criterion in choosing a distributed query execution strategy. We illustrate this with two simple sample queries. Suppose that the EMPLOYEE and DEPARTMENT relations in Figure are distributed at two sites as shown in Figure We will assume in this example that neither relation is fragmented. According to Figure the size of the EMPLOYEE relation is bytes and the size of the DEPARTMENT relation is bytes. Consider the query Q For each employee retrieve the employee name and the name of the department for which the employee works. This can be stated as follows in the relational algebra Q πFname Lname Dname The result of this query will include records assuming that every employee is related to a department. Suppose that each record in the query result is bytes long. Fname EMPLOYEE Site records each record s bytes long Ssn f eld s bytes long Dno f eld s bytes long Site M Lname Ssn Bdate Address Sex Salary Superssn Dno Dname DEPARTMENT Dnumber Mgrssn Mgrstartdate Fname f eld s bytes long Lname f eld s bytes long records each record s bytes long Dnumber f eld s bytes long Mgrssn f eld s bytes long Dname f eld s bytes long Figure Example to illustrate volume of data transferred. Query Processing and Optimization in Distributed Databases The query is submitted at a distinct site which is called the result site because the query result is needed there. Neither the EMPLOYEE nor the DEPARTMENT relations reside at site There are three simple strategies for executing this distributed query Transfer both the EMPLOYEE and the DEPARTMENT relations to the result site and perform the join at site In this case a total of + bytes must be transferred. Transfer the EMPLOYEE relation to site execute the join at site and send the result to site The size of the query result is bytes so + bytes must be transferred. Transfer the DEPARTMENT relation to site execute the join at site and send the result to site In this case + bytes must be transferred. If minimizing the amount of data transfer is our optimization criterion we should choose strategy Now consider another query Q For each department retrieve the department name and the name of the department manager. This can be stated as follows in the relational algebra Q πFname Lname Dname Again suppose that the query is submitted at site The same three strategies for executing query Q apply to Q except that the result of Qincludes only records assuming that each department has a manager Transfer both the EMPLOYEE and the DEPARTMENT relations to the result site and perform the join at site In this case a total of + bytes must be transferred. Transfer the EMPLOYEE relation to site execute the join at site and send the result to site The size of the query result is bytes so + bytes must be transferred. Transfer the DEPARTMENT relation to site execute the join at site and send the result to site In this case + bytes must be transferred. Again we would choose strategy time by an overwhelming margin over strategies and The preceding three strategies are the most obvious ones for the case where the result site whose size is bytes whereas for Q we transfer F πMgrssn whose size is bytes. Join the transferred file with the EMPLOYEE relation at site and transfer the required attributes from the resulting file to site For Q we transfer R πDno Fname Lname whose size is bytes whereas for Q we transfer R πMgrssn Fname Lname whose size is bytes. Execute the query by joining the transferred file R or Rwith DEPARTMENT and present the result to the user at site Using this strategy we transfer bytes for Q and bytes for Q. We limited the EMPLOYEE attributes and tuples transmitted to site in step to only those that will actually be joined with a DEPARTMENT tuple in step For query Q this turned out to include all EMPLOYEE tuples so little improvement was achieved. However for Q only out of the EMPLOYEE tuples were needed. The semijoin operation was devised to formalize this strategy. A semijoin operation R A B S where A and B are domain compatible attributes of R and S respectively produces the same result as the relational algebra expression πR. In a distributed environment where R and S reside at different sites the semijoin is typically implemented by first transferring F πB to the site where R resides and then joining F with R thus leading to the strategy discussed here. Notice that the semijoin operation is not commutative that is R S ≠S R Query Processing and Optimization in Distributed Databases Query and Update Decomposition In a DDBMS with no distribution transparency the user phrases a query directly in terms of specific fragments. For example consider another query Q Retrieve the names and hours per week for each employee who works on some project controlled by department which is specified on the distributed database where the relations at sites and are shown in Figure and those at site are shown in Figure as in our earlier example. A user who submits such a query must specify whether it references the and relations at site and the attribute lists are . For the fragments shown in Figure we have the guard conditions and attribute lists shown in Figure When the DDBMS decomposes an update request it can determine which fragments must be updated by examining their guard conditions. For example a user request to insert a new EMPLOYEE tuple ‘Alex’ ‘B’ ‘Coleman’ Sandstone Houston TX’ M would be decomposed by the DDBMS into two insert requests the first inserts the preceding tuple in the EMPLOYEE fragment at site and the second inserts the projected tuple ‘Alex’ ‘B’ ‘Coleman’ in the fragment at site For query decomposition the DDBMS can determine which fragments may contain the required tuples by comparing the query condition with the guard Chapter Distributed Databases attribute list Fname Minit Lname Ssn Salary Superssn Dno guard condition attribute list guard condition attribute list guard condition attribute list guard condition attribute list guard condition Essn IN attribute list Fname Minit Lname Ssn Salary Superssn Dno guard condition attribute list guard condition attribute list guard condition attribute list guard condition attribute list guard condition Essn IN Site fragments. Site fragments. conditions. For example consider the query Q Retrieve the names and hours per week for each employee who works on some project controlled by department This can be specified in SQL on the schema in Figure as follows Q SELECT Fname Lname Hours FROM EMPLOYEE PROJECT WORKSON WHERE AND Pnumber Pno AND Essn Ssn Overview of Transaction Management in Distributed Databases Suppose that the query is submitted at site which is where the query result will be needed. The DDBMS can determine from the guard condition on and that all tuples satisfying the conditions reside at site Hence it may decompose the query into the following relational algebra subqueries ← ← πEssn Fname Essn SsnEMPLOYEE RESULT ← πFname Lname This decomposition can be used to execute the query by using a semijoin strategy. The DDBMS knows from the guard conditions that contains exactly those tuples satisfying . The manager stores bookkeeping information related to each transaction such as a unique identifier originating site name and so on. For READ operations it returns a local copy if valid and available. For WRITE operations it ensures that updates are visible across all sites containing copies of the data item. For ABORT operations the manager ensures that no effects of the transaction are reflected in any site of the distributed database. For COMMIT operations it ensures that the effects of a write are persistently recorded on all databases containing copies of the data item. Atomic termination of distributed transactions is commonly implemented using the two phase commit protocol. We give more details of this protocol in the following section. Chapter Distributed Databases The transaction manager passes to the concurrency controller the database operation and associated information. The controller is responsible for acquisition and release of associated locks. If the transaction requires access to a locked resource it is delayed until the lock is acquired. Once the lock is acquired the operation is sent to the runtime processor which handles the actual execution of the database operation. Once the operation is completed locks are released and the transaction manager is updated with the result of the operation. We discuss commonly used distributed concurrency methods in Section Two Phase Commit Protocol In Section we described the two phase commit protocol which requires a global recovery manager or coordinator to maintain information needed for recovery in addition to the local recovery managers and the information they maintain The two phase commit protocol has certain drawbacks that led to the development of the three phase commit protocol which we discuss next. Three Phase Commit Protocol The biggest drawback of is that it is a blocking protocol. Failure of the coordinator blocks all participating sites causing them to wait until the coordinator recovers. This can cause performance degradation especially if participants are holding locks to shared resources. Another problematic scenario is when both the coordinator and a participant that has committed crash together. In the two phase commit protocol a participant has no way to ensure that all participants got the commit message in the second phase. Hence once a decision to commit has been made by the coordinator in the first phase participants will commit their transactions in the second phase independent of receipt of a global commit message by other participants. Thus in the situation that both the coordinator and a committed participant crash together the result of the transaction becomes uncertain or nondeterministic. Since the transaction has already been committed by one participant it cannot be aborted on recovery by the coordinator. Also the transaction cannot be optimistically committed on recovery since the original vote of the coordinator may have been to abort. These problems are solved by the three phase commit protocol which essentially divides the second commit phase into two subphases called prepare tocommit and commit. The prepare to commit phase is used to communicate the result of the vote phase to all participants. If all participants vote yes then the coordinator instructs them to move into the prepare to commit state. The commit subphase is identical to its two phase counterpart. Now if the coordinator crashes during this subphase another participant can see the transaction through to completion. It can simply ask a crashed participant if it received a prepare to commit message. If it did not then it safely assumes to abort. Thus the state of the protocol can be recovered irrespective of which participant crashes. Also by limiting the time required for a transaction to commit or abort to a maximum time out period the protocol ensures that a transaction attempting to commit via releases locks on time out. Overview of Concurrency Control and Recovery in Distributed Databases The main idea is to limit the wait time for participants who have committed and are waiting for a global commit or abort from the coordinator. When a participant receives a precommit message it knows that the rest of the participants have voted to commit. If a precommit message has not been received then the participant will abort and release all locks. Operating System Support for Transaction Management The following are the main benefits of operating system supported transaction management Typically DBMSs use their own to guarantee mutually exclusive access to shared resources. Since these semaphores are implemented in userspace at the level of the DBMS application software the OS has no knowledge about them. Hence if the OS deactivates a DBMS process holding a lock other DBMS processes wanting this lock resource get queued. Such a situation can cause serious performance degradation. OS level knowledge of semaphores can help eliminate such situations. Specialized hardware support for locking can be exploited to reduce associated costs. This can be of great importance since locking is one of the most common DBMS operations. Providing a set of common transaction support operations though the kernel allows application developers to focus on adding new features to their products as opposed to reimplementing the common functionality for each application. For example if different DDBMSs are to coexist on the same machine and they chose the two phase commit protocol then it is more beneficial to have this protocol implemented as part of the kernel so that the DDBMS developers can focus more on adding new features to their products. Overview of Concurrency Control and Recovery in Distributed Databases For concurrency control and recovery purposes numerous problems arise in a distributed DBMS environment that are not encountered in a centralized DBMS environment. These include the following Dealing with multiple copies of the data items. The concurrency control method is responsible for maintaining consistency among these copies. The recovery method is responsible for making a copy consistent with other copies if the site on which the copy is stored fails and recovers later. are data structures used for synchronized and exclusive access to shared resources for preventing race conditions in a parallel computing system. Chapter Distributed Databases Failure of individual sites. The DDBMS should continue to operate with its running sites if possible when one or more individual sites fail. When a site recovers its local database must be brought up to date with the rest of the sites before it rejoins the system. Failure of communication links. The system must be able to deal with the failure of one or more of the communication links that connect the sites. An extreme case of this problem is that network partitioning may occur. This breaks up the sites into two or more partitions where the sites within each partition can communicate only with one another and not with sites in other partitions. Distributed commit. Problems can arise with committing a transaction that is accessing databases stored on multiple sites if some sites fail during the commit process. The two phase commit protocol site. Retrieving updated copies to replace stale data may be delayed until an access to this data occurs. In general fragments of relations across sites should be uniquely accessible. Also to ensure data distribution transparency users should be allowed to create synonyms for remote objects and use these synonyms for subsequent referrals. Chapter Distributed Databases Current Trends in Distributed Databases Current trends in distributed data management are centered on the Internet in which petabytes of data can be managed in a scalable dynamic and reliable fashion. Two important areas in this direction are cloud computing and peer to peer databases. Cloud Computing Cloud computing is the paradigm of offering computer infrastructure platforms and software as services over the Internet. It offers significant economic advantages by limiting both up front capital investments toward computer infrastructure as well as total cost of ownership. It has introduced a new challenge of managing petabytes of data in a scalable fashion. Traditional database systems for managing enterprise data proved to be inadequate in handling this challenge which has resulted in a major architectural revision. The Claremont by a group of senior database researchers envisions that future research in cloud computing will result in the emergence of new data management architectures and the interplay of structured and unstructured data as well as other developments. Performance costs associated with partial failures and global synchronization were key performance bottlenecks of traditional database solutions. The key insight is that the hash value nature of the underlying datasets used by these organizations lends itself naturally to partitioning. For instance search queries essentially involve a recursive process of mapping keywords to a set of related documents which can benefit from such a partitioning. Also the partitions can be treated independently thereby eliminating the need for a coordinated commit. Another problem with traditional DDBMSs is the lack of support for efficient dynamic partitioning of data which limited scalability and resource utilization. Traditional systems treated system metadata and application data alike with the system data requiring strict consistency and availability guarantees. But application data has variable requirements on these characteristics depending on its nature. For example while a search engine can afford weaker consistency guarantees an online text editor like Google Docs which allows concurrent users has strict consistency requirements. The metadata of a distributed database system should be decoupled from its actual data in order to ensure scalability. This decoupling can be used to develop innovative solutions to manage the actual data by exploiting their inherent suitability to partitioning and using traditional database solutions to manage critical system metadata. Since metadata is only a fraction of the total data set it does not prove to be a performance bottleneck. Single object semantics of these implementations enables higher tolerance to nonavailability of certain sections of data. Access to data is typically by a single object in an atomic fashion. Hence transaction support to such data is not as stringent as for traditional There is a varied set of Claremont Report on Database Research” is available at may refer to the work done by Das et al. for further details. Distributed Databases in Oracle cloud services available today including application services storage services . More and more data centric applications are expected to leverage data services in the cloud. While most current cloud services are data analysis intensive it is expected that business logic will eventually be migrated to the cloud. The key challenge in this migration would be to ensure the scalability advantages for multiple object semantics inherent to business logic. For a detailed treatment of cloud computing refer to the relevant bibliographic references in this chapter’s Selected Bibliography. Peer to Peer Database Systems A peer to peer database system aims to integrate advantages of computing such as scalability attack resilience and self organization with the features of decentralized data management. Nodes are autonomous and are linked only to a small number of peers individually. It is permissible for a node to behave purely as a collection of files without offering a complete set of traditional DBMS functionality. While FDBS and MDBS mandate the existence of mappings between local and global federated schemas PDBSs attempt to avoid a global schema by providing mappings between pairs of information sources. In PDBS each peer potentially models semantically related data in a manner different from other peers and hence the task of constructing a central mediated schema can be very challenging. PDBSs aim to decentralize data sharing. Each peer has a schema associated with its domain specific stored data. The PDBS constructs a semantic of mappings between peer schemas. Using this path a peer to which a query has been submitted can obtain information from any relevant peer connected through this path. In multidatabase systems a separate global query processor is used whereas in a system a query is shipped from one peer to another until it is processed completely. A query submitted to a node may be forwarded to others based on the mapping graph of semantic paths. Edutella and Piazza are examples of PDBSs. Details of these systems can be found from the sources mentioned in this chapter’s Selected Bibliography. Distributed Databases in Oracle provides support for homogeneous heterogeneous and client server architectures of distributed databases. In a homogeneous architecture a minimum of two Oracle databases reside on at least one machine. Although the location and platform of the databases are transparent to client applications they would need to semantic path describes the higher level relationship between two domains that are dissimilar but not unrelated. discussion is based on available documentation at Chapter Distributed Databases distinguish between local and remote objects semantically. Using synonyms this need can be overcome wherein users can access the remote objects with the same syntax as local objects. Different versions of DBMSs can be used although it must be noted that Oracle offers backward compatibility but not forward compatibility between its versions. For example it is possible that some of the SQL extensions that were incorporated into Oracle may not be understood by Oracle In a heterogeneous architecture at least one of the databases in the network is a non Oracle system. The Oracle database local to the application hides the underlying heterogeneity and offers the view of a single local underlying Oracle database. Connectivity is handled by use of an ODBC or OLE DB compliant protocol or by Oracle’s Heterogeneous Services and Transparent Gateway agent components. A discussion of the Heterogeneous Services and Transparent Gateway agents is beyond the scope of this book and the reader is advised to consult the online Oracle documentation. In the client server architecture the Oracle database system is divided into two parts a front end as the client portion and a back end as the server portion. The client portion is the front end database application that interacts with the user. The client has no data access responsibility and merely handles the requesting processing and presentation of data managed by the server. The server portion runs Oracle and handles the functions related to concurrent shared access. It accepts SQL and PL SQL statements originating from client applications processes them and sends the results back to the client. Oracle client server applications provide location transparency by making the location of data transparent to users several features like views synonyms and procedures contribute to this. Global naming is achieved by using TABLENAME@DATABASENAME to refer to tables uniquely. Oracle uses a two phase commit protocol to deal with concurrent distributed transactions. The COMMIT statement triggers the two phase commit mechanism. The RECO background process automatically resolves the outcome of those distributed transactions in which the commit was interrupted. The RECO of each local Oracle server automatically commits or rolls back any in doubt distributed transactions consistently on all involved nodes. For long term failures Oracle allows each local DBA to manually commit or roll back any in doubt transactions and free up resources. Global consistency can be maintained by restoring the database at each site to a predetermined fixed point in the past. Oracle’s distributed database architecture is shown in Figure A node in a distributed database system can act as a client as a server or both depending on the situation. The figure shows two sites where databases called HQ and Sales are kept. For example in the application shown running at the headquarters for an SQL statement issued against local data the HQ computer acts as a server whereas for a statement against remote data the HQ computer acts as a client. Communication in such a distributed heterogeneous environment is facilitated through Oracle Net Services which supports standard network protocols and APIs. Under Oracle’s client server implementation of distributed databases Net Services Distributed Databases in Oracle Server DEPT Table Application HQ Database Connect to Ident ed by Oracle Net EMP Table Sales Database Transact on Network INSERT INTO EMP@SALES DELETE FROM DEPT SELECT FROM EMP@SALES COMMIT Server Oracle Net Database L nk Figure Oracle distributed database system. Source From Oracle Copyright © Oracle Corporation All rights reserved. is responsible for establishing and managing connections between a client application and database server. It is present in each node on the network running an Oracle client application database server or both. It packages SQL statements into one of the many communication protocols to facilitate client to server communication and then packages the results back similarly to the client. The support offered by Net Services to heterogeneity refers to platform specifications only and not the database software. Support for DBMSs other than Oracle is through Oracle’s Heterogeneous Services and Transparent Gateway. Each database has a unique global name provided by a hierarchical arrangement of network domain names that is prefixed to the database name to make it unique. Oracle supports database links that define a one way communication path from one Oracle database to another. For example CREATE DATABASE LINK .americas Chapter Distributed Databases establishes a connection to the sales database in Figure under the network domain us that comes under domain americas. Using links a user can access a remote object on another database subject to ownership rights without the need for being a user on the remote database. Data in an Oracle DDBS can be replicated using snapshots or replicated master tables. Replication is provided at the following levels Basic replication. Replicas of tables are managed for read only access. For updates data must be accessed at a single primary site. Advanced replication. This extends beyond basic replication by allowing applications to update table replicas throughout a replicated DDBS. Data can be read and updated at any site. This requires additional software called Oracle’s advanced replication option. A snapshot generates a copy of a part of the table by means of a query called the snapshot defining query. A simple snapshot definition looks like this CREATE SNAPSHOT SALESORDERS AS SELECT FROM Oracle groups snapshots into refresh groups. By specifying a refresh interval the snapshot is automatically refreshed periodically at that interval by up to ten Snapshot Refresh Processes . If the defining query of a snapshot contains a distinct or aggregate function a GROUP BY or CONNECT BY clause or join or set operations the snapshot is termed a complex snapshot and requires additional processing. Oracle is an industry standard protocol for directory services. LDAP enables the use of a partitioned Directory Information Tree across multiple LDAP servers which in turn can return references to other servers as a result of a directory query. Online directories and LDAP are particularly important in distributed databases wherein access of metadata related to transparencies discussed in Section must be scalable secure and highly available. Oracle supports LDAP Version and online directories through Oracle Internet Directory a general purpose directory service for fast access and centralized management of metadata pertaining to distributed network resources and users. It runs as an application on an Oracle database and communicates with the database through Oracle Net Services. It also provides password based anonymous and certificate based user authentication using SSL Version Figure illustrates the architecture of the Oracle Internet Directory. The main components are Oracle directory server. Handles client requests and updates for information pertaining to people and resources. Oracle directory replication server. Stores a copy of the LDAP data from Oracle directory servers as a backup. Directory administrator Supports both GUI based and command linebased interfaces for directory administration. Summary In this chapter we provided an introduction to distributed databases. This is a very broad topic and we discussed only some of the basic techniques used with distributed databases. First we discussed the reasons for distribution and the potential advantages of distributed databases over centralized systems. Then the concept of Chapter Distributed Databases Oracle Appl cat on Server Database Oracle Net Connect ons Oracle D rectory Repl cat on Server Oracle D rectory LDAP over SSL Server LDAP Clients Directory Administration Figure Oracle Internet Directory overview. Source From Oracle Copyright © Oracle Corporation All rights reserved. distribution transparency and the related concepts of fragmentation transparency and replication transparency were defined. We categorized DDBMSs by using criteria such as the degree of homogeneity of software modules and the degree of local autonomy. We distinguished between parallel and distributed system architectures and then introduced the generic architecture of distributed databases from both a component as well as a schematic architectural perspective. The issues of federated database management were then discussed in some detail focusing on the needs of supporting various types of autonomies and dealing with semantic heterogeneity. We also reviewed the client server architecture concepts and related them to distributed databases. We discussed the design issues related to data fragmentation replication and distribution and we distinguished between horizontal and vertical fragments of relations. The use of data replication to improve system reliability and availability was then discussed. We illustrated some of the techniques used in distributed query processing and discussed the cost of communication among sites which is considered a major factor in distributed query optimization. The different techniques for executing joins were compared and we then presented the semijoin technique for joining relations that reside on different sites. Then we discussed transaction management including different commit protocols and operating system support for transaction management. We briefly discussed the concurrency Review Questions control and recovery techniques used in DDBMSs and then reviewed some of the additional problems that must be dealt with in a distributed environment that do not appear in a centralized environment. We reviewed catalog management in distributed databases and summarized their relative advantages and disadvantages. We then introduced Cloud Computing and Peer to Peer Database Systems as new focus areas in DDBs in response to the need of managing petabytes of information accessible over the Internet today. We described some of the facilities in Oracle to support distributed databases. We also discussed online directories and the LDAP protocol in brief. Review Questions What are the main reasons for and potential advantages of distributed databases What additional functions does a DDBMS have over a centralized DBMS Discuss what is meant by the following terms degree of homogeneity of a DDBMS degree of local autonomy of a DDBMS federated DBMS distribution transparency fragmentation transparency replication transparency multidatabase system. Discuss the architecture of a DDBMS. Within the context of a centralized DBMS briefly explain new components introduced by the distribution of data. What are the main software modules of a DDBMS Discuss the main functions of each of these modules in the context of the client server architecture. Compare the two tier and three tier client server architectures. What is a fragment of a relation What are the main types of fragments Why is fragmentation a useful concept in distributed database design Why is data replication useful in DDBMSs What typical units of data are replicated What is meant by data allocation in distributed database design What typical units of data are distributed over sites How is a horizontal partitioning of a relation specified How can a relation be put back together from a complete horizontal partitioning How is a vertical partitioning of a relation specified How can a relation be put back together from a complete vertical partitioning Discuss the naming problem in distributed databases. What are the different stages of processing a query in a DDBMS Discuss the different techniques for executing an equijoin of two files located at different sites. What main factors affect the cost of data transfer Chapter Distributed Databases Discuss the semijoin method for executing an equijoin of two files located at different sites. Under what conditions is an equijoin strategy efficient Discuss the factors that affect query decomposition. How are guard conditions and attribute lists of fragments used during the query decomposition process How is the decomposition of an update request different from the decomposition of a query How are guard conditions and attribute lists of fragments used during the decomposition of an update request List the support offered by operating systems to a DDBMS and also their benefits. Discuss the factors that do not appear in centralized systems that affect concurrency control and recovery in distributed systems. Discuss the two phase commit protocol used for transaction management in a DDBMS. List its limitations and explain how they are overcome using the three phase commit protocol. Compare the primary site method with the primary copy method for distributed concurrency control. How does the use of backup sites affect each When are voting and elections used in distributed databases Discuss catalog management in distributed databases. What are the main challenges facing a traditional DDBMS in the context of today’s Internet applications How does cloud computing attempt to address them Discuss briefly the support offered by Oracle for homogeneous heterogeneous and client server based distributed database architectures. Discuss briefly online directories their management and their role in distributed databases. Exercises Consider the data distribution of the COMPANY database where the fragments at sites and are as shown in Figure and the fragments at site are as shown in Figure For each of the following queries show at least two strategies of decomposing and executing the query. Under what conditions would each of your strategies work well a. For each employee in department retrieve the employee name and the names of the employee’s dependents. b. Print the names of all employees who work in department but who work on some project not controlled by department Exercises Consider the following relations BOOKS BOOKSTORE STOCK Totalstock is the total number of books in stock and Inventoryvalue is the total inventory value for the store in dollars. a. Give an example of two simple predicates that would be meaningful for the BOOKSTORE relation for horizontal partitioning. b. How would a derived horizontal partitioning of STOCK be defined based on the partitioning of BOOKSTORE c. Show predicates by which BOOKS may be horizontally partitioned by topic. d. Show how the STOCK may be further partitioned from the partitions in by adding the predicates in . Consider a distributed database for a bookstore chain called National Books with three sites called EAST MIDDLE and WEST. The relation schemas are given in Exercise Consider that BOOKS are fragmented by $price amounts into $price up to $price from to $price from to $price and above Similarly BOOKSTORES are divided by ZIP Codes into EAST Zip up to MIDDLE Zip to WEST Zip to Assume that STOCK is a derived fragment based on BOOKSTORE only. a. Consider the query SELECT Book# Totalstock FROM Books WHERE $price AND $price Assume that fragments of BOOKSTORE are nonreplicated and assigned based on region. Assume further that BOOKS are allocated as EAST MIDDLE WEST Assuming the query was submitted in EAST what remote subqueries does it generate b. If the price of Book# is updated from to at site MIDDLE what updates does that generate Write in English and then in SQL. Chapter Distributed Databases c. Give a sample query issued at WEST that will generate a subquery for MIDDLE. d. Write a query involving selection and projection on the above relations and show two possible query trees that denote different ways of execution. Consider that you have been asked to propose a database architecture in a large organization to consolidate all data including legacy databases as well as relational databases which are geographically distributed so that global applications can be supported. Assume that alternative one is to keep all databases as they are while alternative two is to first convert them to relational and then support the applications over a distributed integrated database. a. Draw two schematic diagrams for the above alternatives showing the linkages among appropriate schemas. For alternative one choose the approach of providing export schemas for each database and constructing unified schemas for each application. b. List the steps that you would have to go through under each alternative from the present situation until global applications are viable. c. Compare these from the issues of i. design time considerations ii. runtime considerations Selected Bibliography The textbooks by Ceri and Pelagatti and Ozsu and Valduriez are devoted to distributed databases. Peterson and Davie Tannenbaum and Stallings cover data communications and computer networks. Comer discusses networks and internets. Ozsu et al. has a collection of papers on distributed object management. Most of the research on distributed database design query processing and optimization occurred in the and we quickly review the important references here. Distributed database design has been addressed in terms of horizontal and vertical fragmentation allocation and replication. Ceri et al. defined the concept of minterm horizontal fragments. Ceri et al. developed an integer programming based optimization model for horizontal fragmentation and allocation. Navathe et al. developed algorithms for vertical fragmentation based on attribute affinity and showed a variety of contexts for vertical fragment allocation. Wilson and Navathe present an analytical model for optimal allocation of fragments. Elmasri et al. discuss fragmentation for the ECR model Karlapalem et al. discuss issues for distributed design of object databases. Navathe et al. discuss mixed fragmentation by combining horizontal and Selected Bibliography vertical fragmentation Karlapalem et al. present a model for redesign of distributed databases. Distributed query processing optimization and decomposition are discussed in Hevner and Yao Kerschberg et al. Apers et al. Ceri and Pelagatti and Bodorick et al. Bernstein and Goodman discuss the theory behind semijoin processing. Wong discusses the use of relationships in relation fragmentation. Concurrency control and recovery schemes are discussed in Bernstein and Goodman Kumar and Hsu compiles some articles related to recovery in distributed databases. Elections in distributed systems are discussed in Garcia Molina Lamport discusses problems with generating unique timestamps in a distributed system. Rahimi and Haug discuss a more flexible way to construct query critical metadata for databases. Ouzzani and Bouguettaya outline fundamental problems in distributed query processing over Web based data sources. A concurrency control technique for replicated data that is based on voting is presented by Thomas Gifford proposes the use of weighted voting and Paris describes a method called voting with witnesses. Jajodia and Mutchler discuss dynamic voting. A technique called available copy is proposed by Bernstein and Goodman and one that uses the idea of a group is presented in ElAbbadi and Toueg Other work that discusses replicated data includes Gladney Agrawal and ElAbbadi ElAbbadi and Toueg Kumar and Segev Mukkamala and Wolfson and Milo Bassiouni discusses optimistic protocols for DDB concurrency control. Garcia Molina and Kumar and Stonebraker discuss techniques that use the semantics of the transactions. Distributed concurrency control techniques based on locking and distinguished copies are presented by Menasce et al. and Minoura and Wiederhold Obermark presents algorithms for distributed deadlock detection. In more recent work Vadivelu et al. propose using backup mechanism and multilevel security to develop algorithms for improving concurrency. Madria et al. propose a mechanism based on a multiversion two phase locking scheme and timestamping to address concurrency issues specific to mobile database systems. Boukerche and Tuck propose a technique that allows transactions to be out of order to a limited extent. They attempt to ease the load on the application developer by exploiting the network environment and producing a schedule equivalent to a temporally ordered serial schedule. Han et al. propose a deadlock free and serializable extended Petri net model for Webbased distributed real time databases. A survey of recovery techniques in distributed systems is given by Kohler Reed discusses atomic actions on distributed data. Bhargava presents an edited compilation of various approaches and techniques for concurrency and reliability in distributed systems. Federated database systems were first defined in McLeod and Heimbigner Techniques for schema integration in federated databases are presented by Elmasri et al. Batini et al. Hayne and Ram and Motro Chapter Distributed Databases Elmagarmid and Helal and Gamal Eldin et al. discuss the update problem in heterogeneous DDBSs. Heterogeneous distributed database issues are discussed in Hsiao and Kamel Sheth and Larson present an exhaustive survey of federated database management. Since late multidatabase systems and interoperability have become important topics. Techniques for dealing with semantic incompatibilities among multiple databases are examined in DeMichiel Siegel and Madnick Krishnamurthy et al. and Wang and Madnick Castano et al. present an excellent survey of techniques for analysis of schemas. Pitoura et al. discuss object orientation in multidatabase systems. Xiao et al. propose an XML based model for a common data model for multidatabase systems and present a new approach for schema mapping based on this model. Lakshmanan et al. propose extending SQL for interoperability and describe the architecture and algorithms for achieving the same. Transaction processing in multidatabases is discussed in Mehrotra et al. Georgakopoulos et al. Elmagarmid et al. and Brietbart et al. among others. Elmagarmid discuss transaction processing for advanced applications including engineering applications discussed in Heiler et al. The workflow systems which are becoming popular to manage information in complex organizations use multilevel and nested transactions in conjunction with distributed databases. Weikum discusses multilevel transaction management. Alonso et al. discuss limitations of current workflow systems. Lopes et al. propose that users define and execute their own workflows using a clientside Web browser. They attempt to leverage Web trends to simplify the user’s work for workflow management. Jung and Yeom exploit data workflow to develop an improved transaction management system that provides simultaneous transparent access to the heterogeneous storages that constitute the HVEM DataGrid. Deelman and Chervanak list the challenges in data intensive scientific workflows. Specifically they look at automated management of data efficient mapping techniques and user feedback issues in workflow mapping. They also argue for data reuse as an efficient means to manage data and present the challenges therein. A number of experimental distributed DBMSs have been implemented. These include distributed INGRES by Epstein et DDTS by Devor and Weeldreyer by Rothnie et System R by Lindsay et SIRIUS DELTA by Ferrier and Stangret and MULTIBASE by Smith et The OMNIBASE system by Rusinkiewicz et al. and the Federated Information Base developed using the Candide data model by Navathe et al. are examples of federated DDBMSs. Pitoura et al. present a comparative survey of the federated database system prototypes. Most commercial DBMS vendors have products using the client server approach and offer distributed versions of their systems. Some system issues concerning client server DBMS architectures are discussed in Carey et al. DeWitt et al. and Wang and Rowe Khoshafian et al. discuss design issues for relational DBMSs in the client server environment. Client server management issues are discussed in many books such as Zantinge and Adriaans Di Stefano discusses data distribution issues specific to grid computing. A major part of this discussion may also apply to cloud computing. Selected Bibliography This page intentionally left blank Advanced Database Models Systems and Applications This page intentionally left blank Enhanced Data Models for Advanced Applications As the use of database systems has grown users have demanded additional functionality from these software packages with the purpose of making it easier to implement more advanced and complex user applications. Object oriented databases and objectrelational systems do provide features that allow users to extend their systems by specifying additional abstract data types for each application. However it is quite useful to identify certain common features for some of these advanced applications and to create models that can represent them. Additionally specialized storage structures and indexing methods can be implemented to improve the performance of these common features. Then the features can be implemented as abstract data types or class libraries and purchased separately from the basic DBMS software package. The term data blade has been used in Informix and cartridge in Oracle to refer to such optional submodules that can be included in a DBMS package. Users can utilize these features directly if they are suitable for their applications without having to reinvent reimplement and reprogram such common features. This chapter introduces database concepts for some of the common features that are needed by advanced applications and are being used widely. We will cover active rules that are used in active database applications temporal concepts that are used in temporal database applications and briefly some of the issues involving spatial databases and multimedia databases. We will also discuss deductive databases. It is important to note that each of these topics is very broad and we give only a brief introduction to each. In fact each of these areas can serve as the sole topic of a complete book. In Section we introduce the topic of active databases which provide additional functionality for specifying active rules. These rules can be automatically triggered chapter Chapter Enhanced Data Models for Advanced Applications by events that occur such as database updates or certain times being reached and can initiate certain actions that have been specified in the rule declaration to occur if certain conditions are met. Many commercial packages include some of the functionality provided by active databases in the form of triggers. Triggers are now part of the and later standards. In Section we introduce the concepts of temporal databases which permit the database system to store a history of changes and allow users to query both current and past states of the database. Some temporal database models also allow users to store future expected information such as planned schedules. It is important to note that many database applications are temporal but they are often implemented without having much temporal support from the DBMS package that is the temporal concepts are implemented in the application programs that access the database. Section gives a brief overview of spatial database concepts. We discuss types of spatial data different kinds of spatial analyses operations on spatial data types of spatial queries spatial data indexing spatial data mining and applications of spatial databases. Section is devoted to multimedia database concepts. Multimedia databases provide features that allow users to store and query different types of multimedia information which includes images video clips audio clips and documents . We discuss automatic analysis of images object recognition in images and semantic tagging of images In Section we discuss deductive an area that is at the intersection of databases logic and artificial intelligence or knowledge bases. A deductive database system includes capabilities to define rules which can deduce or infer additional information from the facts that are stored in a database. Because part of the theoretical foundation for some deductive database systems is mathematical logic such rules are often referred to as logic databases. Other types of systems referred to as expert database systems or knowledge based systems also incorporate reasoning and inferencing capabilities such systems use techniques that were developed in the field of artificial intelligence including semantic networks frames production systems or rules for capturing domain specific knowledge. Section summarizes the chapter. Readers may choose to peruse the particular topics they are interested in as the sections in this chapter are practically independent of one another. is a summary of Deductive Databases. The full chapter from the third edition which provides a more comprehensive introduction is available on the book’s Web site. Active Database Concepts and Triggers Active Database Concepts and Triggers Rules that specify actions that are automatically triggered by certain events have been considered important enhancements to database systems for quite some time. In fact the concept of triggers a technique for specifying certain types of active rules has existed in early versions of the SQL specification for relational databases and triggers are now part of the and later standards. Commercial relational DBMSs such as Oracle and Microsoft SQLServer have various versions of triggers available. However much research into what a general model for active databases should look like has been done since the early models of triggers were proposed. In Section we will present the general concepts that have been proposed for specifying rules for active databases. We will use the syntax of the Oracle commercial relational DBMS to illustrate these concepts with specific examples since Oracle triggers are close to the way rules are specified in the SQL standard. Section will discuss some general design and implementation issues for active databases. We give examples of how active databases are implemented in the STARBURST experimental DBMS in Section since STARBURST provides for many of the concepts of generalized active databases within its framework. Section discusses possible applications of active databases. Finally Section describes how triggers are declared in the standard. Generalized Model for Active Databases and Oracle Triggers The model that has been used to specify active database rules is referred to as the Event Condition Action model. A rule in the ECA model has three components The event that triggers the rule These events are usually database update operations that are explicitly applied to the database. However in the general model they could also be temporal or other kinds of external events. The condition that determines whether the rule action should be executed Once the triggering event has occurred an optional condition may be evaluated. If no condition is specified the action will be executed once the event occurs. If a condition is specified it is first evaluated and only if it evaluates to true will the rule action be executed. The action to be taken The action is usually a sequence of SQL statements but it could also be a database transaction or an external program that will be automatically executed. Let us consider some examples to illustrate these concepts. The examples are based on a much simplified variation of the COMPANY database application from Figure and is shown in Figure with each employee having a name Social example would be a temporal event specified as a periodic time such as Trigger this rule every day at . Chapter Enhanced Data Models for Advanced Applications Name Ssn Salary Dno Supervisorssn EMPLOYEE Dname Dno Totalsal Managerssn DEPARTMENT Figure A simplified COMPANY database used for active rule examples. Security number salary department to which they are currently assigned and a direct supervisor foreign key to EMPLOYEE . For this example we assume that NULL is allowed for Dno indicating that an employee may be temporarily unassigned to any department. Each department has a name number the total salary of all employees assigned to the department and a manager . Notice that the Totalsal attribute is really a derived attribute whose value should be the sum of the salaries of all employees who are assigned to the particular department. Maintaining the correct value of such a derived attribute can be done via an active rule. First we have to determine the events that may cause a change in the value of Totalsal which are as follows Inserting new employee tuples Changing the salary of existing employees Changing the assignment of existing employees from one department to another Deleting employee tuples In the case of event we only need to recompute Totalsal if the new employee is immediately assigned to a department that is if the value of the Dno attribute for the new employee tuple is not NULL . Hence this would be the condition to be checked. A similar condition could be checked for event is currently assigned to a department. For event we will always execute an action to maintain the value of Totalsal correctly so no condition is needed . The action for events and is to automatically update the value of Totalsal for the employee’s department to reflect the newly inserted updated or deleted employee’s salary. In the case of event a twofold action is needed one to update the Totalsal of the employee’s old department and the other to update the Totalsal of the employee’s new department. The four active rules and to the above situation can be specified in the notation of the Oracle DBMS as shown in Figure Let us consider rule to illustrate the syntax of creating triggers in Oracle. Active Database Concepts and Triggers CREATE TRIGGER AFTER INSERT ON EMPLOYEE FOR EACH ROW WHEN UPDATE DEPARTMENT SET Totalsal Totalsal + WHERE Dno CREATE TRIGGER AFTER UPDATE OF Salary ON EMPLOYEE FOR EACH ROW WHEN UPDATE DEPARTMENT SET Totalsal Totalsal + – WHERE Dno CREATE TRIGGER AFTER UPDATE OF Dno ON EMPLOYEE FOR EACH ROW BEGIN UPDATE DEPARTMENT SET Totalsal Totalsal + WHERE Dno UPDATE DEPARTMENT SET Totalsal Totalsal – WHERE Dno END CREATE TRIGGER AFTER DELETE ON EMPLOYEE FOR EACH ROW WHEN UPDATE DEPARTMENT SET Totalsal Totalsal – WHERE Dno CREATE TRIGGER BEFORE INSERT OR UPDATE OF Salary Supervisorssn ON EMPLOYEE FOR EACH ROW WHEN informsupervisor Figure Specifying active rules as triggers in Oracle notation. Triggers for automatically maintaining the consistency of Totalsal of DEPARTMENT. Trigger for comparing an employee’s salary with that of his or her supervisor. Chapter Enhanced Data Models for Advanced Applications The CREATE TRIGGER statement specifies a trigger name for The AFTER clause specifies that the rule will be triggered after the events that trigger the rule occur. The triggering events an insert of a new employee in this example are specified following the AFTER The ON clause specifies the relation on which the rule is specified EMPLOYEE for The optional keywords FOR EACH ROW specify that the rule will be triggered once for each row that is affected by the triggering The optional WHEN clause is used to specify any conditions that need to be checked after the rule is triggered but before the action is executed. Finally the action to be taken is specified as a PL SQL block which typically contains one or more SQL statements or calls to execute external procedures. The four triggers and illustrate a number of features of active rules. First the basic events that can be specified for triggering the rules are the standard SQL update commands INSERT DELETE and UPDATE. They are specified by the keywords INSERT DELETE and UPDATE in Oracle notation. In the case of UPDATE one may specify the attributes to be updated for example by writing UPDATE OF Salary Dno. Second the rule designer needs to have a way to refer to the tuples that have been inserted deleted or modified by the triggering event. The keywords NEW and OLD are used in Oracle notation NEW is used to refer to a newly inserted or newly updated tuple whereas OLD is used to refer to a deleted tuple or to a tuple before it was updated. Thus rule is triggered after an INSERT operation is applied to the EMPLOYEE relation. In the condition is checked and if it evaluates to true meaning that the newly inserted employee tuple is related to a department then the action is executed. The action updates the DEPARTMENT tuple related to the newly inserted employee by adding their salary to the Totalsal attribute of their related department. Rule is similar to but it is triggered by an UPDATE operation that updates the SALARY of an employee rather than by an INSERT. Rule is triggered by an update to the Dno attribute of EMPLOYEE which signifies changing an employee’s assignment from one department to another. There is no condition to check in so the action is executed whenever the triggering event occurs. The action updates both the old department and new department of the reassigned employees by adding their salary to Totalsal of their new department and subtracting their salary from Totalsal of their old department. Note that this should work even if the value of Dno is NULL because in this case no department will be selected for the rule we will see it is also possible to specify BEFORE instead of AFTER which indicates that the rule is triggered before the triggering event is executed. we will see that an alternative is to trigger the rule only once even if multiple rows are affected by the triggering event. and can also be written without a condition. However it may be more efficient to execute them with the condition since the action is not invoked unless it is required. Active Database Concepts and Triggers trigger CREATE TRIGGER trigger name triggering events ON table name [ FOR EACH ROW ] [ WHEN condition ] trigger actions triggering events trigger event OR trigger event trigger event INSERT I DELETE I UPDATE [ OF column name column name ] trigger action PL SQL block Figure A syntax summary for specifying triggers in the Oracle system . It is important to note the effect of the optional FOR EACH ROW clause which signifies that the rule is triggered separately for each tuple. This is known as a row level trigger. If this clause was left out the trigger would be known as a statement level trigger and would be triggered once for each triggering statement. To see the difference consider the following update operation which gives a percent raise to all employees assigned to department This operation would be an event that triggers rule UPDATE EMPLOYEE SET Salary Salary WHERE Dno Because the above statement could update multiple records a rule using row level semantics such as in Figure would be triggered once for each row whereas a rule using statement level semantics is triggered only once. The Oracle system allows the user to choose which of the above options is to be used for each rule. Including the optional FOR EACH ROW clause creates a row level trigger and leaving it out creates a statement level trigger. Note that the keywords NEW and OLD can only be used with row level triggers. As a second example suppose we want to check whenever an employee’s salary is greater than the salary of his or her direct supervisor. Several events can trigger this rule inserting a new employee changing an employee’s salary or changing an employee’s supervisor. Suppose that the action to take would be to call an external procedure informsupervisor which will notify the supervisor. The rule could then be written as in can specify a set of tuples one has to distinguish between whether the rule should be considered once for the whole statement or whether it should be considered separately for each row affected by the statement. The standard Figure An example to illustrate the termination problem for active rules. also allows the user to start rule consideration explicitly via a PROCESS RULES command. Chapter Enhanced Data Models for Advanced Applications problem briefly consider the rules in Figure Here rule is triggered by an INSERT event on and its action includes an update event on of However rule triggering event is an UPDATE event on of and its action includes an INSERT event on In this example it is easy to see that these two rules can trigger one another indefinitely leading to nontermination. However if dozens of rules are written it is very difficult to determine whether termination is guaranteed or not. If active rules are to reach their potential it is necessary to develop tools for the design debugging and monitoring of active rules that can help users design and debug their rules. Examples of Statement Level Active Rules in STARBURST We now give some examples to illustrate how rules can be specified in the STARBURST experimental DBMS. This will allow us to demonstrate how statement level rules can be written since these are the only types of rules allowed in STARBURST. The three active rules and in Figure correspond to the first three rules in Figure but they use STARBURST notation and statement level semantics. We can explain the rule structure using rule The CREATE RULE statement specifies a rule for The ON clause specifies the relation on which the rule is specified EMPLOYEE for The WHEN clause is used to specify the events that trigger the The optional IF clause is used to specify any conditions that need to be checked. Finally the THEN clause is used to specify the actions to be taken which are typically one or more SQL statements. In STARBURST the basic events that can be specified for triggering the rules are the standard SQL update commands INSERT DELETE and UPDATE. These are specified by the keywords INSERTED DELETED and UPDATED in STARBURST notation. Second the rule designer needs to have a way to refer to the tuples that have been modified. The keywords INSERTED DELETED NEW UPDATED and OLDUPDATED are used in STARBURST notation to refer to four transition tables that include the newly inserted tuples the deleted tuples the updated tuples before they were updated and the updated tuples after they were updated respectively. Obviously depending on the triggering events only some of these transition tables may be available. The rule writer can refer to these tables when writing the condition and action parts of the rule. Transition tables contain tuples of the same type as those in the relation specified in the ON clause of the rule for and this is the EMPLOYEE relation. In statement level semantics the rule designer can only refer to the transition tables as a whole and the rule is triggered only once so the rules must be written differently than for row level semantics. Because multiple employee tuples may be that the WHEN keyword specifies events in STARBURST but is used to specify the rule condition in SQL and Oracle triggers. Active Database Concepts and Triggers CREATE RULE ON EMPLOYEE WHEN INSERTED IF EXISTS THEN UPDATE DEPARTMENT AS D SET + FROM INSERTED AS I WHERE WHERE IN CREATE RULE ON EMPLOYEE WHEN UPDATED IF EXISTS OR EXISTS THEN UPDATE DEPARTMENT AS D SET + FROM NEW UPDATED AS N WHERE – FROM OLD UPDATED AS O WHERE WHERE IN OR IN CREATE RULE ON EMPLOYEE WHEN UPDATED THEN UPDATE DEPARTMENT AS D SET + FROM NEW UPDATED AS N WHERE WHERE IN UPDATE DEPARTMENT AS D SET Totalsal – FROM OLD UPDATED AS O WHERE WHERE IN Figure Active rules using statement level semantics in STARBURST notation. inserted in a single insert statement we have to check if at least one of the newly inserted employee tuples is related to a department. In the condition EXISTS is checked and if it evaluates to true then the action is executed. The action updates in a single statement the DEPARTMENT tuple related to the newly inserted employee by adding their salaries to the Totalsal attribute of each related department. Because more than one newly inserted employee may belong to the same Chapter Enhanced Data Models for Advanced Applications department we use the SUM aggregate function to ensure that all their salaries are added. Rule is similar to but is triggered by an UPDATE operation that updates the salary of one or more employees rather than by an INSERT. Rule is triggered by an update to the Dno attribute of EMPLOYEE which signifies changing one or more employees’ assignment from one department to another. There is no condition in so the action is executed whenever the triggering event The action updates both the old department and new department of the reassigned employees by adding their salary to Totalsal of each new department and subtracting their salary from Totalsal of each old department. In our example it is more complex to write the statement level rules than the rowlevel rules as can be illustrated by comparing Figures and However this is not a general rule and other types of active rules may be easier to specify when using statement level notation than when using row level notation. The execution model for active rules in STARBURST uses deferred consideration. That is all the rules that are triggered within a transaction are placed in a set called the conflict set which is not considered for evaluation of conditions and execution until the transaction ends . STARBURST also allows the user to explicitly start rule consideration in the middle of a transaction via an explicit PROCESS RULES command. Because multiple rules must be evaluated it is necessary to specify an order among the rules. The syntax for rule declaration in STARBURST allows the specification of ordering among the rules to instruct the system about the order in which a set of rules should be Additionally the transition tables INSERTED DELETED NEW UPDATED and OLD UPDATED contain the net effect of all the operations within the transaction that affected each table since multiple operations may have been applied to each table during the transaction. Potential Applications for Active Databases We now briefly discuss some of the potential applications of active rules. Obviously one important application is to allow notification of certain conditions that occur. For example an active database may be used to monitor say the temperature of an industrial furnace. The application can periodically insert in the database the temperature reading records directly from temperature sensors and active rules can be written that are triggered whenever a temperature record is inserted with a condition that checks if the temperature exceeds the danger level and results in the action to raise an alarm. in the Oracle examples rules and can be written without a condition. However it may be more efficient to execute them with the condition since the action is not invoked unless it is required. no order is specified between a pair of rules the system default order is based on placing the rule declared first ahead of the other rule. Temporal Database Concepts Active rules can also be used to enforce integrity constraints by specifying the types of events that may cause the constraints to be violated and then evaluating appropriate conditions that check whether the constraints are actually violated by the event or not. Hence complex application constraints often known as business rules may be enforced that way. For example in the UNIVERSITY database application one rule may monitor the GPA of students whenever a new grade is entered and it may alert the advisor if the GPA of a student falls below a certain threshold another rule may check that course prerequisites are satisfied before allowing a student to enroll in a course and so on. Other applications include the automatic maintenance of derived data such as the examples of rules through that maintain the derived attribute Totalsal whenever individual employee tuples are changed. A similar application is to use active rules to maintain the consistency of materialized views O and N to refer to the OLD tuple and NEW tuple respectively. Trigger in Figure shows how the statement level trigger from Figure may be specified in For a statement level trigger the REFERENCING clause is used to refer to the table of all new tuples as N whereas the table of all old tuples is referred to as O. Temporal Database Concepts Temporal databases in the broadest sense encompass all database applications that require some aspect of time when organizing their information. Hence they provide a good example to illustrate the need for developing a set of unifying concepts for application developers to use. Temporal database applications have been Chapter Enhanced Data Models for Advanced Applications developed since the early days of database usage. However in creating these applications it is mainly left to the application designers and developers to discover design program and implement the temporal concepts they need. There are many examples of applications where some aspect of time is needed to maintain the information in a database. These include healthcare where patient histories need to be maintained insurance where claims and accident histories are required as well as information about the times when insurance policies are in effect reservation systems in general where information on the dates and times when reservations are in effect are required scientific databases where data collected from experiments includes the time when each data is measured and so on. Even the two examples used in this book may be easily expanded into temporal applications. In the COMPANY database we may wish to keep SALARY JOB and PROJECT histories on each employee. In the UNIVERSITY database time is already included in the SEMESTER and YEAR of each SECTION of a COURSE the grade history of a STUDENT and the information on research grants. In fact it is realistic to conclude that the majority of database applications have some temporal information. However users often attempt to simplify or ignore temporal aspects because of the complexity that they add to their applications. In this section we will introduce some of the concepts that have been developed to deal with the complexity of temporal database applications. Section gives an overview of how time is represented in databases the different types of temporal CREATE TRIGGER AFTER UPDATE OF Salary ON EMPLOYEE REFERENCING OLD ROW AS O NEW ROW AS N FOR EACH ROW WHEN UPDATE DEPARTMENT SET Totalsal Totalsal + – WHERE Dno CREATE TRIGGER AFTER UPDATE OF Salary ON EMPLOYEE REFERENCING OLD TABLE AS O NEW TABLE AS N FOR EACH STATEMENT WHEN EXISTS OR EXISTS UPDATE DEPARTMENT AS D SET + FROM N WHERE – FROM O WHERE WHERE Dno IN UNION Figure Trigger illustrating the syntax for defining triggers in Temporal Database Concepts information and some of the different dimensions of time that may be needed. Section discusses how time can be incorporated into relational databases. Section gives some additional options for representing time that are possible in database models that allow complex structured objects such as object databases. Section introduces operations for querying temporal databases and gives a brief overview of the language which extends SQL with temporal concepts. Section focuses on time series data which is a type of temporal data that is very important in practice. Time Representation Calendars and Time Dimensions For temporal databases time is considered to be an ordered sequence of points in some granularity that is determined by the application. For example suppose that some temporal application never requires time units that are less than one second. Then each time point represents one second using this granularity. In reality each second is a time duration not a point since it may be further divided into milliseconds microseconds and so on. Temporal database researchers have used the term chronon instead of point to describe this minimal granularity for a particular application. The main consequence of choosing a minimum granularity say one second is that events occurring within the same second will be considered to be simultaneous events even though in reality they may not be. Because there is no known beginning or ending of time one needs a reference point from which to measure specific time points. Various calendars are used by various cultures Chinese Islamic Hindu Jewish Coptic and so on with different reference points. A calendar organizes time into different time units for convenience. Most calendars group seconds into a minute minutes into an hour hours into a day and days into a week. Further grouping of days into months and months into years either follow solar or lunar natural phenomena and are generally irregular. In the Gregorian calendar which is used in most western countries days are grouped into months that are or days and months are grouped into a year. Complex formulas are used to map the different time units to one another. In the temporal data types TIME TIMESTAMP INTERVAL and PERIOD Information. A temporal database will store information concerning when certain events occur or when certain facts are considered to be true. There are several different types of temporal information. Point events or facts are typically associated in the database with a single time point in some granularity. For example a bank deposit event may be associated with the timestamp when the deposit was made or the total monthly sales of a product may be associated with a particular month Name EMPVT Ssn Salary Dno Supervisorssn Vst Vet Name Salary Supervisorssn Ssn TstTet Dname DEPTVT EMPTT Dname Totalsal Managerssn Dno Dno Tst Tet DEPTTT Dno Totalsal Managerssn Vst Vet Name Salary Supervisorssn Ssn Dno Tst Tet EMPBT Dname Totalsal Managerssn Dno Tst Tet DEPTBT Vst Vet Vst Vet Figure Different types of temporal relational databases. Valid time database schema. Transaction time database schema. Bitemporal database schema. other interpretations are intended for time the user can define the semantics and program the applications appropriately and it is called a user defined time. The next section shows how these concepts can be incorporated into relational databases and Section shows an approach to incorporate temporal concepts into object databases. Incorporating Time in Relational Databases Using Tuple Versioning Valid Time Relations. Let us now see how the different types of temporal databases may be represented in the relational model. First suppose that we would like to include the history of changes as they occur in the real world. Consider again the database in Figure and let us assume that for this application the granularity is day. Then we could convert the two relations EMPLOYEE and DEPARTMENT into valid time relations by adding the attributes Vst and Vet whose data type is DATE in order to provide day granularity. This is shown in Figure where the relations have been renamed EMPVT and DEPTVT respectively. Consider how the EMPVT relation differs from the nontemporal EMPLOYEE relation only during the time period [ ] whereas in EMPLOYEE each tuple represents only the current state or current version of each employee. In EMPVT the current version of each employee typically has a special value now as its valid end time. This special value now is a temporal variable that implicitly represents the current time as time progresses. The nontemporal EMPLOYEE relation would only include those tuples from the EMPVT relation whose Vet is now. Figure shows a few tuple versions in the valid time relations EMPVT and DEPTVT. There are two versions of Smith three versions of Wong one version of Brown and one version of Narayan. We can now see how a valid time relation should behave when information is changed. Whenever one or more attributes of an employee are updated rather than actually overwriting the old values as would happen in a nontemporal relation the system should create a new version and close the current version by changing its Vet to the end time. Hence when the user issued the command to update the salary of Smith effective on June to the second version of Smith was created to indicate that the version has become a closed or history version and that the new version of Smith is now the current one. Temporal Database Concepts It is important to note that in a valid time relation the user must generally provide the valid time of an update. For example the salary update of Smith may have been entered in the database on May at . say even though the salary change in the real world is effective on June This is called a proactive update since it is applied to the database before it becomes effective in the real world. If the update is applied to the database after it becomes effective in the real world it is called a retroactive update. An update that is applied at the same time as it becomes effective is called a simultaneous update. The action that corresponds to deleting an employee in a nontemporal database would typically be applied to a valid time database by closing the current version of the employee being deleted. For example if Smith leaves the company effective January then this would be applied by changing Vet of the current version of Smith from now to In Figure there is no current version for Brown because he presumably left the company on and was logically deleted. However because the database is temporal the old information on Brown is still there. The operation to insert a new employee would correspond to creating the first tuple version for that employee and making it the current version with the Vst being the effective time when the employee starts work. In Figure the tuple on Narayan illustrates this since the first version has not been updated yet. Notice that in a valid time relation the nontemporal key such as Ssn in EMPLOYEE is no longer unique in each tuple . The new relation key for EMPVT is a combination of the nontemporal key and the valid start time attribute Vst so we use as primary key. This is because at any point in time there should be at most one valid version of each entity. Hence the constraint that any two tuple versions representing the same entity should have nonintersecting valid time periods should hold on valid time relations. Notice that if the nontemporal primary key value may change over time it is important to have a unique surrogate key attribute whose value never changes for each real world entity in order to relate all versions of the same real world entity. Valid time relations basically keep track of the history of changes as they become effective in the real world. Hence if all real world changes are applied the database keeps a history of the real world states that are represented. However because updates insertions and deletions may be applied retroactively or proactively there is no record of the actual database state at any point in time. If the actual database states are important to an application then one should use transaction time relations. Transaction Time Relations. In a transaction time database whenever a change is applied to the database the actual timestamp of the transaction that applied the change is recorded. Such a database is most useful when changes are applied simultaneously in the majority of cases for example real time stock trading or banking transactions. If we convert the nontemporal database in combination of the nontemporal key and the valid end time attribute Vet could also be used. Chapter Enhanced Data Models for Advanced Applications Figure into a transaction time database then the two relations EMPLOYEE and DEPARTMENT are converted into transaction time relations by adding the attributes Tst and Tet whose data type is typically TIMESTAMP. This is shown in Figure where the relations have been renamed EMPTT and DEPTTT respectively. In EMPTT each tuple V represents a version of an employee’s information that was created at actual time and was removed at actual time . In EMPTT the current version of each employee typically has a special value uc as its transaction end time which indicates that the tuple represents correct information until it is changed by some other A transaction time database has also been called a rollback database because a user can logically roll back to the actual database state at any past point in time T by retrieving all tuple versions V whose transaction time period [ ] includes time point T. Bitemporal Relations. Some applications require both valid time and transaction time leading to bitemporal relations. In our example Figure shows how the EMPLOYEE and DEPARTMENT nontemporal relations in Figure would appear as bitemporal relations EMPBT and DEPTBT respectively. Figure shows a few tuples in these relations. In these tables tuples whose transaction end time Tet is uc are the ones representing currently valid information whereas tuples whose Tet is an absolute timestamp are tuples that were valid until that timestamp. Hence the tuples with uc in Figure correspond to the valid time tuples in Figure The transaction start time attribute Tst in each tuple is the timestamp of the transaction that created that tuple. Now consider how an update operation would be implemented on a bitemporal relation. In this model of bitemporal no attributes are physically changed in any tuple except for the transaction end time attribute Tet with a value of To illustrate how tuples are created consider the EMPBT relation. The current version V of an employee has uc in its Tet attribute and now in its Vet attribute. If some attribute say Salary is updated then the transaction T that performs the update should have two parameters the new value of Salary and the valid time VT when the new salary becomes effective . Assume that VT− is the uc variable in transaction time relations corresponds to the now variable in valid time relations. The semantics are slightly different though. the term rollback does not have the same meaning as transaction rollback . Then the following physical changes would be applied to the EMPBT table Make a copy of the current version V set to VT− to TS to uc and insert in EMPBT is a copy of the previous current version V after it is closed at valid time VT−. Make a copy of the current version V set to VT to now to the new salary value to TS to uc and insert in EMPBT represents the new current version. Set to TS since the current version is no longer representing correct information. As an illustration consider the first three tuples and in EMPBT in Figure Before the update of Smith’s salary from to only was in EMPBT and it was the current version and its Tet was uc. Then a transaction T whose timestamp TS is updates the salary to with the effective valid time of The tuple is created which is a copy of except that its Vet is set to one day less than the new valid time and its Tst is the timestamp of the updating transaction. The tuple is also created which has the new salary its Vst is set to and its Tst is also the timestamp of the updating transaction. Finally the Tet of is set to the timestamp of Chapter Enhanced Data Models for Advanced Applications the updating transaction Note that this is a retroactive update since the updating transaction ran on June but the salary change is effective on June Similarly when Wong’s salary and department are updated to and the updating transaction’s timestamp is and the effective valid time for the update is Hence this is a proactive update because the transaction ran on January but the effective date was February In this case tuple is logically replaced by and Next let us illustrate how a delete operation would be implemented on a bitemporal relation by considering the tuples and in the EMPBT relation of Figure Here employee Brown left the company effective August and the logical delete is carried out by a transaction T with TS Before this was the current version of Brown and its Tet was uc. The logical delete is implemented by setting to to invalidate it and creating the final version for Brown with its Vet and all other tuples would be in another relation. This allows the database administrator to have different access paths such as indexes for each relation and keeps the size of the current table reasonable. Another possibility is to create a third table for corrected tuples whose Tet is not uc. Another option that is available is to vertically partition the attributes of the temporal relation into separate relations so that if a relation has many attributes a whole new tuple version is created whenever any one of the attributes is updated. If the attributes are updated asynchronously each new version may differ in only one of the attributes thus needlessly repeating the other attribute values. If a separate relation is created to contain only the attributes that always change synchronously with the primary key replicated in each relation the database is said to be in temporal normal form. However to combine the information a variation of join known as temporal intersection join would be needed which is generally expensive to implement. It is important to note that bitemporal databases allow a complete record of changes. Even a record of corrections is possible. For example it is possible that two tuple versions of the same employee may have the same valid time but different attribute values as long as their transaction times are disjoint. In this case the tuple with the later transaction time is a correction of the other tuple version. Even incorrectly entered valid times may be corrected this way. The incorrect state of the data Temporal Database Concepts base will still be available as a previous database state for querying purposes. A database that keeps such a complete record of changes and corrections is sometimes called an append only database. Incorporating Time in Object Oriented Databases Using Attribute Versioning The previous section discussed the tuple versioning approach to implementing temporal databases. In this approach whenever one attribute value is changed a whole new tuple version is created even though all the other attribute values will be identical to the previous tuple version. An alternative approach can be used in database systems that support complex structured objects such as object databases notation for object databases . Each time varying attribute is represented as a list of tuples Validstarttime Validendtime Value ordered by valid start time. Whenever an attribute is changed in this model the current attribute version is closed and a new attribute version for this attribute only is appended to the list. This allows attributes to change asynchronously. The current value for each attribute has now for its Validendtime. When using attribute versioning it is useful to include a lifespan temporal attribute associated with the whole object whose value is one or more valid time periods that indicate the valid time of existence for the whole object. Logical deletion of the object is implemented by closing the lifespan. The constraint that any time period of an attribute within an object should be a subset of the object’s lifespan should be enforced. For bitemporal databases each attribute version would have a tuple with five components Validstarttime Validendtime Transstarttime Transendtime Value The object lifespan would also include both valid and transaction time dimensions. Therefore the full capabilities of bitemporal databases can be available with attribute versioning. Mechanisms similar to those discussed earlier for updating tuple versions can be applied to updating attribute versions. Chapter Enhanced Data Models for Advanced Applications class TEMPORALSALARY attribute Date Validstarttime attribute Date Validendtime attribute float Salary class TEMPORALDEPT attribute Date Validstarttime attribute Date Validendtime attribute DEPARTMENTVT Dept class TEMPORALSUPERVISOR attribute Date Validstarttime attribute Date Validendtime attribute EMPLOYEEVT Supervisor class TEMPORALLIFESPAN attribute Date Valid start time attribute Date Valid end time class EMPLOYEEVT attribute list TEMPORALLIFESPAN lifespan attribute string Name attribute string Ssn attribute list TEMPORALSALARY Salhistory attribute list TEMPORALDEPT Depthistory attribute list TEMPORALSUPERVISOR Supervisorhistory Figure Possible ODL schema for a temporal valid time EMPLOYEEVT object class using attribute versioning. Temporal Querying Constructs and the Language So far we have discussed how data models may be extended with temporal constructs. Now we give a brief overview of how query operations need to be extended for temporal querying. We will briefly discuss the language which extends SQL for querying valid time transaction time and bitemporal relational databases. In nontemporal relational databases the typical selection conditions involve attribute conditions and tuples that satisfy these conditions are selected from the set of Temporal Database Concepts current tuples. Following that the attributes of interest to the query are specified by a projection operation or temporal. The CREATE TABLE statement is extended with an optional AS clause to allow users to declare different temporal options. The following options are available AS VALID STATE GRANULARITY AS VALID EVENT GRANULARITY AS TRANSACTION AS VALID STATE GRANULARITY AND TRANSACTION AS VALID EVENT GRANULARITY AND TRANSACTION The keywords STATE and EVENT are used to specify whether a time period or time point is associated with the valid time dimension. In rather than have the user actually see how the temporal tables are implemented the language adds query language constructs to specify various types of temporal selections temporal projections temporal aggregations transformation among granularities and many other concepts. The book by Snodgrass et al. describes the language. Spatial Database Concepts Time Series Data Time series data is used very often in financial sales and economics applications. They involve data values that are recorded according to a specific predefined sequence of time points. Therefore they are a special type of valid event data where the event time points are predetermined according to a fixed calendar. Consider the example of closing daily stock prices of a particular company on the New York Stock Exchange. The granularity here is day but the days that the stock market is open are known . Hence it has been common to specify a computational procedure that calculates the particular calendar associated with a time series. Typical queries on time series involve temporal aggregation over higher granularity intervals for example finding the average or maximum weekly closing stock price or the maximum and minimum monthly closing stock price from the daily information. As another example consider the daily sales dollar amount at each store of a chain of stores owned by a particular company. Again typical temporal aggregates would be retrieving the weekly monthly or yearly sales from the daily sales information or comparing same store monthly sales with previous monthly sales and so on. Because of the specialized nature of time series data and the lack of support for it in older DBMSs it has been common to use specialized time series management systems rather than general purpose DBMSs for managing such information. In such systems it has been common to store time series values in sequential order in a file and apply specialized time series procedures to analyze the information. The problem with this approach is that the full power of high level querying in languages such as SQL will not be available in such systems. More recently some commercial DBMS packages are offering time series extensions such as the Oracle time cartridge and the time series data blade of Informix Universal Server. In addition the language provides some support for time series in the form of event tables. Spatial Database Introduction to Spatial Databases Spatial databases incorporate functionality that provides support for databases that keep track of objects in a multidimensional space. For example cartographic databases that store maps include two dimensional spatial descriptions of their objects from countries and states to rivers cities roads seas and so on. The systems that manage geographic data and related applications are known as contribution of Pranesh Parimala Ranganathan to this section is appreciated. Chapter Enhanced Data Models for Advanced Applications Table Common Types of Analysis for Spatial Data Analysis Type Type of Operations and Measurements Measurements Distance perimeter shape adjacency and direction Spatial analysis statistics Pattern autocorrelation and indexes of similarity and topology using spatial and nonspatial data Flow analysis Connectivity and shortest path Location analysis Analysis of points and lines within a polygon Terrain analysis Slope aspect catchment area drainage network Search Thematic search search by region Geographical Information Systems and they are used in areas such as environmental applications transportation systems emergency response systems and battle management. Other databases such as meteorological databases for weather information are three dimensional since temperatures and other meteorological information are related to three dimensional spatial points. In general a spatial database stores objects that have spatial characteristics that describe them and that have spatial relationships among them. The spatial relationships among the objects are important and they are often needed when querying the database. Although a spatial database can in general refer to an n dimensional space for any n we will limit our discussion to two dimensions as an illustration. A spatial database is optimized to store and query data related to objects in space including points lines and polygons. Satellite images are a prominent example of spatial data. Queries posed on these spatial data where predicates for selection deal with spatial parameters are called spatial queries. For example “What are the names of all bookstores within five miles of the College of Computing building at Georgia Tech ” is a spatial query. Whereas typical databases process numeric and character data additional functionality needs to be added for databases to process spatial data types. A query such as “List all the customers located within twenty miles of company headquarters” will require the processing of spatial data types typically outside the scope of standard relational algebra and may involve consulting an external geographic database that maps the company headquarters and each customer to a map based on their address. Effectively each customer will be associated to a latitude longitude position. A traditional B+ tree index based on customers’ zip codes or other nonspatial attributes cannot be used to process this query since traditional indexes are not capable of ordering multidimensional coordinate data. Therefore there is a special need for databases tailored for handling spatial data and spatial queries. Table shows the common analytical operations involved in processing geographic or spatial Measurement operations are used to measure some of GIS analysis operations as proposed in Albrecht Spatial Database Concepts global properties of single objects and to measure the relative position of different objects in terms of distance and direction. Spatial analysis operations which often use statistical techniques are used to uncover spatial relationships within and among mapped data layers. An example would be to create a map known as a prediction map that identifies the locations of likely customers for particular products based on the historical sales and demographic information. Flow analysis operations help in determining the shortest path between two points and also the connectivity among nodes or regions in a graph. Location analysis aims to find if the given set of points and lines lie within a given polygon . The process involves generating a buffer around existing geographic features and then identifying or selecting features based on whether they fall inside or outside the boundary of the buffer. Digital terrain analysis is used to build three dimensional models where the topography of a geographical location can be represented with an x y z data model known as Digital Terrain Model . The x and y dimensions of a DTM represent the horizontal plane and z represents spot heights for the respective x y coordinates. Such models can be used for analysis of environmental data or during the design of engineering projects that require terrain information. Spatial search allows a user to search for objects within a particular spatial region. For example thematic search allows us to search for objects related to a particular theme or class such as “Find all water bodies within miles of Atlanta” where the class is water. There are also topological relationships among spatial objects. These are often used in Boolean predicates to select objects based on their spatial relationships. For example if a city boundary is represented as a polygon and freeways are represented as multilines a condition such as “Find all freeways that go through Arlington Texas” would involve an intersects operation to determine which freeways intersect the city boundary . Spatial Data Types and Models This section briefly describes the common data types and models for storing spatial data. Spatial data comes in three basic forms. These forms have become a de facto standard due to their wide use in commercial systems. Map includes various geographic or spatial features of objects in a map such as an object’s shape and the location of the object within the map. The three basic types of features are points lines and polygons . Points are used to represent spatial characteristics of objects whose locations correspond to a single coordinate in the scale of a particular application. Depending on the scale some examples of point objects could be buildings cellular towers or stationary vehicles. Moving types of geographic data are based on ESRI’s guide to GIS. See .com implementinggis data Chapter Enhanced Data Models for Advanced Applications vehicles and other moving objects can be represented by a sequence of point locations that change over time. Lines represent objects having length such as roads or rivers whose spatial characteristics can be approximated by a sequence of connected lines. Polygons are used to represent spatial characteristics of objects that have a boundary such as countries states lakes or cities. Notice that some objects such as buildings or cities can be represented as either points or polygons depending on the scale of detail. Attribute data is the descriptive data that GIS systems associate with map features. For example suppose that a map contains features that represent counties within a US state . Attributes for each county feature could include population largest city town area in square miles and so on. Other attribute data could be included for other features in the map such as states cities congressional districts census tracts and so on. Image data includes data such as satellite images and aerial photographs which are typically created by cameras. Objects of interest such as buildings and roads can be identified and overlaid on these images. Images can also be attributes of map features. One can add images to other map features so that clicking on the feature would display the image. Aerial and satellite images are typical examples of raster data. Models of spatial information are sometimes grouped into two broad categories field and object. A spatial application is modeled using either a field or an object based model depending on the requirements and the traditional choice of model for the application. Field models are often used to model spatial data that is continuous in nature such as terrain elevation temperature data and soil variation characteristics whereas object models have traditionally been used for applications such as transportation networks land parcels buildings and other objects that possess both spatial and non spatial attributes. Spatial Operators Spatial operators are used to capture all the relevant geometric properties of objects embedded in the physical space and the relations between them as well as to perform spatial analysis. Operators are classified into three broad categories. Topological operators. Topological properties are invariant when topological transformations are applied. These properties do not change after transformations like rotation translation or scaling. Topological operators are hierarchically structured in several levels where the base level offers operators the ability to check for detailed topological relations between regions with a broad boundary and the higher levels offer more abstract operators that allow users to query uncertain spatial data independent of the underlying geometric data model. Examples include open close and inside . Spatial Database Concepts Projective operators. Projective operators such as convex hull are used to express predicates about the concavity convexity of objects as well as other spatial relations . Metric operators. Metric operators provide a more specific description of the object’s geometry. They are used to measure some global properties of single objects and to measure the relative position of different objects in terms of distance and direction. Examples include length and distance . Dynamic Spatial Operators. The operations performed by the operators mentioned above are static in the sense that the operands are not affected by the application of the operation. For example calculating the length of the curve has no effect on the curve itself. Dynamic operations alter the objects upon which the operations act. The three fundamental dynamic operations are create destroy and update. A representative example of dynamic operations would be updating a spatial object that can be subdivided into translate rotate scale up or down reflect and shear . Spatial Queries. Spatial queries are requests for spatial data that require the use of spatial operations. The following categories illustrate three typical types of spatial queries Range query. Finds the objects of a particular type that are within a given spatial area or within a particular distance from a given location. Nearest neighbor query. Finds an object of a particular type that is closest to a given location. Spatial joins or overlays. Typically joins the objects of two types based on some spatial condition such as the objects intersecting or overlapping spatially or being within a certain distance of one another. Spatial Data Indexing A spatial index is used to organize objects into a set of buckets so that objects in a particular spatial region can be easily located. Each bucket has a bucket region a part of space containing all objects stored in the bucket. The bucket regions are usually rectangles for point data structures these regions are disjoint and they partition the space so that each point belongs to precisely one bucket. There are essentially two ways of providing a spatial index. Chapter Enhanced Data Models for Advanced Applications Specialized indexing structures that allow efficient search for data objects based on spatial search operations are included in the database system. These indexing structures would play a similar role to that performed by B+ tree indexes in traditional database systems. Examples of these indexing structures are grid files and R trees. Special types of spatial indexes known as spatial join indexes can be used to speed up spatial join operations. Instead of creating brand new indexing structures the two dimensional spatial data is converted to single dimensional data so that traditional indexing techniques can be used. The algorithms for converting from to are known as space filling curves. We will not discuss these methods in detail . We give an overview of some of the spatial indexing techniques next. Grid Files. We introduced grid files for indexing of data on multiple attributes in Chapter They can also be used for indexing and higher ndimensional spatial data. The fixed grid method divides an n dimensional hyperspace into equal size buckets. The data structure that implements the fixed grid is an n dimensional array. The objects whose spatial locations lie within a cell can be stored in a dynamic structure to handle overflows. This structure is useful for uniformly distributed data like satellite imagery. However the fixed grid structure is rigid and its directory can be sparse and large. R Trees. The R tree is a height balanced tree which is an extension of the B+ tree for k dimensions where k For two dimensions spatial objects are approximated in the R tree by their minimum bounding rectangle which is the smallest rectangle with sides parallel to the coordinate system axis that contains the object. R trees are characterized by the following properties which are similar to the properties for B+ trees in a leaf node is where I is the MBR for the spatial object whose identifier is object identifier. Every node except the root node must be at least half full. Thus a leaf node that is not the root should contain m entries where m M. Similarly a non leaf node that is not the root should contain m entries where m M and I is the MBR that contains the union of all the rectangles in the node pointed at by child pointer. All leaf nodes are at the same level and the root node should have at least two pointers unless it is a leaf node. All MBRs have their sides parallel to the axes of the global coordinate system. Spatial Database Concepts Other spatial storage structures include quadtrees and their variations. Quadtrees generally divide each space or subspace into equally sized areas and proceed with the subdivisions of each subspace to identify the positions of various objects. Recently many newer spatial access structures have been proposed and this area remains an active research area. Spatial Join Index. A spatial join index precomputes a spatial join operation and stores the pointers to the related object in an index structure. Join indexes improve the performance of recurring join queries over tables that have low update rates. Spatial join conditions are used to answer queries such as “Create a list of highwayriver combinations that The spatial join is used to identify and retrieve these pairs of objects that satisfy the cross spatial relationship. Because computing the results of spatial relationships is generally time consuming the result can be computed once and stored in a table that has the pairs of object identifiers that satisfy the spatial relationship which is essentially the join index. A join index can be described by a bipartite graph G where contains the tuple ids of relation R and contains the tuple ids of relation S. Edge set contains an edge for vr in R and vs in S if there is a tuple corresponding to in the join index. The bipartite graph models all of the related tuples as connected vertices in the graphs. Spatial join indexes are used in operations it is also called the location prediction problem. Similarly where to expect hotspots in crime activity is also a location prediction problem. Spatial association. Spatial association rules are defined in terms of spatial predicates rather than items. A spatial association rule is of the form ^ ^ ^ Pn ⇒ ^ ^ ^ Qm where at least one of the Pi ’s or Qj ’s is a spatial predicate. For example the rule isa ^ touches ⇒ isa Chapter Enhanced Data Models for Advanced Applications is an example of an association rule which will have a certain support s and confidence c. Spatial colocation rules attempt to generalize association rules to point to collection data sets that are indexed by space. There are several crucial differences between spatial and nonspatial associations including The notion of a transaction is absent in spatial situations since data is embedded in continuous space. Partitioning space into transactions would lead to an overestimate or an underestimate of interest measures for example support or confidence. Size of item sets in spatial databases is small that is there are many fewer items in the item set in a spatial situation than in a nonspatial situation. In most instances spatial items are a discrete version of continuous variables. For example in the United States income regions may be defined as regions where the mean yearly income is within certain ranges such as below from to and above Spatial Clustering attempts to group database objects so that the most similar objects are in the same cluster and objects in different clusters are as dissimilar as possible. One application of spatial clustering is to group together seismic events in order to determine earthquake faults. An example of a spatial clustering algorithm is density based clustering which tries to find clusters based on the density of data points in a region. These algorithms treat clusters as dense regions of objects in the data space. Two variations of these algorithms are density based spatial clustering of applications with noise and density based clustering DBSCAN is a density based clustering algorithm because it finds a number of clusters starting from the estimated density distribution of corresponding nodes. Applications of Spatial Data Spatial data management is useful in many disciplines including geography remote sensing urban planning and natural resource management. Spatial database management is playing an important role in the solution of challenging scientific problems such as global climate change and genomics. Due to the spatial nature of genome data GIS and spatial database management systems have a large role to play in the area of bioinformatics. Some of the typical applications include pattern recognition genome of support and confidence for association rules are discussed as part of data mining in Section was proposed by Martin Ester Hans Peter Kriegel Jörg Sander and Xiaowei Xu was proposed by Hinnenberg and Gabriel Multimedia Database Concepts browser development and visualization maps. Another important application area of spatial data mining is the spatial outlier detection. A spatial outlier is a spatially referenced object whose nonspatial attribute values are significantly different from those of other spatially referenced objects in its spatial neighborhood. For example if a neighborhood of older houses has just one brand new house that house would be an outlier based on the nonspatial attribute ‘houseage’. Detecting spatial outliers is useful in many applications of geographic information systems and spatial databases. These application domains include transportation ecology public safety public health climatology and location based services. Multimedia Database Concepts Multimedia databases provide features that allow users to store and query different types of multimedia information which includes images video clips audio clips and documents . The main types of database queries that are needed involve locating multimedia sources that contain certain objects of interest. For example one may want to locate all video clips in a video database that include a certain person say Michael Jackson. One may also want to retrieve video clips based on certain activities included in them such as video clips where a soccer goal is scored by a certain player or team. The above types of queries are referred to as content based retrieval because the multimedia source is being retrieved based on its containing certain objects or activities. Hence a multimedia database must use some model to organize and index the multimedia sources based on their contents. Identifying the contents of multimedia sources is a difficult and time consuming task. There are two main approaches. The first is based on automatic analysis of the multimedia sources to identify certain mathematical characteristics of their contents. This approach uses different techniques depending on the type of multimedia source . The second approach depends on manual identification of the objects and activities of interest in each multimedia source and on using this information to index the sources. This approach can be applied to all multimedia sources but it requires a manual preprocessing phase where a person has to scan each multimedia source to identify and catalog the objects and activities it contains so that they can be used to index the sources. In the first part of this section we will briefly discuss some of the characteristics of each type of multimedia source images video audio and text documents. Then we will discuss approaches for automatic analysis of images followed by the problem of object recognition in images. We end this section with some remarks on analyzing audio sources. An image is typically stored either in raw form as a set of pixel or cell values or in compressed form to save space. The image shape descriptor describes the geometric shape of the raw image which is typically a rectangle of cells of a certain width and height. Hence each image can be represented by an m by n grid of cells. Each cell Chapter Enhanced Data Models for Advanced Applications contains a pixel value that describes the cell content. In black and white images pixels can be one bit. In gray scale or color images a pixel is multiple bits. Because images may require large amounts of space they are often stored in compressed form. Compression standards such as GIF JPEG or MPEG use various mathematical transformations to reduce the number of cells stored but still maintain the main image characteristics. Applicable mathematical transforms include Discrete Fourier Transform Discrete Cosine Transform and wavelet transforms. To identify objects of interest in an image the image is typically divided into homogeneous segments using a homogeneity predicate. For example in a color image adjacent cells that have similar pixel values are grouped into a segment. The homogeneity predicate defines conditions for automatically grouping those cells. Segmentation and compression can hence identify the main characteristics of an image. A typical image database query would be to find images in the database that are similar to a given image. The given image could be an isolated segment that contains say a pattern of interest and the query is to locate other images that contain that same pattern. There are two main techniques for this type of search. The first approach uses a distance function to compare the given image with the stored images and their segments. If the distance value returned is small the probability of a match is high. Indexes can be created to group stored images that are close in the distance metric so as to limit the search space. The second approach called the transformation approach measures image similarity by having a small number of transformations that can change one image’s cells to match the other image. Transformations include rotations translations and scaling. Although the transformation approach is more general it is also more time consuming and difficult. A video source is typically represented as a sequence of frames where each frame is a still image. However rather than identifying the objects and activities in every individual frame the video is divided into video segments where each segment comprises a sequence of contiguous frames that includes the same objects activities. Each segment is identified by its starting and ending frames. The objects and activities identified in each video segment can be used to index the segments. An indexing technique called frame segment trees has been proposed for video indexing. The index includes both objects such as persons houses and cars as well as activities such as a person delivering a speech or two people talking. Videos are also often compressed using standards such as MPEG. Audio sources include stored recorded messages such as speeches class presentations or even surveillance recordings of phone messages or conversations by law enforcement. Here discrete transforms can be used to identify the main characteristics of a certain person’s voice in order to have similarity based indexing and retrieval. We will briefly comment on their analysis in Section A text document source is basically the full text of some article book or magazine. These sources are typically indexed by identifying the keywords that appear in the text and their relative frequencies. However filler words or common words called stopwords are eliminated from the process. Because there can be many keywords Multimedia Database Concepts when attempting to index a collection of documents techniques have been developed to reduce the number of keywords to those that are most relevant to the collection. A dimensionality reduction technique called singular value decompositions which is based on matrix transformations can be used for this purpose. An indexing technique called telescoping vector trees can then be used to group similar documents. Chapter discusses document processing in detail. Automatic Analysis of Images Analysis of multimedia sources is critical to support any type of query or search interface. We need to represent multimedia source data such as images in terms of features that would enable us to define similarity. The work done so far in this area uses low level visual features such as color texture and shape which are directly related to the perceptual aspects of image content. These features are easy to extract and represent and it is convenient to design similarity measures based on their statistical properties. Color is one of the most widely used visual features in content based image retrieval since it does not depend upon image size or orientation. Retrieval based on color similarity is mainly done by computing a color histogram for each image that identifies the proportion of pixels within an image for the three color channels . However RGB representation is affected by the orientation of the object with respect to illumination and camera direction. Therefore current image retrieval techniques compute color histograms using competing invariant representations such as HSV . HSV describes colors as points in a cylinder whose central axis ranges from black at the bottom to white at the top with neutral colors between them. The angle around the axis corresponds to the hue the distance from the axis corresponds to the saturation and the distance along the axis corresponds to the value . Texture refers to the patterns in an image that present the properties of homogeneity that do not result from the presence of a single color or intensity value. Examples of texture classes are rough and silky. Examples of textures that can be identified include pressed calf leather straw matting cotton canvas and so on. Just as pictures are represented by arrays of pixels textures are represented by arrays of texels . These textures are then placed into a number of sets depending on how many textures are identified in the image. These sets not only contain the texture definition but also indicate where in the image the texture is located. Texture identification is primarily done by modeling it as a twodimensional gray level variation. The relative brightness of pairs of pixels is computed to estimate the degree of contrast regularity coarseness and directionality. Shape refers to the shape of a region within an image. It is generally determined by applying segmentation or edge detection to an image. Segmentation is a regionbased approach that uses an entire region whereas edge detection is a boundary based approach that uses only the outer boundary characteristics of entities. Shape representation is typically required to be invariant to translation Chapter Enhanced Data Models for Advanced Applications rotation and scaling. Some well known methods for shape representation include Fourier descriptors and moment invariants. Object Recognition in Images Object recognition is the task of identifying real world objects in an image or a video sequence. The system must be able to identify the object even when the images of the object vary in viewpoints size scale or even when they are rotated or translated. Some approaches have been developed to divide the original image into regions based on similarity of contiguous pixels. Thus in a given image showing a tiger in the jungle a tiger subimage may be detected against the background of the jungle and when compared with a set of training images it may be tagged as a tiger. The representation of the multimedia object in an object model is extremely important. One approach is to divide the image into homogeneous segments using a homogeneous predicate. For example in a colored image adjacent cells that have similar pixel values are grouped into a segment. The homogeneity predicate defines conditions for automatically grouping those cells. Segmentation and compression can hence identify the main characteristics of an image. Another approach finds measurements of the object that are invariant to transformations. It is impossible to keep a database of examples of all the different transformations of an image. To deal with this object recognition approaches find interesting points in an image that are invariant to transformations. An important contribution to this field was made by who used scaleinvariant features from images to perform reliable object recognition. This approach is called scale invariant feature transform . The SIFT features are invariant to image scaling and rotation and partially invariant to change in illumination and camera viewpoint. They are well localized in both the spatial and frequency domains reducing the probability of disruption by occlusion clutter or noise. In addition the features are highly distinctive which allows a single feature to be correctly matched with high probability against a large database of features providing a basis for object and scene recognition. For image matching and recognition SIFT features are first extracted from a set of reference images and stored in a database. Object recognition is then performed by comparing each feature from the new image with the features stored in the database and finding candidate matching features based on the Euclidean distance of their feature vectors. Since the keypoint features are highly distinctive a single feature can be correctly matched with good probability in a large database of features. In addition to SIFT there are a number of competing methods available for object recognition under clutter or partial occlusion. For example RIFT a rotation invariant generalization of SIFT identifies groups of local affine regions that remain approximately affinely rigid across a range of views of an object and across multiple instances of the same object class. Semantic Tagging of Images The notion of implicit tagging is an important one for image recognition and comparison. Multiple tags may attach to an image or a subimage for instance in the example we referred to above tags such as “tiger ” “jungle ” “green ” and “stripes” may be associated with that image. Most image search techniques retrieve images based on user supplied tags that are often not very accurate or comprehensive. To improve search quality a number of recent systems aim at automated generation of these image tags. In case of multimedia data most of its semantics is present in its content. These systems use image processing and statistical modeling techniques to analyze image content to generate accurate annotation tags that can then be used to retrieve images by content. Since different annotation schemes will use different vocabularies to annotate images the quality of image retrieval will be poor. To solve this problem recent research techniques have proposed the use of concept hierarchies taxonomies or ontologies using OWL in which terms and their relationships are clearly defined. These can be used to infer higherlevel concepts based on tags. Concepts like “sky” and “grass” may be further divided into “clear sky” and “cloudy sky” or “dry grass” and “green grass” in such a taxonomy. These approaches generally come under semantic tagging and can be used in conjunction with the above feature analysis and object identification strategies. Analysis of Audio Data Sources Audio sources are broadly classified into speech music and other audio data. Each of these are significantly different from the other hence different types of audio data are treated differently. Audio data must be digitized before it can be processed and stored. Indexing and retrieval of audio data is arguably the toughest among all types of media because like video it is continuous in time and does not have easily measurable characteristics such as text. Clarity of sound recordings is easy to perceive humanly but is hard to quantify for machine learning. Interestingly speech data often uses speech recognition techniques to aid the actual audio content as this can make indexing this data a lot easier and more accurate. This is sometimes referred to as text based indexing of audio data. The speech metadata is typically content dependent in that the metadata is generated from the audio content for example the length of the speech the number of speakers and so on. However some of the metadata might be independent of the actual content such as the length of the speech and the format in which the data is stored. Music indexing on the other hand is done based on the statistical analysis of the audio signal also known as content based indexing. Content based indexing often makes use of the key features of sound intensity pitch timbre and rhythm. It is possible to compare different pieces of audio data and retrieve information from them based on the calculation of certain features as well as application of certain transforms. Chapter Enhanced Data Models for Advanced Applications Introduction to Deductive Databases Overview of Deductive Databases In a deductive database system we typically specify rules through a declarative language a language in which we specify what to achieve rather than how to achieve it. An inference engine within the system can deduce new facts from the database by interpreting these rules. The model used for deductive databases is closely related to the relational data model and particularly to the domain relational calculus formalism . SUPERVISE. SUPERVISE. SUPERVISE. SUPERVISE. SUPERVISE. SUPERVISE. . . . Rules SUPERIOR – SUPERVISE – SUPERVISE SUPERIOR – SUPERIOR SUPERIOR ramesh joyce franklin james joh ahmad jennifer alicia Figure Prolog notation. The supervisory tree. values in a predicate are either numeric or character strings they are represented as identifiers that start with a lowercase letter whereas variable names always start with an uppercase letter. Consider the example shown in Figure which is based on the relational database in Figure but in a much simplified form. There are three predicate names supervise superior and subordinate. The SUPERVISE predicate is defined via a set of facts each of which has two arguments a supervisor name followed by the name of a direct supervisee of that supervisor. These facts correspond to the actual data that is stored in the database and they can be considered as constituting a set of tuples in a relation SUPERVISE with two attributes whose schema is SUPERVISE Thus SUPERVISE states the fact that X supervises Y. Notice the omission of the attribute names in the Prolog notation. Attribute names are only represented by virtue of the position of each argument in a predicate the first argument represents the supervisor and the second argument represents a direct subordinate. The other two predicate names are defined by rules. The main contributions of deductive databases are the ability to specify recursive rules and to provide a framework for inferring new information based on the specified rules. A rule is of the form head – body where – is read as if and only if. A rule usually has a single predicate to the left of the – symbol called the head or left hand side or conclusion of the rule and one or more predicates to the right of the – symbol called the body or right hand side or premise of the rule. A predicate with constants as arguments is said to be ground we also refer to it as an instantiated predicate. The arguments of the predicates that appear in a rule typically include a number of variable symbols although predicates can also contain Chapter Enhanced Data Models for Advanced Applications constants as arguments. A rule specifies that if a particular assignment or binding of constant values to the variables in the body makes all the RHS predicates true it also makes the head true by using the same assignment of constant values to variables. Hence a rule provides us with a way of generating new facts that are instantiations of the head of the rule. These new facts are based on facts that already exist corresponding to the instantiations of predicates in the body of the rule. Notice that by listing multiple predicates in the body of a rule we implicitly apply the logical AND operator to these predicates. Hence the commas between the RHS predicates may be read as meaning and. Consider the definition of the predicate SUPERIOR in Figure whose first argument is an employee name and whose second argument is an employee who is either a direct or an indirect subordinate of the first employee. By indirect subordinate we mean the subordinate of some subordinate down to any number of levels. Thus SUPERIOR stands for the fact that X is a superior of Y through direct or indirect supervision. We can write two rules that together specify the meaning of the new predicate. The first rule under Rules in the figure states that for every value of X and Y if SUPERVISE the rule body is true then SUPERIOR the rule head is also true since Y would be a direct subordinate of X . This rule can be used to generate all direct superior subordinate relationships from the facts that define the SUPERVISE predicate. The second recursive rule states that if SUPERVISE and SUPERIOR are both true then SUPERIOR is also true. This is an example of a recursive rule where one of the rule body predicates in the RHS is the same as the rule head predicate in the LHS. In general the rule body defines a number of premises such that if they are all true we can deduce that the conclusion in the rule head is also true. Notice that if we have two rules with the same head it is equivalent to saying that the predicate is true if either one of the bodies is true hence it is equivalent to a logical OR operation. For example if we have two rules X – Y and X – Z they are equivalent to a rule X – Y OR Z. The latter form is not used in deductive systems however because it is not in the standard form of rule called a Horn clause as we discuss in Section A Prolog system contains a number of built in predicates that the system can interpret directly. These typically include the equality comparison operator which returns true if X and Y are identical and can also be written as X Y by using the standard infix Other comparison operators for numbers such as and can be treated as binary predicates. Arithmetic functions such as + – and can be used as arguments in predicates in Prolog. In contrast Datalog does not allow functions such as arithmetic operations as arguments indeed this is one of the main differences between Prolog and Datalog. However extensions to Datalog have been proposed that do include functions. Prolog system typically has a number of different equality predicates that have different interpretations. Introduction to Deductive Databases A query typically involves a predicate symbol with some variable arguments and its meaning is to deduce all the different constant combinations that when bound to the variables can make the predicate true. For example the first query in Figure requests the names of all subordinates of james at any level. A different type of query which has only constant symbols as arguments returns either a true or a false result depending on whether the arguments provided can be deduced from the facts and rules. For example the second query in Figure returns true since SUPERIOR can be deduced. Datalog Notation In Datalog as in other logic based languages a program is built from basic objects called atomic formulas. It is customary to define the syntax of logic based languages by describing the syntax of atomic formulas and identifying how they can be combined to form a program. In Datalog atomic formulas are literals of the form an where p is the predicate name and n is the number of arguments for predicate p. Different predicate symbols can have different numbers of arguments and the number of arguments n of predicate p is sometimes called the arity or degree of p. The arguments can be either constant values or variable names. As mentioned earlier we use the convention that constant values either are numeric or start with a lowercase character whereas variable names always start with an uppercase character. A number of built in predicates are included in Datalog which can also be used to construct atomic formulas. The built in predicates are of two main types the binary comparison predicates and over ordered domains and the comparison predicates and over ordered or unordered domains. These can be used as binary predicates with the same functional syntax as other predicates for example by writing less. Additionally a formula can have quantifiers namely the universal quantifier and the existential quantifier . In clausal form a formula must be transformed into another formula with the following characteristics All variables in the formula are universally quantified. Hence it is not necessary to include the universal quantifiers explicitly the quantifiers are removed and all variables in the formula are implicitly quantified by the universal quantifier. In clausal form the formula is made up of a number of clauses where each clause is composed of a number of literals connected by OR logical connectives only. Hence each clause is a disjunction of literals. The clauses themselves are connected by AND logical connectives only to form a formula. Hence the clausal form of a formula is a conjunction of clauses. It can be shown that any formula can be converted into clausal form. For our purposes we are mainly interested in the form of the individual clauses each of which is a disjunction of literals. Recall that literals can be positive literals or negative literals. Consider a clause of the form OR OR OR NOT OR OR OR OR Qm This clause has n negative literals and m positive literals. Such a clause can be transformed into the following equivalent logical formula AND AND AND Pn ⇒ OR OR OR Qm where ⇒ is the implies symbol. The formulas and are equivalent meaning that their truth values are always the same. This is the case because if all the Pi literals are true the formula is true only if at least one of the Qi ’s is true which is the meaning of the ⇒ symbol. For formula if all the Pi literals are true their negations are all false so in this case formula is true only if at least one of the Qi ’s is true. In Datalog rules are expressed as a restricted form of clauses called Horn clauses in which a clause can contain at most one positive literal. Hence a Horn clause is either of the form NOT OR OR OR NOT OR Q or of the form NOT OR OR OR NOT The Horn clause in can be transformed into the clause AND AND AND Pn ⇒ Q which is written in Datalog as the following rule Q – Pn. Introduction to Deductive Databases SUPERIOR – SUPERVISE. – SUPERVISE SUPERIOR. . SUPERVISE. SUPERIOR. . where each Xi is a variable or a constant A Prolog or Datalog system has an internal inference engine that can be used to process and compute the results of such queries. Prolog inference engines typically return one result to the query at a time and must be prompted to return additional results. On the contrary Datalog returns results set at a time. Interpretations of Rules There are two main alternatives for interpreting the theoretical meaning of rules proof theoretic and model theoretic. In practical systems the inference mechanism within a system defines the exact interpretation which may not coincide with either of the two theoretical interpretations. The inference mechanism is a computational procedure and hence provides a computational interpretation of the meaning of rules. In this section first we discuss the two theoretical interpretations. Then we briefly discuss inference mechanisms as a way of defining the meaning of rules. In the proof theoretic interpretation of rules we consider the facts and rules to be true statements or axioms. Ground axioms contain no variables. The facts are ground axioms that are given to be true. Rules are called deductive axioms since they can be used to deduce new facts. The deductive axioms can be used to construct proofs that derive new facts from existing facts. For example Figure shows how to prove the fact SUPERIOR from the rules and facts Chapter Enhanced Data Models for Advanced Applications given in Figure The proof theoretic interpretation gives us a procedural or computational approach for computing an answer to the Datalog query. The process of proving whether a certain fact holds is known as theorem proving. The second type of interpretation is called the model theoretic interpretation. Here given a finite or an infinite domain of constant we assign to a predicate every possible combination of values as arguments. We must then determine whether the predicate is true or false. In general it is sufficient to specify the combinations of arguments that make the predicate true and to state that all other combinations make the predicate false. If this is done for every predicate it is called an interpretation of the set of predicates. For example consider the interpretation shown in Figure for the predicates SUPERVISE and SUPERIOR. This interpretation assigns a truth value to every possible combination of argument values for the two predicates. An interpretation is called a model for a specific set of rules if those rules are always true under that interpretation that is for any values assigned to the variables in the rules the head of the rules is true when we substitute the truth values assigned to the predicates in the body of the rule by that interpretation. Hence whenever a particular substitution to the variables in the rules is applied if all the predicates in the body of a rule are true under the interpretation the predicate in the head of the rule must also be true. The interpretation shown in Figure is a model for the two rules shown since it can never cause the rules to be violated. Notice that a rule is violated if a particular binding of constants to the variables makes all the predicates in the rule body true but makes the predicate in the rule head false. For example if SUPERVISE and SUPERIOR are both true under some interpretation but SUPERIOR is not true the interpretation cannot be a model for the recursive rule SUPERIOR – SUPERVISE SUPERIOR In the model theoretic approach the meaning of the rules is established by providing a model for these rules. A model is called a minimal model for a set of rules if we cannot change any fact from true to false and still get a model for these rules. For example consider the interpretation in Figure and assume that the SUPERVISE predicate is defined by a set of known facts whereas the SUPERIOR predicate is defined as an interpretation for the rules. Suppose that we add the predicate SUPERIOR to the true predicates. This remains a model for the rules shown but it is not a minimal model since changing the truth value of SUPERIOR from true to false still provides us with a model for the rules. The model shown in Figure is the minimal model for the set of facts that are defined by the SUPERVISE predicate. In general the minimal model that corresponds to a given set of facts in the modeltheoretic interpretation should be the same as the facts generated by the most commonly chosen domain is finite and is called the Herbrand Universe. Introduction to Deductive Databases Rules SUPERIOR – SUPERVISE. SUPERIOR – SUPERVISE SUPERIOR. Interpretation Known Facts SUPERVISE is true. SUPERVISE is true. SUPERVISE is true. SUPERVISE is true. SUPERVISE is true. SUPERVISE is true. SUPERVISE is true. SUPERVISE is false for all other possible combinations Derived Facts SUPERIOR is true. SUPERIOR is true. SUPERIOR is true. SUPERIOR is true. SUPERIOR is true. SUPERIOR is true. SUPERIOR is true. SUPERIOR is true. SUPERIOR is true. SUPERIOR is true. SUPERIOR is true. SUPERIOR is true. SUPERIOR is false for all other possible combinations Figure An interpretation that is a minimal model. theoretic interpretation for the same original set of ground and deductive axioms. However this is generally true only for rules with a simple structure. Once we allow negation in the specification of rules the correspondence between interpretations does not hold. In fact with negation numerous minimal models are possible for a given set of facts. A third approach to interpreting the meaning of rules involves defining an inference mechanism that is used by the system to deduce facts from the rules. This inference mechanism would define a computational interpretation to the meaning of the rules. The Prolog logic programming language uses its inference mechanism to define the meaning of the rules and facts in a Prolog program. Not all Prolog programs correspond to the proof theoretic or model theoretic interpretations it depends on the type of rules in the program. However for many simple Prolog programs the Prolog inference mechanism infers the facts that correspond either to the proof theoretic interpretation or to a minimal model under the model theoretic interpretation. Chapter Enhanced Data Models for Advanced Applications EMPLOYEE. MALE. EMPLOYEE. MALE. EMPLOYEE. MALE. EMPLOYEE. MALE. EMPLOYEE. MALE. EMPLOYEE. EMPLOYEE. FEMALE. EMPLOYEE. FEMALE. FEMALE. SALARY. SALARY. SALARY. SALARY. SALARY. SALARY. SALARY. WORKSON. WORKSON. WORKSON. WORKSON. WORKSON. WORKSON. WORKSON. WORKSON. WORKSON WORKSON. WORKSON. WORKSON. WORKSON. WORKSON. Figure Fact predicates for part of the database from Figure Datalog Programs and Their Safety There are two main methods of defining the truth values of predicates in actual Datalog programs. Fact defined predicates are defined by listing all the combinations of values that make the predicate true. These correspond to base relations whose contents are stored in a database system. Figure shows the fact defined predicates EMPLOYEE MALE FEMALE DEPARTMENT SUPERVISE PROJECT and WORKSON which correspond to part of the relational database shown in Figure Rule defined predicates are defined by being the head of one or more Datalog rules they correspond to virtual rela Introduction to Deductive Databases SUPERIOR – SUPERVISE. SUPERIOR – SUPERVISE SUPERIOR. SUBORDINATE – SUPERIOR. SUPERVISOR – EMPLOYEE SUPERVISE. – EMPLOYEE SALARY Y – SUPERVISOR . MAINPRODUCTXEMP – EMPLOYEE WORKSON Y PRESIDENT – EMPLOYEE NOT . Figure Rule defined predicates. tions whose contents can be inferred by the inference engine. Figure shows a number of rule defined predicates. A program or a rule is said to be safe if it generates a finite set of facts. The general theoretical problem of determining whether a set of rules is safe is undecidable. However one can determine the safety of restricted forms of rules. For example the rules shown in Figure are safe. One situation where we get unsafe rules that can generate an infinite number of facts arises when one of the variables in the rule can range over an infinite domain of values and that variable is not limited to ranging over a finite relation. For example consider the following rule BIGSALARY – Here we can get an infinite result if Y ranges over all possible integers. But suppose that we change the rule as follows BIGSALARY – EMPLOYEE Salary In the second rule the result is not infinite since the values that Y can be bound to are now restricted to values that are the salary of some employee in the database presumably a finite set of values. We can also rewrite the rule as follows BIGSALARY – EMPLOYEE Salary In this case the rule is still theoretically safe. However in Prolog or any other system that uses a top down depth first inference mechanism the rule creates an infinite loop since we first search for a value for Y and then check whether it is a salary of an employee. The result is generation of an infinite number of Y values even though these after a certain point cannot lead to a set of true RHS predicates. One definition of Datalog considers both rules to be safe since it does not depend on a particular inference mechanism. Nonetheless it is generally advisable to write such a rule in the safest form with the predicates that restrict possible bindings of variables placed first. As another example of an unsafe rule consider the following rule HASSOMETHING – EMPLOYEE Chapter Enhanced Data Models for Advanced Applications RELONE. RELTWO. RELTHREE. SELECTONEAEQC – RELONE. Y Z – RELONE Y Y Z – RELONE Y Z – RELONE. Y Z – RELONE PROJECTTHREEONGH – RELTHREE. UNIONONETWO – RELONE. UNIONONETWO – RELTWO. INTERSECTONETWO – RELONE RELTWO. DIFFERENCETWOONE – RELTWO NOT. CART PROD ONETHREE – RELONE RELTHREE. NATURALJOINONETHREECEQG – RELONE RELTHREE. Figure Predicates for illustrating relational operations. Here an infinite number of Y values can again be generated since the variable Y appears only in the head of the rule and hence is not limited to a finite set of values. To define safe rules more formally we use the concept of a limited variable. A variable X is limited in a rule if it appears in a regular predicate in the body of the rule it appears in a predicate of the form X c or c X or and in the rule body where c and are constant values or it appears in a predicate of the form X Y or Y X in the rule body where Y is a limited variable. A rule is said to be safe if all its variables are limited. Use of Relational Operations It is straightforward to specify many operations of the relational algebra in the form of Datalog rules that define the result of applying these operations on the database relations . This means that relational queries and views can easily be specified in Datalog. The additional power that Datalog provides is in the specification of recursive queries and views based on recursive queries. In this section we Introduction to Deductive Databases show how some of the standard relational operations can be specified as Datalog rules. Our examples will use the base relations RELONE RELTWO and RELTHREE whose schemas are shown in Figure In Datalog we do not need to specify the attribute names as in Figure rather the arity of each predicate is the important aspect. In a practical system the domain of each attribute is also important for operations such as UNION INTERSECTION and JOIN and we assume that the attribute types are compatible for the various operations as discussed in Chapter Figure illustrates a number of basic relational operations. Notice that if the Datalog model is based on the relational model and hence assumes that predicates specify sets of tuples duplicate tuples in the same predicate are automatically eliminated. This may or may not be true depending on the Datalog inference engine. However it is definitely not the case in Prolog so any of the rules in Figure that involve duplicate elimination are not correct for Prolog. For example if we want to specify Prolog rules for the UNION operation with duplicate elimination we must rewrite them as follows UNIONONETWO – RELONE. UNIONONETWO – RELTWO NOT . However the rules shown in Figure should work for Datalog if duplicates are automatically eliminated. Similarly the rules for the PROJECT operation shown in Figure should work for Datalog in this case but they are not correct for Prolog since duplicates would appear in the latter case. Evaluation of Nonrecursive Datalog Queries In order to use Datalog as a deductive database system it is appropriate to define an inference mechanism based on relational database query processing concepts. The inherent strategy involves a bottom up evaluation starting with base relations the order of operations is kept flexible and subject to query optimization. In this section we discuss an inference mechanism based on relational operations that can be applied to nonrecursive Datalog queries. We use the fact and rule base shown in Figures and to illustrate our discussion. If a query involves only fact defined predicates the inference becomes one of searching among the facts for the query result. For example a query such as DEPARTMENT is a selection of all employee names X who work for the Research department. In relational algebra it is the query “Research” which can be answered by searching through the fact defined predicate department. The query involves relational SELECT and PROJECT operations on a base relation and it can be handled by the database query processing and optimization techniques discussed in Chapter Chapter Enhanced Data Models for Advanced Applications SUPERVISOR PRESIDENT MAINPRODUCTEMP WORKSON EMPLOYEE SALARY SUPERVISE DEPARTMENT PROJECT FEMALE MALE SUBORDINATE SUPERIOR Figure Predicate dependency graph for Figures and When a query involves rule defined predicates the inference mechanism must compute the result based on the rule definitions. If a query is nonrecursive and involves a predicate p that appears as the head of a rule p – pn the strategy is first to compute the relations corresponding to pn and then to compute the relation corresponding to p. It is useful to keep track of the dependency among the predicates of a deductive database in a predicate dependency graph. Figure shows the graph for the fact and rule predicates shown in Figures and The dependency graph contains a node for each predicate. Whenever a predicate A is specified in the body of a rule and the head of that rule is the predicate B we say that B depends on A and we draw a directed edge from A to B. This indicates that in order to compute the facts for the predicate B we must first compute the facts for all the predicates A in the rule body. If the dependency graph has no cycles we call the rule set nonrecursive. If there is at least one cycle we call the rule set recursive. In Figure there is one recursively defined predicate namely SUPERIOR which has a recursive edge pointing back to itself. Additionally because the predicate subordinate depends on SUPERIOR it also requires recursion in computing its result. A query that includes only nonrecursive predicates is called a nonrecursive query. In this section we discuss only inference mechanisms for nonrecursive queries. In Figure any query that does not involve the predicates SUBORDINATE or SUPERIOR is nonrecursive. In the predicate dependency graph the nodes corresponding to fact defined predicates do not have any incoming edges since all factdefined predicates have their facts stored in a database relation. The contents of a fact defined predicate can be computed by directly retrieving the tuples in the corresponding database relation. Summary The main function of an inference mechanism is to compute the facts that correspond to query predicates. This can be accomplished by generating a relational expression involving relational operators as SELECT PROJECT JOIN UNION and SET DIFFERENCE that when executed provides the query result. The query can then be executed by utilizing the internal query processing and optimization operations of a relational database management system. Whenever the inference mechanism needs to compute the fact set corresponding to a nonrecursive rule defined predicate p it first locates all the rules that have p as their head. The idea is to compute the fact set for each such rule and then to apply the UNION operation to the results since UNION corresponds to a logical OR operation. The dependency graph indicates all predicates q on which each p depends and since we assume that the predicate is nonrecursive we can always determine a partial order among such predicates q. Before computing the fact set for p first we compute the fact sets for all predicates q on which p depends based on their partial order. For example if a query involves the predicate we must first compute both SUPERVISOR and Since the latter two depend only on the fact defined predicates EMPLOYEE SALARY and SUPERVISE they can be computed directly from the stored database relations. This concludes our introduction to deductive databases. Additional material may be found at the book’s Website where the complete Chapter from the third edition is available. This includes a discussion on algorithms for recursive query processing. We have included an extensive bibliography of work in deductive databases recursive query processing magic sets combination of relational databases with deductive rules and GLUE NAIL! System at the end of this chapter. Summary In this chapter we introduced database concepts for some of the common features that are needed by advanced applications active databases temporal databases spatial databases multimedia databases and deductive databases. It is important to note that each of these is a broad topic and warrants a complete textbook. First we introduced the topic of active databases which provide additional functionality for specifying active rules. We introduced the Event Condition Action model for active databases. The rules can be automatically triggered by events that occur such as a database update and they can initiate certain actions that have been specified in the rule declaration if certain conditions are true. Many commercial packages have some of the functionality provided by active databases in the form of triggers. We discussed the different options for specifying rules such as row level versus statement level before versus after and immediate versus deferred. We gave examples of row level triggers in the Oracle commercial system and statement level rules in the STARBURST experimental system. The syntax for triggers in the standard was also discussed. We briefly discussed some design issues and some possible applications for active databases. Chapter Enhanced Data Models for Advanced Applications Next we introduced some of the concepts of temporal databases which permit the database system to store a history of changes and allow users to query both current and past states of the database. We discussed how time is represented and distinguished between the valid time and transaction time dimensions. We discussed how valid time transaction time and bitemporal relations can be implemented using tuple versioning in the relational model with examples to illustrate how updates inserts and deletes are implemented. We also showed how complex objects can be used to implement temporal databases using attribute versioning. We looked at some of the querying operations for temporal relational databases and gave a brief introduction to the language. Then we turned to spatial databases. Spatial databases provide concepts for databases that keep track of objects that have spatial characteristics. We discussed the types of spatial data types of operators for processing spatial data types of spatial queries and spatial indexing techniques including the popular R trees. Then we discussed some spatial data mining techniques and applications of spatial data. We discussed some basic types of multimedia databases and their important characteristics. Multimedia databases provide features that allow users to store and query different types of multimedia information which includes images video clips audio clips and documents . We provided a brief overview of the various types of media sources and how multimedia sources may be indexed. Images are an extremely common type of data among databases today and are likely to occupy a large proportion of stored data in databases. We therefore provided a more detailed treatment of images their automatic analysis recognition of objects within images and their semantic tagging all of which contribute to developing better systems to retrieve images by content which still remains a challenging problem. We also commented on the analysis of audio data sources. We concluded the chapter with an introduction to deductive databases. We gave an overview of Prolog and Datalog notation. We discussed the clausal form of formulas. Datalog rules are restricted to Horn clauses which contain at most one positive literal. We discussed the proof theoretic and model theoretic interpretation of rules. We briefly discussed Datalog rules and their safety and the ways of expressing relational operators using Datalog rules. Finally we discussed an inference mechanism based on relational operations that can be used to evaluate nonrecursive Datalog queries using relational query optimization techniques. While Datalog has been a popular language with many applications unfortunately implementations of deductive database systems such as LDL or VALIDITY have not become widely commercially available. Review Questions Review Questions What are the differences between row level and statement level active rules What are the differences among immediate deferred and detached consideration of active rule conditions What are the differences among immediate deferred and detached execution of active rule actions Briefly discuss the consistency and termination problems when designing a set of active rules. Discuss some applications of active databases. Discuss how time is represented in temporal databases and compare the different time dimensions. What are the differences between valid time transaction time and bitemporal relations Describe how the insert delete and update commands should be implemented on a valid time relation. Describe how the insert delete and update commands should be implemented on a bitemporal relation. Describe how the insert delete and update commands should be implemented on a transaction time relation. What are the main differences between tuple versioning and attribute versioning How do spatial databases differ from regular databases What are the different types of spatial data Name the main types of spatial operators and different classes of spatial queries. What are the properties of R trees that act as an index for spatial data Describe how a spatial join index between spatial objects can be constructed. What are the different types of spatial data mining State the general form of a spatial association rule. Give an example of a spatial association rule. What are the different types of multimedia sources How are multimedia sources indexed for content based retrieval Chapter Enhanced Data Models for Advanced Applications What important features of images are used to compare them What are the different approaches to recognizing objects in images How is semantic tagging of images used What are the difficulties in analyzing audio sources What are deductive databases Write sample rules in Prolog to define that courses with course number above are graduate courses and that DBgrads are those graduate students who enroll in and Define clausal form of formulas and Horn clauses. What is theorem proving and what is proof theoretic interpretation of rules What is model theoretic interpretation and how does it differ from prooftheoretic interpretation What are fact defined predicates and rule defined predicates What is a safe rule Give examples of rules that can define relational operations SELECT PROJECT JOIN and SET operations. Discuss the inference mechanism based on relational operations that can be applied to evaluate nonrecursive Datalog queries. Exercises Consider the COMPANY database described in Figure Using the syntax of Oracle triggers write active rules to do the following a. Whenever an employee’s project assignments are changed check if the total hours per week spent on the employee’s projects are less than or greater than if so notify the employee’s direct supervisor. b. Whenever an employee is deleted delete the PROJECT tuples and DEPENDENT tuples related to that employee and if the employee manages a department or supervises employees set the Mgrssn for that department to NULL and set the Superssn for those employees to NULL. Repeat but use the syntax of STARBURST active rules. Consider the relational schema shown in Figure Write active rules for keeping the Sumcommissions attribute of SALESPERSON equal to the sum of the Commission attribute in SALES for each sales person. Your rules should also check if the Sumcommissions exceeds if it does call a procedure Notifymanager. Write both statement level rules in STARBURST notation and row level rules in Oracle. Exercises Sid Vid Commission SALES Salespersonid Name Title Phone Sumcommissions SALESPERSON Figure Database schema for sales and salesperson commissions in Exercise Consider the UNIVERSITY EER schema in Figure Write some rules that could be implemented via active rules to enforce some common integrity constraints that you think are relevant to this application. Discuss which of the updates that created each of the tuples shown in Figure were applied retroactively and which were applied proactively. Show how the following updates if applied in sequence would change the contents of the bitemporal EMPBT relation in Figure For each update state whether it is a retroactive or proactive update. a. On the salary of Narayan is updated to effective on b. On the salary of Smith was corrected to show that it should have been entered as effective on c. On the database was changed to indicate that Narayan was leaving the company effective on d. On the database was changed to indicate the hiring of a new employee called Johnson with the tuple ‘Johnson’ NULL effective on e. On the database was changed to indicate that Wong was leaving the company effective on f. On the database was changed to indicate the rehiring of Brown with the same department and supervisor but with salary effective on Show how the updates given in Exercise if applied in sequence would change the contents of the valid time EMPVT relation in Figure Add the following facts to the sample database in Figure SUPERVISE SUPERVISE. First modify the supervisory tree in Figure to reflect this change. Then construct a diagram showing the top down evaluation of the query SUPERIOR using rules and from Figure Chapter Enhanced Data Models for Advanced Applications Consider the following set of facts for the relation PARENT where Y is the parent of X PARENT PARENT PARENT PARENT PARENT PARENT. Consider the rules ANCESTOR – PARENT ANCESTOR – PARENT ANCESTOR which define ancestor Y of X as above. a. Show how to solve the Datalog query ANCESTOR and show your work at each step. b. Show the same query by computing only the changes in the ancestor relation and using that in rule each time. [This question is derived from Bancilhon and Ramakrishnan Consider a deductive database with the following rules ANCESTOR – FATHER ANCESTOR – FATHER ANCESTOR Notice that FATHER means that Y is the father of X ANCESTOR means that Y is the ancestor of X. Consider the following fact base FATHER FATHER FATHER. a. Construct a model theoretic interpretation of the above rules using the given facts. b. Consider that a database contains the above relations FATHER another relation BROTHER and a third relation BIRTH where B is the birth date of person X. State a rule that computes the first cousins of the following variety their fathers must be brothers. c. Show a complete Datalog program with fact based and rule based literals that computes the following relation list of pairs of cousins where the first person is born after and the second after You may use greater than as a built in predicate. Consider the following rules REACHABLE – FLIGHT REACHABLE – FLIGHT REACHABLE where REACHABLE means that city Y can be reached from city X and FLIGHT means that there is a flight to city Y from city X. Selected Bibliography a. Construct fact predicates that describe the following i. Los Angeles New York Chicago Atlanta Frankfurt Paris Singapore Sydney are cities. ii. The following flights exist LA to NY NY to Atlanta Atlanta to Frankfurt Frankfurt to Atlanta Frankfurt to Singapore and Singapore to Sydney. b. Is the given data cyclic If so in what sense c. Construct a model theoretic interpretation How will this query be executed List the series of steps it will go through. e. Consider the following rule defined predicates ROUND TRIP REACHABLE – REACHABLE REACHABLE DURATION Draw a predicate dependency graph for the above predicates. means that you can take a flight from X to Y in Z f. Consider the following query What cities are reachable in hours from Atlanta Show how to express it in Datalog. Assume built in predicates like greater than. Can this be converted into a relational algebra statement in a straightforward way Why or why not g. Consider the predicate population where Y is the population of city X. Consider the following query List all possible bindings of the predicate pair where Y is a city that can be reached in two flights from city X which has over million people. Show this query in Datalog. Draw a corresponding query tree in relational algebraic terms. Selected Bibliography The book by Zaniolo et al. consists of several parts each describing an advanced database concept such as active temporal and spatial text multimedia databases. Widom and Ceri and Ceri and Fraternali focus on active database concepts and systems. Snodgrass describes the language and data model. Khoshafian and Baker Faloutsos and Subrahmanian describe multimedia database concepts. Tansel et al. is a collection of chapters on temporal databases. STARBURST rules are described in Widom and Finkelstein Early work on active databases includes the HiPAC project discussed in Chakravarthy et al. Chapter Enhanced Data Models for Advanced Applications and Chakravarthy A glossary for temporal databases is given in Jensen et al. Snodgrass focuses on TQuel an early temporal query language. Temporal normalization is defined in Navathe and Ahmed Paton and Paton and Diaz survey active databases. Chakravarthy et al. describe SENTINEL and object based active systems. Lee et al. discuss time series management. The book by Shekhar and Chawla consists of all aspects of spatial databases including spatial data models spatial storage and indexing and spatial data mining. Scholl et al. is another textbook on spatial data management. Albrecht describes in detail the various GIS analysis operations. Clementini and Di Felice give a detailed description of the spatial operators. Güting describes the spatial data structures and querying languages for spatial database systems. Guttman proposed R trees for spatial data indexing. Manolopoulos et al. is a book on the theory and applications of R trees. Papadias et al. discuss query processing using R trees for spatial networks. Ester et al. provide a comprehensive discussion on the algorithms and applications of spatial data mining. Koperski and Han discuss association rule discovery from geographic databases. Brinkhoff et al. provide a comprehensive overview of the usage of R trees for efficient processing of spatial joins. Rotem describes spatial join indexes comprehensively. Shekhar and Xiong is a compilation of various sources that discuss different aspects of spatial database management systems and GIS. The density based clustering algorithms DBSCAN and DENCLUE are proposed by Ester et al. and Hinnenberg and Gabriel respectively. Multimedia database modeling has a vast amount of literature it is difficult to point to all important references here. IBM’s QBIC system described in Niblack et al. was one of the first comprehensive approaches for querying images based on content. It is now available as a part of IBM’s database image extender. Zhao and Grosky discuss content based image retrieval. Carneiro and Vasconselos present a database centric view of semantic image annotation and retrieval. Content based retrieval of subimages is discussed by Luo and Nascimento Tuceryan and Jain discuss various aspects of texture analysis. Object recognition using SIFT is discussed in Lowe Lazebnik et al. describe the use of local affine regions to model objects . Among other object recognition approaches G RIF is described in Kim et al. Bay et al. discuss SURF Ke and Sukthankar present PCA SIFT and Mikolajczyk and Schmid describe GLOH. Fan et al. present a technique for automatic image annotation by using concept sensitive objects. Fotouhi et al. was the first international workshop on many faces of multimedia semantics which is continuing annually. Thuraisingham classifies audio data into different categories and by treating each of these categories differently elaborates on the use of metadata for audio. Prabhakaran has also discussed how speech processing techniques can add valuable metadata information to the audio piece. The early developments of the logic and database approach are surveyed by Gallaire et al. Reiter provides a reconstruction of relational database theory while Levesque provides a discussion of incomplete knowledge in light of logic. Gallaire and Minker provide an early book on this topic. A detailed treatment of logic and databases appears in Ullman Volume and there is a related chapter in Volume Ceri Gottlob and Tanca present a comprehensive yet concise treatment of logic and databases. Das is a comprehensive book on deductive databases and logic programming. The early history of Datalog is covered in Maier and Warren Clocksin and Mellish is an excellent reference on Prolog language. Aho and Ullman provide an early algorithm for dealing with recursive queries using the least fixed point operator. Bancilhon and Ramakrishnan give an excellent and detailed description of the approaches to recursive query processing with detailed examples of the naive and seminaive approaches. Excellent survey articles on deductive databases and recursive query processing include Warren and Ramakrishnan and Ullman A complete description of the seminaive approach based on relational algebra is given in Bancilhon Other approaches to recursive query processing include the recursive query subquery strategy of Vieille which is a top down interpreted strategy and the Henschen Naqvi top down compiled iterative strategy. Balbin and Ramamohanrao discuss an extension of the seminaive differential approach for multiple predicates. The original paper on magic sets is by Bancilhon et al. Beeri and Ramakrishnan extend it. Mumick et al. show the applicability of magic sets to nonrecursive nested SQL queries. Other approaches to optimizing rules without rewriting them appear in Vieille Kifer and Lozinskii propose a different technique. Bry discusses how the top down and bottomup approaches can be reconciled. Whang and Navathe describe an extended disjunctive normal form technique to deal with recursion in relational algebra expressions for providing an expert system interface over a relational DBMS. Chang describes an early system for combining deductive rules with relational databases. The LDL system prototype is described in Chimenti et al. Krishnamurthy and Naqvi introduce the choice notion in LDL. Zaniolo discusses the language issues for the LDL system. A language overview of CORAL is provided in Ramakrishnan et al. and the implementation is described in Ramakrishnan et al. An extension to support object oriented features called CORAL++ is described in Srivastava et al. Ullman provides the basis for the NAIL! system which is described in Morris et al. Phipps et al. describe the GLUE NAIL! deductive database system. Zaniolo reviews the theoretical background and the practical importance of deductive databases. Nicolas gives an excellent history of the developments leading up to Deductive Object Oriented Database systems. Falcone et al. survey the DOOD landscape. References on the VALIDITY system include Friesen et al. Vieille and Dietrich et al. Selected Bibliography This page intentionally left blank Introduction to Information Retrieval and Web I n most of the chapters in this book so far we have discussed techniques for modeling designing querying transaction processing of and managing structured data. In Section we discussed the difference between structured semistructured and unstructured data. Information retrieval deals mainly with unstructured data and the techniques for indexing searching and retrieving information from large collections of unstructured documents. In this chapter we will provide an introduction to information retrieval. This is a very broad topic so we will focus on the similarities and differences between information retrieval and database technologies and on the indexing techniques that form the basis of many information retrieval systems. This chapter is organized as follows. In Section we introduce information retrieval concepts and discuss how IR differs from traditional databases. Section is devoted to a discussion of retrieval models which form the basis for IR search. Section covers different types of queries in IR systems. Section discusses text preprocessing and Section provides an overview of IR indexing which is at the heart of any IR system. In Section we describe the various evaluation metrics for IR systems performance. Section details Web analysis and its relationship to information retrieval and Section briefly introduces the current trends in IR. Section summarizes the chapter. For a limited overview of IR we suggest that students read Sections through chapter chapter is coauthored with Saurav Sahay of the Georgia Institute of Technology. Chapter Introduction to Information Retrieval and Web Search Information Retrieval Concepts Information retrieval is the process of retrieving documents from a collection in response to a query by a user. This section provides an overview of information retrieval concepts. In Section we introduce information retrieval in general and then discuss the different kinds and levels of search that IR encompasses. In Section we compare IR and database technologies. Section gives a brief history of IR. We then present the different modes of user interaction with IR systems in Section In Section we describe the typical IR process with a detailed set of tasks and then with a simplified process flow and end with a brief discussion of digital libraries and the Web. Introduction to Information Retrieval We first review the distinction between structured and unstructured data called HOUSES with the attributes HOUSES This is an example of structured data. We can compare this relation with homebuying contract documents which are examples of unstructured data. These types of documents can vary from city to city and even county to county within a given state in the United States. Typically a contract document in a particular state will have a standard list of clauses described in paragraphs within sections of the document with some predetermined text and some variable areas whose content is to be supplied by the specific buyer and seller. Other variable information would include interest rate for financing down payment amount closing dates and so on. The documents could also possibly include some pictures taken during a home inspection. The information content in such documents can be considered unstructured data that can be stored in a variety of possible arrangements and formats. By unstructured information we generally mean information that does not have a well defined formal model and corresponding formal language for representation and reasoning but rather is based on understanding of natural language. With the advent of the World Wide Web the volume of unstructured information stored in messages and documents that contain textual and multimedia information has exploded. These documents are stored in a variety of standard formats including HTML XML such information to satisfy the needs of users. The problems that IR deals with are exacerbated by the fact that the number of Web pages and the number of social interaction events is already in the billions and is growing at a phenomenal rate. All forms of unstructured data described above are being added at the rates of millions per day expanding the searchable space on the Web at rapidly increasing rates. Information Retrieval Concepts Historically information retrieval is “the discipline that deals with the structure analysis organization storage searching and retrieval of information” as defined by Gerald Salton an IR We can enhance the definition slightly to say that it applies in the context of unstructured documents to satisfy a user’s information needs. This field has existed even longer than the database field and was originally concerned with retrieval of cataloged information in libraries based on titles authors topics and keywords. In academic programs the field of IR has long been a part of Library and Information Science programs. Information in the context of IR does not require machine understandable structures such as in relational database systems. Examples of such information include written texts abstracts documents books Web pages e mails instant messages and collections from digital libraries. Therefore all loosely represented or semistructured information is also part of the IR discipline. We introduced XML modeling and retrieval in Chapter and discussed advanced data types including spatial temporal and multimedia data in Chapter RDBMS vendors are providing modules to support many of these data types as well as XML data in the newer versions of their products sometimes referred to as extended RDBMSs or object relational database management systems or content of a particular database. IR systems use a user’s information need expressed as a free form search request for interpretation by the system. Whereas the IR field historically dealt with cataloging processing and accessing text in the form of documents for decades in today’s world the use of Web search engines is becoming the dominant way to find information. The traditional problems of text indexing and making collections of documents searchable have been transformed by making the Web itself into a quickly accessible repository of human knowledge. An IR system can be characterized at different levels by types of users types of data and the types of the information need along with the size and scale of the information repository it addresses. Different IR systems are designed to address specific problems that require a combination of different characteristics. These characteristics can be briefly described as follows Types of Users. The user may be an expert user who is searching for specific information that is clear in his her mind and forms relevant queries for the task or a layperson user with a generic information need. The latter cannot create highly relevant queries for search . Types of Data. Search systems can be tailored to specific types of data. For example the problem of retrieving information about a specific topic may be handled more efficiently by customized search systems that are built to collect and retrieve only information related to that specific topic. The information repository could be hierarchically organized based on a concept or topic hierarchy. These topical domain specific or vertical IR systems are not as large as or as diverse as the generic World Wide Web which contains information on all kinds of topics. Given that these domain specific collections exist and may have been acquired through a specific process they can be exploited much more efficiently by a specialized system. Types of Information Need. In the context of Web search users’ information needs may be defined as navigational informational or Navigational search refers to finding a particular piece of information that a user needs quickly. The purpose of informational search is to find current information about a topic . The goal of transactional search is to reach a site where further interaction happens . Levels of Scale. In the words of Nobel Laureate Herbert Simon What information consumes is rather obvious it consumes the attention of its recipients. Hence a wealth of information creates a poverty of attention and a need to allocate that attention efficiently among the overabundance of information sources that might consume it. This overabundance of information sources in effect creates a high noise to signal ratio in IR systems. Especially on the Web where billions of pages are indexed IR interfaces are built with efficient scalable algorithms for distributed searching indexing caching merging and fault tolerance. IR search engines can be limited in level to more specific collections of documents. Enterprise search systems offer IR solutions for searching different entities in an enterprise’s intranet which consists of the network of computers within that enterprise. The searchable entities include e mails corporate documents manuals charts and presentations as well as reports related to people meetings and projects. They still typically deal with hundreds of millions of entities in large global enterprises. On a smaller scale there are personal information systems such as those on desktops and laptops called desktop search engines for retrieving files folders and different kinds of entities stored on the computer. There are peer to peer systems such as Broder for details. Simon “Designing Organizations for an Information Rich Information Retrieval Concepts BitTorrent which allows sharing of music in the form of audio files as well as specialized search engines for audio such as Lycos and Yahoo! audio search. Databases and IR Systems A Comparison Within the computer science discipline databases and IR systems are closely related fields. Databases deal with structured information retrieval through well defined formal languages for representation and manipulation based on the theoretically founded data models. Efficient algorithms have been developed for operators that allow rapid execution of complex queries. IR on the other hand deals with unstructured search with possibly vague query or search semantics and without a welldefined logical schematic representation. Some of the key differences between databases and IR systems are listed in Table Whereas databases have fixed schemas defined in some data model such as the relational model an IR system has no fixed data model it views data or documents according to some scheme such as the vector space model to aid in query processing as the query result providing an exact answer to the query for the current state of the database. In IR systems there is no fixed language for defining the structure of the document or for operating on the document queries tend to be a set of query terms or a free form natural language phrase. An IR query result is a list of document ids or some pieces of text or multimedia objects or a list of links to Web pages. The result of a database query is an exact answer if no matching records are found in the relation the result is empty . On the other hand the answer to a user request in an IR query represents the IR system’s best attempt at retrieving the Table A Comparison of Databases and IR Systems Databases Structured data Schema driven Relational model is predominant Structured query model Rich metadata operations Query returns data Results are based on exact matching IR Systems Unstructured data No fixed schema various data models Free form query models Rich data operations Search request returns list or pointers to documents Results are based on approximate matching and measures of effectiveness Chapter Introduction to Information Retrieval and Web Search information most relevant to that query. Whereas database systems maintain a large amount of metadata and allow their use in query optimization the operations in IR systems rely on the data values themselves and their occurrence frequencies. Complex statistical analysis is sometimes performed to determine the relevance of each document or parts of a document to the user request. A Brief History of IR Information retrieval has been a common task since the times of ancient civilizations which devised ways to organize store and catalog documents and records. Media such as papyrus scrolls and stone tablets were used to record documented information in ancient times. These efforts allowed knowledge to be retained and transferred among generations. With the emergence of public libraries and the printing press large scale methods for producing collecting archiving and distributing documents and books evolved. As computers and automatic storage systems emerged the need to apply these methods to computerized systems arose. Several techniques emerged in the such as the seminal work of H. P. who proposed using words and their frequency counts as indexing units for documents and using measures of word overlap between queries and documents as the retrieval criterion. It was soon realized that storing large amounts of text was not difficult. The harder task was to search for and retrieve that information selectively for users with specific information needs. Methods that explored word distribution statistics gave rise to the choice of keywords based on their distribution and keywordbased weighting schemes. The earlier experiments with document retrieval systems such as in the adopted the inverted file organization based on keywords and their weights as the method of indexing organization proved inadequate if queries required fast near real time response times. Proper organization of these files became an important area of study document classification and clustering schemes ensued. The scale of retrieval experiments remained a challenge due to lack of availability of large text collections. This soon changed with the World Wide Web. Also the Text Retrieval Conference was launched by NIST in as a part of the TIPSTER with the goal of providing a platform for evaluating information retrieval methodologies and facilitating technology transfer to develop IR products. A search engine is a practical application of information retrieval to large scale document collections. With significant advances in computers and communications technologies people today have interactive access to enormous amounts of user generated distributed content on the Web. This has spurred the rapid growth Luhn “A statistical approach to mechanized encoding and searching of literary Salton Yang and Yu details see Buckley et al. details see Harman in search engine technology where search engines are trying to discover different kinds of real time content found on the Web. The part of a search engine responsible for discovering analyzing and indexing these new documents is known as a crawler. Other types of search engines exist for specific domains of knowledge. For example the biomedical literature search database was started in the and is now supported by the PubMed search which gives access to over million abstracts. While continuous progress is being made to tailor search results to the needs of an end user the challenge remains in providing high quality pertinent and timely information that is precisely aligned to the information needs of individual users. Modes of Interaction in IR Systems In the beginning of Section we defined information retrieval as the process of retrieving documents from a collection in response to a query by a user. Typically the collection is made up of documents containing unstructured data. Other kinds of documents include images audio recordings video strips and maps. Data may be scattered nonuniformly in these documents with no definitive structure. A query is a set of terms used by the searcher to specify an information need . An informational request or a search query may also be a natural language phrase or a question . There are two main modes of interaction with IR systems retrieval and browsing which although similar in goal are accomplished through different interaction tasks. Retrieval is concerned with the extraction of relevant information from a repository of documents through an IR query while browsing signifies the activity of a user visiting or navigating through similar or related documents based on the user’s assessment of relevance. During browsing a user’s information need may not be defined a priori and is flexible. Consider the following browsing scenario A user specifies ‘Atlanta’ as a keyword. The information retrieval system retrieves links to relevant result documents containing various aspects of Atlanta for the user. The user comes across the term ‘Georgia Tech’ in one of the returned documents and uses some access technique and visits documents about Georgia Tech in the same or a different Website . There the user finds an entry for ‘Athletics’ that leads the user to information about various athletic programs at Georgia Tech. Eventually the user ends his search at the Fall schedule for the Yellow Jackets football team which he finds to be of great interest. This user activity is known as browsing. Hyperlinks are used to interconnect Web pages and are mainly used for browsing. Anchor texts are text phrases within documents used to label hyperlinks and are very relevant to browsing. Information Retrieval Concepts Chapter Introduction to Information Retrieval and Web Search Web search combines both aspects browsing and retrieval and is one of the main applications of information retrieval today. Web pages are analogous to documents. Web search engines maintain an indexed repository of Web pages usually using the technique of inverted indexing corporate manuals and reports government notices Web page articles blogs tweets books and journal papers. There are two main approaches to IR statistical and semantic. In a statistical approach documents are analyzed and broken down into chunks of text and each word or phrase is counted weighted and measured for relevance or importance. These words and their properties are then compared with the query terms for potential degree of match to produce a ranked list of resulting documents that contain the words. Statistical approaches are further classified based on the method employed. The three main statistical approaches are Boolean vector space and probabilistic Type of similarity measure Keywords Boolean phrase proximity wildcard queries etc. Conversion from humanly understandable to internal format Situation assessment Query expansion heuristics Storing user’s feedback Personalization Pattern analysis of relevant results Metadata Integration Ranking results Showing useful metadata External data ontologies Document Retrieval Figure Generic IR framework. containing these words and the document properties such as date of creation author and type of document are fetched from the inverted index and compared with the query. This comparison results in a ranked list shown to the user. The user can then provide feedback on the results that triggers implicit or explicit query expansion to fetch results that are more relevant for the user. Most IR systems allow for an interactive search where the query and the results are successively refined. Retrieval Models In this section we briefly describe the important models of IR. These are the three main statistical models Boolean vector space and probabilistic and the semantic model. Chapter Introduction to Information Retrieval and Web Search Documents EXTRACT FEEDBACK QUERY FETCH PROCESS Inverted Index COMPARE Query x Documents RANK Two tickets tickled slightly angst riden orifices. Two Jabberwockies sacrificed subways and two mosly bourgeois orifices towed Kermit. Five very progressive fountains annoyingly tickled the partly speedy dog even though two putrid sheep laughed almost noisily. Document Two tickets tickled slightly angst riden orifices. Two Jabberwockies sacrificed subways and two mosly bourgeois orifices towed Kermit. Five very progressive fountains annoyingly tickled the partly speedy dog even though two putrid sheep laughed almost noisily. Document Two tickets tickled slightly angst riden orifices. Two Jabberwockies sacrificed subways and two mosly bourgeois orifices towed Kermit. Five very progressive fountains annoyingly tickled the partly speedy dog even though two putrid sheep laughed almost noisily. Document Two tickets tickled slightly angst riden orifices. Two Jabberwockies sacrificed subways and two mosly bourgeois orifices towed Kermit. Five very progressive fountains annoyingly tickled the partly speedy dog even though two putrid sheep laughed almost noisily. Document Result Two tickets tickled slightly angst riden orifices. Two Jabberwockies sacrificed subways and two mosly bourgeois orifices towed Kermit. Five very progressive fountains annoyingly tickled the partly speedy dog even though two putrid sheep laughed almost noisily. Result Two tickets tickled slightly angst riden orifices. Two Jabberwockies sacrificed subways and two mosly bourgeois orifices towed Kermit. Five very progressive fountains annoyingly tickled the partly speedy dog even though two putrid sheep laughed almost noisily. Result Index SEARCH INTENT Figure Simplified IR process pipeline. Boolean Model In this model documents are represented as a set of terms. Queries are formulated as a combination of terms using the standard Boolean logic set theoretic operators such as AND OR and NOT. Retrieval and relevance are considered as binary concepts in this model so the retrieved elements are an “exact match” retrieval of relevant documents. There is no notion of ranking of resulting documents. All retrieved documents are considered equally important a major simplification that does not consider frequencies of document terms or their proximity to other terms compared against the query terms. Boolean retrieval models lack sophisticated ranking algorithms and are among the earliest and simplest information retrieval models. These models make it easy to associate metadata information and write queries that match the contents of the Retrieval Models documents as well as other properties of documents such as date of creation author and type of document. Vector Space Model The vector space model provides a framework in which term weighting ranking of retrieved documents and relevance feedback are possible. Documents are represented as features and weights of term features in an n dimensional vector space of terms. Features are a subset of the terms in a set of documents that are deemed most relevant to an IR search for this particular set of documents. The process of selecting these important terms and their properties as a sparse list out of the very large number of available terms is independent of the model specification. The query is also specified as a terms vector and this is compared to the document vectors for similarity relevance assessment. The similarity assessment function that compares two vectors is not inherent to the model different similarity functions can be used. However the cosine of the angle between the query and document vector is a commonly used function for similarity assessment. As the angle between the vectors decreases the cosine of the angle approaches one meaning that the similarity of the query with a document vector increases. Terms are weighted proportional to their frequency counts to reflect the importance of terms in the calculation of relevance measure. This is different from the Boolean model which does not take into account the frequency of words in the document for relevance match. In the vector model the document term weight wij is represented based on some variation of the TF or TF IDF scheme . TF IDF is a statistical weight measure that is used to evaluate the importance of a document word in a collection of documents. The following formula is typically used In the formula given above we use the following symbols dj is the document vector. q is the query vector. wij is the weight of term i in document j. wiq is the weight of term i in query vector q. |V| is the number of dimensions in the vector that is the total number of important keywords . TF IDF uses the product of normalized frequency of a term i in document Dj and the inverse document frequency of the term i to weight a term in a cosine || || || || | | d q d q d q w w w j j j i ij iq V ij × × × i V i iq V w × || || Chapter Introduction to Information Retrieval and Web Search document. The idea is that terms that capture the essence of a document occur frequently in the document but if such a term were to be a good term that discriminates the document from others it must occur in only a few documents in the general population . IDF values can be easily computed for a fixed collection of documents. In case of Web search engines taking a representative sample of documents approximates IDF computation. The following formulas can be used In these formulas the meaning of the symbols is TFij is the normalized term frequency of term i in document Dj . f ij is the number of occurrences of term i in document Dj . IDFi is the inverse document frequency weight for term i. N is the number of documents in the collection. ni is the number of documents in which term i occurs. Note that if a term i occurs in all documents then ni N and hence IDFi log becomes zero nullifying its importance and creating a situation where division by zero can occur. The weight of term i in document j wij is computed based on its TFIDF value in some techniques. To prevent division by zero it is common to add a to the denominator in the formulae such as the cosine formula above. Sometimes the relevance of the document with respect to a query is directly measured as the sum of the TF IDF values of the terms in the Query Q The normalization factor is incorporated into the TF IDF formula itself thereby measuring relevance of a document to the query by the computation of the dot product of the query and document vectors. The algorithm is a well known relevance feedback algorithm based on the vector space model that modifies the initial query vector and its weights in response to user identified relevant documents. It expands the original query vector q to a new vector qe as follows q q D d D d e r r d D ir ir r r ir ir d D + − ∈ ∈ α β γ || | | rel D Q TF IDF j i Q ij i × ∈ TF f f IDF N n ij ij ij i V i i to| | log Rocchio Retrieval Models Here Dr and Dir are relevant and nonrelevant document sets and α β and γ are parameters of the equation. The values of these parameters determine how the feedback affects the original query and these may be determined after a number of trialand error experiments. Probabilistic Model The similarity measures in the vector space model are somewhat ad hoc. For example the model assumes that those documents closer to the query in cosine space are more relevant to the query vector. In the probabilistic model a more concrete and definitive approach is taken ranking documents by their estimated probability of relevance with respect to the query and the document. This is the basis of the Probability Ranking Principle developed by In the probabilistic framework the IR system has to decide whether the documents belong to the relevant set or the nonrelevant set for a query. To make this decision it is assumed that a predefined relevant set and nonrelevant set exist for the query and the task is to calculate the probability that the document belongs to the relevant set and compare that with the probability that the document belongs to the nonrelevant set. Given the document representation D of a document estimating the relevance R and nonrelevance NR of that document involves computation of conditional probability P and P. These conditional probabilities can be calculated using Bayes’ P P × P P P P × P P A document D is classified as relevant if P P. Discarding the constant P this is equivalent to saying that a document is relevant if P × P P × P The likelihood ratio P P is used as a score to determine the likelihood of the document with representation D belonging to the relevant set. The term independence or Naïve Bayes assumption is used to estimate P using computation of P for term t i . The likelihood ratios P P of documents are used as a proxy for ranking based on the assumption that highly ranked documents will have a high likelihood of belonging to the relevant a description of the Cheshire II system see Robertson theorem is a standard technique for measuring likelihood see Howson and Urbach for example. should refer to Croft et al. pages for a detailed description. Chapter Introduction to Information Retrieval and Web Search With some reasonable assumptions and estimates about the probabilistic model along with extensions for incorporating query term weights and document term weights in the model a probabilistic ranking algorithm called of dj . avdl is the average document length of the collection. The Okapi relevance score of a document dj for a query q is given by the equation below where matching. This allows retrieval of relevant documents that share meaningful associations with other documents in the query result even when these associations are not inherently observed or statistically captured. Semantic approaches include different levels of analysis such as morphological syntactic and semantic analysis to retrieve documents more effectively. In morphological analysis roots and affixes are analyzed to determine the parts of speech of the words. Following morphological analysis syntactic analysis follows to parse and analyze complete phrases in documents. Finally the semantic methods have to resolve word ambiguities and or generate relevant synonyms based on the semantic relationships between levels of structural entities in documents . okapi ln . . d q N df df k f k bb j i i ij − + + × University of London Okapi System by Robertson Walker and Hancock Beaulieu Types of Queries in IR Systems The development of a sophisticated semantic system requires complex knowledge bases of semantic information as well as retrieval heuristics. These systems often require techniques from artificial intelligence and expert systems. Knowledge bases like and have been developed for use in knowledge based IR systems based on semantic models. The Cyc knowledge base for example is a representation of a vast quantity of commonsense knowledge about assertions interrelating more than concepts for reasoning about the objects and events of everyday life. WordNet is an extensive thesaurus that is very popular and is used by many systems and is under continuous development as a preprocessing step before sending the filtered query keywords to the IR engine. Most IR systems do not pay attention to the ordering of these words in the query. All retrieval models provide support for keyword queries. Boolean Queries Some IR systems allow using the AND OR NOT + and – Boolean operators in combinations of keyword formulations. AND requires that both terms be found. OR lets either term be found. NOT means any record containing the second term will be excluded. ‘’ means the Boolean operators can be nested using parentheses. ‘+’ is equivalent to AND requiring the term the ‘+’ should be placed directly in front Lenat Miller for a detailed description of WordNet. Chapter Introduction to Information Retrieval and Web Search of the search is equivalent to AND NOT and means to exclude the term the ‘–’ should be placed directly in front of the search term not wanted. Complex Boolean queries can be built out of these operators and their combinations and they are evaluated according to the classical rules of Boolean algebra. No ranking is possible because a document either satisfies such a query or does not satisfy it . A document is retrieved for a Boolean query if the query is logically true as an exact match in the document. Users generally do not use combinations of these complex Boolean operators and IR systems support a restricted version of these set operators. Boolean retrieval models can directly support different Boolean operator implementations for these kinds of queries. Phrase Queries When documents are represented using an inverted keyword index for searching the relative order of the terms in the document is lost. In order to perform exact phrase retrieval these phrases should be encoded in the inverted index or implemented differently . A phrase query consists of a sequence of words that makes up a phrase. The phrase is generally enclosed within double quotes. Each retrieved document must contain at least one instance of the exact phrase. Phrase searching is a more restricted and specific version of proximity searching that we mention below. For example a phrase searching query could be ‘conceptual database design’. If phrases are indexed by the retrieval model any retrieval model can be used for these query types. A phrase thesaurus may also be used in semantic models for fast dictionary searching for phrases. Proximity Queries Proximity search refers to a search that accounts for how close within a record multiple terms should be to each other. The most commonly used proximity search option is a phrase search that requires terms to be in the exact order. Other proximity operators can specify how close terms should be to each other. Some will also specify the order of the search terms. Each search engine can define proximity operators differently and the search engines use various operator names such as NEAR ADJ or AFTER. In some cases a sequence of single words is given together with a maximum allowed distance between them. Vector space models that also maintain information about positions and offsets of tokens have robust implementations for this query type. However providing support for complex proximity operators becomes computationally expensive because it requires the time consuming preprocessing of documents and is thus suitable for smaller document collections rather than for the Web. Wildcard Queries Wildcard searching is generally meant to support regular expressions and pattern matching based searching in text. In IR systems certain kinds of wildcard search support may be implemented usually words with any trailing characters . Providing support for wildcard searches in IR systems involves preprocessing overhead and is not considered worth the cost by many Web search engines today. Retrieval models do not directly provide support for this query type. Natural Language Queries There are a few natural language search engines that aim to understand the structure and meaning of queries written in natural language text generally as a question or narrative. This is an active area of research that employs techniques like shallow semantic parsing of text or query reformulations based on natural language understanding. The system tries to formulate answers for such queries from retrieved results. Some search systems are starting to provide natural language interfaces to provide answers to specific types of questions such as definition and factoid questions which ask for definitions of technical terms or common facts that can be retrieved from specialized databases. Such questions are usually easier to answer because there are strong linguistic patterns giving clues to specific types of sentences for example ‘defined as’ or ‘refers to’. Semantic models can provide support for this query type. Text Preprocessing In this section we review the commonly used text preprocessing techniques that are part of the text processing task in Figure Stopword Removal Stopwords are very commonly used words in a language that play a major role in the formation of a sentence but which seldom contribute to the meaning of that sentence. Words that are expected to occur in percent or more of the documents in a collection are typically referred to as stopwords and they are rendered potentially useless. Because of the commonness and function of these words they do not contribute much to the relevance of a document for a query search. Examples include words such as the of to a and in said for that was on he is with at by and it. These words are presented here with decreasing frequency of occurrence from a large corpus of documents called The fist six of these words account for percent of all words in the listing and the most frequent words account for percent of all text. Removal of stopwords from a document must be performed before indexing. Articles prepositions conjunctions and some pronouns are generally classified as stopwords. Queries must also be preprocessed for stopword removal before the actual retrieval process. Removal of stopwords results in elimination of possible spurious indexes thereby reducing the size of an index structure by about details see Croft et al. pages Chapter Introduction to Information Retrieval and Web Search percent or more. However doing so could impact the recall if the stopword is an integral part of a query . Many search engines do not employ query stopword removal for this reason. Stemming A stem of a word is defined as the word obtained after trimming the suffix and prefix of an original word. For example ‘comput’ is the stem word for computer computing and computation. These suffixes and prefixes are very common in the English language for supporting the notion of verbs tenses and plural forms. Stemming reduces the different forms of the word formed by inflection and derivation to a common stem. A stemming algorithm can be applied to reduce any word to its stem. In English the most famous stemming algorithm is Martin Porter’s stemming algorithm. The Porter is a simplified version of Lovin’s technique that uses a reduced set of about rules and organizes them into sets conflicts within one subset of rules are resolved before going on to the next. Using stemming for preprocessing data results in a decrease in the size of the indexing structure and an increase in recall possibly at the cost of precision. Utilizing a Thesaurus A thesaurus comprises a precompiled list of important concepts and the main word that describes each concept for a particular domain of knowledge. For each concept in this list a set of synonyms and related words is also Thus a synonym can be converted to its matching concept during preprocessing. This preprocessing step assists in providing a standard vocabulary for indexing and searching. Usage of a thesaurus also known as a collection of synonyms has a substantial impact on the recall of information systems. This process can be complicated because many words have different meanings in different contexts. is a large biomedical thesaurus of millions of concepts and a semantic network of meta concepts and relationships that organize the Metathesaurus . Hyphens and punctuation marks may be handled in different ways. Either the entire phrase with the hyphens punctuation marks may be used or they may be eliminated. In some systems the character representing the hyphen punctuation mark may be removed or may be replaced with a space. Different information retrieval systems follow different rules of processing. Handling hyphens automatically can be complex it can either be done as a classification problem or more commonly by some heuristic rules. Most information retrieval systems perform case insensitive search converting all the letters of the text to uppercase or lowercase. It is also worth noting that many of these text preprocessing steps are language specific such as involving accents and diacritics and the idiosyncrasies that are associated with a particular language. Information Extraction Information extraction is a generic term used for extracting structured content from text. Text analytic tasks such as identifying noun phrases facts events people places and relationships are examples of IE tasks. These tasks are also called named entity recognition tasks and use rule based approaches with either a thesaurus regular expressions and grammars or probabilistic approaches. For IR and search applications IE technologies are mostly used to identify contextually relevant features that involve text analysis matching and categorization for improving the relevance of search systems. Language technologies using part of speech tagging are applied to semantically annotate the documents with extracted features to aid search relevance. Inverted Indexing The simplest way to search for occurrences of query terms in text collections can be performed by sequentially scanning the text. This kind of online searching is only appropriate when text collections are quite small. Most information retrieval systems process the text collections to create indexes and operate upon the inverted index data structure metric that we described in Section For a given term this weighting scheme distinguishes to some extent the documents in which the term occurs more often from those in which the term occurs very little or never. These weights are normalized to account for varying document lengths further ensuring that longer documents with proportionately more occurrences of a word are not favored for retrieval over shorter documents with proportionately fewer occurrences. These processed document term streams are then inverted into term document streams for further IR steps. Figure shows an illustration of term document position vectors for the four illustrative terms example inverted index and market which refer to the three documents and the position where they occur in those documents. The different steps involved in inverted index construction can be summarized as follows Break the documents into vocabulary terms by tokenizing cleansing stopword removal stemming and or use of an additional thesaurus as vocabulary. Collect document statistics and store the statistics in a document lookup table. Invert the document term stream into a term document stream along with additional information such as term frequencies term positions and term weights. Searching for relevant documents from the inverted index given a set of query terms is generally a three step process. Vocabulary search. If the query comprises multiple terms they are separated and treated as independent terms. Each term is searched in the vocabulary. Various data structures like variations of B+ tree or hashing may be This example shows an example of an inverted index. Inverted index is a data structure for associating terms to documents. Stock market index is used for capturing the sentiments of the financial market. Stock market index is used for capturing the sentiments of the financial market. ID Term example inverted index market Document position Document Document Document Chapter Introduction to Information Retrieval and Web Search used to optimize the search process. Query terms may also be ordered in lexicographic order to improve space efficiency. Document information retrieval. The document information for each term is retrieved. Manipulation of retrieved information. The document information vector for each term obtained in step is now processed further to incorporate various forms of query logic. Various kinds of queries like prefix range context and proximity queries are processed in this step to construct the final result based on the document collections returned in step Evaluation Measures of Search Relevance Without proper evaluation techniques one cannot compare and measure the relevance of different retrieval models and IR systems in order to make improvements. Figure Example of an inverted index. Evaluation Measures of Search Relevance Evaluation techniques of IR systems measure the topical relevance and user relevance. Topical relevance measures the extent to which the topic of a result matches the topic of the query. Mapping one’s information need with “perfect” queries is a cognitive task and many users are not able to effectively form queries that would retrieve results more suited to their information need. Also since a major chunk of user queries are informational in nature there is no fixed set of right answers to show to the user. User relevance is a term used to describe the “goodness” of a retrieved result with regard to the user’s information need. User relevance includes other implicit factors such as user perception context timeliness the user’s environment and current task needs. Evaluating user relevance may also involve subjective analysis and study of user retrieval tasks to capture some of the properties of implicit factors involved in accounting for users’ bias for judging performance. In Web information retrieval no binary classification decision is made on whether a document is relevant or nonrelevant to a query retrieval model uses this scheme as we discussed in Section Instead a ranking of the documents is produced for the user. Therefore some evaluation measures focus on comparing different rankings produced by IR systems. We discuss some of these measures next. Recall and Precision Recall and precision metrics are based on the binary relevance assumption . Recall is defined as the number of relevant documents retrieved by a search divided by the total number of existing relevant documents. Precision is defined as the number of relevant documents retrieved by a search divided by the total number of documents retrieved by that search. Figure is a pictorial representation of the terms retrieved vs. relevant and shows how search results relate to four different sets of documents. Relevant Yes No Hits TP False Alarms FP Misses FN Correct Rejections TN Retrieved Yes No ☺ ☺ Figure Retrieved vs. relevant search results. Chapter Introduction to Information Retrieval and Web Search Table Precision and Recall for Ranked Retrieval Doc. No. Rank Position i Relevant Precision Recall Yes Yes Yes No No No Yes Yes No Yes The notation for Figure is as follows TP true positive FP false positive FN false negative TN true negative The terms true positive false positive false negative and true negative are generally used in any type of classification tasks to compare the given classification of an item with the desired correct classification. Using the term hits for the documents that truly or “correctly” match the user request we can define Recall |Hits| |Relevant| Precision |Hits| |Retrieved| Recall and precision can also be defined in a ranked retrieval setting. The Recall at rank position i for document di q is the fraction of relevant documents from q to di q in the result set for the query. Let the set of relevant documents from q to di q in that set be Si with cardinality | Si |. Let . Then Recall r |Si | |Dq| The Precision at rank position i or document di q is the fraction of documents from q to di q in the result set that are relevant Precision p |Si | i Table illustrates the p r and average precision metrics. It can be seen that recall can be increased by presenting more results to the user but this approach runs the risk of decreasing the precision. In the Evaluation Measures of Search Relevance example the number of relevant documents for some query The rank position and the relevance of an individual document are shown. The precision and recall value can be computed at each position within the ranked list as shown in the last two columns. Average Precision Average precision is computed based on the precision at each relevant document in the ranking. This measure is useful for computing a single precision value to compare different retrieval algorithms on a query q. Consider the sample precision values of relevant documents in Table The average precision for the example in Table is + + + + + percent . Many good algorithms tend to have high top k average precision for small values of k with correspondingly low values of recall. Recall Precision Curve A recall precision curve can be drawn based on the recall and precision values at each rank position where the x axis is the recall and the y axis is the precision. Instead of using the precision and recall at each rank position the curve is commonly plotted using recall levels r at percent percent percent. The curve usually has a negative slope reflecting the inverse relationship between precision and recall. F Score F score is the harmonic mean of the precision and recall values. High precision is achieved almost always at the expense of recall and vice versa. It is a matter of the application’s context whether to tune the system for high precision or high recall. F score is a single measure that combines precision and recall to compare different result sets One of the properties of harmonic mean is that the harmonic mean of two numbers tends to be closer to the smaller of the two. Thus F is automatically biased toward the smaller of the precision and recall values. Therefore for a high F score both precision and recall must be high. F p r + F pr p r + P pi D d D q i q q avg ∈ | | Chapter Introduction to Information Retrieval and Web Search Web Search and The emergence of the Web has brought millions of users to search for information which is stored in a very large number of active sites. To make this information accessible search engines such as Google and Yahoo! have to crawl and index these sites and document collections in their index databases. Moreover search engines have to regularly update their indexes given the dynamic nature of the Web as new Web sites are created and current ones are updated or deleted. Since there are many millions of pages available on the Web on different topics search engines have to apply many sophisticated techniques such as link analysis to identify the importance of pages. There are other types of search engines besides the ones that regularly crawl the Web and create automatic indexes these are human powered vertical search engines or metasearch engines. These search engines are developed with the help of computerassisted systems to aid the curators with the process of assigning indexes. They consist of manually created specialized Web directories that are hierarchically organized indexes to guide user navigation to different resources on the Web. Vertical search engines are customized topic specific search engines that crawl and index a specific collection of documents on the Web and provide search results from that specific collection. Metasearch engines are built on top of search engines they query different search engines simultaneously and aggregate and provide search results from these sources. Another source of searchable Web documents is digital libraries. Digital libraries can be broadly defined as collections of electronic resources and services for the delivery of materials in a variety of formats. These collections may include a university’s library catalog catalogs from a group of participating universities as in the State of Florida University System or a compilation of multiple external resources on the World Wide Web such as Google Scholar or the IEEE ACM index. These interfaces provide universal access to different types of content such as books articles audio and video situated in different database systems and remote repositories. Similar to real libraries these digital collections are maintained via a catalog and organized in categories for online reference. Digital libraries “include personal distributed and centralized collections such as online public access catalogs and bibliographic databases distributed document databases scholarly and professional discussion lists and electronic journals other online databases forums and bulletin Web Analysis and Its Relationship to Information Retrieval In addition to browsing and searching the Web another important activity closely related to information retrieval is to analyze or mine information on the Web for contributions of Pranesh P. Ranganathan and Hari P. Kumar to this section is appreciated. and Kling page Web Search and Analysis new information of interest. allowing users to browse from page to page. A hyperlink has two components a destination page and an anchor text describing the link. For example a person can link to the Yahoo! Website on his Web page with anchor text such as “My favorite Anchor texts can be thought of as being implicit endorsements. They provide very important latent human annotation. A person linking to other Web pages from his Web page is assumed to have some relation to those Web pages. Web search engines aim to distill results per their relevance and authority. There are many redundant hyperlinks like the links to the homepage on every Web page of the Web site. Such hyperlinks must be eliminated from the search results by the search engines. A hub is a Web page or a Website that links to a collection of prominent sites on a common topic. A good authority is a page that is pointed to by many good hubs while a good hub is a page that points to many good authorities. These ideas are used by the HITS ranking algorithm which is described in Section It is often found that authoritative pages are not very self descriptive and authorities on broad topics seldom link directly to one another. These properties of hyperlinks are being actively used to improve Web search engine result ranking and organize the results as hubs and authorities. We briefly discuss a couple of ranking algorithms below. Analyzing the Link Structure of Web Pages The goal of Web structure analysis is to generate structural summary about the Website and Web pages. It focuses on the inner structure of documents and deals with the link structure using hyperlinks at the interdocument level. The structure and content of Web pages are often combined for information retrieval by Web search engines. Given a collection of interconnected Web documents interesting and informative facts describing their connectivity in the Web subset can be discovered. Web structure analysis is also used to reveal the structure of Web pages which Web Search and Analysis helps with navigation and makes it possible to compare integrate Web page schemes. This aspect of Web structure analysis facilitates Web document classification and clustering on the basis of structure. The PageRank Ranking Algorithm. As discussed earlier ranking algorithms are used to order search results based on relevance and authority. Google uses the wellknown PageRank which is based on the “importance” of each page. Every Web page has a number of forward links and backlinks . It is very difficult to determine all the backlinks of a Web page while it is relatively straightforward to determine its forward links. According to the PageRank algorithm highly linked pages are more important than pages with fewer links. However not all backlinks are important. A backlink to a page from a credible source is more important than a link from some arbitrary page. Thus a page has a high rank if the sum of the ranks of its backlinks is high. PageRank was an attempt to see how good an approximation to the “importance” of a page can be obtained from the link structure. The computation of page ranking follows an iterative approach. PageRank of a Web page is calculated as a sum of the PageRanks of all its backlinks. PageRank treats the Web like a Markov model. An imaginary Web surfer visits an infinite string of pages by clicking randomly. The PageRank of a page is an estimate of how often the surfer winds up at a particular page. PageRank is a measure of query independent importance of a page node. For example let P be the PageRank of any page X and C be the number of outgoing links from page X and let d be the damping factor in the range d Usually d is set to Then PageRank for a page A can be calculated as P – d + d + + P C Here Tn are the pages that point to Page A . PageRank forms a probability distribution over Web pages so the sum of all Web pages’ PageRanks is one. The HITS Ranking Algorithm. The algorithm proposed by Jon Kleinberg is another type of ranking algorithm exploiting the link structure of the Web. The algorithm presumes that a good hub is a document that points to many hubs and a good authority is a document that is pointed at by many other authorities. The algorithm contains two main steps a sampling component and a weightpropagation component. The sampling component constructs a focused collection S of pages with the following properties S is relatively small. S is rich in relevant pages. S contains most of the strongest authorities. PageRank algorithm was proposed by Lawrence Page and Sergey Brin founders of Google. For more information see Kleinberg Chapter Introduction to Information Retrieval and Web Search The weight component recursively calculates the hub and authority values for each document as follows Initialize hub and authority values for all pages in S by setting them to While a. For each page in S calculate authority value Sum of hub values of all pages pointing to the current page. b. For each page in S calculate hub value Sum of authority values of all pages pointed at by the current page. c. Normalize hub and authority values such that sum of all hub values in S equals and the sum of all authority values in S equals Web Content Analysis As mentioned earlier Web content analysis refers to the process of discovering useful information from Web content data documents. The Web content data consists of unstructured data such as free text from electronically stored documents semistructured data typically found as HTML documents with embedded image data and more structured data such as tabular data and pages in HTML XML or other markup languages generated as output from databases. More generally the term Web content refers to any real data in the Web page that is intended for the user accessing that page. This usually consists of but is not limited to text and graphics. We will first discuss some preliminary Web content analysis tasks and then look at the traditional analysis tasks of Web page classification and clustering later. Structured Data Extraction. Structured data on the Web is often very important as it represents essential information such as a structured table showing the airline flight schedule between two cities. There are several approaches to structured data extraction. One includes writing a wrapper or a program that looks for different structural characteristics of the information on the page and extracts the right content. Another approach is to manually write an extraction program for each Website based on observed format patterns of the site which is very labor intensive and time consuming. It does not scale to a large number of sites. A third approach is wrapper induction or wrapper learning where the user first manually labels a set of training set pages and the learning system generates rules based on the learning pages that are applied to extract target items from other Web pages. A fourth approach is the automatic approach which aims to find patterns grammars from the Web pages and then uses wrapper generation to produce a wrapper to extract data automatically. Web Information Integration. The Web is immense and has millions of documents authored by many different persons and organizations. Because of this Web pages that contain similar information may have different syntax and different words that describe the same concepts. This creates the need for integrating Web Search and Analysis information from diverse Web pages. Two popular approaches for Web information integration are Web query interface integration to enable querying multiple Web databases that are not visible in external interfaces and are hidden in the “deep The deep consists of those pages that do not exist until they are created dynamically as the result of a specific database search which produces some of the information in the page of a domain of knowledge. Because each term of source deep Web as defined by Bergman Chapter Introduction to Information Retrieval and Web Search ontology is based on the primitives the terms become more easily comparable than in multiple ontology approaches. The advantage of a hybrid approach is that new sources can be easily added without the need to modify the mappings or the shared vocabulary. In multiple and hybrid approaches several research issues such as ontology mapping alignment and merging need to be addressed. Building Concept Hierarchies. One common way of organizing search results is via a linear ranked list of documents. But for some users and applications a better way to display results would be to create groupings of related documents in the search result. One way of organizing documents in a search result and for organizing information in general is by creating a concept hierarchy. The documents in a search result are organized into groups in a hierarchical fashion. Other related techniques to organize docments are through classification and clustering and database based . The agent based approach involves the development of sophisticated artificial intelligence systems that can act autonomously or semi autonomously on behalf of a particular user to discover and process Web based information. Generally the agent based Web analysis systems can be placed into the following three categories Intelligent Web agents are software agents that search for relevant information using characteristics of a particular application domain to organize and interpret the discovered information. For example an intelligent agent that retrieves product information from a variety of vendor sites using only general information about the product domain. Information Filtering Categorization is another technique that utilizes Web agents for categorizing Web documents. These Web agents use methods from information retrieval and semantic information based on the links among various documents to organize documents into a concept hierarchy. Personalized Web agents are another type of Web agents that utilize the personal preferences of users to organize search results or to discover information and documents that could be of value for a particular user. User Web Search and Analysis preferences could be learned from previous user choices or from other individuals who are considered to have similar preferences to the user. The database based approach aims to infer the structure of the Website or to transform a Web site to organize it as a database so that better information management and querying on the Web become possible. This approach of Web content analysis primarily tries to model the data on the Web and integrate it so that more sophisticated queries than keyword based search can be performed. These could be achieved by finding the schema of Web documents building a Web document warehouse a Web knowledge base or a virtual database. The database based approach may use a model such as the Object Exchange Model that represents semistructured data by a labeled graph. The data in the OEM is viewed as a graph with objects as the vertices and labels on the edges. Each object is identified by an object identifier and a value that is either atomic such as integer string GIF image or HTML document or complex in the form of a set of object references. The main focus of the database based approach has been with the use of multilevel databases and Web query systems. A multilevel database at its lowest level is a database containing primitive semistructured information stored in various Web repositories such as hypertext documents. At the higher levels metadata or generalizations are extracted from lower levels and organized in structured collections such as relational or object oriented databases. In a Web query system information about the content and structure of Web documents is extracted and organized using database like techniques. Query languages similar to SQL can then be used to search and query Web documents. They combine structural queries based on the organization of hypertext documents and content based queries. Web Usage Analysis Web usage analysis is the application of data analysis techniques to discover usage patterns from Web data in order to understand and better serve the needs of Webbased applications. This activity does not directly contribute to information retrieval but it is important to improve or enhance the users’ search experience. Web usage data describes the pattern of usage of Web pages such as IP addresses page references and the date and time of accesses for a user user group or an application. Web usage analysis typically consists of three main phases preprocessing pattern discovery and pattern analysis. Preprocessing. Preprocessing converts the information collected about usage statistics and patterns into a form that can be utilized by the pattern discovery methods. We use the term “page view” to refer to pages viewed or visited by a user. There are several different types of preprocessing techniques available Usage preprocessing analyzes the available collected data about usage patterns of users applications and groups of users. Because this data is often incomplete the process is difficult. Data cleaning techniques are necessary to Kosala and Blockeel Chapter Introduction to Information Retrieval and Web Search eliminate the impact of irrelevant items in the analysis result. Frequently usage data is identified by an IP address and consists of clicking streams that are collected at the server. Better data is available if a usage tracking process is installed at the client site. Content preprocessing is the process of converting text image scripts and other content into a form that can be used by the usage analysis. Often this consists of performing content analysis such as classification or clustering. The clustering or classification techniques can group usage information for similar types of Web pages so that usage patterns can be discovered for specific classes of Web pages that describe particular topics. Page views can also be classified according to their intended use such as for sales or for discovery or for other uses. Structure preprocessing The structure preprocessing can be done by parsing and reformatting the information about hyperlinks and structure between viewed pages. One difficulty is that the site structure may be dynamic and may have to be constructed for each server session. Pattern Discovery The techniques that are used in pattern discovery are based on methods from the fields of statistics machine learning pattern recognition data analysis data mining and other similar areas. These techniques are adapted so they take into consideration the specific knowledge and characteristics for Web Analysis. For example in association rule discovery These pages may not be directly connected to one another via hyperlinks. For example association rule discovery may reveal a correlation between users who visited a page containing electronic products to those who visit a page about sporting equipment. Clustering. In the Web usage domain there are two kinds of interesting clusters to be discovered usage clusters and page clusters. Clustering of users tends to establish groups of users exhibiting similar browsing patterns. Web Search and Analysis Such knowledge is especially useful for inferring user demographics in order to perform market segmentation in E commerce applications or provide personalized Web content to the users. Clustering of pages is based on the content of the pages and pages with similar contents are grouped together. This type of clustering can be utilized in Internet search engines and in tools that provide assistance to Web browsing. Classification. In the Web domain one goal is to develop a profile of users belonging to a particular class or category. This requires extraction and selection of features that best describe the properties of a given class or category of users. As an example an interesting pattern that may be discovered would be of users who placed an online order in Product Books are in the age group and live in rented apartments. Sequential patterns. These kinds of patterns identify sequences of Web accesses which may be used to predict the next set of Web pages to be accessed by a certain class of users. These patterns can be used by marketers to produce targeted advertisements on Web pages. Another type of sequential pattern pertains to which items are typically purchased following the purchase of a particular item. For example after purchasing a computer a printer is often purchased Dependency modeling. Dependency modeling aims to determine and model significant dependencies among the various variables in the Web domain. As an example one may be interested to build a model representing the different stages a visitor undergoes while shopping in an online store based on the actions chosen . Pattern Analysis The final step is to filter out those rules or patterns that are considered to be not of interest from the discovered patterns. The particular analysis methodology based on the application. One common technique for pattern analysis is to use a query language such as SQL to detect various patterns and relationships. Another technique involves loading of usage data into a data warehouse with ETL tools and performing OLAP operations to view it along multiple dimensions by use by location by language used and in other ways or facets. Hence the object can be classified in multiple ways based on multiple taxonomies. A facet defines properties or characteristics of a class of objects. The properties should be mutually exclusive and exhaustive. For example a collection of art objects might be classified using an artist facet an era facet a type facet a country of origin facet a media facet a collection facet and so on. Faceted search uses faceted classification that enables a user to navigate information along multiple paths corresponding to different orderings of the facets. This contrasts with traditional taxonomies in which the hierarchy of categories is fixed and unchanging. University of California Berkeley’s Flamenco is one of the earlier examples of a faceted search system. Social Search The traditional view of Web navigation and browsing assumes that a single user is searching for information. This view contrasts with previous research by library scientists who studied users’ information seeking habits. This research demonstrated that additional individuals may be valuable information resources during information search by a single user. More recently research indicates that there is often direct user cooperation during Web based information search. Some studies report that significant segments of the user population are engaged in explicit collaboration on joint search tasks on the Web. Active collaboration by multiple parties also occur in certain cases at other times and perhaps for a majority of searches users often interact with others remotely asynchronously and even involuntarily and implicitly. Socially enabled online information search is a new phenomenon facilitated by recent Web technologies. Collaborative social search involves different ways for active involvement in search related activities such as co located search remote collaboration on search tasks use of social network for search use of expertise networks involving social data mining or collective intelligence to improve the search process and even social interactions to facilitate information seeking and sense making. This social search activity may be done synchronously asynchronously colocated or in remote shared workspaces. Social psychologists have experimentally validated that the act of social discussions has facilitated cognitive performance. People in social groups can provide solutions pointers to databases or to other people validation and legitimization of ideas and can serve as memory aids and help with problem reformulation. Guided participation is a process in which people co construct knowledge in concert with peers in their community. Information seeking is mostly a solitary activity on the Web today. Some recent work on collaborative search reports several interesting findings and the potential of this technology for better information access. Conversational Search Conversational Search is an interactive and collaborative information finding interaction. The participants engage in a conversation and perform a social search activity that is aided by intelligent agents. The collaborative search activity helps the describes faceted metadata for image search. Chapter Introduction to Information Retrieval and Web Search agent learn about conversations with interactions and feedback from participants. It uses the semantic retrieval model with natural language understanding to provide the users with faster and relevant search results. It moves search from being a solitary activity to being a more participatory activity for the user. The search agent performs multiple tasks of finding relevant information and connecting the users together participants provide feedback to the agent during the conversations that allows the agent to perform better. Summary In this chapter we covered an important area called information retrieval that is closely related to databases. With the advent of the Web unstructured data with text images audio and video is proliferating at phenomenal rates. While database management systems have a very good handle on structured data the unstructured data containing a variety of data types is being stored mainly on ad hoc information repositories on the Web that are available for consumption primarily via IR systems. Google Yahoo and similar search engines are IR systems that make the advances in this field readily available for the average end user giving them a richer search experience with continuous improvement. We started by defining the basic terminology of IR presented the query and browsing modes of interaction in IR systems and provided a comparison of the IR and database technologies. We presented schematics of the IR process at a detailed and an overview level and then discussed digital libraries which are repositories of targeted content on the Web for academic institutions as well as professional communities and gave a brief history of IR. We presented the various retrieval models including Boolean vector space probabilistic and semantic models. They allow for a measurement of whether a document is relevant to a user query and provide similarity measurement heuristics. We then discussed various evaluation metrics such as recall and precision and F score to measure the goodness of the results of IR queries. Then we presented different types of queries besides keyword based queries which dominate there are other types including Boolean phrase proximity natural language and others for which explicit support needs to be provided by the retrieval model. Text preprocessing is important in IR systems and various activities like stopword removal stemming and the use of thesauruses were discussed. We then discussed the construction and use of inverted indexes which are at the core of IR systems and contribute to factors involving search efficiency. Relevance feedback was briefly addressed it is important to modify and improve the retrieval of pertinent information for the user through his interaction and engagement in the search process. We did a somewhat detailed introduction to analysis of the Web as it relates to information retrieval. We divided this treatment into the analysis of content structure and usage of the Web. Web search was discussed including an analysis of the Web link structure followed by an introduction to algorithms for ranking the results from a Web search such as PageRank and HITS. Finally we briefly discussed Review Questions current trends including faceted search social search and conversational search. This is an introductory treatment of a vast field and the reader is referred to specialized textbooks on information retrieval and search engines. Review Questions What is structured data and unstructured data Give an example of each from your experience with data that you may have used. Give a general definition of information retrieval . What does information retrieval involve when we consider information on the Web Discuss the types of data and the types of users in today’s information retrieval systems. What is meant by navigational informational and transformational search What are the two main modes of interaction with an IR system Describe with examples. Explain the main differences between database and IR systems mentioned in Table Describe the main components of the IR system as shown in Figure What are digital libraries What types of data are typically found in them Name some digital libraries that you have accessed. What do they contain and how far back does the data go Give a brief history of IR and mention the landmark developments. What is the Boolean model of IR What are its limitations What is the vector space model of IR How does a vector get constructed to represent a document Define the TF IDF scheme of determining the weight of a keyword in a document. What is the necessity of including IDF in the weight of a term What are probabilistic and semantic models of IR Define recall and precision in IR systems. Give the definition of precision and recall in a ranked list of results at position i. How is F score defined as a metric of information retrieval In what way does it account for both precision and recall What are the different types of queries in an IR system Describe each with an example. What are the approaches to processing phrase and proximity queries Chapter Introduction to Information Retrieval and Web Search Describe the detailed IR process shown in Figure What is stopword removal and stemming Why are these processes necessary for better information retrieval What is a thesaurus How is it beneficial to IR What is information extraction What are the different types of information extraction from structured text What are vocabularies in IR systems What role do they play in the indexing of documents Take five documents with about three sentences each with some related content. Construct an inverted index of all important stems from these documents. Describe the process of constructing the result of a search request using an inverted index. Define relevance feedback. Describe the three types of Web analyses discussed in this chapter. List the important tasks mentioned that are involved in analyzing Web content. Describe each in a couple of sentences. What are the three categories of agent based Web content analyses mentioned in this chapter What is the database based approach to analyzing Web content What are Web query systems What algorithms are popular in ranking or determining the importance of Web pages Which algorithm was proposed by the founders of Google What is the basic idea behind the PageRank algorithm What are hubs and authority pages How does the HITS algorithm use these concepts What can you learn from Web usage analysis What data does it generate What mining operations are commonly performed on Web usage data Give an example of each. What are the applications of Web usage mining What is search relevance How is it determined Define faceted search. Make up a set of facets for a database containing all types of buildings. For example two facets could be “building value or price” and “building type ”. What is social search What does collaborative social search involve Define and explain conversational search. Selected Bibliography Selected Bibliography Information retrieval and search technologies are active areas of research and development in industry and academia. There are many IR textbooks that provide detailed discussion on the materials that we have briefly introduced in this chapter. A recent book entitled Search Engines Information Retrieval in Practice by Croft Metzler and Strohman gives a practical overview of search engine concepts and principles. Introduction to Information Retrieval by Manning Raghavan and Schutze is an authoritative book on information retrieval. Another introductory textbook in IR is Modern Information Retrieval by Ricardo Baeza Yates and Berthier Ribeiro Neto which provides detailed coverage of various aspects of IR technology. Gerald Salton’s and van Rijsbergen’s classic books on information retrieval provide excellent descriptions of the foundational research done in the IR field until the late Salton also introduced the vector space model as a model of IR. Manning and Schutze provide a good summary of natural language technologies and text preprocessing. “Interactive Information Retrieval in Digital Environments” by Xie provides a good human centered approach to information retrieval. The book Managing Gigabytes by Witten Moffat and Bell provides detailed discussions for indexing techniques. The TREC book by Voorhees and Harman provides a description of test collection and evaluation procedures in the context of TREC competitions. Broder classifies Web queries into three distinct classes navigational informational and transactional and presents a detailed taxonomy of Web search. Covi and Kling give a broad definition for digital libraries in their paper and discuss organizational dimensions of effective digital library use. Luhn did some seminal work in IR at IBM in the on autoindexing and business intelligence that received a lot of attention at that time. The SMART system into sets of “similar” elements. Examples An entire population of treatment data on a disease may be divided into groups based on the similarity of side effects produced. The adult population in the United States may be categorized into five groups from most likely to buy to least likely to buy a new product. The Web accesses made by a collection of users against a set of documents may be analyzed in terms of the keywords of documents to reveal clusters or categories of users. For most applications the desired knowledge is a combination of the above types. We expand on each of the above knowledge types in the following sections. Association Rules Market Basket Model Support and Confidence One of the major technologies in data mining involves the discovery of association rules. The database is regarded as a collection of transactions each involving a set of Chapter Data Mining Concepts items. A common example is that of market basket data. Here the market basket corresponds to the sets of items a consumer buys in a supermarket during one visit. Consider four such transactions in a random sample shown in Figure An association rule is of the form X Y where X xn and Y ym are sets of items with xi and yj being distinct items for all i and all j. This association states that if a customer buys X he or she is also likely to buy Y. In general any association rule has the form LHS RHS where LHS and RHS are sets of items. The set LHS ∪ RHS is called an itemset the set of items purchased by customers. For an association rule to be of interest to a data miner the rule should satisfy some interest measure. Two common interest measures are support and confidence. The support for a rule LHS RHS is with respect to the itemset it refers to how frequently a specific itemset occurs in the database. That is the support is the percentage of transactions that contain all of the items in the itemset LHS ∪ RHS. If the support is low it implies that there is no overwhelming evidence that items in LHS ∪ RHS occur together because the itemset occurs in only a small fraction of transactions. Another term for support is prevalence of the rule. The confidence is with regard to the implication shown in the rule. The confidence of the rule LHS RHS is computed as the support support. We can think of it as the probability that the items in RHS will be purchased given that the items in LHS are purchased by a customer. Another term for confidence is strength of the rule. As an example of support and confidence consider the following two rules milk juice and bread juice. Looking at our four sample transactions in Figure we see that the support of milk juice is percent and the support of bread juice is only percent. The confidence of milk juice is percent and the confidence of bread juice is percent . As we can see support and confidence do not necessarily go hand in hand. The goal of mining association rules then is to generate all possible rules that exceed some minimum user specified support and confidence thresholds. The problem is thus decomposed into two subproblems Generate all itemsets that have a support that exceeds the threshold. These sets of items are called large itemsets. Note that large here means large support. Transactionid Time Itemsbought milk bread cookies juice milk juice milk eggs bread cookies coffee Figure Sample transactions in market basket model. Association Rules For each large itemset all the rules that have a minimum confidence are generated as follows For a large itemset X and Y ⊂ X let Z X – Y then if support support minimum confidence the rule Z Y is a valid rule. Generating rules by using all large itemsets and their supports is relatively straightforward. However discovering all large itemsets together with the value for their support is a major problem if the cardinality of the set of items is very high. A typical supermarket has thousands of items. The number of distinct itemsets is where m is the number of items and counting support for all possible itemsets becomes very computation intensive. To reduce the combinatorial search space algorithms for finding association rules utilize the following properties A subset of a large itemset must also be large . Conversely a superset of a small itemset is also small . The first property is referred to as downward closure. The second property called the antimonotonicity property helps to reduce the search space of possible solutions. That is once an itemset is found to be small then any extension to that itemset formed by adding one or more items to the set will also yield a small itemset. Apriori Algorithm The first algorithm to use the downward closure and antimontonicity properties was the Apriori algorithm shown as Algorithm We illustrate Algorithm using the transaction data in Figure using a minimum support of The candidate are milk bread juice cookies eggs coffee and their respective supports are and The first four items qualify for since each support is greater than or equal to In the first iteration of the repeat loop we extend the frequent to create the candidate frequent contains milk bread milk juice bread juice milk cookies bread cookies and juice cookies . Notice for example that milk eggs does not appear in since eggs is small and does not appear in The supports for the six sets contained in are and and are computed by scanning the set of transactions. Only the second milk juice and the fifth bread cookies have support greater than or equal to These two form the frequent Algorithm Apriori Algorithm for Finding Frequent Itemsets Input Database of m transactions D and a minimum support mins represented as a fraction of m. Chapter Data Mining Concepts Output Frequent itemsets Lk Begin steps or statements are numbered for better readability Compute support count m for each individual item i i i n by scanning the database once and counting the number of transactions that item i j appears in The candidate frequent will be the set of items i i i n The subset of items containing i j from where support mins becomes the frequent k termination false repeat Create the candidate frequent by combining members of Lk that have items in common In addition only consider as elements of those items such that every subset of size k appears in Lk Scan the database once and compute the support for each member of if the support for a member of mins then add that member to If is empty then termination true else k k + until termination End In the next iteration of the repeat loop we construct candidate frequent by adding additional items to sets in However for no extension of itemsets in will all subsets be contained in For example consider milk juice bread the milk bread is not in hence milk juice bread cannot be a frequent by the downward closure property. At this point the algorithm terminates with equal to milk bread juice cookies and equal to milk juice bread cookies . Several other algorithms have been proposed to mine association rules. They vary mainly in terms of how the candidate itemsets are generated and how the supports for the candidate itemsets are counted. Some algorithms use such data structures as bitmaps and hashtrees to keep information about itemsets. Several algorithms have been proposed that use multiple scans of the database because the potential number of itemsets can be too large to set up counters during a single scan. We will examine three improved algorithms for association rule mining the Sampling algorithm the Frequent Pattern Tree algorithm and the Partition algorithm. Association Rules Sampling Algorithm The main idea for the Sampling algorithm is to select a small sample one that fits in main memory of the database of transactions and to determine the frequent itemsets from that sample. If those frequent itemsets form a superset of the frequent itemsets for the entire database then we can determine the real frequent itemsets by scanning the remainder of the database in order to compute the exact support values for the superset itemsets. A superset of the frequent itemsets can usually be found from the sample by using for example the Apriori algorithm with a lowered minimum support. In some rare cases some frequent itemsets may be missed and a second scan of the database is needed. To decide whether any frequent itemsets have been missed the concept of the negative border is used. The negative border with respect to a frequent itemset S and set of items I is the minimal itemsets contained in PowerSet and not in S. The basic idea is that the negative border of a set of frequent itemsets contains the closest itemsets that could also be frequent. Consider the case where a set X is not contained in the frequent itemsets. If all subsets of X are contained in the set of frequent itemsets then X would be in the negative border. We illustrate this with the following example. Consider the set of items I A B C D E and let the combined frequent itemsets of size to be S A B C D AB AC BC AD CD ABC . The negative border is E BD ACD . The set E is the only not contained in S BD is the only not in S but whose subsets are and ACD is the only whose subsets are all in S. The negative border is important since it is necessary to determine the support for those itemsets in the negative border to ensure that no large itemsets are missed from analyzing the sample data. Support for the negative border is determined when the remainder of the database is scanned. If we find that an itemset X in the negative border belongs in the set of all frequent itemsets then there is a potential for a superset of X to also be frequent. If this happens then a second pass over the database is needed to make sure that all frequent itemsets are found. Frequent Pattern Tree and FP Growth Algorithm The Frequent Pattern Tree is motivated by the fact that Apriori based algorithms may generate and test a very large number of candidate itemsets. For example with frequent the Apriori algorithm would have to generate or candidate The FP Growth algorithm is one approach that eliminates the generation of a large number of candidate itemsets. ⎜ ⎟ Chapter Data Mining Concepts The algorithm first produces a compressed version of the database in terms of an FP tree . The FP tree stores relevant itemset information and allows for the efficient discovery of frequent itemsets. The actual mining process adopts a divide and conquer strategy where the mining process is decomposed into a set of smaller tasks that each operates on a conditional FP tree a subset of the original tree. To start with we examine how the FP tree is constructed. The database is first scanned and the frequent along with their support are computed. With this algorithm the support is the count of transactions containing the item rather than the fraction of transactions containing the item. The frequent are then sorted in nonincreasing order of their support. Next the root of the FP tree is created with a NULL label. The database is scanned a second time and for each transaction T in the database the frequent in T are placed in order as was done with the frequent We can designate this sorted list for T as consisting of a first item the head and the remaining items the tail. The itemset information is inserted into the FP tree recursively starting at the root node as follows If the current node N of the FP tree has a child with an item name head then increment the count associated with node N by else create a new node N with a count of link N to its parent and link N with the item header table . If the tail is nonempty then repeat step using as the sorted list only the tail that is the old head is removed and the new head is the first item from the tail and the remaining items become the new tail. The item header table created during the process of building the FP tree contains three fields per entry for each frequent item item identifier support count and node link. The item identifier and support count are self explanatory. The node link is a pointer to an occurrence of that item in the FP tree. Since multiple occurrences of a single item may appear in the FP tree these items are linked together as a list where the start of the list is pointed to by the node link in the item header table. We illustrate the building of the FP tree using the transaction data in Figure Let us use a minimum support of One pass over the four transactions yields the following frequent with associated support . We create a NULL root node for the FP tree and insert milk as a child of the root bread as a child of milk cookies as a child of bread and juice as a child of cookies. We adjust the entries for the frequent items in the item header table. For the second transaction we have the sorted list milk juice . Starting at the root we see that a child node with label milk exists so we move to that node and update Association Rules its count . We see that there is no child of the current node with label juice so we create a new node with label juice. The item header table is adjusted. The third transaction only has item milk . Again starting at the root we see that the node with label milk exists so we move to that node increment its count and adjust the item header table. The final transaction contains frequent items bread cookies . At the root node we see that a child with label bread does not exist. Thus we create a new child of the root initialize its counter and then insert cookies as a child of this node and initialize its count. After the item header table is updated we end up with the FP tree and item header table as shown in Figure If we examine this FP tree we see that it indeed represents the original transactions in a compressed format and constructs its conditional pattern base and then its conditional FP tree. The conditional pattern base is made up of a set of prefix paths that is where the frequent item is a suffix. For example if we consider the item juice we see from Figure that there are two paths in the FP tree that end with juice and . The two associated prefix paths are and . The conditional FP tree is constructed from the patterns in the conditional pattern base. The mining is recursively performed on this FP tree. The frequent patterns are formed by concatenating the suffix pattern with the frequent patterns produced from a conditional FP tree. Item Support Link Milk Bread Cookies Juice Milk Bread Bread Cookies Juice Juice Cookies NULL Figure FP tree and item header table. Chapter Data Mining Concepts Algorithm FP Growth Algorithm for Finding Frequent Itemsets Input FP tree and a minimum support mins Output frequent patterns procedure FP growth Begin if tree contains a single path P then for each combination beta of the nodes in the path generate pattern with support minimum support of nodes in beta else for each item i in the header of the tree do begin generate pattern beta with support construct beta’s conditional pattern base construct beta’s conditional FP tree betatree if betatree is not empty then FP growth end End We illustrate the algorithm using the data in Figure and the tree in Figure The procedure FP growth is called with the two parameters the original FP tree and NULL for the variable alpha. Since the original FP tree has more than a single path we execute the else part of the first if statement. We start with the frequent item juice. We will examine the frequent items in order of lowest support . The variable beta is set to juice with support equal to Following the node link in the item header table we construct the conditional pattern base consisting of two paths . These are with their support milk bread cookies juice milk juice bread cookies Partition Algorithm Another algorithm called the Partition algorithm is summarized below. If we are given a database with a small number of potential large itemsets say a few thousand then the support for all of them can be tested in one scan by using a partitioning technique. Partitioning divides the database into nonoverlapping subsets these are individually considered as separate databases and all large itemsets for that partition called local frequent itemsets are generated in one pass. The Apriori algorithm can then be used efficiently on each partition if it fits entirely in main memory. Partitions are chosen in such a way that each partition can be accommodated in main memory. As such a partition is read only once in each pass. The only caveat with the partition method is that the minimum support used for each partition has a slightly different meaning from the original value. The minimum support is based on the size of the partition rather than the size of the database for determining local frequent itemsets. The actual support threshold value is the same as given earlier but the support is computed only for a partition. At the end of pass one we take the union of all frequent itemsets from each partition. This forms the global candidate frequent itemsets for the entire database. When these lists are merged they may contain some false positives. That is some of the itemsets that are frequent in one partition may not qualify in several other partitions and hence may not exceed the minimum support when the original database is considered. Note that there are no false negatives no large itemsets will be missed. The global candidate large itemsets identified in pass one are verified in pass two that is their actual support is measured for the entire database. At the end of phase two all global large itemsets are identified. The Partition algorithm lends itself naturally to a parallel or distributed implementation for better efficiency. Further improvements to this algorithm have been Other Types of Association Rules Association Rules among Hierarchies. There are certain types of associations that are particularly interesting for a special reason. These associations occur among Savasere et al. for details of the algorithm the data structures used to implement it and its performance comparisons. Cheung et al. and Lin and Dunham Chapter Data Mining Concepts Beverages Carbonated Noncarbonated Orange Apple Others Plain Clear Colas Clear drinks Mixed drinks Bottled juices Bottled water Wine coolers Desserts Ice cream Baked Frozen yogurt Rich cream Reduce Healthy Figure Taxonomy of items in a supermarket. hierarchies of items. Typically it is possible to divide items among disjoint hierarchies based on the nature of the domain. For example foods in a supermarket items in a department store or articles in a sports shop can be categorized into classes and subclasses that give rise to hierarchies. Consider Figure which shows the taxonomy of items in a supermarket. The figure shows two hierarchies beverages and desserts respectively. The entire groups may not produce associations of the form beverages desserts or desserts beverages. However associations of the type Healthy brand frozen yogurt bottled water or Rich cream brand ice cream wine cooler may produce enough confidence and support to be valid association rules of interest. Therefore if the application area has a natural classification of the itemsets into hierarchies discovering associations within the hierarchies is of no particular interest. The ones of specific interest are associations across hierarchies. They may occur among item groupings at different levels. Multidimensional Associations. Discovering association rules involves searching for patterns in a file. In Figure we have an example of a file of customer transactions with three dimensions Transactionid Time and Itemsbought. However our data mining tasks and algorithms introduced up to this point only involve one dimension Itemsbought. The following rule is an example of including the label of the single dimension Itemsbought Itemsbought. It may be of interest to find association rules that involve multiple dimensions for Association Rules example Itemsbought. Rules like these are called multidimensional association rules. The dimensions represent attributes of records of a file or in terms of relations columns of rows of a relation and can be categorical or quantitative. Categorical attributes have a finite set of values that display no ordering relationship. Quantitative attributes are numeric and their values display an ordering relationship for example . Itemsbought is an example of a categorical attribute and Transactionid and Time are quantitative. One approach to handling a quantitative attribute is to partition its values into nonoverlapping intervals that are assigned labels. This can be done in a static manner based on domain specific knowledge. For example a concept hierarchy may group values for Salary into three distinct classes low income Salary middle income Salary and high income In a database with items there are possible combinations of items a majority of which do not appear even once in the database. If the absence of a certain item combination is taken to mean a negative association then we potentially have millions and millions of negative association rules with RHSs that are of no interest at all. The problem then is to find only interesting negative rules. In general we are interested in cases in which two specific sets of items appear very rarely in the same transaction. This poses two problems. For a total item inventory of items the probability of any two being bought together is If we find the actual support for these two occurring together to be zero that does not represent a significant departure from expectation and hence is not an interesting association. The other problem is more serious. We are looking for item combinations with very low support and there are millions and millions with low or even zero support. For example a data set of million transactions has most of the billion pairwise combinations of items missing. This would generate billions of useless rules. Therefore to make negative association rules interesting we must use prior knowledge about the itemsets. One approach is to use hierarchies. Suppose we use the hierarchies of soft drinks and chips shown in Figure Chapter Data Mining Concepts Soft drinks Joke Wakeup Topsy Chips Days Nightos Party’Os Figure Simple hierarchy of soft drinks and chips. A strong positive association has been shown between soft drinks and chips. If we find a large support for the fact that when customers buy Days chips they predominantly buy Topsy and not Joke and not Wakeup that would be interesting because we would normally expect that if there is a strong association between Days and Topsy there should also be such a strong association between Days and Joke or Days and In the frozen yogurt and bottled water groupings shown in Figure suppose the Reduce versus Healthy brand division is and the Plain and Clear brands division is among respective categories. This would give a joint probability of Reduce frozen yogurt being purchased with Plain bottled water as percent among the transactions containing a frozen yogurt and bottled water. If this support however is found to be only percent it would indicate a significant negative association among Reduce yogurt and Plain bottled water again that would be interesting. The problem of finding negative association is important in the above situations given the domain knowledge in the form of item generalization hierarchies and the distribution of items . The scope of discovery of negative associations is limited in terms of knowing the item hierarchies and distributions. Exponential growth of negative associations remains a challenge. Additional Considerations for Association Rules Mining association rules in real life databases is complicated by the following factors The cardinality of itemsets in most situations is extremely large and the volume of transactions is very high as well. Some operational databases in retailing and communication industries collect tens of millions of transactions per day. Transactions show variability in such factors as geographic location and seasons making sampling difficult. Item classifications exist along multiple dimensions. Hence driving the discovery process with domain knowledge particularly for negative rules is extremely difficult. simplicity we are assuming a uniform distribution of transactions among members of a hierarchy. Classification Quality of data is variable significant problems exist with missing erroneous conflicting as well as redundant data in many industries. Classification Classification is the process of learning a model that describes different classes of data. The classes are predetermined. For example in a banking application customers who apply for a credit card may be classified as a poor risk fair risk or good risk. Hence this type of activity is also called supervised learning. Once the model is built it can be used to classify new data. The first step learning the model is accomplished by using a training set of data that has already been classified. Each record in the training data contains an attribute called the class label which indicates which class the record belongs to. The model that is produced is usually in the form of a decision tree or a set of rules. Some of the important issues with regard to the model and the algorithm that produces the model include the model’s ability to predict the correct class of new data the computational cost associated with the algorithm and the scalability of the algorithm. We will examine the approach where our model is in the form of a decision tree. A decision tree is simply a graphical representation of the description of each class or in other words a representation of the classification rules. A sample decision tree is pictured in Figure We see from Figure that if a customer is married and if salary then they are a good risk for a bank credit card. This is one of the rules that describe the class good risk. Traversing the decision tree from the root to each leaf node forms other rules for this class and the two other classes. Algorithm shows the procedure for constructing a decision tree from a training data set. Initially all training samples are at the root of the tree. The samples are partitioned Marr ed Salary Acctbalance Fa r r sk Good r sk Poor r sk Yes No Fa r r sk Good r sk Poor r sk Age Figure Sample decision tree for credit card applications. Chapter Data Mining Concepts recursively based on selected attributes. The attribute used at a node to partition the samples is the one with the best splitting criterion for example the one that maximizes the information gain measure. Algorithm Algorithm for Decision Tree Induction Input Set of training data records Rm and set of attributes An Output Decision tree procedure Buildtree Begin create a node N if all records belong to the same class C then return N as a leaf node with class label C if attributes is empty then return N as a leaf node with class label C such that the majority of records belong to it select attribute Ai from attributes label node N with Ai for each known value vj of Ai do begin add a branch from node N for the condition Ai vj Sj subset of records where Ai vj if Sj is empty then add a leaf L with class label C such that the majority of records belong to it and return L else add the node returned by Buildtree end End Before we illustrate Algorithm we will explain the information gain measure in more detail. The use of entropy as the information gain measure is motivated by the goal of minimizing the information needed to classify the sample data in the resulting partitions and thus minimizing the expected number of conditional tests needed to classify a new record. The expected information needed to classify training data of s samples where the Class attribute has n values vn and s i is the number of samples belonging to class label vi is given by where pi is the probability that a random sample belongs to the class with label vi . An estimate for pi is s i s. Consider an attribute A with values vm used as the test attribute for splitting in the decision tree. Attribute A partitions the samples into the subsets Sm where samples in each Sj have a value of vj for attribute A. Each Sj may contain samples that belong to any of the classes. The number of IS S S p p n ii i n log − Classification samples in Sj that belong to class i can be denoted as s ij. The entropy associated with using attribute A as the test attribute is defined as snj can be defined using the formulation for sn with pi being replaced by pij where pij sij sj . Now the information gain by partitioning on attribute A Gain is defined as I – E. We can use the sample training data from Figure to illustrate the algorithm. The attribute RID represents the record identifier used for identifying an individual record and is an internal attribute. We use it to identify a particular record in our example. First we compute the expected information needed to classify the training data of records as I I would be – If we follow similar steps for computing the gain with respect to the other three attributes we end up with E and Gain E and Gain E and Gain Since the greatest gain occurs for attribute Salary it is chosen as the partitioning attribute. The root of the tree is created with label Salary and has three branches one for each value of Salary. For two of the three values that is and all the samples that are partitioned accordingly + + × RID Married Salary Acctbalance Age Loanworthy no yes yes yes yes . no no no no no yes . yes Figure Sample training data for classification algorithm. Chapter Data Mining Concepts Salary Class is “no” Class is “no” Class is “yes” Class is “yes” . . . Age Figure Decision tree based on sample training data where the leaf nodes are represented by a set of RIDs of the partitioned records. and records with RIDs and for fall within the same class loanworthy no and loanworthy yes respectively for those two values. So we create a leaf node for each. The only branch that needs to be expanded is for the value with two samples records with RIDs and in the training data. Continuing the process using these two records we find that Gain is Gain is and Gain is We can choose either Age or Acctbalance since they both have the largest gain. Let us choose Age as the partitioning attribute. We add a node with label Age that has two branches less than and greater or equal to Each branch partitions the remaining sample data such that one sample record belongs to each branch and hence one class. Two leaf nodes are created and we are finished. The final decision tree is pictured in Figure Clustering The previous data mining task of classification deals with partitioning data based on using a preclassified training sample. However it is often useful to partition data without having a training sample this is also known as unsupervised learning. For example in business it may be important to determine groups of customers who have similar buying patterns or in medicine it may be important to determine groups of patients who show similar reactions to prescribed drugs. The goal of clustering is to place records into groups such that records in a group are similar to each other and dissimilar to records in other groups. The groups are usually disjoint. An important facet of clustering is the similarity function that is used. When the data is numeric a similarity function based on distance is typically used. For example the Euclidean distance can be used to measure similarity. Consider two ndimensional data points rj and rk. We can consider the value for the ith dimension as rji and rki for the two records. The Euclidean distance between points rj and rk in n dimensional space is calculated as Distance . . . rr r r r r r r j k j k j k jn kn − + − ++ − Clustering The smaller the distance between two points the greater is the similarity as we think of them. A classic clustering algorithm is the k Means algorithm Algorithm Algorithm k Means Clustering Algorithm Input a database D of m records rm and a desired number of clusters k Output set of k clusters that minimizes the squared error criterion Begin randomly choose k records as the centroids for the k clusters repeat assign each record ri to a cluster such that the distance between ri and the cluster centroid is the smallest among the k clusters recalculate the centroid for each cluster based on the records assigned to the cluster until no change End The algorithm begins by randomly choosing k records to represent the centroids mk of the clusters Ck. All the records are placed in a given cluster based on the distance between the record and the cluster mean. If the distance between mi and record rj is the smallest among all cluster means then record rj is placed in cluster Ci . Once all records have been initially placed in a cluster the mean for each cluster is recomputed. Then the process repeats by examining each record again and placing it in the cluster whose mean is closest. Several iterations may be needed but the algorithm will converge although it may terminate at a local optimum. The terminating condition is usually the squared error criterion. For clusters Ck with means mk the error is defined as We will examine how Algorithm works with the records in Figure Assume that the number of desired clusters k is Let the algorithm choose records with RID for cluster and RID for cluster as the initial cluster centroids. The remaining records will be assigned to one of those clusters during the ErrorDistance ∈ r mj i i r C k j i RID Age Yearsofservice Figure Sample records for clustering example . Chapter Data Mining Concepts first iteration of the repeat loop. The record with RID has a distance from of and a distance from of so it joins cluster The record with RID has a distance from of and a distance from of so it joins cluster The record with RID has a distance from of and a distance from of so it joins cluster The record with RID has a distance from of and a distance from of so it joins cluster Now the new means for the two clusters are computed. The mean for a cluster Ci with n records of m dimensions is the vector The new mean for is and the new mean for is A second iteration proceeds and the six records are placed into the two clusters as follows records with RIDs are placed in and records with RIDs are placed in The mean for and is recomputed as and respectively. In the next iteration all records stay in their previous clusters and the algorithm terminates. Traditionally clustering algorithms assume that the entire data set fits in main memory. More recently researchers have developed algorithms that are efficient and are scalable for very large databases. One such algorithm is called BIRCH. BIRCH is a hybrid approach that uses both a hierarchical clustering approach which builds a tree representation of the data as well as additional clustering methods which are applied to the leaf nodes of the tree. Two input parameters are used by the BIRCH algorithm. One specifies the amount of available main memory and the other is an initial threshold for the radius of any cluster. Main memory is used to store descriptive cluster information such as the center of a cluster and the radius of the cluster . The radius threshold affects the number of clusters that are produced. For example if the radius threshold value is large then few clusters of many records will be formed. The algorithm tries to maintain the number of clusters such that their radius is below the radius threshold. If available memory is insufficient then the radius threshold is increased. The BIRCH algorithm reads the data records sequentially and inserts them into an in memory tree structure which tries to preserve the clustering structure of the data. The records are inserted into the appropriate leaf nodes based on the distance between the record and the cluster center. The leaf node where the insertion happens may have to split depending upon the updated center and radius of the cluster and the radius threshold parameter. Additionally when splitting extra cluster information is stored and if memory becomes insufficient then the radius threshold will be increased. Increasing the radius threshold may actually produce a side effect of reducing the number of clusters since some nodes may be merged. Overall BIRCH is an efficient clustering method with a linear computational complexity in terms of the number of records to be clustered. C n r n r i ji r C jm ji ji r C ∈ ∈ Approaches to Other Data Mining Problems Approaches to Other Data Mining Problems Discovery of Sequential Patterns The discovery of sequential patterns is based on the concept of a sequence of itemsets. We assume that transactions such as the supermarket basket transactions we discussed previously are ordered by time of purchase. That ordering yields a sequence of itemsets. For example milk bread juice bread eggs cookies milk coffee may be such a sequence of itemsets based on three visits by the same customer to the store. The support for a sequence S of itemsets is the percentage of the given set U of sequences of which S is a subsequence. In this example milk bread juice bread eggs and bread eggs cookies milk coffee are considered subsequences. The problem of identifying sequential patterns then is to find all subsequences from the given sets of sequences that have a user defined minimum support. The sequence is a predictor of the fact that a customer who buys itemset is likely to buy itemset and then and so on. This prediction is based on the frequency of this sequence in the past. Various algorithms have been investigated for sequence detection. Discovery of Patterns in Time Series Time series are sequences of events each event may be a given fixed type of a transaction. For example the closing price of a stock or a fund is an event that occurs every weekday for each stock and fund. The sequence of these values per stock or fund constitutes a time series. For a time series one may look for a variety of patterns by analyzing sequences and subsequences as we did above. For example we might find the period during which the stock rose or held steady for n days or we might find the longest period over which the stock had a fluctuation of no more than percent over the previous closing price or we might find the quarter during which the stock had the most percentage gain or percentage loss. Time series may be compared by establishing measures of similarity to identify companies whose stocks behave in a similar fashion. Analysis and mining of time series is an extended functionality of temporal data management Chapter Data Mining Concepts which contains values that are results from a series of n tests for one patient. The target variable that we wish to predict is P the probability of survival of the patient. Then the rule for regression takes the form ⇒ P x or x P ≤ y The choice depends on whether we can predict a unique value of P or a range of values for P. If we regard P as a function P f the function is called a regression function to predict P. In general if the function appears as Y f Xn and f is linear in the domain variables xi the process of deriving f from a given set of tuples for Xn y is called linear regression. Linear regression is a commonly used statistical technique for fitting a set of observations or points in n dimensions with the target variable y. Regression analysis is a very common tool for analysis of data in many research domains. The discovery of the function to predict the target variable is equivalent to a data mining operation. Neural Networks A neural network is a technique derived from artificial intelligence research that uses generalized regression and provides an iterative method to carry it out. Neural networks use the curve fitting approach to infer a function from a set of samples. This technique provides a learning approach it is driven by a test sample that is used for the initial inference and learning. With this kind of learning method responses to new inputs may be able to be interpolated from the known samples. This interpolation however depends on the world model developed by the learning method. Neural networks can be broadly classified into two categories supervised and unsupervised networks. Adaptive methods that attempt to reduce the output error are supervised learning methods whereas those that develop internal representations without sample outputs are called unsupervised learning methods. Neural networks self adapt that is they learn from information about a specific problem. They perform well on classification tasks and are therefore useful in data mining. Yet they are not without problems. Although they learn they do not provide a good representation of what they have learned. Their outputs are highly quantitative and not easy to understand. As another limitation the internal representations developed by neural networks are not unique. Also in general neural networks have trouble modeling time series data. Despite these shortcomings they are popular and frequently used by several commercial vendors. Approaches to Other Data Mining Problems Genetic Algorithms Genetic algorithms are a class of randomized search procedures capable of adaptive and robust search over a wide range of search space topologies. Modeled after the adaptive emergence of biological species from evolutionary mechanisms and introduced by GAs have been successfully applied in such diverse fields as image analysis scheduling and engineering design. Genetic algorithms extend the idea from human genetics of the four letter alphabet of the human DNA code. The construction of a genetic algorithm involves devising an alphabet that encodes the solutions to the decision problem in terms of strings of that alphabet. Strings are equivalent to individuals. A fitness function defines which solutions can survive and which cannot. The ways in which solutions can be combined are patterned after the cross over operation of cutting and combining strings from a father and a mother. An initial population of a well varied population is provided and a game of evolution is played in which mutations occur among strings. They combine to produce a new generation of individuals the fittest individuals survive and mutate until a family of successful solutions develops. The solutions produced by GAs are distinguished from most other search techniques by the following characteristics A GA search uses a set of solutions during each generation rather than a single solution. The search in the string space represents a much larger parallel search in the space of encoded solutions. The memory of the search done is represented solely by the set of solutions available for a generation. A genetic algorithm is a randomized algorithm since search mechanisms use probabilistic operators. While progressing from one generation to the next a GA finds near optimal balance between knowledge acquisition and exploitation by manipulating encoded solutions. Genetic algorithms are used for problem solving and clustering problems. Their ability to solve problems in parallel provides a powerful tool for data mining. The drawbacks of GAs include the large overproduction of individual solutions the random character of the searching process and the high demand on computer processing. In general substantial computing power is required to achieve anything of significance with genetic algorithms. seminal work entitled Adaptation in Natural and Artificial Systems introduced the idea of genetic algorithms. Chapter Data Mining Concepts Applications of Data Mining Data mining technologies can be applied to a large variety of decision making contexts in business. In particular areas of significant payoffs are expected to include the following Marketing. Applications include analysis of consumer behavior based on buying patterns determination of marketing strategies including advertising store location and targeted mailing segmentation of customers stores or products and design of catalogs store layouts and advertising campaigns. Finance. Applications include analysis of creditworthiness of clients segmentation of account receivables performance analysis of finance investments like stocks bonds and mutual funds evaluation of financing options and fraud detection. Manufacturing. Applications involve optimization of resources like machines manpower and materials and optimal design of manufacturing processes shop floor layouts and product design such as for automobiles based on customer requirements. Health Care. Applications include discovery of patterns in radiological images analysis of microarray experimental data to cluster genes and to relate to symptoms or diseases analysis of side effects of drugs and effectiveness of certain treatments optimization of processes within a hospital and the relationship of patient wellness data with doctor qualifications. Commercial Data Mining Tools Currently commercial data mining tools use several common techniques to extract knowledge. These include association rules clustering neural networks sequencing and statistical analysis. We discussed these earlier. Also used are decision trees which are a representation of the rules used in classification or clustering and statistical analyses which may include regression and many other techniques. Other commercial products use advanced techniques such as genetic algorithms casebased reasoning Bayesian networks nonlinear regression combinatorial optimization pattern matching and fuzzy logic. In this chapter we have already discussed some of these. Most data mining tools use the ODBC interface. ODBC is an industry standard that works with databases it enables access to data in most of the popular database programs such as Access dBASE Informix Oracle and SQL Server. Some of these software packages provide interfaces to specific database programs the most common are Oracle Access and SQL Server. Most of the tools work in the Microsoft Windows environment and a few work in the UNIX operating system. The trend is for all products to operate under the Microsoft Windows environment. One tool Data Surveyor mentions ODMG compliance see Chapter where we discuss the ODMG object oriented standard. Commercial Data Mining Tools In general these programs perform sequential processing in a single machine. Many of these products work in the client server mode. Some products incorporate parallel processing in parallel computer architectures and work as a part of online analytical processing tools. User Interface Most of the tools run in a graphical user interface environment. Some products include sophisticated visualization techniques to view data and rules and are even able to manipulate data this way interactively. Text interfaces are rare and are more common in tools available for UNIX such as IBM’s Intelligent Miner. Application Programming Interface Usually the application programming interface is an optional tool. Most products do not permit using their internal functions. However some of them allow the application programmer to reuse their code. The most common interfaces are C libraries and Dynamic Link Libraries . Some tools include proprietary database command languages. In Table we list representative data mining tools. To date there are almost one hundred commercial data mining products available worldwide. Non . products include Data Surveyor from the Netherlands and PolyAnalyst from Russia. Future Directions Data mining tools are continually evolving building on ideas from the latest scientific research. Many of these tools incorporate the latest algorithms taken from artificial intelligence statistics and optimization. Currently fast processing is done using modern database techniques such as distributed processing in client server architectures in parallel databases and in data warehousing. For the future the trend is toward developing Internet capabilities more fully. Additionally hybrid approaches will become commonplace and processing will be done using all resources available. Processing will take advantage of both parallel and distributed computing environments. This shift is especially important because modern databases contain very large amounts of information. Not only are multimedia databases growing but also image storage and retrieval are slow operations. Also the cost of secondary storage is decreasing so massive information storage will be feasible even for small companies. Thus data mining programs will have to deal with larger sets of data of more companies. Most of data mining software will use the ODBC standard to extract data from business databases proprietary input formats can be expected to disappear. There is a definite need to include nonstandard data including images and other multimedia data as source data for data mining. Table Some Representative Data Mining Tools Company Product Technique Platform Interface AcknoSoft Kate Decision trees Case based reasoning Windows UNIX Microsoft Access Angoss Knowledge SEEKER Decision trees Statistics Windows ODBC Business Objects Business Miner Neural nets Machine learning Windows ODBC CrossZ QueryObject Statistical analysis Optimization algorithm Windows MVS UNIX ODBC Data Distilleries Data Surveyor Comprehensive can mix different types of data mining UNIX ODBC ODMGcompliant DBMiner Technology Inc. DBMiner OLAP analysis Associations Classification Clustering algorithms Windows Microsoft OLAP IBM Intelligent Miner Classification Association rules Predictive models UNIX IBM Megaputer Intelligence PolyAnalyst Symbolic knowledge acquisition Evolutionary programming Windows ODBC Oracle NCR Management Discovery Tool Association rules Windows ODBC Purple Insight MineSet Decision trees Association rules UNIX Oracle Sybase Informix SAS Enterprise Miner Decision trees Association rules Neural nets Regression Clustering UNIX Windows Macintosh ODBC Oracle ODBC Open Data Base Connectivity ODMG Object Data Management Group Chapter Data Mining Concepts Summary In this chapter we surveyed the important discipline of data mining which uses database technology to discover additional knowledge or patterns in the data. We gave an illustrative example of knowledge discovery in databases which has a wider scope than data mining. For data mining among the various techniques we focused on the details of association rule mining classification and clustering. We presented algorithms in each of these areas and illustrated with examples of how those algorithms work. A variety of other techniques including the AI based neural networks and genetic algorithms were also briefly discussed. Active research is ongoing in data mining and we have outlined some of the expected research directions. In the future database technology products market a great deal of data mining activity is expected. We summarized out of nearly one hundred data mining tools available future research is expected to extend the number and functionality significantly. Review Questions What are the different phases of the knowledge discovery from databases Describe a complete application scenario in which new knowledge may be mined from an existing database of transactions. What are the goals or tasks that data mining attempts to facilitate What are the five types of knowledge produced from data mining What are association rules as a type of knowledge Give a definition of support and confidence and use them to define an association rule. What is the downward closure property How does it aid in developing an efficient algorithm for finding association rules that is with regard to finding large itemsets What was the motivating factor for the development of the FP tree algorithm for association rule mining Describe an association rule among hierarchies with an example. What is a negative association rule in the context of the hierarchy in Figure What are the difficulties of mining association rules from large databases What are classification rules and how are decision trees related to them What is entropy and how is it used in building decision trees How does clustering differ from classification Describe neural networks and genetic algorithms as techniques for data mining. What are the main difficulties in using these techniques Review Questions Chapter Data Mining Concepts Transid Itemspurchased milk bread eggs milk juice juice butter milk bread eggs coffee eggs coffee coffee juice milk bread cookies eggs cookies butter milk bread RID Age City Gender Education Repeatcustomer NY F college YES SF M graduate YES NY F college YES NY F college NO LA M high school NO NY F college YES NY F graduate YES LA M college YES NY F high school NO NY F college YES Exercises Apply the Apriori algorithm to the following data set. The set of items is milk bread cookies eggs butter coffee juice . Use for the minimum support value. Show two rules that have a confidence of or greater for an itemset containing three items from Exercise For the Partition algorithm prove that any frequent itemset in the database must appear as a local frequent itemset in at least one partition. Show the FP tree that would be made for the data from Exercise Apply the FP Growth algorithm to the FP tree from Exercise and show the frequent itemsets. Apply the classification algorithm to the following set of data records. The class attribute is Repeatcustomer. Consider the following set of two dimensional records Selected Bibliography RID Also consider two different clustering schemes where contains records and contains records and where contains records and contains records Which scheme is better and why Use the k Means algorithm to cluster the data from Exercise We can use a value of for K and we can assume that the records with RIDs and are used for the initial cluster centroids . The k Means algorithm uses a similarity metric of distance between a record and a cluster centroid. If the attributes of the records are not quantitative but categorical in nature such as Incomelevel with values low medium high or Married with values Yes No or Stateofresidence with values Alabama Alaska Wyoming then the distance metric is not meaningful. Define a more suitable similarity metric that can be used for clustering data records that contain categorical data. Selected Bibliography Literature on data mining comes from several fields including statistics mathematical optimization machine learning and artificial intelligence. Chen et al. give a good summary of the database perspective on data mining. The book by Han and Kamber is an excellent text describing in detail the different algorithms and techniques used in the data mining area. Work at IBM Almaden research has produced a large number of early concepts and algorithms as well as results from some performance studies. Agrawal et al. report the first major study on association rules. Their Apriori algorithm for market basket data in Agrawal and Srikant is improved by using partitioning in Savasere et al. Toivonen proposes sampling as a way to reduce the processing effort. Cheung et al. extends the partitioning to distributed environments Lin and Dunham propose techniques to overcome problems with data skew. Agrawal et al. discuss the performance perspective on association rules. Mannila et al. Park et al. and Amir et al. present additional efficient algorithms related to association rules. Han et al. present the FP tree algorithm Chapter Data Mining Concepts discussed in this chapter. Srikant and proposes mining generalized rules. Savasere et al. present the first approach to mining negative associations. Agrawal et al. describe the Quest system at IBM. Sarawagi et al. describe an implementation where association rules are integrated with a relational database management system. Piatesky Shapiro and Frawley have contributed papers from a wide range of topics related to knowledge discovery. Zhang et al. present the BIRCH algorithm for clustering large databases. Information about decision tree learning and the classification algorithm presented in this chapter can be found in Mitchell Adriaans and Zantinge Fayyad et al. and Weiss and Indurkhya are books devoted to the different aspects of data mining and its use in prediction. The idea of genetic algorithms was proposed by Holland a good survey of genetic algorithms appears in Srinivas and Patnaik Neural networks have a vast literature a comprehensive introduction is available in Lippman Tan et al. provides a comprehensive introduction to data mining and has a detailed set of references. Readers are also advised to consult proceedings of two prominent annual conferences in data mining the Knowledge Discovery and Data Mining Conference which has been running since and the SIAM International Conference on Data Mining which has been running since Links to past conferences may be found at http . Overview of Data Warehousing and OLAP The increasing processing power and sophistication of analytical tools and techniques have resulted in the development of what are known as data warehouses. These data warehouses provide storage functionality and responsiveness to queries beyond the capabilities of transaction oriented databases. Accompanying this ever increasing power is a great demand to improve the data access performance of databases. As we have seen throughout this book traditional databases balance the requirement of data access with the need to ensure data integrity. In modern organizations users of data are often completely removed from the data sources. Many people only need readaccess to data but still need fast access to a larger volume of data than can conveniently be downloaded to the desktop. Often such data comes from multiple databases. Because many of the analyses performed are recurrent and predictable software vendors and systems support staff are designing systems to support these functions. Presently there is a great need to provide decision makers from middle management upward with information at the correct level of detail to support decision making. Data warehousing online analytical processing and data mining provide this functionality. We gave an introduction to data mining techniques in Chapter In this chapter we give a broad overview of data warehousing and OLAP technologies. Introduction Definitions and Terminology In Chapter we defined a database as a collection of related data and a database system as a database and database software together. A data warehouse is also a collection of information as well as a supporting system. However a clear distinction chapter Chapter Overview of Data Warehousing and OLAP exists. Traditional databases are transactional . Data warehouses have the distinguishing characteristic that they are mainly intended for decision support applications. They are optimized for data retrieval not routine transaction processing. Because data warehouses have been developed in numerous organizations to meet particular needs there is no single canonical definition of the term data warehouse. Professional magazine articles and books in the popular press have elaborated on the meaning in a variety of ways. Vendors have capitalized on the popularity of the term to help market a variety of related products and consultants have provided a large variety of services all under the data warehousing banner. However data warehouses are quite distinct from traditional databases in their structure functioning performance and purpose. W. H. characterized a data warehouse as a subject oriented integrated nonvolatile time variant collection of data in support of management’s decisions. Data warehouses provide access to data for complex analysis knowledge discovery and decision making. They support high performance demands on an organization’s data and information. Several types of applications OLAP DSS and data mining applications are supported. We define each of these next. OLAP is a term used to describe the analysis of complex data from the data warehouse. In the hands of skilled knowledge workers OLAP tools use distributed computing capabilities for analyses that require more storage and processing power than can be economically and efficiently located on an individual desktop. DSS also known as EIS executive information systems not to be confused with enterprise integration systems support an organization’s leading decision makers with higher level data for complex and important decisions. Data mining which includes insertions updates and deletions while also supporting information query requirements. Traditional relational databases are optimized to process queries that may touch a small part of the database and transactions that deal with insertions or updates of a few tuples per relation to process. Thus they cannot be optimized for OLAP DSS or data mining. By contrast data warehouses are designed precisely to support efficient extraction processing and presentation for analytic and decision making purposes. In comparison to traditional databases data warehouses generally contain very large amounts of data from multiple sources that may include databases from different data models and sometimes files acquired from independent systems and platforms. is credited with initially using the term warehouse. The latest edition of his work is Inmon Characteristics of Data Warehouses Databases Clean ng Backflush ng Reformatt ng Data m ng DSS EIS OLAP Other data nputs Updates new data Metadata Data Data warehouse Figure Sample transactions in market basket model. Characteristics of Data Warehouses To discuss data warehouses and distinguish them from transactional databases calls for an appropriate data model. The multidimensional data model to make better and faster decisions. Figure gives an overview of the conceptual structure of a data warehouse. It shows the entire data warehousing process which includes possible cleaning and reformatting of data before loading it into the warehouse. This process is handled by tools known as ETL tools. At the back end of the process OLAP data mining and DSS may generate new relevant information such as rules this information is shown in the figure going back into the warehouse. The figure also shows that data sources may include files. and Dayal provide an excellent tutorial on the topic with this as a starting definition. Chapter Overview of Data Warehousing and OLAP Data warehouses have the following distinctive Multidimensional conceptual view Generic dimensionality Unlimited dimensions and aggregation levels Unrestricted cross dimensional operations Dynamic sparse matrix handling Client server architecture Multiuser support Accessibility Transparency Intuitive data manipulation Consistent reporting performance Flexible reporting Because they encompass large volumes of data data warehouses are generally an order of magnitude larger than the source databases. The sheer volume of data is an issue that has been dealt with through enterprise wide data warehouses virtual data warehouses and data marts Enterprise wide data warehouses are huge projects requiring massive investment of time and resources. Virtual data warehouses provide views of operational databases that are materialized for efficient access. Data marts generally are targeted to a subset of the organization such as a department and are more tightly focused. Data Modeling for Data Warehouses Multidimensional models take advantage of inherent relationships in data to populate data in multidimensional matrices called data cubes. For data that lends itself to dimensional formatting query performance in multidimensional matrices can be much better than in the relational data model. Three examples of dimensions in a corporate data warehouse are the corporation’s fiscal periods products and regions. A standard spreadsheet is a two dimensional matrix. One example would be a spreadsheet of regional sales by product for a particular time period. Products could be shown as rows with sales revenues for each region comprising the columns. Adding a time dimension and Salley coined the term OLAP and mentioned these characteristics. We have reordered their original list. Data Modeling for Data Warehouses Reg Region Product Reg Reg Figure A two dimensional matrix model. such as an organization’s fiscal quarters would produce a three dimensional matrix which could be represented using a data cube. Figure shows a three dimensional data cube that organizes product sales data by fiscal quarters and sales regions. Each cell could contain data for a specific product Product Reg Reg Region Reg Qtr Qtr Fiscalquarter Qtr Qtr Figure A three dimensional data cube model. Chapter Overview of Data Warehousing and OLAP Reg Region Reg Reg Reg Qtr Qtr F sca quarter Qtr Qtr P P Product P P P Figure Pivoted version of the data cube from Figure specific fiscal quarter and specific region. By including additional dimensions a data hypercube could be produced although more than three dimensions cannot be easily visualized or graphically presented. The data can be queried directly in any combination of dimensions bypassing complex database queries. Tools exist for viewing data according to the user’s choice of dimensions. Changing from one dimensional hierarchy to another is easily accomplished in a data cube with a technique called pivoting . In this technique the data cube can be thought of as rotating to show a different orientation of the axes. For example you might pivot the data cube to show regional sales revenues as rows the fiscal quarter revenue totals as columns and the company’s products in the third dimension . Figure shows a roll up display that moves from individual products to a coarser grain of product categories. Shown in Figure a drill down display provides the opposite capability furnishing a finergrained view perhaps disaggregating country sales by region and then regional sales by subregion and also breaking up products by styles. Data Modeling for Data Warehouses Products Products Products Products Reg on Product categories Region Region Region Figure The roll up operation. The multidimensional storage model involves two types of tables dimension tables and fact tables. A dimension table consists of tuples of attributes of the dimension. A fact table can be thought of as having tuples one per a recorded fact. This fact contains some measured or observed variable and identifies it with pointers to dimension tables. The fact table contains the data and the dimensions identify each tuple in that data. Figure contains an example of a fact table that can be viewed from the perspective of multiple dimension tables. Two common multidimensional schemas are the star schema and the snowflake schema. The star schema consists of a fact table with a single table for each dimension . A technique called bitmap indexing constructs a bit vector for each value in a domain Building a Data Warehouse Fact table I Bus ness results Prodno Prodname Proddescr Prodstyle Prodl ne Dimension table Product Product Quarter Reg on Revenue Fact table II Bus ness forecast Product Futureqtr Reg on Pro ectedrevenue Figure A fact constellation. being indexed. It works very well for domains of low cardinality. There is a bit placed in the jth position in the vector if the jth row contains the value being indexed. For example imagine an inventory of cars with a bitmap index on car size. If there are four car sizes economy compact mid size and full size there will be four bit vectors each containing bits for a total index size of Bitmap indexing can provide considerable input output and storage space advantages in low cardinality domains. With bit vectors a bitmap index can provide dramatic improvements in comparison aggregation and join performance. In a star schema dimensional data can be indexed to tuples in the fact table by join indexing. Join indexes are traditional indexes to maintain relationships between primary key and foreign key values. They relate the values of a dimension of a star schema to rows in the fact table. For example consider a sales fact table that has city and fiscal quarter as dimensions. If there is a join index on city for each city the join index maintains the tuple IDs of tuples containing that city. Join indexes may involve multiple dimensions. Data warehouse storage can facilitate access to summary data by taking further advantage of the nonvolatility of data warehouses and a degree of predictability of the analyses that will be performed using them. Two approaches have been used smaller tables including summary data such as quarterly sales or revenue by product line and encoding of level into existing tables. By comparison the overhead of creating and maintaining such aggregations would likely be excessive in a volatile transaction oriented database. Building a Data Warehouse In constructing a data warehouse builders should take a broad view of the anticipated use of the warehouse. There is no way to anticipate all possible queries or analyses during the design phase. However the design should specifically support ad hoc querying that is accessing data with any meaningful combination of values for the attributes in the dimension or fact tables. For example a marketingintensive consumer products company would require different ways of organizing the data warehouse than would a nonprofit charity focused on fund raising. An appropriate schema should be chosen that reflects anticipated usage. Chapter Overview of Data Warehousing and OLAP Acquisition of data for the warehouse involves the following steps The data must be extracted from multiple heterogeneous sources for example databases or other data feeds such as those containing financial market data or environmental data. Data must be formatted for consistency within the warehouse. Names meanings and domains of data from unrelated sources must be reconciled. For instance subsidiary companies of a large corporation may have different fiscal calendars with quarters ending on different dates making it difficult to aggregate financial data by quarter. Various credit cards may report their transactions differently making it difficult to compute all credit sales. These format inconsistencies must be resolved. The data must be cleaned to ensure validity. Data cleaning is an involved and complex process that has been identified as the largest labor demanding component of data warehouse construction. For input data cleaning must occur before the data is loaded into the warehouse. There is nothing about cleaning data that is specific to data warehousing and that could not be applied to a host database. However since input data must be examined and formatted consistently data warehouse builders should take this opportunity to check for validity and quality. Recognizing erroneous and incomplete data is difficult to automate and cleaning that requires automatic error correction can be even tougher. Some aspects such as domain checking are easily coded into data cleaning routines but automatic recognition of other data problems can be more challenging. After such problems have been taken care of similar data from different sources must be coordinated for loading into the warehouse. As data managers in the organization discover that their data is being cleaned for input into the warehouse they will likely want to upgrade their data with the cleaned data. The process of returning cleaned data to the source is called backflushing to a multidimensional model. The data must be loaded into the warehouse. The sheer volume of data in the warehouse makes loading the data a significant task. Monitoring tools for loads as well as methods to recover from incomplete or incorrect loads are required. With the huge volume of data in the warehouse incremental updating is usually the only feasible approach. The refresh policy will probably emerge as a compromise that takes into account the answers to the following questions How up to date must the data be Can the warehouse go offline and for how long What are the data interdependencies Building a Data Warehouse What is the storage availability What are the distribution requirements What is the loading time As we have said databases must strike a balance between efficiency in transaction processing and supporting query requirements but a data warehouse is typically optimized for access from a decision maker’s needs. Data storage in a data warehouse reflects this specialization and involves the following processes Storing the data according to the data model of the warehouse Creating and maintaining required data structures Creating and maintaining appropriate access paths Providing for time variant data as new data are added Supporting the updating of warehouse data Refreshing the data Purging data Although adequate time can be devoted initially to constructing the warehouse the sheer volume of data in the warehouse generally makes it impossible to simply reload the warehouse in its entirety later on. Alternatives include selective refreshing of data and separate warehouse versions . When the warehouse uses an incremental data refreshing mechanism data may need to be periodically purged for example a warehouse that maintains data on the previous twelve business quarters may periodically purge its data each year. Data warehouses must also be designed with full consideration of the environment in which they will reside. Important design considerations include the following Usage projections The fit of the data model Characteristics of available sources Design of the metadata component Modular component design Design for manageability and change Considerations of distributed and parallel architecture We discuss each of these in turn. Warehouse design is initially driven by usage projections that is by expectations about who will use the warehouse and how they will use it. Choice of a data model to support this usage is a key initial decision. Usage projections and the characteristics of the warehouse’s data sources are both taken into account. Modular design is a practical necessity to allow the warehouse to evolve with the organization and its information environment. Additionally a well Chapter Overview of Data Warehousing and OLAP built data warehouse must be designed for maintainability enabling the warehouse managers to plan for and manage change effectively while providing optimal support to users. You may recall the term metadata from Chapter metadata was defined as the description of a database including its schema definition. The metadata repository is a key data warehouse component. The metadata repository includes both technical and business metadata. The first technical metadata covers details of acquisition processing storage structures data descriptions warehouse operations and maintenance and access support functionality. The second business metadata includes the relevant business rules and organizational details supporting the warehouse. The architecture of the organization’s distributed computing environment is a major determining characteristic for the design of the warehouse. There are two basic distributed architectures the distributed warehouse and the federated warehouse. For a distributed warehouse all the issues of distributed databases are relevant for example replication partitioning communications and consistency concerns. A distributed architecture can provide benefits particularly important to warehouse performance such as improved load balancing scalability of performance and higher availability. A single replicated metadata repository would reside at each distribution site. The idea of the federated warehouse is like that of the federated database a decentralized confederation of autonomous data warehouses each with its own metadata repository. Given the magnitude of the challenge inherent to data warehouses it is likely that such federations will consist of smaller scale components such as data marts. Large organizations may choose to federate data marts rather than build huge data warehouses. Typical Functionality of a Data Warehouse Data warehouses exist to facilitate complex data intensive and frequent ad hoc queries. Accordingly data warehouses must provide far greater and more efficient query support than is demanded of transactional databases. The data warehouse access component supports enhanced spreadsheet functionality efficient query processing structured queries ad hoc queries data mining and materialized views. In particular enhanced spreadsheet functionality includes support for state of theart spreadsheet applications as well as for OLAP applications programs. These offer preprogrammed functionalities such as the following Roll up. Data is summarized with increasing generalization . Drill down. Increasing levels of detail are revealed . Pivot. Cross tabulation is performed. Slice and dice. Projection operations are performed on the dimensions. Sorting. Data is sorted by ordinal value. Data Warehouse versus Views Selection. Data is available by value or range. Derived attributes. Attributes are computed by operations on stored and derived values. Because data warehouses are free from the restrictions of the transactional environment there is an increased efficiency in query processing. Among the tools and techniques used are query transformation index intersection and union special ROLAP and MOLAP functions SQL extensions advanced join methods and intelligent scanning . Improved performance has also been attained with parallel processing. Parallel server architectures include symmetric multiprocessor cluster and massively parallel processing and combinations of these. Knowledge workers and decision makers use tools ranging from parametric queries to ad hoc queries to data mining. Thus the access component of the data warehouse must provide support for structured queries . Together these make up a managed query environment. Data mining itself uses techniques from statistical analysis and artificial intelligence. Statistical analysis can be performed by advanced spreadsheets by sophisticated statistical analysis software or by custom written programs. Techniques such as lagging moving averages and regression analysis are also commonly employed. Artificial intelligence techniques which may include genetic algorithms and neural networks are used for classification and are employed to discover knowledge from the data warehouse that may be unexpected or difficult to specify in queries. . Materialized views have been explored for their performance enhancement. Views however provide only a subset of the functions and capabilities of data warehouses. Views and data warehouses are alike in that they both have read only extracts from databases and subject orientation. However data warehouses are different from views in the following ways Data warehouses exist as persistent storage instead of being materialized on demand. Data warehouses are not usually relational but rather multidimensional. Views of a relational database are relational. Data warehouses can be indexed to optimize performance. Views cannot be indexed independent of the underlying databases. Data warehouses characteristically provide specific support of functionality views cannot. Chapter Overview of Data Warehousing and OLAP Data warehouses provide large amounts of integrated and often temporal data generally more than is contained in one database whereas views are an extract of a database. Difficulties of Implementing Data Warehouses Some significant operational issues arise with data warehousing construction administration and quality control. Project management the design construction and implementation of the warehouse is an important and challenging consideration that should not be underestimated. The building of an enterprise wide warehouse in a large organization is a major undertaking potentially taking years from conceptualization to implementation. Because of the difficulty and amount of lead time required for such an undertaking the widespread development and deployment of data marts may provide an attractive alternative especially to those organizations with urgent needs for OLAP DSS and or data mining support. The administration of a data warehouse is an intensive enterprise proportional to the size and complexity of the warehouse. An organization that attempts to administer a data warehouse must realistically understand the complex nature of its administration. Although designed for read access a data warehouse is no more a static structure than any of its information sources. Source databases can be expected to evolve. The warehouse’s schema and acquisition component must be expected to be updated to handle these evolutions. A significant issue in data warehousing is the quality control of data. Both quality and consistency of data are major concerns. Although the data passes through a cleaning function during acquisition quality and consistency remain significant issues for the database administrator. Melding data from heterogeneous and disparate sources is a major challenge given differences in naming domain definitions identification numbers and the like. Every time a source database changes the data warehouse administrator must consider the possible interactions with other elements of the warehouse. Usage projections should be estimated conservatively prior to construction of the data warehouse and should be revised continually to reflect current requirements. As utilization patterns become clear and change over time storage and access paths can be tuned to remain optimized for support of the organization’s use of its warehouse. This activity should continue throughout the life of the warehouse in order to remain ahead of demand. The warehouse should also be designed to accommodate the addition and attrition of data sources without major redesign. Sources and source data will evolve and the warehouse must accommodate such change. Fitting the available source data into the data model of the warehouse will be a continual challenge a task that is as much art as science. Because there is continual rapid change in technologies both the requirements and capabilities of the warehouse will change considerably over time. Additionally data warehousing technology itself will continue to evolve for some time so that component structures and functional Review Questions ities will continually be upgraded. This certain change is excellent motivation for having fully modular design of components. Administration of a data warehouse will require far broader skills than are needed for traditional database administration. A team of highly skilled technical experts with overlapping areas of expertise will likely be needed rather than a single individual. Like database administration data warehouse administration is only partly technical a large part of the responsibility requires working effectively with all the members of the organization with an interest in the data warehouse. However difficult that can be at times for database administrators it is that much more challenging for data warehouse administrators as the scope of their responsibilities is considerably broader. Design of the management function and selection of the management team for a database warehouse are crucial. Managing the data warehouse in a large organization will surely be a major task. Many commercial tools are available to support management functions. Effective data warehouse management will certainly be a team function requiring a wide set of technical skills careful coordination and effective leadership. Just as we must prepare for the evolution of the warehouse we must also recognize that the skills of the management team will of necessity evolve with it. Summary In this chapter we surveyed the field known as data warehousing. Data warehousing can be seen as a process that requires a variety of activities to precede it. In contrast data mining ROLAP MOLAP and DSS . Describe the characteristics of a data warehouse. Divide them into functionality of a warehouse and advantages users derive from it. What is the multidimensional data model How is it used in data warehousing Define the following terms star schema snowflake schema fact constellation data marts. Chapter Overview of Data Warehousing and OLAP What types of indexes are built for a warehouse Illustrate the uses for each with an example. Describe the steps of building a warehouse. What considerations play a major role in the design of a warehouse Describe the functions a user can perform on a data warehouse and illustrate the results of these functions on a sample multidimensional data warehouse. How is the concept of a relational view related to a data warehouse and data marts In what way are they different List the difficulties in implementing a data warehouse. List the open issues and research problems in data warehousing. Selected Bibliography Inmon is credited for giving the term wide acceptance. Codd and Salley popularized the term online analytical processing and defined a set of characteristics for data warehouses to support OLAP. Kimball is known for his contribution to the development of the data warehousing field. Mattison is one of the several books on data warehousing that gives a comprehensive analysis of techniques available in data warehouses and the strategies companies should use in deploying them. Ponniah gives a very good practical overview of the data warehouse building process from requirements collection to deployment maintenance. Bischoff and Alexander is a compilation of advice from experts. Chaudhuri and Dayal give an excellent tutorial on the topic while Widom points to a number of outstanding research problems. Alternative Diagrammatic Notations for ER Models Figure shows a number of different diagrammatic notations for representing ER and EER model concepts. Unfortunately there is no standard notation different database design practitioners prefer different notations. Similarly various CASE tools and OOA methodologies use various notations. Some notations are associated with models that have additional concepts and constraints beyond those of the ER and EER models described in Chapters through while other models have fewer concepts and constraints. The notation we used in Chapter is quite close to the original notation for ER diagrams which is still widely used. We discuss some alternate notations here. Figure shows different notations for displaying entity types classes attributes and relationships. In Chapters through we used the symbols marked in Figure rectangle oval and diamond. Notice that symbol for entity types classes symbol for attributes and symbol for relationships are similar but they are used by different methodologies to represent three different concepts. The straight line symbol for representing relationships is used by several tools and methodologies. Figure shows some notations for attaching attributes to entity types. We used notation . Notation uses the third notation for attributes from Figure The last two notations in Figure and are popular in OOA methodologies and in some CASE tools. In particular the last notation displays both the attributes and the methods of a class separated by a horizontal line. appendixA Appendix A Alternative Diagrammatic Notations for ER Models Entity type class symbols E E Attribute symbols Relationship symbols A R A A R R Ssn Name Address EMPLOYEE EMPLOYEE Ssn Name Address . . . Ssn Name Address EMPLOYEE Ssn Name Address Hireemp Fireemp EMPLOYEE N N C C G Gs C C C C G Figure Alternative notations. Symbols for entity type class attribute and relationship. Displaying attributes. Displaying cardinality ratios. Various notations. Notations for displaying specialization generalization. Appendix A Alternative Diagrammatic Notations for ER Models Figure shows various notations for representing the cardinality ratio of binary relationships. We used notation in Chapters through Notation known as the chicken feet notation is quite popular. Notation uses the arrow as a functional reference and resembles our notation for foreign keys in the relational model used in Bachman diagrams and the network data model uses the arrow in the reverse direction . For a relationship uses a straight line without any chicken feet makes both halves of the diamond white and places arrowheads on both sides. For an M N relationship uses chicken feet at both ends of the line makes both halves of the diamond black and does not display any arrowheads. Figure shows several variations for displaying constraints which are used to display both cardinality ratio and total partial participation. We mostly used notation . Notation is the alternative notation we used in Figure and discussed in Section Recall that our notation specifies the constraint that each entity must participate in at least min and at most max relationship instances. Hence for a relationship both max values are for M N both max values are n. A min value greater than specifies total participation . In methodologies that use the straight line for displaying relationships it is common to reverse the positioning of the constraints as shown in a variation common in some tools is shown in . Another popular technique which follows the same positioning as is to display the min as o or as | as shown in . Figure shows some notations for displaying specialization generalization. We used notation in Chapter where a d in the circle specifies that the subclasses and are disjoint and an o in the circle specifies overlapping subclasses. Notation uses G to specify disjoint and Gs to specify overlapping some notations use the solid arrow while others use the empty arrow . Notation uses a triangle pointing toward the superclass and notation uses a triangle pointing toward the subclasses it is also possible to use both notations in the same methodology with indicating generalization and indicating specialization. Notation places the boxes representing subclasses within the box representing the superclass. Of the notations based on some use a single lined arrow and others use a double lined arrow . The notations shown in Figure show only some of the diagrammatic symbols that have been used or suggested for displaying database conceptual schemes. Other notations as well as various combinations of the preceding have also been used. It would be useful to establish a standard that everyone would adhere to in order to prevent misunderstandings and reduce confusion. This page intentionally left blank Parameters of Disks The most important disk parameter is the time required to locate an arbitrary disk block given its block address and then to transfer the block between the disk and a main memory buffer. This is the random access time for accessing a disk block. There are three time components to consider as follows Seek time . This is the time needed to mechanically position the read write head on the correct track for movable head disks. For movable head disks this time varies depending on the distance between the current track under the read write head and the track specified in the block address. Usually the disk manufacturer provides an average seek time in milliseconds. The typical range of average seek time is to msec. This is the main culprit for the delay involved in transferring blocks between disk and memory. Rotational delay . Once the read write head is at the correct track the user must wait for the beginning of the required block to rotate into position under the read write head. On average this takes about the time for half a revolution of the disk but it actually ranges from immediate access to a full disk revolution . If the speed of disk rotation is p revolutions per minute then the average rotational delay rd is given by rd min p msec msec A typical value for p is rpm which gives a rotational delay of rd msec. For fixed head disks where the seek time is negligible this component causes the greatest delay in transferring a disk block. appendixB Appendix B Parameters of Disks Block transfer time . Once the read write head is at the beginning of the required block some time is needed to transfer the data in the block. This block transfer time depends on the block size track size and rotational speed. If the transfer rate for the disk is tr bytes msec and the block size is B bytes then btt B tr msec If we have a track size of Kbytes and p is rpm then the transfer rate in bytes msec is tr bytes msec In this case btt msec where B is the block size in bytes. The average time needed to find and transfer a block given its block address is estimated by msec This holds for either reading or writing a block. The principal method of reducing this time is to transfer several blocks that are stored on one or more tracks of the same cylinder then the seek time is required for the first block only. To transfer consecutively k noncontiguous blocks that are on the same cylinder we need approximately s + msec In this case we need two or more buffers in main storage because we are continuously reading or writing the k blocks as we discussed in Chapter The transfer time per block is reduced even further when consecutive blocks on the same track or cylinder are transferred. This eliminates the rotational delay for all but the first block so the estimate for transferring k consecutive blocks is s + rd + msec A more accurate estimate for transferring consecutive blocks takes into account the interblock gap that takes the gap size into account when reading consecutively stored blocks. If the gap size is G bytes then btr tr bytes msec The bulk transfer rate is the rate of transferring useful bytes in the data blocks. The disk read write head must go over all bytes on a track as the disk rotates including the bytes in the interblock gaps which store control information but not real data. When the bulk transfer rate is used the time needed to transfer the useful data in one block out of several consecutive blocks is B btr. Hence the estimated time to read k blocks consecutively stored on the same cylinder becomes s + rd + msec Appendix B Parameters of Disks Another parameter of disks is the rewrite time. This is useful in cases when we read a block from the disk into a main memory buffer update the buffer and then write the buffer back to the same disk block on which it was stored. In many cases the time required to update the buffer in main memory is less than the time required for one disk revolution. If we know that the buffer is ready for rewriting the system can keep the disk heads on the same track and during the next disk revolution the updated buffer is rewritten back to the disk block. Hence the rewrite time Trw is usually estimated to be the time needed for one disk revolution Trw rd msec msec To summarize the following is a list of the parameters we have discussed and the symbols we use for them Seek time s msec Rotational delay rd msec Block transfer time btt msec Rewrite time Trw msec Transfer rate tr bytes msec Bulk transfer rate btr bytes msec Block size B bytes Interblock gap size G bytes Disk speed p rpm This page intentionally left blank Overview of the QBE Language The Query By Example language is important because it is one of the first graphical query languages with minimum syntax developed for database systems. It was developed at IBM Research and is available as an IBM commercial product as part of the QMF interface option to The language was also implemented in the Paradox DBMS and is related to a point and click type interface in the Microsoft Access DBMS. It differs from SQL in that the user does not have to explicitly specify a query using a fixed syntax rather the query is formulated by filling in templates of relations that are displayed on a monitor screen. Figure shows how these templates may look for the database of Figure The user does not have to remember the names of attributes or relations because they are displayed as part of these templates. Additionally the user does not have to follow rigid syntax rules for query specification rather constants and variables are entered in the columns of the templates to construct an example related to the retrieval or update request. QBE is related to the domain relational calculus as we shall see and its original specification has been shown to be relationally complete. Basic Retrievals in QBE In QBE retrieval queries are specified by filling in one or more rows in the templates of the tables. For a single relation query we enter either constants or example elements in the columns of the template of that relation. An example element stands for a domain variable and is specified as an example value preceded by the underscore character . Additionally a P. prefix is entered in certain columns to indicate that we would like to print appendixC Appendix C Overview of the QBE Language values in those columns for our result. The constants specify values that must be exactly matched in those columns. For example consider the query Retrieve the birth date and address of John B. Smith. In Figures through we show how this query can be specified in a progressively more terse form in QBE. In Figure an example of an employee is presented as the type of row that we are interested in. By leaving John B. Smith as constants in the Fname Minit and Lname columns we are specifying an exact match in those columns. The rest of the columns are preceded by an underscore indicating that they are domain variables . The P. prefix is placed in the Bdate and Address columns to indicate that we would like to output value in those columns. can be abbreviated as shown in Figure There is no need to specify example values for columns in which we are not interested. Moreover because example values are completely arbitrary we can just specify variable names for them as shown in Figure Finally we can also leave out the example values entirely as shown in Figure and just specify a P. under the columns to be retrieved. To see how retrieval queries in QBE are similar to the domain relational calculus compare Figure with in domain calculus as follows uv | EMPLOYEE and q ‘John’ and r ‘B’ and s ‘Smith’ DEPARTMENT Fname Minit Lname Ssn Bdate Address Sex Salary Superssn Dno EMPLOYEE DEPTLOCATIONS Dnumber Dlocation PROJECT Pname Pnumber Plocation Dnum WORKSON Essn Pno Hours DEPENDENT Essn Dependentname Sex Bdate Relationship Dname Dnumber Mgrssn Mgrstartdate Figure The relational schema of Figure as it may be displayed by QBE. Appendix C Overview of the QBE Language EMPLOYEE Fname Minit Lname Ssn Bdate Address Sex Salary Superssn Dno John B Smith Main Houston TX M EMPLOYEE Fname Minit Lname Ssn Bdate Address Sex Salary Superssn Dno John B Smith Main Houston TX EMPLOYEE Fname Minit Lname Ssn Bdate Address Sex Salary Superssn Dno John B Smith EMPLOYEE Fname Minit Lname Ssn Bdate Address Sex Salary Superssn Dno John B Smith P. P. Figure Four ways to specify the query in QBE. We can think of each column in a QBE template as an implicit domain variable hence Fname corresponds to the domain variable q Minit corresponds to r and Dno corresponds to z. In the QBE query the columns with P. correspond to variables specified to the left of the bar in domain calculus whereas the columns with constant values correspond to tuple variables with equality selection conditions on them. The condition EMPLOYEE and the existential quantifiers are implicit in the QBE query because the template corresponding to the EMPLOYEE relation is used. In QBE the user interface first allows the user to choose the tables needed to formulate a query by displaying a list of all relation names. Then the templates for the chosen relations are displayed. The user moves to the appropriate columns in the templates and specifies the query. Special function keys are provided to move among templates and perform certain functions. We now give examples to illustrate basic facilities of QBE. Comparison operators other than may be entered in a column before typing a constant value. For example the query List the social security numbers of employees who work more than hours per week on project number can be specified as shown in Figure For more complex conditions the user can ask for a condition box which is created by pressing a particular function key. The user can then type the complex with the ¬ symbol is not allowed in a condition box. Appendix C Overview of the QBE Language For example the query List the social security numbers of employees who work more than hours per week on either project or project can be specified as shown in Figure Some complex conditions can be specified without a condition box. The rule is that all conditions specified on the same row of a relation template are connected by the and logical connective whereas conditions specified on distinct rows are connected by or . Hence can also be specified as shown in Figure by entering two distinct rows in the template. Now consider query List the social security numbers of employees who work on both project and project this cannot be specified as in Figure which lists those who work on either project or project The example variable ES will bind itself to Essn values in – – tuples as well as to those in – – tuples. Figure shows how to specify correctly where the condition in the box makes the EX and EY variables bind only to identical Essn values. In general once a query is specified the resulting values are displayed in the template under the appropriate columns. If the result contains more rows than can be displayed on the screen most QBE implementations have function keys to allow scrolling up and down the rows. Similarly if a template or several templates are too wide to appear on the screen it is possible to scroll sideways to examine all the templates. A join operation is specified in QBE by using the same in the columns to be joined. For example the query List the name and address of all employees who variable is called an example element in QBE manuals. WORKSON Essn Pno Hours WORKSON Essn Pno Hours P. PX HX HX and Essn Pno Hours Figure Specifying complex conditions in QBE. The query The query with a condition box. The query without a condition box. Appendix C Overview of the QBE Language WORKSON Essn Pno Hours WORKSON Essn Pno Hours EX EY CONDITIONS Figure Specifying EMPLOYEES who work on both projects. Incorrect specification of an AND condition. Correct specification. work for the ‘Research’ department can be specified as shown in Figure Any number of joins can be specified in a single query. We can also specify a result table to display the result of the join query as shown in Figure this is needed if the result includes attributes from two or more relations. If no result table is specified the system provides the query result in the columns of the various relations which may make it difficult to interpret. Figure also illustrates the feature of QBE for specifying that all attributes of a relation should be retrieved by placing the P. operator under the relation name in the relation template. To join a table with itself we specify different variables to represent the different references to the table. For example query For each employee retrieve the employee’s first and last name as well as the first and last name of his or her immediate supervisor can be specified as shown in Figure where the variables starting with E refer to an employee and those starting with S refer to a supervisor. Grouping Aggregation and Database Modification in QBE Next consider the types of queries that require grouping or aggregate functions. A grouping operator G. can be specified in a column to indicate that tuples should be grouped by the value of that column. Common functions can be specified such as CNT. and MIN. In QBE the functions and CNT. are applied to distinct values within a group in the default case. If we want these functions to apply to all values we must use the prefix ALL. This convention is different in SQL where the default is to apply a function to all values. in QBE is unrelated to the universal quantifier. Appendix C Overview of the QBE Language Figure shows query which counts the number of distinct salary values in the EMPLOYEE relation. Query does not provide this support. Hence the QMF version of QBE which we discuss here is not relationally complete. Queries such as Find employees who work on all projects controlled by department cannot be specified. EMPLOYEE Fname Minit Lname Ssn Bdate Address Sex Salary Superssn Dno FN Research P. FN LN Addr DX LN Addr DX DEPARTMENT Dname Dnumber Mgrssn Mgrstartdate R E S U LT P. R E S U LT EMPLOYEE Fname Minit Lname Ssn Bdate Address Sex Salary Superssn Dno Xssn Xssn Figure Illustrating JOIN and result relations in QBE. The query The query Appendix C Overview of the QBE Language EMPLOYEE Fname Minit Lname Ssn Bdate Address Sex Salary Superssn Dno . EMPLOYEE Fname Minit Lname Ssn Bdate Address Sex Salary Superssn Dno .ALL EMPLOYEE Fname Minit Lname Ssn Bdate Address Sex Salary Superssn Dno .ALL .ALL . PROJECT Pname Pnumber Plocation P. PX Dnum WORKSON Essn Pno Hours .EX CONDITIONS Figure Functions and grouping in QBE. The query The query The query The query There are three QBE operators for modifying the database I. for insert D. for delete and U. for update. The insert and delete operators are specified in the template column under the relation name whereas the update operator is specified under the columns to be updated. Figure shows how to insert a new EMPLOYEE tuple. For deletion we first enter the D. operator and then specify the tuples to be deleted by a condition EMPLOYEE Fname Minit Lname Ssn Bdate Address Sex Salary Superssn Dno I. Marini Richard K Oak Forest Katy TX EMPLOYEE Fname Minit Lname Ssn Bdate Address Sex Salary Superssn Dno D. EMPLOYEE Fname Minit Lname Ssn Bdate Address Sex Salary Superssn Dno John Smith Figure Modifying the database in QBE. Insertion. Deletion. Update in QBE. request to increase the salary of ‘John Smith’ by percent and also to reassign him to department number QBE also has data definition capabilities. The tables of a database can be specified interactively and a table definition can also be updated by adding renaming or removing a column. We can also specify various characteristics for each column such as whether it is a key of the relation what its data type is and whether an index should be created on that field. QBE also has facilities for view definition authorization storing query definitions for later use and so on. QBE does not use the linear style of SQL rather it is a two dimensional language because users specify a query moving around the full area of the screen. Tests on users have shown that QBE is easier to learn than SQL especially for nonspecialists. In this sense QBE was the first user friendly visual relational database language. More recently numerous other user friendly interfaces have been developed for commercial database systems. The use of menus graphics and forms is now becoming quite common. Filling forms partially to issue a search request is akin to using QBE. Visual query languages which are still not so common are likely to be offered with commercial relational databases in the future. 